{
    "abstract": "Abstract\nAlthough some research effort has been devoted to the comparison of probability- and nonprobability-based Web surveys,\ndifferent types of nonprobability-based samples have not been thoroughly examined. This exploratory study compares\nthe data quality between online panel and intercept samples. Online panel refers to a pre-recruited and profiled pool of\nrespondents. An intercept sample is a pool of respondents that are obtained through banners, ads, or promotions. Anyone\ncan click on them and subsequently respond to a survey. Respondents are not pre-recruited or profiled. Three surveys with\n52, 29, and 19 questions, respectively, were administered to both samples. Propensity score weighting adjustment is used for\nthe analyses. The results show that the completion rates are higher for the panel than the intercept sample. The completion\ntimes are similar for these two samples. Data quality, on average, tends to be higher for panel than intercept samples.\n",
    "reduced_content": "Creative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons\nAttribution-NonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use,\nreproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open\nAccess pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nMethodological Innovations\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\nmio.sagepub.com\nIntroduction\nResearch has shown that many factors contribute to the rapid\ngrowth of Web surveys, including high Internet penetration,\nlow cost, timeliness, and so on (see, for example, Fricker and\nchallenge that Web surveys face is the lack of a sampling\nframe of the general population and hence making inference\nfrom it. Couper (2000) classifies Web surveys into two broad\ncategories: nonprobability-based and probability-based. He\nfurther classifies nonprobability-based Web surveys into\nthree groups, namely, polls as entertainment, unrestricted\nself-selected surveys, and volunteer opt-in panels.\nThis study focuses on the last two types of nonprobabil-\nity-based Web surveys and compares data quality between\nthem through six surveys. As indicated by the name, unre-\nstricted self-selected surveys, or a web intercept sample, usu-\nally use open invitations (e.g. banners or ads on websites)\nand impose no restriction on survey access. The identity of\nthe potential respondents is unknown, and the personal con-\ntact information is usually not sought and no re-contact is\nattempted. It relies on recruiting anyone who happens to be\npassing through a website at a particular time. In contrast,\nopt-in panels collect panelists' contact information, along\nwith profile data at the sign-up stage for contacting and\ntargeting respondents in later surveys. Previous research has\nexamined different nonprobability sample acquisition strate-\nNonprobability samples have been used in many fields.\nFor example, researchers in fields like psychology and mar-\nket research have long used nonprobability samples for\nexperimental studies. For those types of studies, the repre-\nsentativeness of the sample is not critical. However, other\naspects of data quality, in particular, the conscientiousness of\nthe respondents, are of critical importance. Satisficing theory\nis often cited as a framework for understanding respondents'\ncognitive process and efforts when responding to survey\nrespondents can be classified into two broad categories: opti-\nmizers and satisficers. Optimizers are those who carefully\nand conscientiously go through each cognitive step, includ-\ning comprehension, retrieval, judgment and estimation,\nreporting, and mapping, before providing an answer. On the\nComparing data quality between online\npanel and intercept samples\nMingnan Liu\n Keywords\nNonprobability survey, web survey, online panel, intercept sample, data quality, satisficing\nSurveyMonkey, Palo Alto, CA, USA\nCorresponding author:\nEmail: mingnanL@SurveyMonkey.com\nOriginal Article\n2 Methodological Innovations\nother hand, satisficers tend to take cognitive shortcuts and\nstop the cognitive process as soon as they reach a good\nenough yet not perfect answer.\nWhy should we expect the data quality as measured by\nsatisficing to differ between the two sample sources? To\nanswer this question, it is necessary to explain the nature of\nthe two sample sources first. The intercept samples in this\nstudy come from offer walls. An offer wall is a page that\nappears within a mobile app that offers users rewards in\nexchange for completing some tasks. In this study, a survey\ninvitation was offered on the offer wall when mobile game\nplayers needed additional game coins to continue with their\ngames. Game players can click on one of the offers (tasks)\ndisplayed on the offer wall, and a survey is one of the offers.\nIf the participants click on the survey invite, they will be\nredirected to the survey. Once they have completed the sur-\nvey, participants are directed back to the offer wall page to\ncollect their rewards and continue with their previous actions,\nin this case, mobile games. The online panel used in this\nstudy is SurveyMonkey Audience, an online nonprobability-\nbased Web access panel. The panelists are recruited from\nmillion people who complete SurveyMonkey sur-\nveys every month. After one completes a survey on the\nSurveyMonkey platform, he or she will be directed to a land-\ning page, called the survey thanks-page. On that web page,\nparticipants are asked to sign up for SurveyMonkey\nAudience. After signing up, panelists need to first take a pro-\nfile survey asking a variety of demographic characteristics\nand behaviors, which are later used for targeting.\nThree major differences between the intercept sample and\nthe online panel under study can influence their data quality.\nFirst, the context of the survey environment could be differ-\nent. For the intercept respondents, the survey is during the\nprocess of another action in which they are engaged. The\nrespondents probably have more motives to rush through\nthe survey so that they can return to their previous actions.\nThis may lead to suboptimal data quality, that is, satisficing\n1987). Although the survey-taking context of the panel\nrespondents is not clear, considering that they are invited by\nemails, they have more control over the time and place at\nwhich they take the survey. Thus, it is less likely for panel\nrespondents to start taking a survey when they are in the mid-\ndle of another task and having to rush through the survey.\nSecond, incentives are different between these two sample\nsources. For the intercept sample, respondents are rewarded\nwith game coins or lives in order for them to continue with\nthe game. They benefit directly from taking the survey. For\nthe panel survey, SurveyMonkey rewards panelists with char-\nitable donations. There is no direct financial benefit from tak-\ning surveys for the panelists. The literature has examined the\nimpact of incentives on the web survey response rate (Bosnjak\nand Wolff, 2007). One study examined cash incentives and\ncharity donation and found that the charity donation actually\ndecreased survey participation (Pedersen and Nielsen, 2014).\nAnother study showed that incentives improved not only sur-\nvey participation but also data quality (Van Veen et al., 2015).\nThe direct incentive used in the intercept sample can probably\nprovide more motivation to the respondents, and hence, they\nare more likely to treat the survey more seriously and give\nmore conscientious responses. On the other hand, the charity\ndonation in the panel survey suggests that respondents are\ndoing the survey for altruistic reasons rather than personal\nbenefit. Research has shown that altruistic-oriented people\nare more likely to accept survey requests (Abraham et al.,\nalso be more likely to provide better data quality. Considering\nthese, the different incentive strategies between the two sam-\nple sources produce competing expectations on their impacts\non data quality.\nThird, the survey experience is different between the two\nsample sources. The online panel respondents, in principle,\nshould have more survey experience than the intercept sam-\nples. The literature shows either negative (Toepoel et al.,\nvey experience and data quality. Given the mixed findings in\nthe literature and the lack of data on survey experience, no\nspecific prediction will be made based on this.\nIt is important to note that in this study, all intercept\nrespondents used mobile devices, while panel respondents\ncould choose from all types of devices, including mobile and\nnon-mobile. Therefore, the mode could be another contribu-\ntor to the difference, if any, between the panel and intercept\nsamples. Previous studies found similar responses from both\nmobile and non-mobile survey participants (Couper et al.,\n2015). However, the current design does not allow a separa-\ntion of the effects.\nData and measures\nboth online panel and intercept samples in the United States.\nThe surveys were conducted in May 2015. Surveys were pro-\ngrammed and administered using the SurveyMonkey plat-\nform. The completion rate is calculated as follows1\nCompletion rate\nof completed surveys\nof surveys finished th\n=\nNo.\nNo. e\ne first page\nThe 52-question survey is the most comprehensive; the\nother two are shorter versions of the 52-question survey. The\ndata quality indicators are drawn from the satisficing litera-\ngests that satisficers, as opposed to optimizers, stop their\ncognitive process as soon as they reach a good enough but\nnot optimal answer when they are responding to survey\nquestions. The study also supplemented the satisficing indi-\ncators with other data quality measures. The measures used\nin the 52-question version include (1) response rounding of\nnumeric questions--the percentage of numeric answers that\nare integers of 5; (2) straightlining of matrix questions--the\npercentage of respondents selecting the same response\noption regardless of the items in a matrix; (3) trap ques-\ntions--the percentage of failed trap responses. An example\nof a trap question is \"Please select B regardless of your real\nanswer,\" and failure means selection of any options other\nthan \"B\" (for more examples, see Jones et al., 2015); (4)\nknowledge question answers--the percentage of inaccurate\nanswers to knowledge questions; (5) sensitive questions--\nthe percentage of socially desirable answers; (6) open-ended\nresponse quality--the number of characters for the open-\nended responses; (7) time to complete in seconds--the\nmedian time for completing the entire survey. The 29-ques-\ntion and 19-question surveys included fewer questions.\nTable 1 summarizes the measures in each survey. All ques-\ntions were required.\nGiven the nature of this study, that is, to compare the sur-\nvey responses from two different sample sources, it is impos-\nsible to conduct a strictly randomized experiment.\nConsequently, the sample compositions, in terms of demo-\ngraphic variables, in each pair of surveys under comparison\ncan be different. To remove as much of the impact of demo-\ngraphic difference as possible, I analyze the data using t-tests\nor chi-square tests, with the propensity score weight.\nSpecifically, I first fit a propensity model to predict participa-\ntion in the online panel versus intercept survey, using varia-\nbles that are potentially related to the response propensity for\nboth surveys. The variables used in the three surveys are dif-\nferent, as the short surveys have fewer variables than the\nlong survey. For the 52-question survey, the variables used in\nthe propensity model include gender, age, race/ethnicity,\neducation level, marital status, household income level,\nhouse ownership, insurance ownership, credit card owner-\nship, census region, party ID, and ideology. The variables for\nthe 29-question survey include gender, age, race/ethnicity,\neducation level, marital status, household income level, cen-\nsus region, party ID, and ideology. The variables for the\n19-question survey include gender, age, race/ethnicity, edu-\ncation level, household income level, and census region.\nNext, I create the propensity weight by taking the inverse of\nthe probability of responding to the online panel versus inter-\ncept sample. In all models, a full propensity distribution was\nused for adjustment. Each propensity model showed bal-\nanced distributions of the variables used in the model, sug-\ngesting that the propensity model performed well.\nFor each survey, I conduct two sets of analyses. I first\nanalyze all the samples together to compare intercept versus\npanel. Then, I separate the mobile panel respondents and\ncompare them with the intercept respondents since all inter-\ncept respondents in this study used mobile devices. The num-\nbers of respondents in the mobile samples for the three panel\nand 19-question surveys. The propensity score weights are\ncalculated separately for these two sets of analyses.\nResults\nCompletion rate\nThe completion rates for intercept surveys were significantly\nlower than the completion rates for panel surveys for all\nthree versions of surveys (Table 2). When comparing com-\npletion rates across the three surveys for intercept and panel\nsamples separately, a clear trend emerges: a short survey\nleads to a higher completion rate.\nTable 3 presents the survey responses between intercept and\npanel respondents.\nTime to complete.The median time to completion for the\nintercept and panel surveys was not significantly different\nbetween the all samples and mobile samples only.\nResponse rounding. One item showed a significant difference,\nin which the panel survey produces more rounded answers\nthan the intercept survey. When only comparing mobile\nrespondents, panel respondents tend to provide more rounded\nanswers than intercept respondents, although the differences\nwere not significant.\nStraightlining. Straightlining is measured through two sets of\nmulti-item rating scales, one with five items and the other\nTable 1. Response quality indicators by surveys.\nResponse latency All questions All questions All questions Median seconds t-test\nRounding 5 numeric questions 4 numeric questions 1 numeric question % Rounded answers t-test\nStraightlining Two multi-item scales\nOne multi-item\nscale (5 items)\nOne multi-item\nscale (5 items)\n% Straight-lined responses t-test\nTrap question 4 questions 3 questions 1 question % Trapped answers t-test\nKnowledge question 4 questions 2 questions 1 question % Incorrect answers t-test\nSensitive question 5 questions 2 questions \u00ad % Desirable answers t-test\nOpen-ended question 1 question 1 question 1 question Number of characters t-test\nSelf-rate difficulty 1 question 1 question 1 question 4 categories 2\n4 Methodological Innovations\nwith six items. For the 6-item scale, panel respondents are\nsignificantly less likely to straightline than intercept respond-\nents (t=-2.11, p<.05). The difference is smaller and not sig-\nnificant for the mobile-only comparison.\nTrap question. Three trap questions were asked in the survey,\nwhere the first one is much easier than the other two. For the\neasy question (Trap 1), almost all respondents answered it\ncorrectly. For the two more difficult questions, significantly\nfewer panel respondents failed than did intercept respond-\nsame trend is found for the mobile-only respondents (Trap 2:\nKnowledge question. Four knowledge questions were asked in\nthe surveys and panel respondents were more likely to pro-\nvide correct answers to all of them than were intercept\nrespondents, although only two are significant in the all\nTable 2. Completion rates between intercept and panel for the three surveys.\n Intercept (1) Panel (2) Intercept (3) Panel (4) Intercept (5) Panel (6)\nTable 3. Data quality indicators for 52-question intercept and panel surveys.\nAll samples Mobile samples only\n Intercept Panel t Intercept Panel t\nRounding (% rounded)\nStraightlining (% straight-lined)\nTrap question (% failed)\nKnowledge question (% incorrect)\nSensitive question (%)\nTask difficulty (%) 2 2\nsamples comparison and one is significant among the mobile\nsamples. In the mobile-only comparison, the intercept sam-\nple provided significantly more wrong answers than the\npanel sample to one question.\nSensitive question.Among the five sensitive questions, only\nthe one on the number of sexual partners shows a significant\ndifference; intercept respondents report more lifetime sexual\npartners than panel respondents (all samples: t=3.28, p<.01;\nOpen-ended question.Panel respondents provided longer\nresponses to the open-ended question than intercept respond-\nents, and this was the case for both the all samples (t=5.89,\nMost of the open-ended responses were valid, although\nthe panel respondents provided even fewer invalid responses\nthan the intercept respondents in the all samples analysis\nTo sum up, there are 21 indicators under comparison, and\nfor all samples, eight data points show significant differ-\nences between the two sample sources. When restricted to\nmobile respondents only, only four comparisons are signifi-\ncantly different. It needs to be pointed out that the mobile\npanel sample size is relatively small (n=178), and therefore,\nthe lack of significant findings may simply be lower. To\nreflect the data quality difference while accounting for the\nsmaller sample size, I calculated the average difference\nbetween the intercept and panel samples (except time to\ncomplete and open-ended response because these are in a\ndifferent metric from the other variables), and it is 3.77 for\nall samples comparison and 2.48 for mobile comparison. A\npositive number suggests lower data quality for the intercept\nthan the panel sample. This means that, on average, inter-\ncept respondents produced somewhat worse data quality\nthan panel respondents.\nThe 29-question survey contained the same set of data qual-\nity indicators as the 52-question survey, although fewer\nquestions for each indicator (Table 4).\nTime to complete. Similar to the 52-question survey, in this\n29-question survey, the median time to completion for inter-\ncept and panel surveys was not significantly different for\nboth the all samples and mobile samples only.\nRounding.Four numeric questions were used to measure\nresponse rounding. Intercept respondents gave significantly\nmore rounded answers to only one question (t\n=\np<.05) in the all samples analysis. The disparity between the\ntwo samples becomes smaller and is not significant in the\nmobile-only analysis.\nTable 4. Data quality indicators for 29-question intercept and panel surveys.\nAll samples Mobile samples only\n Intercept Panel t Intercept Panel t\nRounding (% rounded)\nStraightlining (% straight-lined)\nTrap question (% failed)\nKnowledge question (% incorrect)\nSensitive question (% desirable)\nTask difficulty (%)\n6 Methodological Innovations\nStraightlining. Only the 5-item scale was asked in this survey and\nthe difference between the two samples is small and not signifi-\ncant for both the all samples and mobile-only comparisons.\nTrap question.The two more difficult trap questions were\nused in this survey. The difference of the percentages of\nfailed respondents for Trap 2 is small between the two sam-\ndifference in the mobile-only sample is not significant.\nKnowledge question. Two knowledge questions were kept for\nthis version of the survey. Panel respondents consistently\nprovided more accurate answers than intercept respondents,\nalthough the difference was significant only for the Knowl-\nedge 4 question (all samples: t\n=\nSensitive question.Two questions on smoking and drug use\nwere asked, and the responses were similar between the two\nsamples and no significant differences were found.\nOpen-ended question.Panel respondents provided longer\nresponses to the open-ended question than intercept respond-\nents, and this was the case for both the all samples (t=6.59,\nAcross all the data quality indicators except for response\nlatency, the average difference is 3.16 for all samples and\n0.71 for the mobile samples comparison.\nWith the exception of the sensitive questions, the 19-ques-\ntion survey included all the data quality indicators. However,\nonly one question was asked for each data quality indicator\n(Table 5).\nTime to complete.The median time to completion for the\nintercept and panel surveys was not significantly different\nwhen all samples were combined. However, for the mobile-\nonly samples, the median time was significantly longer for\nthe panel than the intercept sample.\nRounding.The differences between the two samples were\nsmall and not significant for both all samples and mobile-\nonly respondents.\nStraightlining.More straight-lined responses existed in the\nintercept than the panel samples, and the differences were\nsignificant for both analyses (all samples: t\nTrap question. Only the easiest trap question was retained in\nthis survey. Hence, the failure rates were quite low and simi-\nlar between the two samples.\nKnowledge question. Panel respondents provided more accurate\nanswers than did the intercept respondents. The difference was\nsignificant for the mobile respondents (t=2.95, p<.01).\nOpen-ended question.Panel respondents provided longer\nresponses to the open-ended question than intercept respond-\nents, and this was the case for both the all samples (t=6.93,\nOn average, the difference between the intercept and\npanel samples is 3.57 for the all samples comparison and\n6.52 for the mobile samples comparison.\nTask difficulty\nAt the end of each survey, the respondents are also asked to\nevaluate the level of difficulty of the survey through a 4-point\nTable 5. Data quality indicators for 19-question intercept and panel surveys.\nAll samples Mobile samples only\n Intercept Panel t Intercept Panel t\nRounding (% rounded)\nStraightlining (% straight-lined)\nTrap question (% failed)\nKnowledge question (% incorrect)\nTask difficulty\nrating scale. For the 52-question survey, the differences\nbetween the panel and intercept samples were not signifi-\ncant. For both the 29- and 19-question surveys, significantly\nmore panel respondents thought the survey to be very easy\nthan intercept respondents, in the all samples comparison.\nThe same pattern emerged for mobile samples only in the\nConclusion and discussion\nDifferent survey sampling strategies have their own pros and\ncons. Previous studies have largely focused on the examina-\ntion of nonprobability survey data quality by benchmarking\nthe survey estimates on external probability benchmarks. To\nour knowledge, this is the first study that compares data qual-\nity between intercept and panel surveys. Instead of examin-\ning the representativeness of the survey data, this study\nfocuses on respondent satisficing, a data quality perspective\nthat is of crucial importance for nonprobability surveys.\nThe completion rates of panel surveys are consistently\nhigher than intercept surveys, regardless of the survey length.\nThis suggests that the online panel has advantages over the\nintercept in terms of cost and efficiency. One thing to point out\nis that the higher completion rate for the panel survey may be\ndue to the fact that PC respondents tend to have greater response\npropensity than mobile respondents in general, and, should we\nremove the PC respondents from the completion rate calcula-\ntion, a different pattern might emerge. Unfortunately, such data\nwere not captured in the platform for me to calculate a mobile-\nonly completion rate for the panel sample.\nFor the 52-question survey, the overall comparison between\nthe two samples revealed eight significant differences among\n21 data points. Among the eight significant data points, six\nshowed indications of superior data quality for the panel com-\npared to the intercept sample. However, after restricting the\ncomparison to mobile-only respondents for the panel, the\nnumber of significant differences drops to four and all but one\nsuggest that a panel survey produces better quality data than\ndoes an intercept survey. Similarly, for the 29-question survey,\namong the 13 data points studied, three showed significant dif-\nferences, among which two indicated that the panel survey\nrespondents had fewer satisficing behaviors. Only one data\nindicator remains significant and it indicates that intercept\nrespondents produce better data among mobile respondents.\nThe 19-question survey only has six data points under com-\nparison, of which two showed significant differences; both\nindicated that panel respondents satisfice less than intercept\nrespondents. Interestingly, for the mobile-only comparison,\none more data quality indicator becomes significant.\nThe fewer significant differences in the mobile samples\ncould be due to the smaller sample size of mobile respond-\nents in the panel survey. Another analytical approach we\ntook was to calculate the average difference between the two\nsample sources across all data quality indicators in each sur-\nvey. The results show that the panel respondents engaged in\nfewer satisficing behaviors than the intercept samples.\nThe cost of each complete response was US$3.00 in the\nUS$7.00 in the 52-question survey, for both panel and inter-\ncept samples. There is no financial benefit in choosing one\nover the other.\nThere are several limitations to this study that future\nresearch could address. First, the panel sample is a combina-\ntion of both mobile and non-mobile respondents, but the\nintercept sample is entirely mobile, and hence with the data\navailable, it is not possible to separate out the sample source\neffect from the mode effect. Other studies have found few\ndifferences in data quality between mobile and PC respond-\nents (for a review, see Couper et al., 2015), and they provide\nsome confidence for me to believe most of the differences\nobserved are due to the sample sources rather than the mode.\nHowever, it is very important for future study to tease out the\nmode effect from sample differences. Second, not all propen-\nsity models are the same. The shorter surveys have fewer\npredictors in the model than do the longer surveys simply\nbecause fewer questions are asked in the shorter surveys.\nConsequently, the power of propensity score weighting\nadjustment is not equal across the three survey lengths and it\nmay therefore contribute to the data quality difference.\nShould there be the same propensity model for all surveys,\nthe difference between intercept and panel might be further\nreduced for the shorter surveys. Third, there is more than one\nway of recruiting and inviting survey participants from inter-\ncept samples. For example, participants could be recruited\nthrough more targeted approaches, by targeting populations\nthat are potentially interested in the survey and using mes-\nsages that appeal to their altruism (for examples, see Barratt\nand incentive strategy might appeal to different sectors of the\npopulation and hence lead to different data quality. Future\nstudies should examine whether the findings in this study\nhold if the intercept samples are recruited differently. Last\nbut not least, this study targeted the general population, with-\nout any targeting or screening. Everyone from the intercept\nand panel samples was eligible for this study. In reality, many\nweb surveys target a specific group or people with specific\ncharacteristics. Whether the findings of this study hold when\nthe goal of the survey is to target a subgroup of the popula-\ntion is unknown, and I encourage future study to explore this.\nDeclaration of conflicting interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research, author-\nship, and/or publication of this article.\nNote\n1. For all three surveys, the first page only included one introduc-\ntory sentence: \"We would like to hear about your experiences\n8 Methodological Innovations\nand your opinions on some important issues. There are no right\nor wrong answers.\" No questions were asked on the first page.\nThe survey platform could only track and calculate the com-\npletion rate conditional on Page 1. The completion rate for the\nentire survey was not available.\nReferences\nAbraham KG, Maitland A and Bianchi SM (2006) Nonresponse\nin the American time use survey: Who is missing from the\ndata and how much does it matter? Public Opinion Quarterly\nAlvarez RM, Sherman RP and VanBeselaere C (2003) Subject\nacquisition for web-based surveys. Political Analysis 11(1):\nBarratt MJ and Lenton S (2015) Representativeness of online\npurposive sampling with Australian cannabis cultivators.\nBarratt MJ, Ferris JA and Lenton S (2015) Hidden popula-\ntions, online purposive sampling, and external validity:\nBosnjak M and Tuten TL (2003) Prepaid and promised incentives in\nweb surveys: An experiment. Social Science Computer Review\nCouper MP (2000) Review: Web surveys--A review of issues and\nCouper MP, Antoun C and Mavletova A (2015) Mobile web sur-\nveys: A total survey error perspective. Paper presented at the\ninternational total survey error conference, Baltimore, MD,\nDykema J, Stevenson J, Klein L, et al. (2013) Effects of e-mailed\nversus mailed invitations and incentives on response rates,\ndata quality, and costs in a web survey of university fac-\nFricker RD and Schonlau M (2002) Advantages and disadvantages\nof internet research surveys: Evidence from the literature.\nG\u00f6ritz AS (2006) Cash lotteries as incentives in online pan-\nG\u00f6ritz AS and Wolff H-G (2007) Lotteries as incentives in longi-\ntudinal web studies. Social Science Computer Review 25(1):\nGroves RM, Singer E and Corning A (2000) Leverage-Saliency\ntheory of survey participation: Description and an illustration.\nJones MS, House LA and Gao Z (2015) Respondent screening and\nrevealed preference axioms: Testing quarantining methods for\nenhanced data quality in web panel surveys. Public Opinion\nKrosnick JA (1991) Response strategies for coping with the cog-\nnitive demands of attitude measures in surveys. Applied\nKrosnick JA (1999) Survey research. Annual Review of Psychology\nKrosnick JA and Alwin DF (1987) An evaluation of a cognitive the-\nory of response-order effects in survey measurement. Public\nMatthijsse SM, de Leeuw ED and Hox JJ (2015) Internet pan-\nels, professional respondents, and data quality. Methodology\nPedersen MJ and Nielsen CV (2014) Improving survey response\nrates in online panels: Effects of low-cost incentives\nand cost-free text appeal interventions. Social Science\nComputer Review. Epub ahead of print 17 December. DOI:\nSheehan KB (2001) E-Mail survey response rates: A review.\nJournal of Computer-Mediated Communication. Epub ahead\nToepoel V, Das M and Van Soest A (2008) Effects of design in\nweb surveys: Comparing trained and fresh respondents. Public\nVan Veen F, G\u00f6ritz AS and Sattler S (2015) Response effects\nof prenotification, prepaid cash, prepaid vouchers, and\npostpaid vouchers: An experimental comparison. Social\nScience Computer Review. Epub ahead of print May. DOI:\nWright KB (2005) Researching internet-based populations:\nAdvantages and disadvantages of online survey research, online\nquestionnaire authoring software packages, and web survey ser-\nvices. Journal of Computer-Mediated Communication. Epub\nAuthor biography\nMingnan Liu, Ph.D., is a survey scientist at SurveyMonkey, Palo Alto,\nCalifornia, USA. He received his Ph.D. in survey methodology from\nthe University of Michigan. His research covers a variety of areas,\nincluding survey measurement, nonresponse, and sampling bias.\nAppendix 1\nQuestion wordings and response options\n(a) 52-question survey\n(b) 29-question survey\n(c) 19-question survey\nResponse rounding\n(a) (b) During the past 30days, how many times did you eat fast food, including drive-through, takeaway, and sitting down\nin the restaurant?\n(a) (b) (c) During the past 30days, on how many days did you have one or more alcoholic drinks?\n(a) (b) On an average day, about how many hours do you watch television?\n(a) (b) How many text messages have you sent and received on your phone in your current billing cycle?\n(a) How many close friends do you have?\nStraightlining\n(a) (b) (c) Please tell us whether you would like to see more or less government spending in each area.\nSpend much more Spend more Spend the same as now Spend less Spend much less\nThe environment o o o o o\nHealth o o o o o\nEducation o o o o o\nThe police and law\nenforcement\no o o o o\nUnemployment benefits o o o o o\n(a) Do you agree or disagree with the following statement?\nAgree\nstrongly\nAgree\nsomewhat\nNeither agree\nnor disagree\nDisagree\nsomewhat\nDisagree\nstrongly\nOur society should do whatever is necessary to\nmake sure that everyone has an equal opportunity\nto succeed\no o o o o\nWe have gone too far in pushing equal rights in\nthis country\no o o o o\nOne of the big problems in this country is that we\ndon't give everyone an equal chance\no o o o o\nThis country would be better off if we worried\nless about how equal people are\no o o o o\nIt is not really that big a problem if some people\nhave more of a chance in life than others\no o o o o\nIf people were treated more equally in this\ncountry, we would have many fewer problems\no o o o o\n10 Methodological Innovations\nTrap questions\n(a) (c) Please select B as your answer choice.\n A\n B\n C\n D\n E\n(a) (b) In the past 30days, did you make a purchase online at any of the following website? Please select all that apply. We\nalso want to see whether people are reading the questions carefully. To show that you've read this much, please mark both\nthe Groupon and None of the above box below. That's right, just select these two options only.\n amazon.com\n BestBuy.com\n ebay.com\n Groupon.com\n overstock.com\n Zappos.com\n None of the above\n(a) (b) Regardless of how frequently you read the newspaper, what would you say are your favorite newspaper sections to\nread? Please check all that apply. We also want to see whether people are reading the questions carefully. To show that\nyou've read this much, please mark both the Classified and None of the above box below. That's right, just select these two\noptions only.\n National\n Local\n Classified\n Sports\n Business\n Science and technology\n Opinion\n None of the above\nKnowledge questions\n(a) (b) (c) The logo for the Olympic Games comprises four interlocking rings.\n True\n False\n(a) Antibiotics will kill viruses as well as bacteria.\n True\n False\n(a) How many times can an individual be elected President of the United States under current laws?\n(a) (b) For how many years is a US Senator elected, that is, how many years are there in one full term of office for a US\nSenator?\nSensitive questions\n(a) (b) Do you now smoke cigarettes every day, some days, or not at all?\n Every day\n Some days\n Not at all\n(a) Have you smoked at least 100 cigarettes in your entire life?\n Yes\n No\n(a) (b) Have you ever, even once, used marijuana or hashish?\n Yes\n No\n(a) Here is a list of terms that people sometimes use to describe themselves. Which option best describes how you think of\nyourself?\n Heterosexual or straight\n Homosexual, gay or lesbian\n Bisexual\n(a) How many sex partners have you had in the last 12months?\nOpen-ended question\n(a) (b) (c) What do you think is the most important problem facing the country today?\nSurvey difficulty\n(a) (b) (c) How easy or difficult do you find the survey is?\n Very easy\n Somewhat easy\n Somewhat hard\n Very hard"
}