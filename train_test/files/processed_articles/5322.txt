{
    "abstract": "Abstract\nDespite all the attention to Big Data and the claims that it represents a ``paradigm shift'' in science, we lack understanding\nabout what are the qualities of Big Data that may contribute to this revolutionary impact. In this paper, we look beyond\nthe quantitative aspects of Big Data (i.e. lots of data) and examine it from a sociotechnical perspective. We argue that a\nkey factor that distinguishes ``Big Data'' from ``lots of data'' lies in changes to the traditional, well-established ``control\nzones'' that facilitated clear provenance of scientific data, thereby ensuring data integrity and providing the foundation for\ncredible science. The breakdown of these control zones is a consequence of the manner in which our network tech-\nnology and culture enable and encourage open, anonymous sharing of information, participation regardless of expertise,\nand collaboration across geographic, disciplinary, and institutional barriers. We are left with the conundrum--how to\nreap the benefits of Big Data while re-creating a trust fabric and an accountable chain of responsibility that make credible\nscience possible.\n",
    "reduced_content": "Original Research Article\nBig Data, data integrity, and the fracturing\nof the control zone\nCarl Lagoze\n Keywords\nBig Data, control zone, paradigm shift, sociotechnical\nBig Data is not only about being big\nThe popular and scholarly literature is filled with excite-\nment about Big Data. A good deal of the enthusiasm\ncomes from the business sector, where Big Data offers\nnew possibilities for direct and micro marketing,\nsupply-chain optimization, and other means of increas-\ning efficiency and profits. This enthusiasm has also\nspread to the public sector, particularly in the areas\nof security and terrorism prevention. In this paper, we\nexamine the impact of Big Data in the context of sci-\nence,1 encompassing the research that takes place in the\nacademic, corporate, and government milieu.\nAdmittedly, the line between commercial research (dis-\ntinguished from corporate research such as that which\ntakes place at IBM Watson) and scientific research can\nbe fuzzy, but we distinguish the former as motivated by\nfinancial concerns (e.g. product improvement for profit\nimprovement), whereas the latter is motivated by the\nsearch for some ``truth''. Some argue that Big Data\nrepresents a new paradigm of science, a ``fourth para-\ndigm'' (Hey et al., 2009), adopting the terminology used\nby Kuhn (1970) to characterize the revolutionary\ntransformation of a scientific field.2 While many view\nthis new paradigm as complementary rather than sub-\nstitutive to pre-existing paradigms (observation, experi-\nmentation, and simulation), others like Chris Anderson\nhave taken a more extreme view, claiming that Big\nData represents the ``end of theory'' (Anderson, 2008).\nOur goal in this paper is to pull back from the hype\nand take a more measured, analytical approach to\nBig Data, focusing on the question ``what are the\ncharacteristics of (some) Big Data that manifest a para-\ndigm shift in the fundamental assumptions of science''?\nWe distinguish between Big Data characteristics that\nhave methodological consequences and those that\nimpact epistemological foundations. We characterize\nthe former as important but not paradigm-shifting.\nIn contrast, we argue that a paradigm shift is\nUniversity of Michigan, School of Information, Ann Arbor, MI, USA\nCorresponding author:\nCarl Lagoze, University of Michigan, 105 S. State Street, Ann Arbor,\nEmail: clagoze@umich.edu\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (http://www.uk.sagepub.com/aboutus/\nopenaccess.htm).\nindeed evident when Big Data impacts epistemological\nfoundations.\nEmbedded in this argument is the assumption that\nthe characteristics we are looking for are not native to\nall uses of big (in size) data. And, in fact, it may be true\nthat data that is not necessarily quantitatively large\nmay have characteristics that are paradigm-shifting\nwhen used in certain contexts and by certain commu-\nnities of use.\nBefore proceeding any further with an analysis of\n``big'' as a qualifying characteristic of (some) ``data'',\nit is important to establish a definition of ``data'',\nwhether big or small. A National Academies report\n(A Question of Balance: Private Rights and the Public\nInterest in Scientific and Technical Databases, 1999)\nprovides a simple and inclusive foundation definition:\n``data are artifacts, numbers, letters, and symbols that\ndescribe an object, idea, condition, situation, or other\nfactors.'' Although this definition is useful, it fails to\ncapture the ``relative'' nature of data (in contrast to it\nhaving an ``essential'' nature). As Borgman (2011)\nstates: ``[d]ata may exist only in the eyes of the\nholder: the recognition that an observation, artifact,\nor record constitute data is itself a scholarly act.''\nThis perspective of data is reflexive; something (e.g.\nimages, text, and Excel worksheet, etc.) is data because\nsomeone uses it as data in a specific context, and tran-\nscendent, it carries across the many disciplines, prac-\ntices, and epistemologies of science.\nThis relational/contextual perspective gives us the\nbasis for examining the Big Data phenomenon in a\nmanner that both crosses epistemological boundaries\nand is contextualized by them. With due recognition\nof the dangers of making generalizations about ``sci-\nence'', we hope to establish some fundamental aspects\nof Big Data that are indeed boundary crossing, while\nremaining shaped by (and shaping) specific disciplinary\npractices.\nHaving established this relativistic definition of data,\nwe return to the notion that ``Big Data is not only\nabout being big''; that there is some combination of\nfeatures or dimensions (perhaps among them size)\nthat may have revolutionary effects on science and\nknowledge production. This multidimensional perspec-\ntive is evident in many of the popularized, mass-market\ndescriptions of Big Data.\nOne popular multidimensional definition of Big\nData is based on the so-called 3Vs: Volume, Velocity,\nand Variety (Laney, 2001). Volume is the size factor.\nVelocity refers to the speed of accumulation, the result-\ning dynamic nature of the data, and the high-scale pro-\ncessing capacity needed to make it useful and keep it\ncurrent. Finally, Variety refers to the mixing together,\nor mashing-up, of heterogeneous data types, models,\nand schema and the need to resolve these differences\nin order to make the data useful. Others have enhanced\nthis list with additional ``Vs'': Validity, the amount of\nbias or noise in the data; Veracity, the correctness and\naccuracy of the data; and Volatility, the persistence and\nlongevity of data (Normandeau, 2013), the first two of\nwhich, Validity and Veracity, are of particular interest\nto the argument of this paper.\nMayer-Schonberger and Cukier in their best-selling\nbook Big Data offer an alternative but complementary\nset of characteristics of Big Data, which they claim\n``challenges the way we live and interact with the\nworld'' (Mayer-Scho\nterize Big Data as revolutionary because it enables/\nembodies ``three shifts [characteristics] in the way we\nanalyze information and transform how we understand\nand organize society.'' The first is the ``more'' charac-\nteristic, which they posit as the foundation for the two\nother characteristics. A notable aspect of ``bigness''\naccording to the authors is its equivalence to ``allness''\n(n \u00bc all). Throughout the book they assert that Big\nData obviates the need for traditional (in their view\nflawed) sampling techniques and increasingly can be\nconsidered a complete view of the object of investiga-\ntion. We later question this argument and initially note\nthat even if the n \u00bc all principle were true, the notion of\ndata providing a ``complete view'' of reality, in the\nobjective sense, is met with skepticism by a number of\nGitelman, 2013). The second characteristic is\n``messy,'' the effect of which is diminished by the n \u00bc all\ncharacteristic. In their words, ``looking at vastly more\ndata also permits us to lessen our desire for exactitude.''\nThe third and final characteristic is the shift in analyt-\nical technique from causality to correlation. ``Most\nstrikingly, society will need to shed some of its obses-\nsion for causality in exchange for simple correlations;\nnot knowing why but only what.'' We will return to\nMayer-Schonberger and Cukier later in this paper to\nfurther critique of their n \u00bc all claim and its implica-\ntions for new paradigm science.\nThese two attempts to define Big Data, and many\nothers like them, fail to adequately capture the nuances\nand contexts of use of Big Data that may make it revo-\nlutionary and the driver of a new scientific paradigm.\nEmploying Kuhn's words, when are Big Data ``tra-\ndition-shattering complements to the tradition-bound\nactivity of normal science'' (Kuhn, 1970)? To answer\nthis question, we need to examine Big Data from a\nsociotechnical perspective (Bijker, 1995; Lamb and\nSawyer, 2005). We need to investigate their social, cul-\ntural, historical, and technical facets and the interplay\nand tensions among these facets that collectively estab-\nlish the impact of Big Data on science and the possible\ntransformation thereof. An analysis of this sort will\nallow us to distinguish the aspects of Big Data that,\n2 Big Data & Society\nno matter how contributory to innovation, may be\nmore evolutionary than revolutionary, from those\nthat are indeed paradigm-shifting. Furthermore, it\nwill help us distinguish between locality--discipline\nand/or field-specific characteristics of Big Data--and\nglobality--aspects of Big Data that may be paradigm-\nshifting across the scholarly enterprise.\nLots of data or Big Data?\nBecause technology is such a basic enabler and compo-\nnent of Big Data practices (i.e. computation including\nhardware, software, and algorithmic components; high-\nspeed networks; massive storage arrays), it is useful to\nbuild our argument on the notions of new technological\nparadigms (Dosi, 1982) and of disruption (Christensen\nand Rosenbloom, 1995; Rosenbloom and Christensen,\n1994). Originating in the business and organizational\nbehavior sector, these two concepts nicely complement\nKuhn's theories, which focus on the scholarly domain.\nDosi distinguishes between evolutionary paths of\ntechnological change and new technological paradigms\nthat represent discontinuities from pre-existing techno-\nlogical paths and address new classes of problems.\nChristensen and Rosenbloom expand on this with the\nnotion of disruptive innovation, which is a discontinu-\nity in not only the technological aspect of a product or\nservice, but also a sociotechnical disruption; a context-\nual change in the set of valuations and values that\nframe and are impacted by the technical innovation.\nChristensen initially applied this theory of disruption\nto product lines (Christensen, 1997), with a prime\nexample being the successive introduction of smaller\nhard disk platters that initially seemed noncompetitive\nwith the disc products of mainstream manufacturers,\nbut eventually and repetitively obliterated the main-\nstream markets due to their framing within the revolu-\ntionary personalization of computing. Christensen has\nalso applied this theoretical framework to health care\n(Christensen et al., 2008a) and education (Christensen\nBy leveraging the theoretical frameworks of Kuhn,\nRosenbloom, and Christiansen, we argue that a disrup-\ntion in science (a.k.a. the creation of a new paradigm) is\nnot just methodological, a way of doing (a.k.a. tech-\nnical), but also must be sociotechnical. It must chal-\nlenge existing epistemological norms, ways of\nknowing and framing the fundamental scientific ques-\ntions of the field; institutional ecologies (Star and\nGriesemer, 1989), agreements on scope, assumed know-\nledge, and boundaries of research work; reward struc-\ntures, paths to tenure and promotion; and\ncommunication regimes, mechanisms, and norms for\ndisseminating knowledge. We will use this scaffolding\nfor the remainder of this essay to distinguish between\nwhat we will call lots of data, the effects of which are by\nand large methodological and technical, and true Big\nData, that which entails epistemological and, as a\nresult, paradigmatic change.\nOur distinguishing between these two terms--lots of\ndata (which entails methodological change and tech-\nnical innovation) and Big Data (which implies the re-\nevaluation of epistemological foundations)--should\nnot be interpreted as an attempt to segregate data\ninto two disjoint silos, i.e. data set 1 is ``lots of data'',\nin contrast to data set 2 that is genuine ``Big Data''.\nOur intention, rather, is to establish these concepts as\ncontinuous dimensions with which instances of data use\ncan be evaluated in order to understand the degree and\norigins of their methodological and/or paradigm-shift-\ning effects, i.e. a use of data set 1 has high ``lots of data''\nimpact but low ``Big Data'' impact while a use of data\nset 2 has low ``lots of data'' impact but high ``Big Data''\nimpact. The term ``instances of data use'', in contrast to\nsimply ``data'', is intentional and refers to the fact that,\nsimilar to the definition of data, the methodological\nand epistemological impact of data must be evaluated\nwithin the context of use. An important facet of this\ncontext is the distinct epistemic culture (Knorr-Cetina,\n1999) of the community of use and its particular per-\nspectives on data and its meaning. In other words, the\nsame data set may ``measure'' differently according to\nthe ``Big Data'' and ``lots of data'' dimensions when\nemployed by different disciplinary communities and/\nor for different purposes.\nAlthough the primary focus of the remainder of this\npaper is the Big Data dimension--when, how, and why\ndoes data use challenge the epistemological foundations\nof science--it is useful, for the purpose of contrast, to\nbriefly examine the companion lots of data dimension.\nThis brevity should not be construed as dismissive\ntowards the significance of these technical challenges\nand the methodological impacts they have. Indeed,\nthere are great challenges here and the scholarly and\npractical effects of meeting these challenges can be pro-\nfound, albeit not paradigm-shifting.\nTwo often-cited instances of data use demonstrate\nthe lots of data dimension. The petabytes of data\nstreaming in from high-energy physics experiments\n(studied thoroughly by Knorr-Cetina, 1999) or those\nthat are components of the Sloan Digital Sky Survey\n(Szalay and Gray, 2001) are certainly Big Data in terms\nof size. But, considered alone, their bigness and the\nissues associated with them are by and large technical.\nThese communities have historic cultures of data shar-\ntheir data has always been ``big'' relative to the quan-\ntitative definitions of the day. This is similar to the situ-\nation with many domains of science that have a legacy\nof exploring and manipulating large data sets, where\n``large'' is historically contextualized relative to the\ntechnical affordances of the time (Gitelman, 2013).\nThe massive quantity of data in these two examples\nclearly introduces issues about new high-capacity stor-\nage systems, high-speed networks to easily move them\nback and forth, and map-reduce algorithms that permit\nparallel computation over these massive data sets. A\nrecent white paper co-authored by leading data science\nresearchers (Agrawal et al., n.d.) provides a useful list\nof the cross-cutting challenges that need to be met to\nrespond to these issues; heterogeneity and incomplete-\nness, scale, timeliness or speed, privacy, and human\ncollaboration. All of these are formidable challenges.\nHowever, the need for these new methodologies and\ntools to manipulate, store, and curate these massive\ndata sets does not correspond to a paradigm-shifting\ndisruption of the historically data-focused epistemic\nculture of the communities of practice that engage\nwith these data.\nA recent paper by Leonelli (2014) in the inaugural\nissue of this journal explores the same issue in the dis-\ncipline of biology.3 Similar to this paper (albeit limited\nto a single discipline), Leonelli aims ``to inform a cri-\ntique of the supposedly revolutionary power of Big\nData science,'' likewise defining revolutionary as syn-\nonymous with creating a new epistemology and a new\nset of norms. Similar to our earlier examples in physics\nand astronomy, she notes that ``data-gathering prac-\ntices in subfields [of the life sciences] have been at the\nheart of inquiry since the early modern era, and have\ngenerated problems ever since.'' She then aims the bulk\nof her critique at Mayer-Scho\n\u00a8 nberger and Cukier's\nclaims that data completeness mitigates data messiness\nand their championing of correlation over causality,\nwhich we will return to later in this paper. She finishes\nby rejecting the notion that Big Data is exerting a revo-\nlutionary effect on the epistemology of biology itself,\nclaiming that ``there is a strong continuity with prac-\ntices of large data collection and assemblage since the\nearly modern period; and the core methods and epi-\nstemic problems of biological research, including\nexploratory experimentation, sampling and the search\nfor causal mechanisms, remain crucial parts of the\ninquiry in the area of science.'' In contrast to epistemic\neffects on the discipline itself, she acknowledges signifi-\ncant methodological challenges ``encountered in\ndeveloping and applying curatorial standards for\ndata . . . '' and in the dissemination of that data.\nOn the other hand, the sensitivity of the evolutionary\nversus revolutionary impact of big (or of even any) data\nto epistemic culture becomes evident in the context of\ndigital humanities (or as some call it computational\nhumanities, and its specializations such as computa-\ntional history). The level of controversy over the ``data-\nfication'' (Mayer-Scho\nor literary artifacts (whether in massive scale such as\nthe Google Books Project or the scale of a single liter-\nary corpus) can be viewed as evidence of resistance to\nthe introduction of a new epistemology, based on data,\nthat is viewed by some as threatening, and perhaps\ninferior, to existing and historically based epistemolo-\nThese examples in physics, astronomy, biology, and\nthe humanities (and many similar ones) lead us to con-\nclude that mere bigness, lots of data (which appears\nto have different meanings in different scholarly\nfields), is not the basis for declaring a new paradigm\nin science. Furthermore, we can be fairly confident that\nsuch a blanket declaration without attention to the con-\nfounding factor of epistemic cultures warrants\nskepticism.\nData integrity and credible science\nWith these caveats in mind, however, we do claim that\nthere might be some cross-cutting framing of data and\ntheir application across the entire scholarly endeavor,\nrecognizing that this framing needs to be parameterized\nto a particular use of data within a particular epistemic\nculture. Then, we need to understand how Big Data\nmight challenge this common framing, thereby becom-\ning ``tradition shattering'' (Kuhn, 1970).\nAt the forefront is the notion of data integrity, which\nwe assert is a consistent and discipline-crossing founda-\ntion of credible science (Committee on Ensuring the\nUtility and Integrity of Research Data in a Digital\nterm integrity rather than correctness or quality; the\nlatter terms ascribe a level of positivism to data that\nmany modern scholars refute (Edwards, 2010;\nGitelman, 2013). Integrity, on the other hand, has a\nmore constructivist tone, implying notions of ``trust'',\n``fitness for use'', and ``consensual understanding'', all\nof which are contextual and relative to epistemic cul-\nture, in contrast to the implicitly binary notion of cor-\nrectness. Looking at this from the perspective of\ninfrastructure to support data sharing (using ``infra-\nstructure'' in its broadest most sociotechnical sense;\nEdwards et al., 2007), we can then draw the links\nfrom integrity to trust, and ultimately to provenance\n(evidence upon which trust is established), and propose\nthat determining the degree of data integrity is based on\nthe ability to answer a number of questions. What is the\norigin of these data? Who has been responsible for\nthem since their origination? Can we apply our stand-\nard notions for trust and integrity to them? Do our\nstandard methodologies for interpreting them and\ndrawing conclusions from them make sense? Big Data\nis then those data that disrupt fundamental notions of\nintegrity and force new ways of thinking and doing to\n4 Big Data & Society\nreestablish it. Said differently, Big Data is data that\nmakes us rethink our notions of credible science.\nOur attention here to the issues of data and scientific\nintegrity is coincident with a growing concern with the\nreliability of scientific knowledge. The notion of a crisis\nin reliability has been discussed in the media (Naik,\n2011), and in scientific journal articles (Brembs and\nMunafo\n` , 2013) and editorials (``Announcement,''\nreliability has been fueled by well-publicized cases of\nscientific fraud and data falsification in a number of\nscientific fields (Harrison et al., 2010; ``Researcher\nFaked Evidence of Human Cloning, Koreans\nition, a number of academics are warning about the\nprevalence of false results in the scientific literature\nBut, as pointed out by Stodden (2014), some of this\nconcern arises from the increasing prevalence of data-\nintensive (Big Data) science across the disciplines, and\nthe application of computational, analytical methods to\nthose data without complete understanding of their\ncharacteristics (e.g. the nature of the sample represented\nby the data). Absent full understanding of the data (and\nin some cases a failure to account for this lack of intim-\nacy with the data), researchers have at times unwittingly\nor sloppily applied methodological tools or epistemo-\nlogical understanding to those data that failed to\naccount for the fundamental differences between them\nand traditional highly-curated and reliable data. As\npointed out by Lazer et al. (2014), `` . . . most Big Data\nthat have received popular attention are not the output\nof instruments designed to produce valid and reliable\ndata amenable for scientific analysis.''\nOf particular concern in this area has been scientific\nresults based on data sources of questionable proven-\nance and integrity such as distributed sensors (Wallis\net al., 2007) and ``black box social media,'' where the\norigin and basis of the data are difficult to determine\n(Driscoll and Walker, 2014) and the algorithmic bias on\nthe conclusions is difficult to unravel (Gillespie, 2014).\nA well-known example of the foibles of the reliance on\ninformally collected data and algorithmic projection is\nthe Google Flu Trends (GFT), which raised huge sci-\nentific optimism about the predictive utility of infor-\nmally collected data when first published in Nature in\nserious setback in 2013 when the GFT predictions for\nthat year were shown to be seriously exaggerated\ning for this setback is beyond the scope of this paper.\nHowever, one acknowledged factor is an overconfi-\ndence in the veracity of the data as a true sample of\nreality, rather than a random snapshot in time and the\nresult of algorithmic dynamics.\nWe acknowledge that this emphasis on data integrity\n(a.k.a. quality) stands somewhat in opposition to the\npopularized claims by Mayer-Scho\n\u00a8 nberger and Cukier\nthat ``looking at vastly more data . . . permits us to\nloosen up a desire for exactitude'' and effectively\nallows us to ignore ``messiness'' in data (Mayer-\nScho\n\u00a8 nberger, 2013). As mentioned earlier, this claim\nand subsequent claims by the authors seem to rely\nheavily on n \u00bc all, that is, Big Data is not a sample\nbut a complete set. We find this claim highly suspicious\nand agree with fellow scholars (Boyd and Crawford,\ndata, no matter what its size, is de facto a sample,\nwith bias implicit due to choice of instrumentation,\nspan of observation, units of measurement, and\nnumerous other factors. In essence, n never equals all;\nall is a limit in mathematical terms that can be\napproached but never attained. This point is also\nemphasized by Leonelli, who states that ``having a lot\nof data is not the same as having all of them; and\ncultivating such a vision of completeness is a very\nrisky and potentially misleading strategy'' (Leonelli,\n2014). Thus, if one denies sampling and its effects on\nmessiness or on our ability to derive meaning from cor-\nrelations, as Mayer-Scho\n\u00a8 nberger and Cukier seem to\ndo, they tread on questionable territory in terms of\nhigh integrity science, and may indeed have an argu-\nment that is more appropriate to business and com-\nmerce. Again quoting Leonelli, ``it is no coincidence\nthat most of the examples given by Mayer-\nScho\n\u00a8 nberger and Cukier come from the industrial\nworld, and particularly globalized retail strategies as\nis the case of Amazon.com'' (Leonelli, 2014).\nAs a point of reference, it is useful to look at the\nnotions of integrity, trust, and provenance in the con-\ntext of archives and archival science, for which they are\nessential concepts. Hirtle (2000) describes the meanings\nof these terms and the manner in which they are core to\nthe definition of the archive in the context of the ship\nConstellation, a tourist destination in Baltimore harbor\nthat was mistakenly identified as a revolutionary war\nship when its vintage was really the US Civil War.\nAccording to Hirtle (2000), ``at the heart of an arch-\nive . . . are records that are created by an agency or\norganization in the course of its business and that\nserve as evidence of the actions of that agency or organ-\nization [italics added].'' Furthermore, ``one way in\nwhich archivists working with . . . records have sought\nto ensure the enduring value of archives as evidence is\nthrough the maintenance of an unbroken provenance for\nthe records [italics added].'' Implicit in the notion of\n``unbroken provenance'' is control over storage and\ntransfer; in order to serve as evidence an archival\nrecord must demonstrate a complete, unbroken, histor-\nical knowledge of the item of interest, who has been in\ncontrol of it, and by what means it has been transferred\nor moved to other authorities. Fans of crime shows on\nTV or of detective novels should find this notion quite\nfamiliar; the evidence presented in a court of law is\nuseless if law enforcement has lost control of it and it\nmay have been tampered with.\nDefining the control zone\nTaking a cue from archival science then, we should\nlook at the role of control (and unbroken provenance)\nas a necessary (but not necessarily sufficient) factor in\ndata integrity. Traditional data origination, sharing,\nand reuse were based on the reality of containable\nand concrete physical data (e.g. written by hand or\nstored on magnetic devices that are kept in drawers\nor file cabinets) and data sharing practices based on\nphysical handoff to known colleagues. The physicality\nof both the data and the transfer of data amounted to a\nwell-defined control zone resulting in a provenance\nchain that was documented and witnessed. Before\nexamining the breakdown of this control zone in the\ncontext of Big Data, in the next section we examine the\nsame notion and its role in the disruption of another\nknowledge infrastructure (Edwards et al., 2013) that\nhas over the past two decades undergone considerable\nchange, the library. In a seminal 1996 article, ``Library\nFunctions, Scholarly Communication, and the\nFoundation of the Digital Library: Laying Claim to\nthe Control Zone'' (Atkinson, 1996), the late Ross\nAtkinson, then Associate University Librarian at\nCornell University, describes how the notion of a\ncontrol zone lay at the foundation of the library.\nAccording to Atkinson, the functioning of the library\ndepends on the definition of a clear boundary, a\ndemarcation of what lies in the library and what is out-\nside. Internal to this boundary, within the control zone,\nthe library can lay claim to those resources that have\nbeen selected as part of the collection, and assert cur-\nation, or stewardship, of those selected resources to\nensure their integrity, availability, and stability over\nthe long-term.\nThe boundary of the traditional library was easy to\ndefine. It was the ``bricks and mortar'' structure with a\nclear and controlled entry point that contained and\nprotected the selected physical resources over which\nthe library asserted control and curatorial responsibil-\nity. Correspondingly, from the patron's point of view,\nthe boundary marked what could be called a ``trust\nzone'', an area to which entry and exit were clearly\nmarked and in which they could presume the existence\nof the integrity guarantees of the library. Integrity, in\nthis case, does not imply veracity of the resources of the\nlibrary, but adherence to principles of proper informa-\ntion stewardship, including accurate description,\nlongevity of the resources, and adherence to some selec-\ntion criteria.\nIn Lagoze (2010), we describe how the move from\nphysical to digital information resources and the\nattendant access to them by the web architecture\nprofoundly disrupted the foundation of the control\nzone. This disruption was not anticipated by early par-\nticipants, practitioners, and researchers in the early\ndigital library initiatives, who foresaw technical\nbut not institutional change. In fact, some predicted\nthat in the end ``[digital] library services would fol-\nlow a familiar model'' (Gladney et al., 1994).\nOthers saw the Internet as another familiar evolution-\nary technical change, similar to past challenges to\nlibraries, stating that ``The anarchy of the Internet\nmay be daunting for the neophyte, but it differs little\nfrom the bibliographic chaos that is the result of\nfive and a half centuries of the printing press''\nTwo decades later, it is clear that the implications of\nmoving from physical to digital information and net-\nwork access to the information is more than a technical\nphenomenon; the implications are more than that\n``digital information crosses boundaries easily'' (Van\nHouse et al., 2003) and in fact are deeply disruptive\nto the library. By viewing the library as a meme,4\nrather than just as an institution or a physical artifact,\nwe can see the roots of the disruption. At the founda-\ntion of it is the foundation of the library itself, the dis-\nintegration of the control zone. The notions of a clear\nboundary, and the attendant concepts of being inside or\noutside, disappear in the web architecture, where users\n(i.e. patrons) no longer enter through a well-defined\ndoor, but ride hyperlinks and land wherever they may\nchoose in the digital library. Attempts to reassert a\nboundary by defining a new digital door or portal\nand establishing branding signposts defining inside vs.\noutside have proven incompatible with the dominant\nweb context and have largely failed. With the collapse\nof the control zone, other fundamental components of\nthe library meme become difficult to implement or ana-\nchronistic relative to the increasingly normative\nbroader web context. These include selection, deciding\nwhat information sources are available to patrons;\nintermediation, acting as a buffer between information\ncreators and information users; bibliographic descrip-\ntion, providing ``order making'' via the catalog; and\nfixity, guaranteeing the immutability of information\nresources.\nIn conclusion, the wholesale transition of our intel-\nlectual, popular, and cultural heritage to the digital\nrealm has been accompanied by a disruptive change\nin our expectations about our knowledge infrastruc-\ntures. The notions of selection, intermediation, biblio-\ngraphic description, and fixity that are core principles\n6 Big Data & Society\nof the library meme stand at odds to the web informa-\ntion meme. These contradictions become sharper as the\nweb has moved over the past decade into the web 2.0\nera and beyond. Expectations of open access to infor-\nmation, active participation in knowledge production\nand annotation, and the integration of social activity\nand knowledge activities are now the expected norm.\nLibraries are certainly part of this modern knowledge\ninfrastructure. But they exist as participants in a world\nof competing ``knowledge institutions'' (e.g. Wikipedia,\nFacebook, Twitter). Meanwhile, notions of informa-\ntion integrity, which were formally grounded in institu-\ntional frameworks such as the library, remain\nproblematic and in search of new ways to certify the\nprovenance of information resources.\nRethinking credible science in the age\nof Big Data\nWith knowledge of this precedent, we can now return\nto Big Data and recognize parallels in the historical\ntransitions of the library and the transformations in\nthe ways that scholarly data are created, shared, and\nused. The relatively well-controlled mechanisms (both\ncultural and technical) for data creation, data sharing,\nand data reuse are under pressure for a number of rea-\nsons. Funders, the public, and fellow scientists are\ndemanding, for good reason, better access to data\nand in general ``open data'' (Huijboom and Broek,\nthe creation of numerous data repositories (Greenberg\nallow easy and generally anonymous access to scientific\ndata on a global scale. Science in general is becoming\nmore collaborative and interdisciplinary (Barry and\net al., 2011) (at least partly due to the multidisciplinary\nscope of grand challenge problems like climate change),\nbreaking down traditional closely-knit teams of col-\nleagues and bringing together scholars with different\nepistemic and methodological cultures. An increasing\nnumber of data sources originate from nontraditional\nmeans, such as social networks for which concerns\nabout integrity and provenance are not priorities.\nMashups of data are becoming increasingly common,\nblurring the lines between formal and informal data.\nScientists seem to have a love/hate relationship with\nthis new reality. While they support the abstract idea\ntheir sharing practices, and sharing preferences, remain\nrelatively closed and motivated by control (Borgman,\nQuantitative social science research provides an\ninteresting example of this data transition and impact\non the control zone. For the past 50 years, quantitative\nsocial science has been built on a shared foundation of\ndata sources originating from survey research, aggre-\ngate government statistics, and in-depth studies of indi-\nvidual places, people, or events. Underlying these data\nis a well-established and well-controlled infrastructure\ncomposed of an international network of highly curated\nand metadata-rich archives of social science such as the\nInter-University Consortium for Political and Social\nResearch5 (ICPSR) and the UK Data Archive.6\nThese archives continue to play an important role in\nquantitative social science research. However, the emer-\ngence and maturation of ubiquitous networked com-\nputing and the ever-growing data cloud have\nintroduced a spectacular quantity and variety of new\ndata sources into this mix. These include social media\ndata sources such as Facebook, Twitter, and other\nonline communities in which individuals reveal massive\namounts of information about themselves that are\ninvaluable for social science research. When combined\nwith more traditional data sources, these provide the\nopportunity for studies at scales and complexities here-\ntofore unimaginable. This transformation has been\ndescribed by Gary King, a Harvard political scientist,\nas the social science data revolution, which is character-\nized by a ``changing evidence base of social science\nnities present formidable new challenges to the fabric\nof social science research. Among those mentioned by\nKing (2011b) include privacy challenges, problems of\nsampling bias in uncontrolled data sets, a change in the\nbasic ``job descriptions'' of social scientists with\ndemand for new skills in statistical methods, computa-\ntional methods, and the like, and the need for new\ncross-disciplinary collaborations (i.e. breaking down\nthe silos that social science scholars formally existed\nin). Clearly this is an example of Big Data rather\nthan just lots of data.\nAnother example of this fracturing of the control\nzone exists in observational science, for example, iden-\ntification and reporting of phenomena (e.g. species) in\necological niches, astronomy, and meteorology. In each\nof these areas there is a growing interest in what has\nbeen termed crowd sourced citizen science, which\nengages numerous volunteers as participants in large-\nscale scientific endeavors (Wiggins and Crowston,\n2010). The opportunities for large-scale citizen science\narise from the ubiquitous networking and computing\ncontext and especially the recent spectacular growth\nin the use of mobile devices. The motivations for lever-\naging this large-scale volunteer workforce as observa-\ntional ``sensors'' are substantial. The geographic scope\nof the observational spaces and the varieties of habitats\nmake reliance on trained observers (e.g. scientists)\ninfeasible. Our particular experience in this area is\nwith the eBird project,7 originated at the Cornell\nLaboratory of Ornithology, a highly successful citizen\nscience project that for over a decade has collected\nobservations from volunteer participants worldwide\n(Sullivan et al., 2014). Those data have subsequently\nbeen used for a large body of highly-regarded and influ-\nential scientific research.\nIt comes as no surprise that crowd sourced citizen\nscience makes a substantial portion of the formal sci-\nentific community uneasy (Sauer et al., 1994), especially\nin fields where people's lives are at stake, such as medi-\ncine (Raven, 2012). These data, by nature, breakdown a\nwell-established control zone whereby data is collected\nby experts or individuals managed by experts who care-\nfully abide by scientific methods. In contrast, citizen\nscience of this type must contend with the problems\nof highly variable observer expertise and experience.\nHow can we trust data or the science that results\nfrom those data when their provenance is rooted in\nsources whose own provenance does not conform to\n``standard'' criteria such as degree, publication record,\nor institutional affiliation?\nThe examples described above are only two of the\nmany instances in which new varieties of Big Data are\nundermining traditional control zones of science. If we\nlook longitudinally, we can see that examples such as\nthese are only the beginning of the problem. The frac-\ntured control zones, and the resulting uncertain prov-\nenance and trust, only intensify through the lifecycle of\nsharing, reuse, and circulation of data in an open net-\nwork in which not all participants are deemed trust-\nworthy according to established norms. Looking\nacross this lifecycle, this dilemma very quickly becomes\ncombinatorially more complex. If the control zone\naround data set A and that around data set B are\npoorly defined, that which results from the reuse and\ncombination of the two is only fuzzier. Of course, this is\nonly the first step in the progressive mashup and\n``cooking'' of these data with other data, a progression\nthat is inevitable when data reuse is easy and strongly\nencouraged.\nDespite the challenges and uncertainties, the inclu-\nsion of these ``uncontrolled'' Big Data into the scientific\nprocess is a reality that will continue and perhaps\nbecome more common. Our ``always there, every-\nwhere'' network culture will continue to make more\nand larger amounts of automatically, accidentally,\nand informally created data available for science. The\nvalue of these data across the scholarly spectrum has\nbeen demonstrated numerous times. Social scientists\ncan conduct studies on large-scale social networks\nthat may not replace, but do significantly complement,\ntraditional research based on small-scale social groups\ntists can now accumulate heretofore unavailable evi-\ndence of global phenomena, such as bird migrations\nand climatological events, by leveraging the active par-\nticipation and contribution of enthusiastic human\nvolunteers.8\nOur goal in this paper has not been to propose a\nnormative framework for this reality, but to simulate\nand add to discussions and investigations of its\nentangled social, cultural, historical, and technical\nimplications. Rather than fall back on hyperbolic\n``Big Data will change the world,'' the scholarly com-\nmunity needs to understand it and investigate its impli-\ncations for science policy and public trust of science.\nWe propose two threads for moving forward: one epis-\ntemological, evaluate our understanding of quality in\nboth data and science and our means for determining it,\nthe other methodological, developing means of recover-\ning traditional quality metrics.\nThe first approach begins by raising the awareness of\nresearchers who use Big Data about its opportunities,\ncomplexities, and dangers. This area is reasonably well\ncovered in Boyd and Crawford's (2011) paper ``Six\nProvocations for Big Data'', which covers many of\nthe caveats in dealing with this kind of data including\n``Claims to Objectivity and Accuracy are Misleading''\nand ``Bigger Data Are Not Always Better Data.'' As\nthe authors point out, a critical component of using Big\nData for research is understanding the integrity of\nthose data, where they originated, what biases are\nbuilt into them, how data cleaning may lead to over\nfitting, and what sampling biases may be embedded in\nthem. In this context, we need to evaluate what quality\nand integrity mean in a networked culture and its\nnumerous possible contexts, in the manner that other\nscholars are investigating parallel issues such as privacy\nAs for methodology, we suggest two technical paths\nthat may offer amelioration of the integrity problem,\nboth based on retrospectively recovering provenance,\nrather than prospectively, as in the traditional\nmanner. In our research with eBird, we have been\ninvestigating ways to reconstruct observer/contributor\nexpertise from the aggregated data. Our realization has\nbeen that expertise is too nuanced a factor to recon-\nstruct, but that experience, interpreted as deliberate\npractice, is an effective path to expert performance\n(Ericsson and Charness, 1994). Evidence of experience\ncan be extracted from the aggregated data; for example,\nfrequency of contributions, the diversity of contribu-\ntions measured by species distribution, etc. By devising\nways to recognize these traces we hope to develop\nmechanisms that aid scientists in determining the\nexpertise (and perhaps integrity) of anonymous data\ncontributors (reference removed for author anonym-\nity). Another approach might be to employ digital\nforensics (Reith et al., 2002), a technique increasingly\npopular in the intelligence and legal communities,\n8 Big Data & Society\nwhich, like our work with expertise, recovers traces of\norigin and provenance metadata from a digital artifact\nitself.\nIn conclusion, we have argued for an understanding\nof the difference between lots of data and Big Data. The\nformer, a quantitative feature with mainly technical\nand methodological implications, has, without a\ndoubt, had important effects on the way science is\ndone and what it makes possible. However, the latter,\na qualitative feature with profound epistemological and\nsociotechnical implications, shakes some of the core\nassumptions of credible science: trust and integrity.\nSimilar to so many aspects of our modern digital cul-\nture such as journalism (e.g. the New York Times versus\nthe flood of grassroots news blogs) and reference infor-\nmation (e.g. Encyclopedia Britannica versus\nWikipedia), it is futile and even undesirable to seek a\nreturn to traditional, rigid control zones. Nevertheless,\nwe are left with the challenge with Big Data to reap its\nbenefits while simultaneously holding science to the\nsame standards that it has been held to for centuries.\nDeclaration of conflicting interests\nThe author declares that there is no conflict of interest.\nFunding\nThis research received no specific grant from any funding\nagency in the public, commercial, or not-for-profit sectors.\nNotes\n1. Throughout this essay we use the term ``science'' as a gen-\neralization for all academic fields, not just the physical,\nlife, etc. sciences.\n2. Some well-known examples of Kuhn's notion of a para-\ndigm shift are the introduction of plate tectonics in geology\nand Einstein's special relativity theory in physics, both of\nwhich challenged primary assumptions of their respective\nfields.\n3. Although Leonelli does undertake a disciplinary-level ana-\nlysis, she acknowledges the flaws of using ``discipline'' as\nthe unit of study, recognizing the fact that within such\ncoarse granularity lies a wide variety of epistemological\nand methodological practices.\n4. We employ the term ``meme'' here to expand what we\nmean by ``library'' beyond its operational, technical, and\ninstitutional characteristics, and consider it in a manner\nsimilar to a semiotic sign (Morris, 1938).\n5. http://www.icpsr.umich.edu\n6. http://www.data-archive.ac.uk\n7. http://ebird.org\n8. One might conjecture about the possibility of machine sen-\nsing to replace the human volunteers. However, bird spe-\ncies observation and identification rely on a highly\nnuanced combination of visual, auditory, habitat, and\nother knowledge that will make automated sensing extre-\nmely difficult to implement.\nReferences\nAgrawal D, Bernstein P, Bertino E, et al. (n.d.) Challenges\nand Opportunities with Big Data. Available at: https://\nwww.purdue.edu/discoverypark/cyber/assets/pdfs/\nAnderson C (2008) The end of theory: will the data deluge\nmake the scientific method obsolete? Wired 1\u00ad5.\nAnnouncement: Reducing our irreproducibility (2013) Nature\nA Question of Balance: Private Rights and the Public Interest\nin Scientific and Technical Databases (1999) Washington,\nDC: The National Academies Press. Available at: http://\nwww.nap.edu/openbook.php?record_id\u00bc9692 (accessed\nAtkinson R (1996) Library functions, scholarly communica-\ntion, and the foundation of the digital library: laying claim\nto the control zone. The Library Quarterly 66(3).\nBarry A and Born G (2013) Interdisciplinarity:\nReconfigurations of the Social and Natural Sciences, 1st\ned. New York, NY: Routledge.\nBijker WE (1995) Of Bicycles, Bakelites, and Bulbs: Toward a\nTheoryofSociotechnicalChange.Cambridge,MA:MITPress.\nBorgman CL (2011) The conundrum of sharing research data.\nJournal of the American Society for Information Science\nBowker G (2014) The theory/data thing. International Journal\nof Communication 8(5).\nBoyd D and Crawford K (2011) Six provocations for Big\nBrembs B and Munafo\nconsequences of journal rank. ArXiv. Available at: http://\nBruns A (2013) Faster than the speed of print: reconciling\n``big data'' social media analysis and academic scholar-\nship. First Monday 18(10). Available at: http://first\nButler D (2013) When Google got flu wrong. Nature\nChristensen CM (1997) The Innovator's Dilemma: When New\nTechnologies Cause Great Firms to Fail. Boston, MA:\nHarvard Business School Press.\nChristensen CM, Grossman JH and Hwang J (2008a) The\nInnovator's Prescription: A Disruptive Solution for Health\nCare. New York, NY: McGraw-Hill.\nChristensen CM, Horn MB and Johnson CW (2008b)\nDisrupting Class: How Disruptive Innovation Will Change\nthe Way the World Learns. New York, NY: McGraw-Hill.\nChristensen CM and Rosenbloom RS (1995) Explaining the\nattacker's advantage: technological paradigms, organiza-\ntional dynamics, and the value network. Research Policy\nCommittee on Ensuring the Utility and Integrity of Research\nData in a Digital Age (2009) Ensuring the Integrity,\nAccessibility, and Stewardship of Research Data in the\nDigital Age. Washington, DC: National Academies Press.\nCragin MH, Palmer CL, Carlson JR, et al. (2010) Data shar-\ning, small science and institutional repositories.\nPhilosophical Transactions. Series A, Mathematical,\nDosi G (1982) Technological paradigms and technological\ntrajectories: a suggested interpretation of the determinants\nand directions of technical change. Research Policy 11(3):\nDriscoll K and Walker S (2014) Big data, big questions work-\ning within a black box: transparency in the collection and\nproduction of big twitter data. International Journal of\nEdwards P, Mayernik MS, Batcheller A, et al. (2011) Science\nfriction: data, metadata, and collaboration. Social Studies\nEdwards PN (2010) A Vast Machine: Computer Models,\nClimate Data, and the Politics of Global Warming.\nCambridge, MA: MIT Press.\nEdwards PN, Jackson SJ, Bowker GC, et al. (2007)\nUnderstanding Infrastructure: Dynamics, Tensions, and\nDesign. Washington, DC: National Science Foundation.\nEdwards PN, Jackson SJ, Chalmers MK, et al. (2013)\nKnowledge Infrastructures: Intellectual Frameworks and\nResearch Challenges. Ann Arbor, MI.\nEricsson KA and Charness N (1994) Expert performance: its\nstructure and acquisition. American Psychologist 49(8):\nGillespie T (2014) The relevance of algorithms. In: Gillespie\nT, Boczkowski P and Foot (eds) Media Technologies:\nEssays on Communication, Materiality, and Society.\nCambridge, MA: MIT Press, p.167.\nGinsberg J, Mohebbi MH, Patel RS, et al. (2009) Detecting\ninfluenza epidemics using search engine query data.\nGinsparg P (1994) First steps towards electronic research\nGitelman L (2013) ``Raw Data'' Is an Oxymoron\n(Infrastructures). Cambridge, MA: The MIT Press, p.192.\nGladney HM, Fox EA, Ahmed Z, et al. (1994) Digital\nLibrary: Gross Structure and Requirements: Report from\na March 1994 Workshop. College Station: IEEE.\nGreenberg J, White HC, Carrier S, et al. (2009) A metadata\nbest practice for a scientific data repository. Journal of\nHahnel M (2012) Exclusive: figshare a new open data project\nthat wants to change the future of scholarly publishing. In:\nImpact of Social Sciences Blog.\nHarrison WTA, Simpson J and Weil M (2010) Editorial. Acta\nCrystallographica Section E Structure Reports Online\nHaythornthwaite C, Lunsford KJ, Bowker GC, et al. (2006)\nChallenges for research and practice in distributed, inter-\ndisciplinary collaboration. In: Hine C (ed) New\nInfrastructures for Knowledge Production: Understanding\nHey T, Tansley S and Tolle K (eds) (2009) The Fourth\nParadigm. Redmond, WA: Microsoft Research.\nHirtle PB (2000) Archival authenticity in a digital age.\nIn: Cullen C, Levy DM, Lynch CA, et al. (eds)\nAuthenticity in a Digital Environment. Washington, DC:\nCouncil on Library and Information Resources.\nHuijboom N and Broek TD (2011) Open data: an inter-\nnational comparison of strategies. European Journal of\nIoannidis JPA (2005) Why most published research findings\nJasny BR, Chin G, Chong L, et al. (2011) Data replication &\nreproducibility. Again, and again, and again. . .\nKing G (2011a) Ensuring the data-rich future of the social\nKing G (2011b) The social science data revolution. Available\nat: http://gking.harvard.edu/files/gking/files/evbase-hori-\nKnorr-Cetina K (1999) Epistemic Cultures: How the Sciences\nMake Knowledge. Cambridge, MA: Harvard University\nPress.\nKuhn TS (1970) The Structure of Scientific Revolutions, 2nd\ned. Chicago: University of Chicago Press.\nLagoze C (2010) Lost Identity: The Assimilation of Digital\nLibraries into the Web (PhD dissertation). Cornell\nUniversity, Ithaca. Available at: http://carllagoze.files.\nLamb R and Sawyer S (2005) On extending social informatics\nfrom a rich legacy of networks and conceptual resources.\nLaney D (2001) {3D} Data Management: Controlling Data\nVolume, Velocity, and Variety.\nLazer D, Kennedy R, King G, et al. (2014) The parable of\nLeonelli S (2014) What difference does quantity make? On the\nepistemology of Big Data in biology. Big Data & Society\nLerner FA (1999) Libraries Through the Ages. New York,\nNY: Continuum.\nMayer-Scho\n\u00a8 nberger V (2013) Big Data: A Revolution that\nWill Transform How We Live, Work, and Think. Boston:\nHoughton Mifflin Harcourt.\nMichener W, Vieglais D, Vision T, et al. (2011) DataONE:\ndata observation network for earth -- preserving data and\nenabling innovation in the biological and environmental\nMilgram S (1967) The small world problem. Psychology\nMolloy JC (2011) The open knowledge foundation: open data\nMorris CW (1938) Foundations of the Theory of Signs.\nChicago: University of Chicago Press.\nMurray-Rust P (2008) Open data in science. Serials Review\nNaik G (2011). Mistakes in scientific studies surge. Wall Street\nJournal. Available at: http://online.wsj.com/news/articles/\nNissenbaum H (2009) Privacy in Context: Technology, Policy,\nand the Integrity of Social Life. Stanford, CA: Stanford\nLaw Books.\nNormandeau N (2013) Beyond volume, variety and vel-\nocity is the issue of big data veracity. Available at:\n10 Big Data & Society\nvariety-velocity-issue-big-data-veracity/ (accessed 15 April\nNowotny H (2001) Re-Thinking Science: Knowledge and the\nPublic in an Age of Uncertainty, 1st ed. Cambridge, UK:\nPolity.\nPo\n\u00a8 schl U (2004) Interactive journal concept for improved sci-\nentific publishing and quality assurance. Learned\nresearch industry gets bigger. Available at: http://blogs.\ncrowdsourced-health-research-industry-gets-bigger.html\nReith M, Carr C and Gunsch G (2002) An examination of\ndigital forensic models. International Journal of Digital\nResearcher faked evidence of human cloning, Koreans report\nRosenberg D (2013) Data before the fact. In: ``Raw Data'' is\nan Oxymoron. Cambridge, MA: MIT Press, pp.15\u00ad30.\nRosenbloom RS and Christensen CM (1994) Technological\ndiscontinuties, organizational capabilities, and strategic\ncommitments. Industrial and Corporate Change 3(3):\nSauer JR, Peterjohn BG and Link WA (1994) Observer\ndifferences in the North American Breeding Bird Survey.\nStar SL and Griesemer JR (1989) Institutional ecology, trans-\nlations and boundary objects: amateurs and professionals\nStodden V (2014) Enabling reproducibility in big data\nresearch: balancing confidentiality and scientific transpar-\nency. In: Privacy, Big Data and the Public Good.\nCambridge, UK: Cambridge University Press. Available\nat: http://www.cambridge.org/us/academic/subjects/\nstatistics-probability/statistical-theory-and-methods/\nprivacy-big-data-and-public-good-frameworks-\nSullivan BL, Aycrigg JL, Barry JH, et al. (2014) The eBird\nenterprise: an integrated approach to development and\napplication of citizen science. Biological Conservation 169\n(January).\nSzalay A and Gray J (2001) The world-wide telescope. Science\nTenopir C, Allard S, Douglass K, et al. (2011) Data sharing\nby scientists: practices and perceptions. PLoS ONE 6(6):\nVan House NA, Bishop AP and Buttenfield BP (2003)\nIntroduction: Digital Libraries as Sociotechnical Systems.\nCambridge, MA: MIT Press.\nVerfaellie M and McGwin J (2011) The case of Diederik\nStapel: Allegations of scientific fraud by prominent\nDutch social psychologist are investigated by multiple uni-\nversities. Psychological Science Agenda 25(12).\nWagner CS, Roessner JD, Bobb K, et al. (2011) Approaches\nto understanding and measuring interdisciplinary scientific\nresearch (IDR): a review of the literature. Journal of\nWallis J, Borgman C, Mayernik M, et al. (2007) Know thy\nsensor: trust, data quality, and data integrity in scientific\ndigital libraries. In: Kova\n\u00b4 cs L, Fuhr N and Meghini C\n(eds) Research and Advanced Technology for Digital\nWiggins A and Crowston K (2010) Distributed scientific\ncollaboration: research opportunities in citizen science.\nIn: Proceedings of ACM CSCW 2010 workshop on the\nchanging dynamics of scientific collaborations.\nZachary WW (1977) An information flow model for conflict\nand fission in small groups. Journal of Anthropological"
}