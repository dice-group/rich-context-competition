{
    "abstract": "Abstract: Predation is a major source of natural selection on primates and may have\nshaped attentional processes that allow primates to rapidly detect dangerous animals.\nBecause ancestral humans were subjected to predation, a process that continues at very low\nfrequencies, we examined the visual processes by which men and women detect dangerous\nanimals (snakes and lions). We recorded the eye movements of participants as they\ndetected images of a dangerous animal (target) among arrays of nondangerous animals\n(distractors) as well as detected images of a nondangerous animal (target) among arrays of\ndangerous animals (distractors). We found that participants were quicker to locate targets\nwhen the targets were dangerous animals compared with nondangerous animals, even when\nspatial frequency and luminance were controlled. The participants were slower to locate\nnondangerous targets because they spent more time looking at dangerous distractors, a\nprocess known as delayed disengagement, and looked at a larger number of dangerous\ndistractors. These results indicate that dangerous animals capture and maintain attention in\nhumans, suggesting that historical predation has shaped some facets of visual orienting and\nits underlying neural architecture in modern humans.\n",
    "reduced_content": "Evolutionary Psychology\n\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\nOriginal Article\nDangerous Animals Capture and Maintain Attention in Humans\nJessica L. Yorzinski, Center for Cognitive Neuroscience, Duke University, Durham, NC, USA. Email:\njly5@duke.edu (Corresponding author).\nMichael J. Penkunas, Department of Psychology, University of California, Davis, CA, USA.\nMichael L. Platt, Duke Institute for Brain Sciences, Center for Cognitive Neuroscience, and Department of\nNeurobiology, Duke University, Durham, NC, USA.\nRichard G. Coss, Department of Psychology, University of California, Davis, CA, USA.\n Keywords: attention, humans, delayed disengagement, eye-tracking, predation, predator\ndetection\n\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\nIntroduction\nPredation has been an important source of natural selection on primates. A variety\nof predators, such as snakes and felids (Hart and Sussman, 2005; Isbell, 2006; Stanford,\n2002), have preyed upon primates for millions of years. Humans are no exception, as their\nHuman attention to dangerous animals\nhominin ancestors also suffered from predation, and they continue to experience predation\nfrom large-bodied felids and snakes in rural areas (Coss, Fitzhugh, Schmid-Holmes,\nGreene, 2011; Treves and Palmqvist, 2007). The ability of primates to rapidly respond to\npotential danger is critical to their survival (Caro, 2005). Before individuals can respond to\npotential danger and engage in defensive action, they must first detect the threat by\ndirecting their attention (covertly or overtly) toward it (Cronin, 2005; Dukas and Kamil,\n2000; Huijding, Mayer, and Koster, 2011; Yorzinski, Patricelli, Babcock, Pearson, and\nIt is possible that our long evolutionary history with predators has shaped our visual\nsystem to quickly detect dangerous animals (Coss, 2003; Isbell, 2006). Visual-search\nexperiments have found that children, including infants as young as 8-months, and adults,\nare faster to detect threatening animals, including snakes, spiders, and lions, compared with\n2013a,b; Rosa, Gamito, Oliveira, Morais, and Saraiva, 2011; Waters, Lipp, and Spence,\n2004). Ontogenetic experiences with threats also influence detection performance\n(Blanchette, 2006; Fox, Griggs, and Mouchlianitis, 2007). People are faster to detect\ncontemporary threats (such as guns and syringes) compared to neutral stimuli even though\nthese contemporary threats are too recent to have influenced our evolutionary history. The\nperceptual processes responsible for this rapid detection of threats are largely unknown.\nBased on studies that directly measure visual attention (using eye-trackers to\nmonitor eye movements), threatening stimuli are often better at attracting and holding\nattention compared to nonthreatening stimuli. Humans are faster at detecting images of\ndangerous people and people experiencing threat than people who are not threatened\n(Nummenmaa, Hyona, and Calvo, 2006). Furthermore, humans, especially high-anxious\nindividuals, are slower to disengage their attention when viewing angry faces compared\nwith happy or neutral faces (Belopolsky, Devue, and Theeuwes, 2012; Reinholdt-Dunne et\nal., 2012). Similarly, people often detect spider and snake images faster than neutral images\nThe purpose of this study was to examine the processes of visual scanning that\nguide humans' abilities to detect dangerous animals rapidly. The eye movements of adult\nparticipants were recorded as they located a single image of a dangerous animal (target)\nembedded in an array of nondangerous animals (distractors) or detected a nondangerous\nanimal (target) embedded in an array of dangerous animals (distractors). The participants\nwere presented with color images and images in which low-level features were minimized\n(spatial frequency and luminance were controlled). We tested whether dangerous animals:\n(i) maintain attention or \"delay disengagement\" during visual search (Fox, Russo, and\nDutton, 2002) and/or (ii) exogenously attract attention through low-level features (e.g.,\nluminance or contrast; Simons, 2000). If dangerous animals are effective at maintaining\nattention, we predicted that humans would spend more time looking at distractors when the\ndistractors were dangerous animals compared with nondangerous animals. If dangerous\nanimals capture attention through low-level features, we predicted that humans would look\nat a larger number of distractors when the distractors were dangerous animals compared\nHuman attention to dangerous animals\nwith nondangerous animals.\nMaterials and Methods\nParticipants\nThirty men and 30 women participated in this study at Duke University from\nNovember 2012 through March 2013. They were all of European heritage and between the\nrecruit participants, and they were told that they would be participating in a study that\nexplored predator recognition. They earned $15 for their participation. The Institutional\nReview Board of Duke University (#7646) approved this study; written consent was\nobtained from all participants.\nAnimal images\nWe created two sets of 96 matrices that displayed images of dangerous and\nnondangerous animals. One set showed images of lions (Panthera leo) and impalas\n(Aepyceros melampus), and the other set showed images of snakes (Serpentes) and lizards\n(Lacertilia). Each set included four treatment blocks of 24 matrices.\nIn the first set, the first treatment block (Target Lion) consisted of 24 matrices that\nwere created from 24 images of lions and impalas. Each matrix consisted of a 3 \u00d7 3 array in\nwhich one lion image (target) and seven impala images (distractors) were displayed (the\n(dpi = 96) and filled the entire screen. Images within the matrices were 293 \u00d7 208 pixels\n(approximately 7.4 degrees wide and 5.2 degrees high); 100 pixels separated images from\neach other and from the edges of the matrices. Lion images appeared three times in each of\nthe eight possible positions across the 24 matrices, and a different lion photograph was\nused in each matrix. Impala images appeared in pseudo-randomized positions within each\nmatrix such that each image appeared seven times across the 24 matrices but was never in\nthe same matrix position more than once and only appeared one time within the same\nmatrix. The lion and impala images consisted of adult males with manes and adult females,\nrespectively, and displayed animals that were standing with all four legs on the ground and\nnot looking directly at the camera. The images depicted each animal in a natural scene and\nnone of the animals were displaying obviously threatening or defensive postures; the\nimages were obtained from online sources.\nA second treatment block (Target Lion Low Level Control) was created using the\n24 matrices that were generated in the first treatment block; the matrices were processed\nusing the SHINE toolbox (default settings; Willenbockel et al., 2010) in MATLAB to\nminimize low-level confounds (images within a matrix were matched for luminance and\nspatial frequency; see Figure 1b). The SHINE toolbox first matches the Fourier amplitude\nspectra of the images (spatial frequency matching) and then matches the luminance\nhistograms (Willenbockel et al., 2010); the low-level features of the resulting images are\ntherefore minimized (because they have the same luminance and spatial frequency) but not\nentirely eliminated since the toolbox does not match other low-level features (such as edges\nor orientation).\nHuman attention to dangerous animals\nFigure 1. Examples of scanpaths from one male participant on matrices from the four\ntreatment blocks of the lion and impala set\nNote. (a) Target Lion; (b) Target Lion Low Level Control; (c) Target Impala; (d) Target Impala Low Level\nControl. The size of the black circles indicates the amount of time the participant spent looking at each\nlocation. Gaze begins in the middle of the image and ends on the target animal.\nThe process used to create images in the first and second treatment blocks was\nrepeated to generate the third and fourth treatment blocks (Target Impala and Target Impala\nLow Level Control) except that one impala image (target) and seven lion images\n(distractors) were used in each image. Therefore, there were a total of 96 matrices in the\nTarget Lion, Target Lion Low Level Control, Target Impala, and Target Impala Low Level\nControl treatment blocks.\nThe second set of 96 matrices was created using the same procedure that we used to\ncreate the first set except that we used images of snakes and lizards. The snake and lizard\nimages were the same as those used in previous studies (see Penkunas and Coss, 2013a,b).\nThe images were used to create the Target Snake, Target Snake Low Level Control, Target\nLizard, and Target Lizard Low Level Control blocks.\nHuman attention to dangerous animals\nEye-tracker\nWe used a Tobii T60 eye-tracker along with Tobi Studio 3.1 and 3.2 (Tobii\nTechnology, Inc., Sweden) to present our images and record the gaze of participants\n(accuracy: 0.5 degrees; data rate: 60 Hz; binocular tracking). Participants were told that we\nwere measuring the size of their pupils but were not told that their eye movements were\nbeing monitored until after they completed the trial. The images were displayed using Tobii\nParticipants were positioned approximately 60 cm from the screen and were unrestrained\n(i.e., no bite bar or chin rest was used). The equipment was calibrated (9 points) before\neach trial began. We used the Tobii Velocity-Threshold Identification filter (I-VT filter;\ngap fill-in: 75 ms; eye selection: average; noise reduction: median; noise reduction\nsamples: 7; velocity calculator window: 20 ms; I-VT classifier threshold: 30 degrees/sec;\nmerge adjacent time: 75 ms; merge adjacent angle: 0.5 degrees; discard short fixations: 60\nms) to classify fixations and saccades. This filter classifies eye movements as fixations or\nsaccades based upon the velocity of eye movements; eye movements below and above the\nvelocity threshold (30 degrees/sec, in this study) are classified as fixations and saccades,\nrespectively. Eye-tracking data consisted of coordinates of where participants were known\nto be looking during each sampling point.\nExperimental procedure\nThe experimenter (JLY) first asked participants to perform two practice trials so\nthey could become familiar with the procedure. In the first practice trial, participants were\nasked to fixate a black dot that appeared in the center of the screen for 1 sec. They were\nthen presented with a 3 \u00d7 3 matrix that consisted of one image of a dog and seven images\nof cats (arranged in the same manner as described above for the predator and nonpredator\nmatrices). They were instructed to press the space bar on the keyboard as soon as they\nlocated the dog image within the matrix. Once they pressed the space bar, the matrix\ndisappeared and the fixation dot reappeared. They repeated this process for 10 matrices.\nThe second practice trial was similar to the first except that the 10 matrices of the dogs and\ncats were altered to minimize low-level confounds (see above).\nAfter completing the two practice trials, participants were then presented with the\nfirst set of 24 matrices. As in the practice trials, they were instructed to fixate a central dot;\nwhen a matrix appeared, they were asked to press the space bar as soon as they found the\ntarget animal. Because the participants fixated this central dot, the middle position of the\nmatrices was left empty (see animal images above) to ensure that participants had to search\nfor the target. They performed this task for each of the four blocks of matrices within the\nset (the order of the blocks was randomized across participants). This process was repeated\na second time with the second set of matrices (the order of the sets was randomized across\nparticipants). Therefore, a participant would perform the search task on eight blocks of\nmatrices: Target Lion, Target Impala, Target Lion Low Level Control, Target Impala Low\nLevel Control, Target Snake, Target Lizard, Target Snake Low Level Control, and Target\nLizard Low Level Control, with the order of the blocks and sets randomized across\nparticipants.\nHuman attention to dangerous animals\nMeasurements and statistical analysis\nUsing a customized MATLAB program, we drew rectangular regions of interest\n(ROI) around each target and distractor. All target and distractor images were the same size\n(293 \u00d7 208 pixels; see Animal Images above) and their ROIs included the entire rectangular\nregion of each image. For each fixation coordinate, we determined which ROI it fell within\nto determine whether the participant was looking at the target image, distractor images, or\nneither the target nor distractor images. We calculated four metrics: the amount of time that\nelapsed before participants fixated on the target (Latency to Fixate Target Animal), the\namount of time that elapsed before participants manually responded by pressing the space\nbar to indicate they detected the target (Latency to Manual Response), the number of\ndifferent distractors the participants fixated (No. of Different Distractors Fixated), and the\naverage time that participants spent looking at each distractor, only including distractors\nthat were fixated (Time Viewing Distractors). For each participant, we calculated the mean\nvalue of the metrics within each of the eight treatment blocks (Target Lion, Target Impala,\nTarget Lion Low Level Control, Target Impala Low Level Control, Target Snake, Target\nLizard, Target Snake Low Level Control, and Target Lizard Low Level Control). In\nmatrices where the data indicated a participant never fixated the target, it was not possible\nto determine whether (i) participants did not fixate the target (and therefore did not\ncorrectly perform the task) or (ii) whether the eye-tracker failed to record the participants'\ngaze when they were fixating the target. We therefore excluded a given matrix from the\nanalysis if a participant's fixations never fell within the target or if more than 10% of the\ngaze data was missing; only 4.6% of the matrices were discarded due to this restriction.\nWe analyzed our data using linear mixed-effects models with repeated measures\nand an unstructured covariance structure (PROC MIXED) in SAS (SAS Institute Inc.,\nCary, NC). We examined whether the latency to fixate the target animal, latency to\nmanually respond, number of different distractor images fixated, and time viewing each\ndistractor image were influenced by the sex of the participant (male or female), animal\nclass of the target (mammal vs. reptile), danger level of the target (dangerous vs. not\ndangerous), type of image (natural image vs. image that controlled for low-level features),\nand their interactions; we included participant identity as a random effect. Because sex of\nthe participant was nonsignificant in all models (ps > 0.2), we dropped this term from the\nmodels. We made a priori predictions regarding differences among treatment blocks and\ncreated contrasts to evaluate these differences; we performed eight comparisons and used a\nBonferroni correction to evaluate significance. Means \u00b1 SEs are provided in the Results\nsection to illustrate effect sizes.\nResults\nThe latency to locate the target image (fixate and manual response), number of\ndifferent distractor images fixated, and time viewing each distractor image varied\ndepending on the animal class (see Table 1A), danger level (see Table 1B), image type (see\nTable 1C), and some of the interactions among these variables (see Table 1D-G).\nParticipants were faster to fixate the target when the target was a dangerous animal (lion\nand snake) compared to when it was a nondangerous animal (impala and lizard) in both the\nHuman attention to dangerous animals\n20 ms; see Latency to Fixate Target Animal, Table 1H and J) and matrices that minimized\n1071 \u00b1 18 ms; see Latency to Fixate Target Animal, Table 1I and K); however, participants\nwere slower to fixate a given target in the low-level matrices compared with the natural\nmatrices (see Latency to Fixate Target Animal, Table 1L-O; see Figure 2a).\nParticipants were also faster to detect the target via manual response when the target\nwas a dangerous animal (lion and snake) compared to when it was a nondangerous animal\nand K); however, participants were slower to detect a given target in the low-level matrices\ncompared with the natural matrices (see Latency to Manual Response, Table 1L-O; see\nFigure 2b). Participants were faster to visually fixate the target than indicate they had\ndetected the target via a manual response (paired t-test: t = 44.1, p < 0.0001).\nIn both the natural matrices and the matrices that minimized low-level features,\nparticipants looked at a greater number of distractor images (see No. of Different\nDistractors Fixated, Table 1H-K; see Figure 3) and spent more time looking at each\ndistractor image (see Time Viewing Distractors, Table 1H-K; see Figure 4) when the\ndistractors were dangerous animals compared with nondangerous animals. Participants\nlooked at fewer distractors (see No. of Different Distractors Fixated, Table 1L-O; see\nFigure 3) and spent less time looking at each distractor (see Time Viewing Distractors,\nTable 1L-O; see Figure 4) for a given target in the natural matrices compared with the\nmatrices matched for luminance and spatial frequency.\nHuman attention to dangerous animals\nTable 1. The effect of animal class, danger level, and image type on the latency to locate\nthe target (via fixations and manual responses), number of different distractors fixated, and\ntime viewing each distractor\nLatency to\nFixate Target\nAnimal\nLatency to\nManual\nResponse\nNo. of Different\nDistractors\nFixated\nTime\nViewing\nDistractors\nOverall\nModel\nA\nB\nC\nD Animal Class \u00d7\nDanger Level\nE Animal Class \u00d7\nImage Type\nF Danger Level \u00d7\nImage Type\nG Animal Class \u00d7\nDanger Level \u00d7\nImage Type\nComparisons\nH\nI Lion Low Level\nControl vs. Impala\nLow Level Control\nJ\nK Snake Low Level\nControl vs. Lizard\nLow Level Control\nL Lion vs. Lion Low\nLevel Control\nM Impala vs. Impala\nLow Level Control\nN Snake vs. Snake\nLow Level Control\nO Lizard vs. Lizard\nLow Level Control\nNote. F values are displayed; p-values are indicated in parentheses unless the result is highly statistically\nsignificant (p < 0.0001) and thus indicated with an asterisk. Effect size (Cohen's d) is reported in brackets.\nThe numerator degrees of freedom is 1 and the denominator degrees of freedom is 59 in all tests.\nHuman attention to dangerous animals\nFigure 2. The latency to (a) first fixate the target animal and (b) manually respond (key\npress) after detecting the target animal\na.\nb.\nNote. Means and standard-error bars are shown; horizontal lines indicate planned comparisons and all\ncomparisons were statistically significant (p < 0.0001).\nHuman attention to dangerous animals\nFigure 3. Number of dangerous and nondangerous animal distractors fixated\nNote. The number of different distractors fixated with respect to the treatment block is displayed. Means and\nstandard-error bars are shown; horizontal lines indicate planned comparisons and all comparisons were\nFigure 4. Duration of time spent looking at animal distractors\nNote. The amount of time spent looking at each distractor with respect to the treatment block is displayed.\nMeans and standard-error bars are shown; horizontal lines indicate planned comparisons and all comparisons\nHuman attention to dangerous animals\nDiscussion\nWe found that participants visually detected dangerous animals (snakes and lions)\nfaster than nondangerous animals (lizards and impalas). These results are consistent with\nprevious studies showing that humans (including infants, children, and adults) are quicker\nto detect dangerous compared with nondangerous animals (Blanchette, 2006; Brosch and\nstudies (but see LoBue and DeLoache, 2010; Rosa et al., 2011), we quantified detection\nbased on eye movements as well as manual responses (a key press). Eye movements are a\nmore ecologically valid method of assessing attention than manual responses. When\nhumans detect potentially dangerous situations, they orient their eyes to the danger before\nthey respond manually (Bannerman, Milders, de Gelder, and Sahraie, 2009). Indeed, the\nlatency to detect animals, both dangerous and nondangerous, in our study was at least 52%\nfaster based on eye movement patterns compared to manual responses.\nIn addition, we found that adults were quicker to detect dangerous animals even\nwhen some low-level features of the images (spatial frequency and luminance) were\ncontrolled. Low-level features can influence attention through bottom-up processing, in\nwhich properties of the image exogenously capture attention (James, 1890). For example,\nspatial frequency (large changes in intensity; Mannan, Ruddock, and Wooding, 1997),\ncolor, form, and luminance (Turatto and Galfano, 2000) can automatically attract attention.\nBecause the participants in our study were faster to detect dangerous animals compared\nwith nondangerous animals, even after controlling for spatial frequency and luminance,\nthese low-level features were not driving the ability of humans to detect danger rapidly.\nSimilarly, previous studies also reported that adults and children detected dangerous\nanimals faster than nondangerous animals even when the images were gray-scale (Flykt,\n2005; Hayakawa, Kawai, and Masataka, 2011). Importantly, we found that humans were\nslower to detect both dangerous and nondangerous animals when these low-level features\nwere controlled, suggesting that these low-level features can generally aid in detection but\nare not specific to detecting dangerous animals.\nThe perceptual processes by which humans rapidly detect danger in natural settings\nare largely unexplored. One hypothesis is that dangerous objects are particularly effective\nat maintaining attention or \"delaying disengagement\" during visual search (Fox et al.,\n2002). In further support of this hypothesis, studies generally find that fearful and angry\nfaces attract more attention than neutral or happy ones (Bannerman et al., 2009; Belopolsky\net al., 2012). Our results provide support for the delayed-disengagement hypothesis. We\nfound that adults detected nondangerous animals slower than dangerous animals because\nthey spent more time looking at each of the dangerous (distractor) images (i.e., they were\nslower to disengage their attention from the dangerous animals). This suggests that it is\ncritical for humans to fixate dangerous animals so that they can assess levels of threat.\nA second hypothesis explaining why humans rapidly detect danger is that\ndangerous animals exogenously attract attention based on their low-level features (\u00d6hman,\n1986; Simons, 2000). Although our results indicated that some low-level features (spatial\nfrequency and luminance) do not affect humans' ability to detect dangerous animals (see\nHuman attention to dangerous animals\nabove), additional low-level features may influence detection. For example, the shape of\nsome dangerous animals may exogenously draw attention. LoBue and DeLoache (2011)\nfound that the coiled shape of snakes facilitated detection. Our results are in agreement\nwith this attentional-capture hypothesis because participants looked at a larger number of\ndangerous (distractor) animals when searching for nondangerous animals, indicating that\nthe dangerous animals drew attention even though the participants were searching for\nnondangerous animals. Therefore, in the context of the current study presenting animal\nimages, humans appear to assess levels of danger rapidly in their surroundings because they\n(i) spend more time looking at dangerous animals (delayed disengagement) and (ii) detect\ndangerous animals through salient visual features.\nBecause primates, including humans, have experienced predation for millions of\nyears, selection has likely shaped their antipredator behaviors (Coss and Ramakrishnan,\nantipredator responses that involve emitting alarm calls, increasing vigilance levels,\navoidance, piloerection, and mobbing or attacking predators (Caro, 2005; Isbell, 1994).\nIndividuals sometimes display these antipredator behaviors in response to predators that\nthey have never even seen before (reviewed in Yorzinski, 2010). In particular, humans\noften react with fear and increase their attention toward dangerous animals, a process that is\nmediated by the amygdala and hippocampus and then modulated by cortical areas for\nregulating action (Lovett-Barron et al., 2014; \u00d6hman, 2005). It is likely beneficial for\nanimals, including humans, to increase their attention toward potential threats so they can\nrespond appropriately (Cresswell, Butler, Whittingham, and Quinn, 2008). Future eye-\ntracking experiments could examine the relationship between visual attention and predator\ndetection in children to better understand the development of visual biases in humans.\n"
}