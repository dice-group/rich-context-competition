{
    "abstract": "Abstract\nThere are several methods for building prediction models. The wealth of currently available\nmodeling techniques usually forces the researcher to judge, a priori, what will likely be the\nbest method. Super learning (SL) is a methodology that facilitates this decision by combining\nall identified prediction algorithms pertinent for a particular prediction problem. SL generates\na final model that is at least as good as any of the other models considered for predicting the\noutcome. The overarching aim of this work is to introduce SL to analysts and practitioners.\nThis work compares the performance of logistic regression, penalized regression, random\nforests, deep learning neural networks, and SL to predict successful substance use disorders\n(SUD) treatment. A nationwide database including 99,013 SUD treatment patients was used.\nAll algorithms were evaluated using the area under the receiver operating characteristic\ncurve (AUC) in a test sample that was not included in the training sample used to fit the pre-\ndiction models. AUC for the models ranged between 0.793 and 0.820. SL was superior to all\nbut one of the algorithms compared. An explanation of SL steps is provided. SL is the first\nstep in targeted learning, an analytic framework that yields double robust effect estimation\nand inference with fewer assumptions than the usual parametric methods. Different aspects\nof SL depending on the context, its function within the targeted learning framework, and the\nbenefits of this methodology in the addiction field are discussed.\n",
    "reduced_content": "Use of a machine learning framework to\npredict substance use disorder treatment\nsuccess\nLaura Acion1,2*, Diana Kelmansky1, Mark van der Laan3, Ethan Sahker2,4,\n1 Instituto de Ca\n\u00b4lculo, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires, CONICET,\nBuenos Aires, Argentina, 2 Iowa Consortium for Substance Abuse Research and Evaluation, University of\nIowa, Iowa City, Iowa, United States of America, 3 Division of Biostatistics, University of California, Berkeley,\nCalifornia, United States of America, 4 Counseling Psychology Program, Department of Psychological and\nQuantitative Foundations, College of Education, University of Iowa, Iowa City, Iowa, United States of\nAmerica, 5 Department of Psychiatry, Roy J and Lucille A Carver College of Medicine, University of Iowa,\nIowa City, Iowa, United States of America, 6 Department of Biostatistics, College of Public Health, University\nof Iowa, Iowa City, Iowa, United States of America\n* laura.acion@ic.fcen.uba.ar\n Introduction\nThere are several methods for building prediction models. Prediction models are often gener-\nated using some form of linear or logistic regression--e.g. [1\u00ad4]. More recently, other learning\nalgorithms such as random forests (RF) or artificial neural networks (ANN) are being used for\nCitation: Acion L, Kelmansky D, van der Laan M,\nSahker E, Jones D, Arndt S (2017) Use of a\nmachine learning framework to predict substance\nuse disorder treatment success. PLoS ONE 12(4):\nEditor: Raymond Niaura, Legacy, Schroeder\nInstitute for Tobacco Research and Policy Studies,\nCopyright: \u00a9 2017 Acion et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricted use, distribution, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availability Statement: All data are available\nat https://www.icpsr.umich.edu/icpsrweb/ICPSR/\nFunding: This work was funded by Consejo de\nInvestigaciones Cienti\n\u00b4ficas y Te\n\u00b4cnicas Postdoctoral\nCompeting interests: The authors have declared\nthat no competing interests exist.\nprediction in the health sciences--e.g. [5\u00ad7]. These new techniques may be able to enhance\nprediction, thus improving the chances of matching patients to the most effective treatments.\nThe wealth of currently available modeling techniques usually forces the researcher to\njudge, a priori, what will likely be the best prediction method. Super learning (SL) [8] is a\nmethodology that facilitates this decision by combining all identified prediction algorithms\npertinent for a particular prediction problem. SL generates a final model that is at least as good\nas any of the other models considered for predicting the outcome. This property of SL is both\ntheoretically [8] and empirically supported [9]. The goal of this paper is to introduce various\nprediction methods, some of which are novel to the field of substance use disorders (SUD)\ntreatment.\nAccounting for individual patient characteristics to maximize positive outcomes is at the\nheart of precision medicine. SUD treatment is one of the many areas that can greatly benefit\nfrom optimizing patients' pathways to the best possible treatment outcome. For example,\nUnited States estimates indicate only 19.8% of cases in need of alcohol use disorders treatment\nwere ever treated [10]. Identifying key predictors of successful treatment can serve to discover\ndisparities, strengths and weaknesses in service delivery, eventually increasing treatment suc-\ncess and reducing unmet treatment needs [11\u00ad13].\nThe first step in identifying key characteristics for matching patients to the most effective\ntreatments is to predict who will succeed at a given treatment. The importance of this topic is\nreflected by the rich literature aiming to identify patients' characteristics increasing the efficacy\nof SUD treatment (e.g. [4, 14\u00ad16]). The methodological focus of this paper leaves a substantive\nliterature review regarding SUD treatment success out of its scope.\nSome of the literature predicting successful SUD treatment takes advantage of large publicly\navailable datasets such as the Substance Abuse and Mental Health Services Administration\nTreatment Episode Data Set--Discharges (TEDS-D) [17]. The main advantage of datasets\nsuch as TEDS-D is the large number of records available including important patient charac-\nteristics and treatment features. Due to the computer power required when large data sets with\nnumerous predictors are used, the advantage of working with large datasets or big data used to\nbe also the greatest disadvantage for applying sophisticated prediction algorithms such as SL\nor other sophisticated machine learning methods. Computer power is currently less of a prob-\nlem thanks to technological advances and an active open source software developer commu-\nnity. For instance, the open source community has optimized SL and other machine learning\nalgorithms for the analysis of big data using open source software [18].\nMost of the methods featured in this paper were not used to analyze TEDS-D before. The\naim is to evaluate what method works best at predicting successful treatment using a real life\nlarge database. Rather than a dataset tailored to showcase the methods properties, we use a\ndataset commonly used in the SUD field. We hypothesized that SL will generate the best pre-\ndiction model as measured by the area under the receiver operating characteristic curve\n(AUC) evaluated in a test sample not included in the training sample used to fit all prediction\nmodels.\nThe use of SL as a prediction tool of success in SUD treatment may contribute to the litera-\nture by bettering the ability to identify treatment outcome disparities that, when addressed,\nmay lead to improve patients' treatment outcomes.\nMaterials and methods\nData\nTo illustrate different prediction analytic approaches, we focused in SUD outpatient treatment\nMachine learning for SUD treatment success prediction\nTEDS-D is an excellent example of a large administrative dataset that may be of interest to\naddiction researchers in real life. This dataset allows us to illustrate the use of the methodolo-\ngies introduced in this work within a realistic setting.\nOutcome. Treatment completed was considered a successful treatment discharge status, all\nother treatment discharge reasons (e.g., left against professional advice, incarcerated, other)\nwere considered as indicators of non-successful treatment. Treatment completion is a standard\nprocess outcome measure because it predicts longer-term outcomes such as less future criminal\ninvolvement, fewer readmissions, employment and income one year after treatment [19\u00ad22].\nPredictors. Twenty eight predictors recorded by TEDS-D were included in the analysis.\nPredictors include 10 patient characteristics (i.e., age, gender, race\u00ade.g., White, Black--, eth-\nnicity\u00adindicating patient's specific Hispanic origin, marital status, education, employment sta-\ntus, pregnant at time of admission, veteran status, and living arrangement), 3 treatment\ncharacteristics (i.e., intensity, medication-assisted opioid therapy, and length of stay), principal\nsource of referral, summary of type of problematic substance (with categories \"alcohol only\",\n\"other drugs only\", or \"alcohol and drugs\"), and mental health problem. TEDS-D records\nthorough information about substances of misuse. This includes the following 12 predictors:\nprimary, secondary, and tertiary substance problem, usual route of administration, frequency\nof use, and age at first use. The substances include: alcohol, cocaine/crack, marijuana/hashish,\nprescription opiates/synthetics, and methamphetamine use. Several other drug use categories\nwere collapsed for analysis because of low percentages.\nInclusion criteria. TEDS-D includes all admissions/discharges rather than individuals.\nConsequently, only records that indicate the individual had no prior SUD treatment were\nincluded in the analyses. Part of our previous work focuses on racial and ethnic minorities [11,\n12, 23, 24]. It is known that racial and ethnic minorities vary in their treatment access and suc-\ncess levels [25\u00ad27]. Thus, we restricted our analysis only to cases indicating a Hispanic/Latino\nethnicity, 18 years old or older, and treatment in outpatient service settings. We focused on\noutpatient service settings because criteria for treatment completion/success (i.e., the outcome\nof interest) and duration for other types of services (e.g., 24-hour inpatient, detoxification-\nonly) and outpatient services are often very different. This is a sample arbitrarily chosen to\nexemplify the different analytical approaches. The choice is based on our previous knowledge\nin this field and not on the results obtained after using the methods described in the following\nparagraphs.\nExclusion criteria. Records with missing data in any of the predictors, the outcome, or\ncharacteristics used for determining inclusion in the study were excluded from the analysis.\nSince not all states collect the same information for their patients, from a total of 9,829,536 rec-\nThese inclusion and exclusion criteria did not affect the performance of SL when compared\nto the rest of the analytical strategies used. A total of 99,013 records representing unique indi-\nviduals were selected according to the inclusion and exclusion criteria and were used in the\nanalysis. This sample was separated in a training set with 80% of the sample (n = 79,210) and a\nBecause these data represent public information and there is no subject identification, the\nUniversity of Iowa Human Subjects Office Institutional Review Board exempted this study\nfrom review.\nStatistical analysis\nThe goal of the analysis was to compare results of classical analytical strategies that are com-\nmonly used to address prediction of treatment success (e.g., logistic regression) alongside\nMachine learning for SUD treatment success prediction\nresults of newer methods for prediction (e.g., ANN). We also compared SL results. SL is an\nensemble machine learning methodology that encompasses all other methodologies in its\nlibrary and has demonstrated (both theoretical and empirical) superiority for prediction [8, 9].\nFormally, the analysis is as follows. Let W = {W1\n, . . ., Wk\n} denote the predictors of interest\nand Y represent the binary outcome treatment success (Yes/No). Let O = (W, Y) be a random\nvariable such that O~P0\n(i.e., the true probability distribution of O is P0\n). Since only records\nthat indicate the individual had no prior treatment in a drug or alcohol program were included\nin the analyses, we assumed that the individuals observed can be represented as independent\nand identically distributed observations of the random variable O. For each individual i, Yi\nand\nWi\nare observed. We sought to estimate \"\n\u00f0Y \u00bc YesjW \u00de--i.e., the probability of succeed-\ning in treatment given the predictors of interest. \"\nis unknown. Hence, we aimed to find the\nbest estimator of \"\n. This was achieved by maximizing the AUC. We did this using logistic\nregression, 3 types of penalized regression, RF, ANN, and the corresponding SL that includes\nthese algorithms in its library. SL also maximized the AUC.\nWe used 2 mechanisms to avoid overfitting. The analysis used 2-fold cross-validation (CV)\nselected individuals that were only included in the test set (Fig 1). All analytic approaches\nexplored were compared using the AUC in the test set. The best analytic strategy for predicting\ntreatment success was identified as the model maximizing AUC. The AUC is a convenient\nmeasure of prediction success that can be interpreted as the probability that any of the algo-\nrithms ranks a randomly chosen successful patient higher than a randomly chosen unsuccess-\nful patient [28]. AUC ranges from 0 to 1, AUC = 1 means perfect prediction and AUC = 0.5\nsuggests chance levels of prediction.\nWe used at least 2 different configurations for each analytic strategy compared. The first\napproach included all 28 predictors available. Since it can be useful to reduce the number of\npredictors considered and simplify the final prediction formula, we also selected a subset of\nFig 1. Analytic work flow.\nMachine learning for SUD treatment success prediction\nthe 10 predictors with highest variable importance in a RF model. In decreasing order of\nimportance, these predictors were: length of stay, age, principal source of referral, primary\nproblematic substance, its age of first use, and its frequency of use, employment level, SUD\ntype (i.e., only alcohol, only drugs, or both alcohol and drugs), education level, and patient's\nspecific Hispanic origin (e.g., Puerto Rican, Mexican, Cuban).\nFor each set of predictors, the following algorithms were compared: logistic regression,\nleast absolute shrinkage and selection operator (lasso), ridge, and elastic net penalized regres-\nsions [29], RF [30], ANN [29, 31], and SL [8]. Since none of the aforementioned regression\nmodels considered interaction effects in the predictive model, an additional model including\nterms for all predictors and selected 2-way interactions was also added to the SL library for\neach type of regression. Two-way interactions were initially screened using all possible logistic\nregression models of 2 predictors at a time and their 2-way interaction. All interactions with p-\nvalues<0.0001 were included along with all predictors in the regression models of the SL\nlibrary. We compared 17 algorithms/algorithm configurations.\nThere is a multitude of other analytic strategies that could be chosen [29]; however, to illus-\ntrate SL use for predicting treatment success when compared to other methods, we consider the\nchosen algorithms are adequate. In this context, where the data generating model is unknown,\nSL will be superior to the rest of the methodologies, regardless of the set of algorithms initially\nchosen. Some characteristics of the chosen algorithms are presented subsequently.\nLogistic regression. Logistic parametric regression is the most commonly used algorithm\nfor prediction in the SUD treatment outcomes field. Logistic regression is easily implemented\nand interpreted. However, logistic regression assumptions are strong and, since the true data\ngenerating model is unknown, these assumptions are usually violated. For example, including\nmany predictors, their interactions, and/or other higher order terms in a logistic regression\nmodel does not guarantee that it is the best model, due, for example, to collinearity between\nthe predictors included (which increases variability) or model misspecification (which intro-\nduces bias).\nPenalized regression. Penalized regression (such as lasso, ridge regression, or elastic nets)\noffers an alternative to parametric regression models. The 3 types of penalized regression\napplied in this work vary in their variance/bias tradeoffs depending on the characteristics of\nthe predictor set. For example, lasso will select only one term from a set of correlated predic-\ntors. This may not be appropriate. In fact, when the number of predictors is small compared to\nthe number of independent observations, ridge regression outperforms lasso when variables\nare highly correlated. Additionally, if the true data generating model has only a few predictors\nbut the candidate models have a large number of predictors; lasso may eliminate predictors\nthat were not in the data generating model. On the other hand, ridge regression will keep all\nterms in the final model. The elastic net penalized regression provides some balance between\nlasso and the ridge regressions.\nIn the example presented here, there are 28 categorical predictors that correspond to 135\nterms when the model is parametrized using dummy variables. The screening step for the\nmost relevant 2-way interactions of these 135 terms, preselected 257 2-way interaction terms.\nThus, the regression models including all predictors and selected interactions had 393 terms\nincluding the intercept. While logistic regression estimated 393 parameters for this model,\nlasso estimated parameters only for terms uncorrelated with each other and zeroed-out the\nrest; ridge regression kept all 393 but down weighted each term. In this way, penalized regres-\nsion allows for tuning large models adapting them to the information provided by the data.\nRandom forests. RF is a recursive partitioning method popular in many fields with high-\ndimensional data (e.g., genomics). RF can evaluate a number of predictor variables even in\nthe presence of complex interactions, including those that are not possible to model using\nMachine learning for SUD treatment success prediction\nregression. RF is an ensemble of classification and regression trees constructed on bootstrap\nsamples. Unlike individual trees, RF is more protective against overfitting.\nArtificial neural networks. An ANN uses interconnected nodes within various layers to\nexplain an outcome given a set of predictors. The relationships between the nodes are defined\nby weights calculated using a given rule. The initial weights are preassigned by the analyst. The\nANN algorithm iterates adjusting the weights. At the end of each iteration, the performance at\noutcome prediction is evaluated. ANNs efficiently generate non-linear classification rules but\ncan be prone to overfitting. More recently, some types of ANN are referred to as deep learning\n[31]. Deep learning allows modeling multiple levels of non-linearity in the data and is scalable\nto large datasets and big data in general. We used deep learning ANN with hidden layer sizes\nSuper learning. SL is a generalization of the stacking algorithm [32], an ensemble ma-\nchine learning method that takes a weighted average of all other algorithms considered for pre-\ndiction and produces a single prediction function (PF) with optimal tradeoff between variance\nand bias. SL is very flexible for learning from the data as it combines the strengths of all meth-\nodologies considered (including different configurations of the same algorithm) while mini-\nmizing modelling flaws. Another advantage of SL is that it eliminates the need to select a priori\na single or a few methodologies for the analysis. SL allows analyzing the data using simulta-\nneously all the methodologies the researcher considers suitable.\nFig 1 depicts the work flow used to analyze the data and details all the steps necessary for\nrunning SL. The input of the analysis consists of the training and test data sets together with\nthe algorithmic library. Since we used 2-fold CV to obtain each algorithm PF, as well as, the SL\nPF, the training set was initially partitioned in 2 blocks. Each algorithm in the library was fitted\nusing each block independently. We used the data block excluded from the model fitting to\ncalculate the CV AUC for each algorithm in the library. We averaged both CV AUCs, resulting\nin a single training set CV AUC for each algorithm. Up to this step, model fitting follows a reg-\nular 2-fold CV modelling path. The PF of discrete SL, a simpler version of SL, is the PF of the\nalgorithm with the minimum CV AUC.\nHowever, SL performs better when a weighted combination of the algorithms' PFs is used.\nThus, the next step for obtaining SL PF is to calculate a weight for each algorithm PF. This is\ndone regressing Y on the values of Y predicted by each algorithm in the library. Next, each\nalgorithm is fitted using the whole training set and the SL PF is obtained by applying the esti-\nmated weights to the algorithm predictions for each observation.\nIt can be demonstrated that using 2-fold CV, the procedure can end here and the AUCs of fit-\nting each algorithm and SL to the whole training dataset would suffice for SL to outperform the\nrest of the algorithms without overfitting. However, we included an additional validation step: all\nthe PFs obtained with the training set, where used to predict SUD treatment success in the test\ndata set. We calculated AUCs compared to evaluate all models adjusted using the test dataset.\nA thorough description of all the algorithms used in this work is out of the scope of this\nmanuscript. The interested reader will find further details about SL in van der Laan and Rose\n[33] and about the rest of the aforementioned methodologies in Friedman et al [29] and Ben-\nModels were fitted using the open source R programming language [34] and the H2O R\ninterface version 3.8.2.2 [35] that optimizes all the analytical methods used for large datasets.\nThe h2oEnsemble package version 0.1.8 [36] was used to fit the SL model. We set all tuning\nparameters for each algorithm in the SL library (e.g., the ANN implementation used in this\nmanuscript has over 20 parameters) to their default values. The analysis took about 2.5 hours\n8 Gb RAM. Most of the analytic time was used to fit the four regression models including\nMachine learning for SUD treatment success prediction\n2-way interaction terms. The other 13 algorithms, including SL, required only about 6 minutes\nof the 2.5 hours. AUC confidence intervals and variances were estimated using the Delong and\ncolleagues methodology [37] as implemented in the R package pROC [38]. Briefly, Delong\net al [37] used the equality between AUC and the Mann-Whitney U statistic and asymptotic\nnormality to analytically derive variance and confidence interval estimators for AUC.\nResults\nTreatment success rate and characteristics for the patients included in the analyses are shown\nin Table 1. For brevity, only gender and the top 10 most important predictors as identified by\nRF were included in the table.\nTable 2 shows the AUC in the test set (N = 19,802) for the 17 algorithms/algorithm configu-\nrations compared. All AUC are between 0.793 and 0.820. This indicates that, for this set of\nmodels, the probability that any of the algorithms will rank a randomly chosen successful\npatient higher than a randomly chosen unsuccessful patient is between 0.793 and 0.820. This is\nusually considered a good performance for prediction models.\nAs hypothesized, SL shows the largest AUC. SL performance is very closely followed by RF\nincluding all predictors. For this particular example, the algorithm with the worst performance\nis logistic regression including all predictors and selected 2-way interactions. The relative\nimprovement in AUC of SL is 3.3% when compared to the worst prediction method. Also, the\nAUC for SL has the smallest estimated variance. The rest of the models considered have esti-\nmated AUC variances up to 19% higher than SL. As revealed by Fig 2, SL AUC is higher than\nAUC for all other models, with the exception of the RF model including all predictors.\nAlso, in most cases, the models including all predictors performed slightly better (but not\nstatistically so) than the models including only the top 10 predictors selected in the initial\nscreening step to simplify the prediction model. All parametric regression models both penal-\nized and non-penalized approaches performed almost identically with respect to AUC for\nmodels not including interactions. This behavior could have been anticipated since a model\nwith only 28 predictors can be considered too small for a data set this size. However, for mod-\nels including all predictors and selected 2-way interactions, lasso outperformed the other 3\nregression models. AUC for lasso was higher than for ridge and logistic regressions.\nDiscussion\nThis work compared 17 models for predicting successful completion of SUD treatment. Both\ntraditional and newer analytic strategies were equated in a nationwide large dataset. Particu-\nlarly, super learning, an ensemble machine learning algorithm, was introduced for the first\ntime to the study of SUD treatment success using large datasets. As expected, SL showed the\nbest predictive performance.\nIn this particular example, SL superiority was meager when compared to more traditional\nmodels such as logistic regression. This result was not at all evident before analyzing these\ndata. We understand this may dampen the interest in this work. However, we think that the\nfinding that there are no major differences in the solutions of all methods is important in itself\nand worth discussing. Unlike what happens with other predictive models, the lack of substan-\ntive difference between SL and the rest of the models used means that: a) any of these methods\ncould be used for these data and b) there are no major problems in the assumptions of the\ndifferent models used. These findings are never evident before or after analyzing a dataset\nwhen analysis is performed using a traditional analytical approach. Sensitivity analysis is one\napproach used to address this limitation of traditional methods. In this sense, SL serves as a\ntool to streamlining and improving sensitivity analysis for prediction. SL does not require that\nMachine learning for SUD treatment success prediction\nTotal N (%)\nSUD Treatment Success\nGender\nEthnicity\nAge (years)\nEducation (years)\nEmployment\nPrimary Substance\nFrequency of Primary Substance Use\n(Continued)\nMachine learning for SUD treatment success prediction\na model is chosen a priori, at the end of the analysis results in the best model, and, if there are\nno substantial differences with the rest of the models, SL provides a solid justification for the\nuse of any of the models. SL is a valuable, but underutilized, tool for obtaining robust predic-\ntion results as required, for example, by the National Institutes of Health [39].\nIn addition, even small improvements in prediction can have a high impact depending on\neach particular problem. In this case, a small prediction improvement could significantly\nimpact patients' outcomes and/or treatment costs. In other cases, small prediction improve-\nments could save lives. Since SL is theoretically proven to perform as well or better than the\nbest model included in its library, the results presented here are relevant to illustrate the use of\nSL in the SUD treatment outcome prediction field.\nIn other applications, SL has shown 44% and 12% increase in performance when compared\nto ANN and RF, respectively [40]. Also, SL has shown as good or significantly better predictive\nperformance than linear regression and other methods when used in 13 real datasets--see Fig\nTable 1. (Continued)\nTotal N (%)\nAge of First Primary Substance Use (years)\nSubstance Abuse Type\nSource of Referral\nLength of Stay (days)\nMachine learning for SUD treatment success prediction\n3.4 in [40]. In fact, stacking algorithms similar to SL are usually among the type of algorithms\nthat win prediction contests such as the Heritage Provider Network Health Prize [41]. The\nHeritage Provider Network Health Prize was a competition to develop a predictive algorithm\nhelping identify patients most likely to be admitted to healthcare providers.\nThe results of this work should be interpreted as an illustration of the presented statistical\nmethods in a realistic setting. The practical conclusions should be seen in light of the data\nlimitations. Analysis was restricted to Hispanics and very few features describing the type of\ntreatment and how treatment was administered were included. There may be considerable res-\nervation with using treatment completion as an indicator of treatment success. While post-\ntreatment follow-up outcome measures of abstinence or reduced use would be best, such\nTable 2. AUC in the test set (N = 19,802) for each algorithm and algorithm parametrization used.\nModel AUC b\nFig 2. 95% confidence intervals for AUC of each model compared.\nMachine learning for SUD treatment success prediction\nnation-wide data collection efforts were out of the scope of this project. While these are impor-\ntant limitations to inform treatment success prediction, these issues do not affect the main\ngoal of this work.\nTraditional analytical approaches for prediction may not only consider the prediction per-\nformance of a model, but also its ecological validity for a given research area. That is one of the\nreasons logistic regression could be preferred over SL for predicting SUD treatment success.\nWhile the model proposed by SL is designed to excel at prediction, it is not meant to be inter-\npreted or to be used for effect estimation. If the true or close to true data generating model is\nincluded in the SL library, SL will likely have a low gain in predictive performance and its lack\nof interpretability could discourage its use. However, most of the time, researchers are uncer-\ntain about the model that generated the data. When ecological validity is important in a predic-\ntion context, SL could be used as a simpler model validator if an interpretable model in its\nlibrary is close to SL in predictive performance. In the example presented here, SL has signifi-\ncantly better predictive performance than logistic regression; thus, it would be difficult to\ndefend any of the logistic regression models used in this application in lieu of SL. On the other\nhand, these data show that, if the ecologically validity of RF models could be easily assessed,\nthe use of the RF model with all the predictors could be used instead of the SL model.\nWhen the goal of the research is to interpret the effect of different predictors in the out-\ncome, SL can be used in the context of the targeted learning framework [33]. SL is the first\nstage for obtaining doubly-robust effect estimates using targeted maximum loss-based estima-\ntion (TMLE) [42]. TMLE is an estimation technique that allows for double robust estimation\nof effects with fewer untestable assumptions than the usual parametric methods used for esti-\nmation. TMLE can be applied either in a causality framework or in a merely associative frame-\nwork (additional details about TMLE is out of the scope of this work, we recommend [33]\nfor further information). Using the prediction model suggested by SL and TMLE, one could\nanswer questions such as \"What is the treatment success rate difference between Hispanics\nwith comorbid psychiatric disorders and those without comorbid disorders?\" and \"Is this dif-\nference different from zero?\" In contrast, statistical inference is not possible in machine learn-\ning methodologies such as RF or deep learning. This important limitation of machine learning\nalgorithms is overcome by targeted learning, the first analytical framework that provides esti-\nmates and hypothesis testing while using machine learning [43]. Future directions of this work\ninclude determining the benefits of applying targeted learning for different effect estimations\nand inference in the addiction field.\n"
}