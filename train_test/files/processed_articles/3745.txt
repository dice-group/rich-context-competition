{
    "abstract": "Abstract\nAlthough the terminology of Big Data has so far gained little traction in economics, the availability of unprecedentedly\nrich datasets and the need for new approaches \u00ad both epistemological and computational \u00ad to deal with them is an\nemerging issue for the discipline. Using interviews conducted with a cross-section of economists, this paper examines\nperspectives on Big Data across the discipline, the new types of data being used by researchers on economic issues, and\nthe range of responses to this opportunity amongst economists. First, we outline the areas in which it is being used,\nincluding the prediction and `nowcasting' of economic trends; mapping and predicting influence in the context of mar-\nketing; and acting as a cheaper or more accurate substitute for existing types of data such as censuses or labour market\ndata. We then analyse the broader current and potential contributions of Big Data to economics, such as the ways in\nwhich econometric methodology is being used to shed light on questions beyond economics, how Big Data is improving\nor changing economic models, and the kinds of collaborations arising around Big Data between economists and other\ndisciplines.\n",
    "reduced_content": "Original Research Article\nEmerging practices and perspectives\non Big Data analysis in economics:\nBigger and better or more of the same?\nLinnet Taylor1, Ralph Schroeder2 and Eric Meyer2\n Keywords\nBig Data, economics, econometrics, interdisciplinarity, epistemology, business\nIntroduction\nBig Data is increasing in importance as a source of\ninformation about the social world. A variety of\nsocial science disciplines have experimented with these\nnew sources and types of data, with perhaps communi-\ncations studies in the lead at the moment. However, the\ndiscipline of economics appears so far to have been\nfairly slow to pick up on the promise of this new cat-\negory of data. In this paper, we focus on the field of\neconomics as a case study to examine the adoption of\nBig Data approaches and epistemologies in the social\nscience disciplines. Although every discipline is different\nin its reasons for adopting or rejecting Big Data ana-\nlysis, economics may be a useful case study because, as\nwe argue below, it occupies an interesting space at the\nintersection between academic and applied knowledge\nused for business purposes (see also Savage and\ntinct trajectory in making inroads in the social sciences\nand in the uses of bigger and richer datasets. At the\nsame time, economics also has a strong body of\ntheory and methodology which may make economists\nsceptical of Big Data sources and approaches and may\npose unique challenges in this discipline concerning reli-\nability and representativeness. For these reasons,\nexploring how economics is encountering Big Data\nmay offer insights into the question of how Big Data\nis shaping \u00ad or not \u00ad the direction of the social sciences.\nOur working definition of Big Data is that there is a\nstep change in the scale and scope of the sources of\nmaterials (and tools for manipulating these sources)\navailable in relation to a given object of interest\n1University of Amsterdam, Amsterdam, The Netherlands\n2Oxford Internet Institute, Oxford, UK\nCorresponding author:\nLinnet Taylor, University of Amsterdam, Plantage Muidergracht 14,\nEmail: l.e.m.taylor@uva.nl\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY-NC-ND: This article is distributed under the terms of the Creative Commons Attribution-Non\nCommercial-NoDerivs 3.0 License (http://www.creativecommons.org/licenses/by-nc-nd/3.0/) which permits non-commercial use,\nreproduction and distribution of the work as published without adaptation or alteration, without further permission provided the original work is\nattributed as specified on the SAGE and Open Access pages (http://www.uk.sagepub.com/aboutus/openaccess.htm).\n(Schroeder, 2014). This definition is different from the\nterminology used in industry (Laney, 2001), which\nrevolves around the `volume, variety and velocity' of\ndata, a definition also adopted by some of the academic\neconomists that we focus on in this paper \u00ad even though\nthey invariably use `Big Data' that fits our definition,\nwhether they are aware of definitions or not. Other\nresearchers from our sample, particularly if they are\nconnected to industry, may also use the terminology\nof computational methods to set their work apart\nfrom previous studies within the academic sphere (e.g.\nTambe, 2012). Indeed, relying purely on the termin-\nology of `Big Data' is problematic, as economists and\nothers working in this area may or may not actually use\nthe term even if they are clearly operating within our\ndefinition or within a broader conception of computa-\ntional methods. This can be seen in the number of\npapers which name `Big Data' as a particular feature\nof their analysis, which is relatively small: a Scopus\nsearch for papers with `Big Data' in the title, abstract\narticles, of which only 32 are categorised as `economics,\neconometrics and finance'. Of course, this is just one\nindicator, which is not able to find uses of Big Data that\ndo not mention the term, but in addition to this, to the\nbest of our knowledge, there is only one current publi-\ncation which explicitly advocates Big Data as an\nimportant force in the future of economics (Einav and\nThe lack of a clear adoption of terminology is not\nsurprising in a new and still emerging area, even if vari-\nous characteristics of Big Data clearly make it an\nimportant resource for economics. Einav and Levin\n(2013) have pointed out three of these main character-\nistics. First, that Big Data sources are frequently avail-\nable in real-time, which can offer an advantage in terms\nof `nowcasting', or identifying economic trends as they\nare occurring. The second relates to the scale of the\ndata: the large size of the datasets becoming available\nresolves the statistical problem of limited observations\nand makes analysis more powerful and potentially\nmore accurate, while their granularity (a characteristic\nMichael Zhang, Assistant Professor at Hong Kong\nUniversity of Science and Technology, terms `nano-\ndata', following Erik Brynjolffson (M. Zhang, inter-\nof understanding individual actions. Third, such data\noften involve aspects of human behaviour which have\npreviously been difficult to observe, for example per-\nsonal connections (such as those within Facebook) or\ngeolocation (such as the place from which a `tweet' was\nsent via Twitter). However, Einav and Levin (2013) also\npoint out some drawbacks which may have led to\neconomists' comparative reluctance to adopt Big Data\nso far. The main one is the unstructured nature of such\ndata and the complexity of the linkages often contained\nwithin it, which upset the usual econometric assump-\ntion that data points are not interdependent, or at least\nare interdependent in certain defined ways. As Einav\nand Levin point out, this complexity presents an econo-\nmetric challenge in terms of untangling this dependence\nstructure and understanding the data.\nBesides the characteristics of sources of Big Data\nwhich make them suitable for economists, there are\nalso certain ways in which economists are well suited\nto being users of Big Data. Big Data analysis demands\ntechnical skills in terms of statistics and coding which\nare part of the standard training for most economists.\nThe econometric challenge of working with Big Data\nusing statistical techniques appropriate for entire popu-\nlations is part of a continuum of such challenges faced\nby economists as data sources have become larger and\nmore complex over time, and the rewards of solving\nsuch problems, in terms of advancing the discipline,\nare potentially significant. Perhaps most importantly,\nthere is a considerable amount of Big Data found\nwithin the traditional territory of economics: financial\ntransactions of all kinds, including increasingly granu-\nlar sources such as loyalty card data and online pur-\nchases, labour market data, and detailed population\ndata. All these concerns suggest that Big Data is a\npotential goldmine for economists and that there may\nbe a demonstrable opportunity cost for many econo-\nmists in not engaging with this type of research. Yet, as\nwe shall see, there are also limits to the uses of Big Data\nin economics, and these shed interesting light on its role\namong the social sciences and beyond.\nResearch questions and methods\nThis paper uses a series of interviews (n \u00bc 17) conducted\nwith economists who have been working with Big Data,\nor data scientists working on questions within the eco-\nnomics or business fields, to examine the issues involved\nand the challenges and rewards of this type of data.\nThese interviews are part of a larger project funded\nby the Alfred P. Sloan Foundation for which we have\ninterviewed more than 125 social scientists over the\nstructured interviewing approach designed to elicit\ninformation about their engagement with Big Data,\nthe tools and skills they use to work with data and\nlearn more about how they gain access to data sources.\nThe questions we ask here and the conclusions we draw\nout concerning economists' use of Big Data are also\ninformed by this larger study, which also relies on\ndesk research, scientometrics, participation in various\nfora such as conferences about Big Data, and our own\nengagement in research in this area. The economists,\nlike our other interviewees, were selected using\n2 Big Data & Society\npurposive sampling to find those working at the\nresearch front of Big Data in social science disciplines,\nand thus do not constitute a representative sample of\nany given discipline. However, given the newness of Big\nData approaches to research, focusing on those\nengaged at the research front makes more sense than\ntrying to understand Big Data use with random sam-\npling techniques or measuring their impact via cit-\nations. Thus, this qualitative study is not necessarily\nrepresentative, but is intended as an exploratory effort\nto uncover the motivations, practices and challenges\nencountered by social scientists, and thus to inform\nfuture directions in this area.\nTwo research questions guide this paper, as follows:\n1. For what purposes are Big Data used (prediction\nand/or nowcasting, marketing research, substituting\nnew or cheaper datasets for older ones, or other\nfactors)?\n2. Which type of economic or other knowledge\nadvance is this use of data contributing to? In\nother words, which subdisciplines, economic meth-\nods, models, and motivations are apparent among\nearly adopters of Big Data approaches in economics?\nWhat constitutes Big Data within\neconomics?\nAs already mentioned, finding a consistent definition of\n`Big Data' in the field of economics is difficult, while\nunderstandings in the social sciences are still emerging\naround what constitutes `big' versus `not big'. An incre-\nmental rise in the number of data points does not serve\nas a definition per se \u00ad as a discipline, economics has\ngenerally aspired to the most extensive and detailed\ndatasets possible, and has a history of adapting and\nevolving statistical techniques to deal with new types\nof data; nor does the need for programming skills \u00ad\nunlike some other social science disciplines, economists\ntend to learn to code in order to use analytical software\nsuch as R and SAS. Within the group of economists\ninterviewed, there were a range of opinions on what\nconstitutes Big Data, with the agreement that the spe-\ncific terminology is fairly recent \u00ad although some were\nworking with what is now being termed `Big Data' a\ndecade ago, most had not heard the term until around\n2010, and agreed that it has not gained much traction\nwithin academic economics in particular. They did\nagree that it was possible to identify a class of data\nwhich was particular in terms of its size and complexity,\nalthough there were several different points of view as\nto which features rendered it genuinely new.\nOne common starting point amongst the economists\ninterviewed was that the emergence of Big Data can be\nsituated within a continuum of developments in the\ndiscipline, and that the practices and perspectives\nwhich define economics are not particularly responsive\nto new levels of size or complexity in the datasets avail-\nable. Within economics, Big Data cannot be charac-\nterised mainly as a shift in the sources of data, as is\npossible, for example, where new social media provide\nthese sources in other social sciences such as media and\ncommunication studies. For instance, Professor David\nHendry notes that there is a difference between macro-\nand micro-economics in terms of the number of obser-\nvations commonly accessible to the researcher, so that\n`in cross-sections relevant to macroeconomics about\n1000 would be seen as Big Data and needing a lot of\ndifferent methods of analysis' (D. Hendry, interviewed\nHowever, many respondents did identify some\naspects of Big Data which have epistemological or\npragmatic implications for those economists who\nchoose to engage with it. On the pragmatic side, Big\nData can be characterized as highly multidimensional\nin terms of the number of variables per observation, the\nnumber of observations, or both, given the accessibility\nof more and more data \u00ad what Professor Hal Varian,\nChief Economist at Google, referred to as `fat data,\nlong data, extensible data and cheap data' (H.\nEinav, an economist at Stanford, similarly identifies a\ntrend towards data sources where `you just know a lot\nof stuff on every observation . . . [such as] histories and\nstuff like that from which you could construct a very\nbroad set of potential variables' (L. Einav, interviewed\nThis multidimensionality is also important because it\nnecessitates new approaches and training. Nathaniel\nHilger, an economics PhD at Harvard, defines it func-\ntionally in terms of the need for new or adapted ana-\nlytical tools: `It [Big Data] starts when you can't use\nSimilarly, Alberto Cavallo, an Assistant Professor at\nMIT, defines it in terms of a messiness that challenges\ncurrent skillsets:\nI think to me the big challenge now has become having\npeople who have enough skills to be able to jump into a\nvery messy data set that has been built for other pur-\nposes and then knowing what to look for and how to\nclean that data, and transform it into meaningful infor-\nmation, and I think that is going to be the big chal-\nFor Prasanna Tambe of NYU's Stern School, granu-\nlarity is the defining feature of the new datasets. He\noffers an analogy with van Leeuwenhoek's invention\nof the microscope in the 17th century:\nTaylor et al. 3\n[With a microscope] you can basically look at one\norganism at a whole new level of detail. And I like\nthat analogy for Big Data as well. That in a lot of\nways we're looking at questions that people have\nlooked at before, but you're just turning up the micro-\nscope. I think that's a pretty apt description when it\ncomes to consumer spending, labour markets, crowd\nfunding, there are so many examples I can think of\nwhere the questions are old but they will need to look\nat them with this new level of analysis that just, sort of\nexplodes the number of policy implications and things\nlike that you can get from them. (P. Tambe, inter-\nOther respondents offered a definition of Big Data\nas datasets relating to human behaviour, i.e. the by-\nproducts of people's use of technology and behaviour\nas consumers in a technologically-enabled market. For\nexample, Duncan Simester, a Professor at MIT, takes\nthe view that Big Data is `micro-level detailed data\ndescribing some type of consumer behaviour. . . . I\ncould imagine that if I was in operations management\nit might be machine cycles . . . but it's a behavioural\nresponse measure' (D. Simester, interviewed 11\nSascha Becker, a Professor at the University of\nWarwick, defines Big Data from a methodological per-\nspective as universal with regard to the phenomenon of\nstudy (`N \u00bc all'), and in turn to its characteristic of\nstretching computational resources:\n[It is Big Data] in the sense that it's the universe, so\nliterally all firms that are multinationals. That, for me,\nwould be one definition of really Big Data as opposed\nto some sample. And that . . . we linked up with the uni-\nverse of all German workers, so we crossed 32 million\nGerman workers with 6000 multinational firms and\nalso non-multinational firms, domestic firms. And\nthat was, I guess, the first instance where simply com-\nputing power set certain limits in my research work. (S.\nFinally, Einav suggests that the advent of the termin-\nology of `Big Data' in the economic sphere may be\nlargely driven by industry. He sees corporations collect-\ning ever more extensive and intensive data from their\ncustomers, and offering access to economists when they\nrealise that their different interests in unlocking the\ndata's value may align:\nExcept for maybe the more sophisticated companies\nout there, many of them just sit on their datasets and\nthey realise they have potentially a gold mine of data\nbut they have no idea what to do with it. So in that\nsense maybe what happens with Big Data is that more\nand more, private and academic enterprises came along\nto say, `Well, you know, you guys are all sitting on huge\ndatasets and it's time for you to actually potentially get\nsome value out of this'. (L. Einav, interviewed 20\nThese different viewpoints are not incompatible: access\nto Big Data appears linked at least partly to the cor-\nporate connections in the field of economics and in\nbusiness in particular, but also to a desire on the part\nof many economists to find new perspectives on endur-\ning questions. The next section outlines some of the\nmain patterns in the uses of Big Data among the econo-\nmists in our interviews.\nRationales for the adoption of Big Data\napproaches\nAs with any new technological development, the adop-\ntion of Big Data approaches has depended on various\nfactors (such as data availability) and is taking place at\ndifferent rates among different groups. The process of\nadoption may be top-down (institutionally driven),\nbottom-up (based on individuals' perceptions of an\nadvantage) or, as is usually the case, a mixture of the\ntwo (Rogers, 2003). Economic analysis using Big Data\nhas slowly been gaining social scientific traction on\nboth these levels. On the institutional level, both the\nAmerican Economic Association and the US National\nBureau of Economic Research (NBER) held panels or\npotential in economic analysis, and the head of Pew\nSurvey Research has described how interest is rising\naround the idea of using Big Data derived from social\nmedia and other transactional sources to supplement,\nand possibly in some cases as a substitute for, govern-\nment statistical data gathered using traditional survey-\nMeanwhile, on the individual level, economists have\nadopted Big Data approaches where these can offer a\nnew take on traditional economic questions such as\nlabour market dynamics (Choi and Varian, 2012), the\neffect of early education on earnings (Chetty et al.,\nthe workings of online markets (Einav et al., 2011).\nSome of these papers involve, or are even led by, com-\nputer scientists (Antenucci et al., 2013) or behavioural\nscientists (Moat et al., 2013), but with economists as co-\nauthors. Beyond this, a minority trend is also emerging\nwhere `Big Data economics' is effectively adopted from\noutside the field entirely, for example where computer\nscientists use Big Data to look at questions bearing on\nissues that are central to economics, such as Bollen\net al. (2011) who studied the relationship between\nTwitter and the stock market.\n4 Big Data & Society\nFurthermore, economists appear to be engaging with\ndifferent aspects of Big Data approaches depending on\ntheir priorities. For instance, `velocity' is frequently\nnamed as an identifying characteristic of Big Data\n(Laney, 2001), but not all economists using Big Data\nare engaging with this aspect. Many are using a real-\ntime feed of some kind, such as the Billion Prices\nProject at MIT (Cavallo, 2011), or the MIT project\nrun by Duncan Simester, where real-time Twitter data\nwill be used in combination with transaction data from\nstores to compare consumer sentiment with actual\npurchasing behaviour (D. Simester, interviewed\nples from a Big Dataset within the company which\nowns the data, often using technology such as\nHadoop or Apache Pig, and then analyse the data\nwith ordinary statistical tools such as Matlab or\nSTATA which do not take advantage of the data's\nreal-time aspect. Professor Hal Varian, Chief\nEconomist at Google, pointed out that this way of\nworking with otherwise unmanageably large datasets\nhas analytic advantages in the context of economic\nanalysis:\nIn a lot of cases drawing a signal from that data is just\nas good as using the data itself. So there are cases where\nthe Big Data advantage can be exaggerated, and where\nsampling is the best procedure. . . . And the advantage\nof sampling of course is that you can draw a repeated\nsample, so you can see how your results vary with the\nsampling distribution. (H. Varian, interviewed 29\nSome economists see themselves as non-adopters, sug-\ngesting that there is nothing new in Big Data. This\nbelief that Big Data represents just a point along a con-\ntinuum of more or less extensive datasets is epitomised\nby the econometrician Professor David Hendry when\nhe says that `whether the dataset's big or small doesn't\nactually matter in establishing change, but if it's big and\nthe system is complex the only way to establish change\nis to model that complexity' (D. Hendry, interviewed\nHowever, many of the economists interviewed who\ndid see Big Data as a step change in the kinds of ana-\nlysis that were possible said that using this type of data\nallowed them to address problems in innovative ways,\nand this also relates to their interest in new technical\napproaches. There was a consensus that the aspects of\nBig Data which seem to attract economists \u00ad that it is\ngranular, population-level data with multiple dimen-\nsions that allow researchers to analyse cases along\nmany variables \u00ad allow economic researchers to test\ntheories of behaviour that were previously untestable,\ncreating a new set of metrics for issues of economic\ninterest which were previously in the realm of theory.\nFor example, Nathaniel Hilger, a former Harvard PhD\nstudent in economics and now an Assistant Professor\nat Brown University, has worked on several pro-\njects involving large-scale administrative data from\nthe US Internal Revenue Service, and believes that\nthis kind of population-level data is potentially\nrevolutionary.\nI think the essential feature of all these [Big Data] pro-\njects is that it uses a very large amount of sand to get\nenough gold to do causal inference on a question that\nhadn't previously been able to be analysed as convin-\ncingly. . . . another benefit of having Big Data is once\nyou get the essential causal effect you're looking for, if\nyou have enough gold, you can then parse the gold to\nlook at the effect on different subgroups and learn even\nmore about what's driving the causal effect. (N. Hilger,\nOne important rationale for using `born-digital' data is\nthat, in contrast to the classic survey-based datasets\nwhich have been the basis for much applied economics\nover the last century, economists can often collect it\nthemselves. The ability to collect large-scale data inde-\npendently can be especially powerful with regard to\nquestions which have previously been the preserve of\ngovernments. One example of this is the Billion Prices\nProject, devised by Alberto Cavallo, now an Assistant\nProfessor at MIT. The project, which involves pro-\ngramming a web scraper to gather online prices for\ngoods and using them to compile an inflation index,\nwas devised as a way to create a more transparent\nand accurate inflation measure in Argentina. The abil-\nity to access real-time price data has effectively pos-\nitioned Cavallo's research as an alternative to\nnational governments' inflation measures. Cavallo\nnotes that the project seems to illustrate the case for\nmore independent data collection among economists:\nWe [economists] have been using the same data sets\nover and over again, and since we wanted new answers,\nwe have been developing new econometric techniques\nto try to transform the data, and get more meaningful\ninformation out of them. But it was reaching a point\nwhere there is nothing else you can do on that side, and\njust having a fresh, new data set brings a whole new\nperspective, and I think people are starting to realise\nthat, and gradually people are becoming more inter-\nested in data collection itself. (A. Cavallo, interviewed\nBesides the two extremes of survey versus scraped data,\nthe option of on-demand data such as that provided by\nGoogle Analytics is also proving a reason to explore\nTaylor et al. 5\nnew opportunities. A currently high-profile example is\nin the field of `nowcasting' (Choi and Varian, 2012),\nwhich uses what might be termed curated synopses of\nhuge datasets, such as people's web searches through\nGoogle, to make highly accurate short-term predic-\ntions. The well-known example from Choi and Varian\nexamines consumer and labour market trends, suggest-\ning that the changing volume of queries about given\nproducts or services on Google closely mirrors\ndemand. The project illustrates how adopting Big\nData approaches may not involve learning new com-\nputational techniques, or necessarily challenging the\ndiscipline's methodological bounds. The important\nquestion may be whether the data gathered using\nthese new sources raises new epistemological con-\ncerns, and whether it takes economists outside their\ncomfort zone in terms of reliability and replicability.\nWe will address these questions in the section that\nfollows.\nThe challenges of interpreting Big Data\nAlthough economists generally have sophisticated stat-\nistical skills and plenty of expertise in coding large data-\nsets, the new sources of data described here present\nchallenges which highlight issues in how Big Data is\nbecoming part of social science research. For example,\nthe size of Big Data may render the idea of statistical\nsignificance, a mainstay of hypothesis-testing, useless.\nVarian says, `when you have a billion observations,\neverything's significant'. Varian and another senior\neconomist, David Hendry, have very different\napproaches to the interpretative issues highlighted by\nBig Data. Varian is prompted to ask whether it is time\nto officially separate the statistical notion of significance\nfrom its more general meaning \u00ad a discussion which has\nbeen underway in the natural sciences, notably medical\nstatistics, for several decades (e.g. Gardner and Altman,\n1986). For Varian, the significance problem highlights\nexisting weaknesses in economic practice which can be\nresolved by taking a broader view of what is worth\nreporting:\nyou really do have to address what we should be\naddressing all along, the importance \u00ad unless we use\nsignificance in a phenomenal sense, or an operational\nsense, not the statistical sense. Because after all, statis-\ntics was designed to deal with datasets of a hundred or\nso observations, when you look at it. So we've devel-\noped some bad habits, I think, in terms of misusing\nstatistical terms. (H. Varian, interviewed 29 January\nDavid Hendry is concerned with the argument, as\nvoiced by Mayer-Scho\nmuch work using Big Data is essentially descriptive,\ndealing with correlation rather than causality.\nHe notes that if economics cannot seek causality, it\nsimilarly loses one of its mainstays:\nit applies in epidemiology, it applies in sociology,\npolitical science and in economics that you get large\ndatasets, and under the null [hypothesis] that there's\nno connection you will find lots of connections unless\nyou're extremely careful about how you analyse it.\nMany of the methods of analysis that I see people\nusing, even through to genetics and studies of DNA,\nare using methods that I think are seriously flawed in\nterms of picking up things that are not there.\nIn contrast to Varian's approach of adopting more of a\nphenomenal lens, Hendry advocates sharpening econo-\nmists' modelling techniques (chief of which is what is\nknown to economists as the `LSE/Hendry approach')\nto make economic analysis more powerful, regardless\nof the size of the dataset.\nDespite these problems, which are both methodo-\nlogical and epistemological, the debate about whether\naccess to extensive, highly granular data heralds the\n`end of theory' (Anderson, 2008) has found only limited\nresonance in economics. Professor Sascha Becker of the\nUniversity of Warwick suggests, like Varian, that Big\nData will cause economists to reevaluate their assump-\ntions, but rather entails a more iterative interaction\nbetween theory and empirical data:\nI think theories [in the light of Big Data], they\ndon't have the same value. It's more about in the\npast maybe we would have theory and then would\ndo simulations and calibrations and then make predic-\ntions about what might happen. And Big Data\nallows you to really go out there and measure stuff.\nBut still you will need theory to understand the mech-\nanisms or even to suggest what you might hope to\nfind in the first place. (S. Becker, interviewed 23\nIf Big Data is causing these economists to reevaluate\nthe explanatory power of economic methodology, it is\nalso causing some to reevaluate the explanatory power\nof economists themselves. The Council of Economic\nAdvisors in the USA makes annual predictions of\nunemployment rates and other indicators, and an\nexperiment at the University of Michigan is being con-\nducted by a team of economists in collaboration with\nMike Cafarella, a Professor of Computer Science, to\ntest whether Twitter may outperform the Council as a\npredictive tool. The premise of the study, Cafarella\nexplains, is to assume that the economists' errors are\n6 Big Data & Society\nrandom and therefore (to the economists) not predict-\nable, and to see whether Twitter can quantify them:\nIf the social media data is actually carrying some brand\nnew information in the universe, something that we\ndidn't have previously, then we should be able to pre-\ndict [professional economic advisors'] error. . . . and at\nleast in the case of unemployment using Twitter we\nwere able to predict about one third of the error.\nThis is a particularly interesting project because\nsocial media has been criticised as having unknown\nbias and therefore being of questionable reliability as\na social scientific tool (Gonza\n\u00b4 lez-Bailo\nforthcoming), yet the Michigan project sets it against\nhuman judgement with the aim of quantifying error\nmargins.\nAnother issue is the emerging uses in the social sci-\nences of data mining. A term which used to denote `bad'\nquantitative social science which lacked a clear hypoth-\nesis, with the advent of Big Data this is becoming a more\ncredible form of research. A study by Michael Zhang\npublished in the American Economic Review demon-\nstrates how attitudes to data mining are changing:\nZhang, who moved into economics with a background\nin computer science, data-mined Wikipedia content\nin order to develop his research questions, noting\nthat he and his collaborator Zhu `needed time to process\nthe data before we actually came to the research\nquestion . . . basically, all these questions came after we\nhad the data'. The data mining led to two papers on\nbehavioural economic questions (Zhang and Zhu,\nsible by the Chinese government's on-off blocking of\nWikipedia.\nSimilarly, Einav and Levin's (2013) work with eBay\nauction data has involved data mining in order to\nsearch for the right questions. They describe how\nrather than seeking out a particular dataset to answer\nan established question, as is common with economists\nwho work on survey data which is curated and there-\nfore has more predictable contents, a windfall of `data\nin the wild' such as the by-products of consumers' eBay\nuse may require a very different strategy to seek the\nright question. It also may require a different timescale\nfrom curated data, weighted towards question develop-\nment rather than model-based analysis:\nSo we kind of came to it not having a particular idea of\nwhat exactly we want to do. We just wanted to formu-\nlate reasonable questions that could kind of leverage\nthe idea that you have the Big Data rather than some\nsort of a smallish portion of it. So initially we were\nbasically for six months just playing with the data,\ntrying to understand, you know, what we could do\nwith it and what could be interesting. (L. Einav, inter-\nEinav and Levin's work illustrates how economists may\nknow that a dataset contains great analytical value\nwithout being able to specify that value in advance.\nAlong with Zhang and Zhu's research, it suggests that\nrather than being exclusively hypothesis-led, economics\nresearch using Big Data may need to work towards a\ndifferent, or broader, definition of methodological\nrigour to take into account data where most of the\nuncertainty is weighted towards the pre-analysis\nphase, and once the data is understood through a par-\nticular question, the extensiveness of the dataset makes\nthe process more of a snapshot than an excavation.\nThe challenge of access\nEinav and Levin (2013) argue that Big Data approaches\nhave the potential to allow economists to ask a great\nvariety of new questions. Arguably, however, these\nquestions depend largely on researchers' ability to\naccess new sources of data, most of which are propri-\netary. The challenge of access to appropriate data for\none's research is not new, and given the proprietary\nnature of much Big Data, similar hierarchies are\nlikely to emerge to those already existing in the discip-\nline of economics, where senior researchers have the\nresources, influence and networks to gain access to\nthe `best' data. Corporate data in particular presents\nsimilar problems regardless of its size, since it is propri-\netary and tends to be offered to researchers only subject\nto non-disclosure agreements which may limit the rep-\nlicability of studies. The disciplinary expectations\naround replicability and access to data may have to\nrelax as more researchers use Big Data for their studies\n\u00ad or in an alternate scenario may grow more stringent\nas datasets invisible to all but the author become more\ncommon. Michael Zhang suggests that a new politics of\ndata will emerge, but that either scenario is possible:\nThe usual practice is to sign some NDA [non-disclosure\nagreement] kind of arrangement and then by the time\nwhen the paper can be published sometimes, you know,\nsome companies, they don't care about the data any-\nmore. . . . so far no journal has a policy to say that you\ncannot publish if you don't share, so, there's no threat\nto authors \u00ad but in future I would imagine people will.\nGetting access to the `best' sources of data is, as noted\nabove, traditionally an issue of hierarchy. The research\nteam working with Raj Chetty at Harvard, for example,\nhas access through him to US Internal Revenue Service\nTaylor et al. 7\ndata of unprecedented size and detail on individuals'\nemployment history, which they have used to produce\ngroundbreaking analysis of, for example, how early\nchildhood education affects people's life chances\n(Chetty et al., 2011). However, Nate Hilger, Chetty's\nresearch team member, described the process of obtain-\ning and sustaining access to such a huge and detailed\ndataset as a significant investment of time and effort.\nThe research team could only access the IRS data in\nsecure data rooms authorised by the IRS central office,\nthey had to get what he described as `fairly, I think,\nhigh level security clearance', which `took months', and\ninvolved the team submitting information on `every-\nwhere we'd lived for the last ten years' (N. Hilger, inter-\nThe investment of time and effort is no less when\naccessing a highly restricted dataset such as the employ-\nment histories stored in LinkedIn's database in the\nfirm's Mountain View headquarters. However, the dif-\nference between IRS data and born-digital data from\nan internet firm is that younger researchers can, with\nthe right contacts, gain access as easily as senior ones.\nThey may even have more chance of access if they have\na high level of relevant technical skills, as did Prasanna\nTambe, who worked on LinkedIn data to produce a\nstudy of employment dynamics (Tambe, 2012).\nTambe originally self-funded his research as a\nsummer project. He got access to the data through a\ncombination of having fairly advanced programming\nskills (learned during a masters in Computer Science\nand a PhD in Economics) and via connections in\nCalifornia which allowed him to spend a significant\nperiod at the company's headquarters working with\nthe data, which (similarly to the IRS data) could not\nbe accessed outside the firm's building. The effort\ninvolved in getting access was similar to that with\nIRS data, but significantly more informal:\nThere's not an easy answer in the sense that what you'd\nlike to be able to give is sort of a blueprint as how you\ncould do this . . . [There are] various companies, and\nthere's maybe half a dozen of the big ones, all have\ntheir sort of own incentives. . . . I knew somebody\nwho knew somebody or just reached out randomly\nand got a response and. . . it took usually multiple con-\nversations or contacts, visits. So it wasn't that it was\nthat easy or direct, a direct interface through which you\ncould access the data. (P. Tambe, interviewed 26 April\nHowever, the return for his effort was a more detailed\ndataset than the IRS could offer. Job websites such as\nLinkedIn, Monster.com and Careerbuilder.com collect\nindividual-level sequential employment histories at a\nlevel of detail not offered by national administrative\ndata. Tambe describes it as `job titles . . . what skills\npeople have, what employers they worked at, occupa-\ntional level detail, all those things were sort of, you\nmight call a new level of granularity for labour data'.\nTambe's experience suggests that a generational and\ngeographic divide between more traditional research\nbased on large survey-based datasets and economists\nusing datasets from the big internet firms, often based\nin Silicon Valley.\nThe challenge of data access is driving significant\ncareer changes in the field of economics. Senior econo-\nmists such as Hal Varian (at Google) and Bernardo\nHuberman (at Hewlett Packard) are working outside\nthe academy for global corporations where they have\naccess to new and privileged sources of data, and such\nmoves are becoming more common amongst mid-\ncareer economists. So far, many are keeping an affili-\nation within academia: Patrick Bajari, Vice President\nand the Chief Economist at Amazon.com, remains a\nProfessor of Economics at the University of\nWashington, and Steven Tadelis, Distinguished\nEconomist at eBay, also holds the post of Associate\nProfessor at UC Berkeley's Haas School of Business.\nOver time, this increasing corporate affiliation on the\npart of some of the best-known economists in the field\nmay even contribute to a scenario where having access\nto corporate data becomes a factor in universities'\nhiring decisions.\nAlthough economists with coding skills such as\nCavallo have acquired important datasets such as\nonline price data through computational methods\nalone, most Big Data remains proprietary. Google's\npublicly available datasets such as Insights and\nTrends are curated, and it is unlikely that the company\nwill make the full extent of search data public any time\nsoon. Varian says that the company does not like the\nprospect of negotiating access with individual research-\ners `dealing with things on a case by case basis', and\ntherefore has decided to `make data available to every-\nbody or to nobody'.\nConclusion: The implications of\nBig Data for economics\nWe have sought to answer two main questions in this\npaper. First, we have outlined the purposes for which\nBig Data is being used, and demonstrated that Big\nData applied to economic questions has the potential\nto be disruptive both methodologically and epistemo-\nlogically. It may reframe some questions that are, or\nshould be, important to economists, and may do so in\nways that lead to new styles of thinking and investiga-\ntion. Furthermore, given that the development of meth-\nodologies for the analysis of Big Data presents various\nchallenges, econometrics may provide a useful bridge\n8 Big Data & Society\nbetween computer science, which can access and\nmanipulate the data and do the calculations but does\nnot traditionally contend with the questions of repre-\nsentativeness or validity, and the social sciences, which\nare interested in these questions.\nBig Data has emerged as a strong fit with behav-\nioural economics and part of the economics discipline\nrelated to industry in particular, but it may also present\nsome interesting challenges for economics within aca-\ndemia. Porter (2010) has written that `statistical reason\nis the beacon of an ideal of impersonal rationality\nachieved through technical methods' (pp. 45\u00ad46). This\npaper has suggested that Big Data has the potential to\nchallenge this notion of rationality \u00ad for example, by\naddressing the messy problem of sentiment analysis in\nsocial media in the cause of predicting economists'\nerror rate on particular questions. It may also have\nthe power, however, to stimulate new thinking on\nissues such as external validity, ways to address sam-\npling bias, and the ways in which, rather than allowing\nthe end of theory, exponentially larger datasets can be\nused in combination with sophisticated modelling\nstrategies to produce more detailed and accurate\nexplanations of social processes.\nSecond, we have addressed the question of how Big\nData is contributing to advances in knowledge. We\nhave shown that the intersection of economics and\nBig Data poses some important questions for eco-\nnomics and social science in general. One can see the\nshift towards Big Data as being a change in methodo-\nlogical emphasis, a change in data management and\nanalytic tactics, but one can also see it on another\nlevel as necessitating a more fundamental shift in per-\nspective from a science based on the notion of the mean\nand the standard deviation from the `normal' to one\nbased on individual particularity \u00ad an epistemological\nchange which brings into question some of the funda-\nmental tenets of economics as a discipline. If one takes\nthis last perspective and looks at Big Data as a qual-\nitative as well as a quantitative change, one can also see\nchallenges for economists in conceptualising these new\ndatasets and methods.\nBesides the epistemological challenge, there are\npragmatic issues to be resolved if the discipline is to\nengage with Big Data more broadly. Several questions\narise from our interviews: will Big Data democratise\naccess to the most valuable data for economists, or\ndoes it make less even the playing field for those in\nless well-resourced positions or institutions? Will it\nlead to new hierarchies forming around different\nforms of access and new sources? Second, will econo-\nmists' statistical and computational skills enable them\nto participate in developing new methodologies and\nanalytical approaches for Big Data, and will their con-\ncern with generalisability and reliability prompt new\napproaches which address Big Data's uncertain\nbiases? Finally, our interviews also raise the question\nof replicability. We have outlined a picture of Big Data\nthat is largely proprietary, with open-access data\nalready curated by firms in ways which are inaccessible\nto researchers. If Big Data holds great promise for eco-\nnomics, as many of our interviewees believe, will the\nway economists present and publish their results have\nto change, and will the field have to accept less access to\ndatasets and limited or no replicability?\nFor these reasons, the Big Data turn, if such it is,\nmay possibly be disruptive in economics, with possible\nanalogous ruptures in other social sciences. We have\nseen this before: the invention of statistics in the\n1800s allowed the emerging discipline of economics to\ntake a new turn in the analysis of social dynamics. It is\nalso possible that instead of Big Data becoming an\naccepted stream within economics, it could give rise\nto a sub-field of its own with separate disciplinary\nand methodological norms, with the implication that\nthose who practise this type of analysis will become\nseparated from or balkanised within the larger discip-\nline. So far, there are enough prominent economists\nengaging with Big Data studies (Brynjolffson, Varian,\nHuberman, Poterba) such that Big Data can be seen as\nan emerging specialism rather than a break from\nthe discipline. However, the emergence of conferences\nor sessions concentrating on Big Data suggests,\nif not marginalisation, then at least that Big Data\nis a specialisation rather than the future of the\nmainstream.\nThis paper has outlined the challenges and potential\nrewards of using Big Data in the field of economics.\nOverall, the evidence presented here suggests that the\nvalue of Big Data to the discipline may lie partly in\ncreating a stimulus for new ways of thinking, but spe-\ncifically in challenging economists to imaginatively\napply an economic perspective to the evolving digital\nlandscape. This work may be anchored in strong lines\nof existing inquiry \u00ad most of those interviewed for this\npaper are applying new datasets to economic issues\nthey have been interested in their whole careers \u00ad but\noften involve a new way of seeing existing information\nand inventive methods to separate the signal from the\nnoise. The most innovative work is being done by those\nwho, as Nathaniel Hilger put it, can devise ways to see\nthe gold amidst the sand.\nDeclaration of conflicting interest\nThe authors declare that there is no conflict of interest.\nFunding\nThis research was supported by a grant from the Alfred P.\nSloan Foundation.\nTaylor et al. 9\nReferences\nAnderson C (2008) The end of theory: the data deluge makes\nthe scientific method obsolete. Wired Magazine, 16 July.\nAvailable at: http://www.wired.com/science/discoveries/\nAntenucci D, Cafarella MJ, Levenstein MC, et al. (2013)\nRingtail: Feature selection for easier nowcasting. In:\nWebDB `13: Sixteenth International Workshop on the\nWeb and Databases. New York, NY.\nBollen J, Mao H and Zeng X (2011) Twitter mood predicts\nthe stock market. Journal of Computational Science 2(1):\nCavallo A (2011) Scraped data and sticky prices. Paper pre-\nsented at the American Economic Association annual con-\nference, Denver, CO. Available at: http://www.\naeaweb.org/aea/2011conference/program/retrieve.php?\nChetty R, Friedman JN, Hilger N, et al. (2011) How does\nyour kindergarten classroom affect your earnings?\nEvidence from Project STAR. The Quarterly Journal of\nChoi H and Varian H (2012) Predicting the present with\nEinav L, Kuchler T, Levin JD, et al. (2011) Learning from\nseller experiments in online markets. NBER Working\nPaper Series No. 17385. Available at: http://www.nber\nEinav L and Levin JD (2013) The data revolution and eco-\nnomic analysis. NBER Working Paper Series No. 19035.\nGardner MJ and Altman DG (1986) Confidence-intervals\nrather than P-values: Estimation rather than hypothesis-\nGonza\n\u00b4 lez-Bailo\nForthcoming). Assessing the bias in samples of large\nonline networks social networks. Social Networks.\nKeeter S (2012) Presidential address: Survey research, its new\nfrontiers, and democracy. Public Opinion Quarterly 76(3):\nLaney D (2001) 3D data management: Controlling data\nvolume, variety and velocity (META Group File 949).\nAvailable at: http://blogs.gartner.com/doug-laney/files/\nVolume-Velocity-and-Variety.pdf.\nMayer-Scho\n\u00a8 nberger V and Cukier K (2013) Big Data: A\nRevolution that Will Transform How We Live, Work, and\nThink. New York: Houghton Mifflin Harcourt.\nMoat HS, Curme C, Avakian A, et al. (2013) Quantifying\nWikipedia usage patterns before stock market moves.\nPorter TM (2010) Statistics and the career of public reason:\nengagement and detachment in a quantified world.\nIn: Crook T and O'Hara G (eds) Statistics and the\nPublic Sphere: Numbers and the People in Modern\nBritain. New York: Routledge, pp. 32\u00ad47.\nRogers E (2003) Diffusion of Innovations, 5th ed. New York:\nFree Press.\nSavage M and Burrows R (2007) The coming crisis of empir-\nSavage M and Burrows R (2009) Some further reflections on\nthe coming crisis of empirical sociology. Sociology 43(4):\nSchroeder R (2014) Big Data: Towards a more scientific\nsocial science and humanities? In: Graham M and\nDutton WH (eds) Society and the Internet: How\nNetworks of Information are Changing our Lives. Oxford:\nOxford University Press.\nTambe P (2012) How the IT workforce affects returns to IT\ninnovation: Evidence from Big Data analytics. NYU Stern\nSchool of Business. Available at: http://www.krannert.\npurdue.edu/faculty/kkarthik/wise12/papers%5Cwise12_\nZhang X and Zhu F (2006) Intrinsic motivation of open con-\ntent contributors: The case of Wikipedia. In: Workshop on\nInformation Systems and Economics.\nZhang X and Zhu F (2011) Group size and incentives to\ncontribute: A natural experiment at Chinese Wikipedia.\n10 Big Data & Society"
}