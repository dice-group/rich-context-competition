{
    "abstract": "Abstract\nThe serial position effect shows that two interrelated cognitive processes underlie immediate recall of a supraspan word list. The current\nstudy used item response theory (IRT) methods to determine whether the serial position effect poses a threat to the construct validity of\nimmediate list recall as a measure of verbal episodic memory. Archival data were obtained from a national sample of 4,212 volunteers\naged 28\u00ad84 in the Midlife Development in the United States study. Telephone assessment yielded item-level data for a single immediate\nrecall trial of the Rey Auditory Verbal Learning Test (RAVLT). Two parameter logistic IRT procedures were used to estimate item para-\nmeters and the Q1\nstatistic was used to evaluate item fit. A two-dimensional model better fit the data than a unidimensional model, supporting\nthe notion that list recall is influenced by two underlying cognitive processes. IRT analyses revealed that 4 of the 15 RAVLT items (1, 12, 14,\nand 15) were misfit (p , .05). Item characteristic curves for items 14 and 15 decreased monotonically, implying an inverse relationship\nbetween the ability level and the probability of recall. Elimination of the four misfit items provided better fit to the data and met necessary\nIRT assumptions. Performance on a supraspan list learning test is influenced by multiple cognitive abilities; failure to account for the serial\nposition of words decreases the construct validity of the test as a measure of episodic memory and may provide misleading results. IRT\nmethods can ameliorate these problems and improve construct validity.\n",
    "reduced_content": "Immediate List Recall as a Measure of Short-Term Episodic Memory:\nInsights from the Serial Position Effect and Item Response Theory\nBrandon E. Gavett1,2,*, Julie E. Horwitz3,4\n1Department of Psychology, University of Colorado at Colorado Springs, Colorado Springs, CO, USA\n2Department of Neurology and Center for the Study of Traumatic Encephalopathy, Boston University School of Medicine, Boston, MA, USA\n3Psychology Service, Edith Nourse Rogers Memorial Veterans Hospital, Bedford, MA, USA\n4Memorial Health System, Colorado Springs, CO, USA\n*Corresponding author at: Department of Psychology, University of Colorado at Colorado Springs, 4047 Columbine Hall, Colorado Springs, CO 80918, USA.\nE-mail address: bgavett@uccs.edu (B.E. Gavett).\n Keywords: Learning and memory; Assessment; Statistical methods\nIntroduction\nList learning tests are commonly used by neuropsychologists to estimate episodic memory abilities (Rabin, Barr, & Burton,\n2005). In a common paradigm, lists with word lengths that exceed an individual's attention span (supraspan) are presented to an\nexaminee for immediate and delayed free recall. This form of memory assessment has demonstrated high levels of sensitivity\nand specificity to disorders causing memory dysfunction, such as amnestic mild cognitive impairment and Alzheimer's\ndisease (AD; Gavett et al., 2009), temporal lobe epilepsy (Grammaldo et al., 2006), and post-concussive syndrome\nOne commonly reported phenomenon inherent in all supraspan free recall list learning tests is the serial position effect\n(Deese & Kaufman, 1957). This effect describes the pattern by which the probability of a word being recalled varies as a func-\ntion of its position in the list. Items from the beginning and the end of a list are more likely to be recalled than items from the\nmiddle of a list. Over the last several decades, theoretical models have been put forth to explain this phenomenon. Atkinson and\nShiffrin (1968) proposed that recall of items from the beginning of the list (primacy effect) was facilitated due to an increased\nopportunity for these words to be rehearsed, and thus encoded into more stable memory stores (also see Rundus, 1971). The\n# The Author 2011. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.\nAtkinson and Shiffrin model also proposed that recall of items from the end of a list (recency effect) was facilitated because\nthese items were still present in the rehearsal buffer at the time of recall (also see Vallar & Papagano, 1986). This model was\nelaborated upon by Craik and Lockhart (1972), who proposed that rather than simply through increased rehearsal, earlier list\nitems were remembered better because of the increased opportunity for the application of deeper encoding strategies. Later,\nBaddeley (1986) proposed a multi-component model of working memory, by which a central executive was responsible for\nacting upon information in low-level auditory and visual stores (the phonological loop and visuospatial sketch pad, respective-\nly). Baddeley's (1986) model of working memory proposed that, in order for information to be encoded into stable memory\nstores, it must be acted upon by the central executive. These abbreviated descriptions of some of the most influential theoretical\nmodels of human memory simply serve to illustrate the fact that immediately recalling a supraspan list of words is thought to be\ncontrolled by multiple cognitive processes. For the sake of consistency with naming conventions in clinical neuropsychology,\nwe have chosen to use the terms attention and short-term memory to describe primary and secondary memory processes,\nrespectively.\nShort-term memory and attention are dissociable constructs of human cognition. For instance, AD causes considerable\ndamage to the neurobiological substrate of episodic memory, the hippocampal\u00adentorhinal complex, early in the course of\nthe disease (Sperling et al., 2010). Consistent with these known neuropathological changes, the clinical manifestations of\nAD show a disruption in episodic memory (Sperling et al., 2010) with general sparing of basic verbal attention early in the\ncourse of the disease when diagnosis is usually most valuable (Linn et al., 1995). This pattern of cognitive impairment\ncaused by AD can be seen on supraspan list learning tasks. Patients with AD show a recency effect similar to cognitively\nhealthy participants--suggesting intact attention--but show a reduced primacy effect consistent with short-term memory im-\npairment (Gainotti & Marra, 1994; Spinnler, Della Sala, Bandera, & Baddeley, 1988). Despite the evidence for this dissoci-\nation between episodic memory and attention span in patients with AD, list learning test performance is nearly always\ninterpreted based on the total number of words recalled from the list, regardless of the position of the word in the list.\nUnfortunately, a total sum does not identify the specific contributions made by attention versus short-term memory to the\ntotal score. For this reason, there is cause to suspect that typical neuropsychological approaches to list learning test interpret-\nation lack construct validity (Cronbach & Meehl, 1955) for making unbiased estimates of episodic memory ability.\nTo address the potential influence of the serial position effect on immediate list recall, Buschke and colleagues (2006) exam-\nined the use of a scoring system that assigned weights to words recalled based on their position in the list. These authors\nhypothesized that, compared with unit weighting, assigning greater weight to words recalled from short-term memory\nstores (primacy effect) and lesser weight to words recalled from basic attention stores (recency effect) would more accurately\ndistinguish individuals with mild AD from those without AD. The results were as expected: the AD group showed recency but\nnot primacy effects and the weighted list learning score (area under the receiver operating characteristic curve [AUC] \u00bc 0.86)\noffered better discrimination of mild AD from controls than the unweighted score (AUC \u00bc 0.77).\nBuschke and colleagues (2006) showed that the clinical detection of AD could be improved upon by accounting for the\nserial position effect in list learning test scoring and interpretation. However, the Buschke and colleagues (2006) study was\nlimited by the use of theoretically derived item position weightings instead of empirically derived weightings. The serial pos-\nition effect clearly shows that items from the middle of the list are the most difficult to remember, and therefore, an argument\ncould be made that these items should be assigned the most weight. Fortunately, there are empirical methods to evaluate item-\nlevel test data for the purposes of deriving ability estimates.\nItem response theory (IRT) is considered a part of modern psychometric theory, but its origins date back to the 1950s.\nAlthough used extensively in standardized educational assessment (e.g., the Graduate Record Examination), IRT has not fac-\ntored largely into the development and validation of neuropsychological assessment instruments (but see Mungas, Reed, &\nKramer, 2003; Mungas et al., 2010). IRT is based on the theory that each individual item that makes up a test contributes\nin some quantifiable way to the estimation of an underlying unidimensional trait (although multidimensional IRT is possible,\nit is rarely used). Depending on the model chosen, each item can be described by one (difficulty [b]), two (difficulty and dis-\ncrimination [a]), or three (difficulty, discrimination, and guessing [g]) parameters. These parameters allow for the estimation\nof an examinee's underlying ability (u) and the quantification of a test's reliability and measurement error (Embretson & Reise,\n2000) across the spectrum of ability levels. IRT differs from classical test theory (CTT) in that IRT requires stronger assump-\ntions to be met with regard to the underlying latent trait and the items that contribute to its measurement (Lord & Novick, 1968;\nNunnally & Bernstein, 1994). When applied to neuropsychological test construction and validation, IRT can be used to provide\nconverging evidence for construct validity (Embretson & Gorin, 2001), to identify items that are not appropriate measures of\nthe intended latent variable, to create test summary scores that weight items based on parameters such as difficulty, and to\nquantify the test's measurement error across all relevant ability levels (Mungas & Reed, 2000). The similarities and differences\nbetween IRT and CTT and ways that two methods augment one another have been addressed in great detail elsewhere (e.g.,\nEmbretson & Reise, 2000; Zickar & Broadfoot, 2009). Therefore, we will highlight the major advantages of IRT that are rele-\nvant in the current study.\nIn CTT, items on a particular test are usually assigned unit weights and summed to produce a total test score. In neuro-\npsychological applications, that test score is then scaled relative to a normative sample to produce an estimate of an individual's\nability on the trait assumed to be measured by the test. In IRT, tests can be scored using procedures that weight items based on\nthe parameter estimates of the chosen model. Items that are low in information (a function of the chosen parameters and a\nsimilar construct to reliability) may be discarded or re-written to yield more desirable information, which serves to improve\nthe test's standard error of measurement. Thus, IRT allows for an understanding of how each item contributes to the measure-\nment of the underlying construct and to the test's measurement precision. In the case of list learning tests, if it is assumed that\nthe test is intended to measure episodic memory, each item's contribution toward measuring this trait can be evaluated and\nitems that do not appear valuable can be discarded. If lower-information items are not discarded, they are not weighted as\nheavily as items that possess higher information when deriving ability estimates based on the underlying construct. While\nIRT may be beneficial for enhancing the construct validity of list recall, the serial position effect, by definition, poses a chal-\nlenge for IRT implementation. In addition to unidimensionality, IRT models also assume local independence; that is, the idea\nthat an examinee's response to an item is based solely on his or her underlying ability level and not some other factor. In the\ncase of list learning performance, the serial position effect clearly demonstrates that item position influences probability of\nrecall, and thus, the local independence assumption appears to be violated. This will be a topic of investigation in the\ncurrent study.\nBased on the list learning research summarized above, it is believed that, consistent with the proposed mechanisms under-\nlying the serial position effect, the number of words immediately recalled from a list is influenced by two related cognitive\nability constructs: attention and short-term episodic memory. If true, this raises questions about the contributions of later\nitems in the list to the test's construct validity as a measure of episodic memory. We hypothesize that a two-factor model\nof list learning performance will more closely approximate the item-level list learning data than a unidimensional model.\nWe also predict that an IRT model can help to identify items that are more affected by attention than memory and that by\neliminating these confounding items, the test's construct validity as a measure of memory will be improved. More specifically,\nwe hypothesize that performance on later items in the list of words is largely a function of attention and therefore contributes\nlittle information about short-term memory abilities. Ignoring these items is not expected to change the measurement properties\nof the test under a unidimensional model of memory. Our goal in the current paper is to test these hypotheses in a sample of\ncognitively healthy adults and to establish the test's psychometric properties under an IRT model. This approach can help to\ndetermine whether the test has acceptable construct validity and measurement invariance for future research in clinical\nsamples.\nMethod\nParticipants and Procedure\nParticipants were volunteers in the Midlife Development in the United States-II (MIDUS-II; Ryff & Lachman, 2007) study,\na follow-up to a national survey of non-institutionalized adults selected by random-digit dialing (Brim, Ryff, & Kessler, 2004).\nPart of the MIDUS-II, completed between 2004 and 2006, included a computer-assisted telephone cognitive assessment using\nthe Brief Test of Adult Cognition by Telephone (Tun & Lachman, 2006) in a sample of 4,212 participants. MIDUS-II\nCognitive functioning archival data are publicly available through the Inter-University Consortium for Political and Social\nResearch website (http://dx.doi.org/10.3886/ICPSR04652). Data were examined for missingness and other problems, as\nflagged by the MIDUS-II researchers. Cases that were flagged as problematic (for any cognitive test) or had missing data\non the immediate recall trial of the list learning test were excluded. We extracted item-level responses from the immediate\nrecall trial of the list learning test. Data were recoded in a binary fashion to indicate whether each item in the 15-word list\nwas recalled by the participant (0 \u00bc not recalled, 1 \u00bc correctly recalled).\nMeasures\nThe list learning test administered to participants in the MIDUS study is a telephone version of the Rey Auditory Verbal\nLearning Test (RAVLT; Rey, 1964; Taylor, 1959), consisting of one immediate recall trial and one delayed recall trial.\nAfter checking for adequate hearing, the 15 list items were read to the participants with a 1-s pause between each word.\nThe participants were then given 90 s to freely recall as many words as possible from the list, in any order. Correct responses,\nintrusions, and repetitions were recorded, but only correct responses on the immediate recall trial were used for the\npresent study.\nStatistical Analysis\nData analysis was conducted using R version 2.11.1 (R Development Core Team, 2010), including the ltm package for IRT\nanalyses (Rizopoulos, 2006). For all significance tests, we set a \u00bc 0.05. We calculated the mean score for each of the 15 items\nand regressed these scores onto the item sequence number (1\u00ad15) using a quadratic term to examine the serial position effect.\nThe unidimensionality assumption of IRT was tested using modified parallel analysis (Drasgow & Lissak, 1983), a technique\nthat employs Monte Carlo simulation to compare the matrix of tetrachoric correlations observed in the actual data under a spe-\ncified IRT model to the simulated data (averaged across 200 simulations) under the same IRT model with unidimensionality\nassumed. We examined eigenvalues from the resulting scree plots and compared our results with the null hypothesis of no\ndifference between the second eigenvalues of the observed data and the simulated data. We also conducted a visual examin-\nation of the scree plots in order to compare them with the exemplars depicted in Drasgow and Lissak (1983) for a determination\nof the robustness of the IRT model to potential violations of the unidimensionality assumption.\nFor all IRT analyses, we selected a two parameter logistic (2PL) model (see, e.g., Edwards, 2009), which estimates the dif-\nficulty (b) and discrimination (a) parameters of each item. These parameter estimates were used to create item characteristic\ncurves, item information curves, test information curves, and to calculate the test's standard error of measurement. For all IRT\nanalyses, we focused on estimating ability levels (u) ranging from 3 SD above to 3 SD below the mean of an assumed normal\nWe examined item fit using the Q1\nstatistic (Yen, 1981), which approximates a x2 distribution. Due to the very large size of\nour sample and associated statistical power, it is likely that most, if not all, item fit statistics will be found to either under or over\nfit the model to a statistically significant degree (Reise, 1990). Therefore, instead of relying on an assumed x2 distribution to\ntest for significant item misfit, we compared the real data with a 100-trial Monte Carlo simulation of the data under the null\nhypothesis (Yen, 1981). Significant differences in the Q1\nstatistic between the real data and the simulated data suggest item\nmisfit.\nBecause strong associations between item discrimination and item position suggest that the local independence assumption\nhas been violated, we calculated the correlations among item position, item discrimination, and squared item discrimination to\ncheck for violations of this assumption (Reise & Waller, 1990).\nResults\nOf the 4,212 participants with data in the MIDUS-II Cognitive functioning data set, 239 were excluded due to missing data\non the immediate word list learning task or due to questionable test validity on any portion of the cognitive evaluation. The\nWe found evidence for the serial position effect in the data, as can be seen in Fig. 1, which illustrates the accuracy obtained\nfor each item in the current sample. The U-shaped trend line in Fig. 1 depicts the quadratic function that best fits these item\naccuracy data. The list learning data in the current sample show clear evidence of the serial position effect.\nHaving established the presence of a serial position effect in the current data, we tested the hypothesis that follows from this\neffect: that performance on a single 15-item word list recall trial is better explained by a model that accounts for two ability\nconstructs (i.e., short-term episodic memory and attention) rather than a model that accounts for only one (i.e., short-term\nepisodic memory). Using modified parallel analysis procedures (Drasgow & Lissak, 1983), a scree plot derived from the\ncurrent data was compared with a Monte Carlo simulation of the data under an assumed unidimensional model (Fig. 2). As\ncan be seen in Fig. 2, there are two eigenvalues .1.0. When compared with the simulated unidimensional data, the second\neigenvalue of the observed data (1.19) is significantly larger than the second eigenvalue of the simulated data (0.31), p \u00bc\n.005. We also compared the model fit of the actual data under a two-factor model with the model fit of the actual data\nunder a unidimensional model. The results suggested that the two-dimensional model offered a significantly better fit to the\ndata than the unidimensional model, D (df \u00bc 15) \u00bc 516.2, p , .001, with the second factor accounting for 8.2% of the total\nvariance. Although these results provide evidence to suggest that the current list learning data are affected by two underlying\ncognitive constructs, the violation of the unidimensionality assumption is not so severe as to invalidate the use of a unidimen-\nsional IRT model according to guidelines set forth by Drasgow and Lissak (1983).\nA 2PL IRT model yielded the item characteristic (top row) and item information curves (bottom row) shown in Fig. 3. Of\nparticular salience, in the top left panel of Fig. 3, is the negative slope of the item characteristic curves for items 14 and 15,\nwhich indicates that the lower an individual's memory ability, the more likely the person is to correctly recall these items; this\neffect is especially pronounced in the final item. Based on these data, 75% of individuals with episodic memory abilities falling\n3 SD below the average will recall the final two words of this 15-item list. In contrast, items 14 and 15 are the least likely to be\nrecalled by individuals with extremely high episodic memory abilities (z-scores  3.0). This pattern was replicated when the\ndata were re-analyzed using a random-halving of the sample (data not shown). As a result of this undesirable pattern, these two\nitems were excluded from further IRT analyses, which is a common approach to test development and refinement (Embretson\n& Reise, 2000). Item fit statistics, shown in Table 1, revealed significant item misfit for items 1, 12, and 15. The item\nFig. 2. Scree plot depicting the results of the modified parallel analysis for the 15-item list. The real data (solid line) show evidence of a two-factor solution\nbased on the second eigenvalue .1.0 and the difference between the second eigenvalues of the real data the simulated data (dashed line).\nFig. 1. The serial position effect in the current list learning data. The trendline represents the quadratic function that best fits the data. Error bars represent 95%\nconfidence intervals.\ncharacteristic and information curves for these items and item 14 are highlighted in Fig. 3. In Figs 3\u00ad6, the x-axis represents the\nunderlying ability (u) measured by the test items, scaled as a z-score (M \u00bc 0.0, SD \u00bc 1.0), such that a u value of zero represents\nan average ability level and higher u z-scores represent higher ability estimates. The parameter estimates for the 15-item version\nof the RAVLT are likely biased by a violation of the local independence assumption; the correlations between item position and\nitem discrimination were quite high (rIP.a\nBecause the findings described above are consistent with our hypothesis, we sought to determine whether excluding the\nscores obtained on items 14 and 15 would change the measurement properties of the test, the fit of items 1 and 12, and the\nposition-discrimination correlations. The item characteristic and item information curves from the 13-item model are presented\non the right side of Fig. 3. The difficulty and discrimination parameters of the 13 items in this model are essentially unchanged\ncompared with the original 15-item model (Table 1). We also derived the total test information functions for both the 13- and\n15-item lists from the sum of the item information curves and used these to calculate the standard error of measurement for both\nversions of the test across the range of ability levels (Fig. 4). The elimination of the final two items from the 15-item test had no\neffect on the test's information or measurement precision. However, an examination of the item fit statistics again revealed\nsignificant misfit for items 1 and 12 (Table 1). Furthermore, the correlations between item position and item discrimination\nremained high (rIP.a\n= -.56), suggesting a violation of the local independence assumption. Therefore, a third\nFig. 3. Item characteristic (top row) and item information (bottom row) curves for the 15-item (left column) and 13-item (right column) 2PL IRT models. In the\n15-item model, items 1, 12, 14, and 15 (broken lines) function dissimilarly from the remaining items (solid lines). Removing items 14 and 15 to yield a 13-item\nlist did not improve the fit of items 1 and 12.\nItem fit analyses for the 11-item model revealed no significant misfit fit for any of the 11 items (Table 2). The exclusion\nof items 1, 12, 14, and 15 also kept the local independence assumption from being violated by attenuating the position-\ndiscrimination correlations (rIP.a\n= -.08). Item characteristic curves and item information curves for the\n11-item list are displayed in Fig. 5. The 11-item test information function is plotted along with the 11-item standard error\nof measurement in Fig. 6.\nDiscussion\nThe serial position effect is one of the most well-established phenomena in the study of human memory. There is a large\nbody of literature to suggest that immediate recall of a supraspan list of words is affected by two interrelated cognitive abilities,\nwhich we refer to here as attention and short-term episodic memory. Although this effect is known to most psychologists,\nneuropsychological test batteries regularly include tests of supraspan list recall as a measure of episodic memory without\ntaking into account the possible influence of the serial position effect. The current results caution against the sole reliance\nupon a simple sum of items recalled, as failing to account for the position of words in a list reduces the construct validity\nof list recall as a measure of episodic memory. As expected, the final two list items were found to be inappropriate in a\nTable 1. IRT parameter estimates and item fit statistics for the 15- and 13-item models\nItem number Stimulus 15-item model 13-item model\np-value b a Q1\np-value\nNote: IRT \u00bc item response theory.\nFig. 4. Test information and standard error of measurement for the 15-item (left) and 13-item (right) 2PL IRT models. The similarity between the curves\nsuggests that the measurement properties of the 15-item model are not impacted by the removal of the final two items.\nunidimensional model of episodic memory ability. The error introduced by these items is egregious; individuals with very low\nability estimates are more likely to recall the last two items than individuals with very high ability estimates (Fig. 3). However,\nwe also found evidence of misfit for items 1 and 12. Because item 1 is strongly associated with the primacy effect, its presence\ncontributes to the violation of the local independence assumption, which precludes accurate parameter estimation under an IRT\nmodel. As the Atkinson and Shiffrin (1968) model suggests, the probability of recalling item 1 may be more strongly related to\nits position than to the examinee's underlying episodic memory skills. This may also be the case for item 12 with respect to the\nrecency effect, but this explanation does not clarify why item 13, which should be expected to produce even stronger recency\neffects, was found to fit well within the model.\nThese findings are generally consistent with those reported by Buschke and colleagues (2006), who found that a weighting\nscheme that assigned increasingly greater credit to earlier items was more effective at detecting episodic memory impairment\ncaused by AD than scoring with unit weighting. Although Buschke and colleagues (2006) correctly diverted points away from\nFig. 5. Item characteristic (left) and item information (right) curves for the 11-item 2PL IRT model.\nFig. 6. Test information and standard error of measurement for the 11-item 2PL IRT model.\nthe final list items, the current findings suggest that assigning the greatest amount of weight to the earliest items may be a sub-\noptimal approach to item weighting, especially considering that item 1, which receives the most weight under the Buschke and\ncolleagues (2006) scoring system, did not fit our model of episodic memory.\nThe current results are also consistent with other research investigating the serial position effect in clinical samples. Patients\nwith neurological illnesses that impair short-term memory through damage to the medial temporal lobes (e.g., AD) have a\nmarkedly reduced primacy effect with a relatively small change in recency effect during supraspan list recall (Gainotti &\nMarra, 1994; Spinnler et al., 1988). Based on our results, the lower an individual's memory ability, the more likely he or\nshe is to recall items 14 and 15. Assigning equal weight to items 14 and 15, or greater weight to item 1 (as suggested by\nBuschke et al., 2006), may impede accurate interpretation of episodic memory test results, which could influence diagnosis\nand possible treatments.\nThe current results illustrate some of the benefits of using both IRT and CTT methods for neuropsychological test construc-\ntion and validation and show why it is necessary for more neuropsychological instruments to be calibrated using IRT models. In\nthe meantime, the utility of list learning tests may be improved through re-norming using methods that do not assign points for\ncorrect responses to misfit items. Without further IRT analyses or re-norming, accurate interpretation of list learning perform-\nance may be facilitated by close examination of the primacy and recency effect produced by test-takers. The Second Edition of\nthe California Verbal Learning Test (Delis, Kaplan, Kramer, & Ober, 2000), for example, offers users the option of deriving\nnormatively adjusted values for primacy and recency effects. Valuable clinical insight into an examinee's episodic memory\nabilities may be facilitated by the use of a process approach to list learning interpretation; that is, with consideration of the\nserial position and corresponding difficulty of individual items rather than simply interpreting the total number of words\nrecalled.\nDespite the information provided by IRT about the relative contribution of items in the derivation of latent ability estimates,\nIRT is not a psychometric panacea. For the measurement of short-term episodic memory, the IRT-derived 11-item list has\nbetter construct validity than the 15-item list, but nevertheless, it does not possess desirable measurement properties. The\ntest information and the standard error of measurement functions (Fig. 6) show that the 11-item list is most appropriate for\nestimating abilities that fall between z-scores of 20.5 and 2.0; however, at these levels of u, the standard error of measurement\nis approximately 1.0. In other words, the widths of the error bars for the 95% confidence intervals around u are approximately\n2.0 SD in both the positive and the negative directions. This imprecision suggests that a single trial immediate list recall test\ndoes not provide clinically useful information.\nThe current study has several limitations that may affect its applicability to clinical situations. First, the test data were col-\nlected via telephone evaluations and may not generalize to in-person testing scenarios. In addition, the methods used to collect\nthe current data were not specifically designed to address the current hypotheses. Instead of randomizing the order of the 15\nwords, the same list order was administered to all participants. It is possible that the item parameters estimated in the current\nIRT model may be affected if some of the words in the list are inherently easier to remember than other words. For example,\ndifferences in word frequency (e.g., Thorndike & Lorge, 1944) may cause some words to be inherently more easy to remember\nthan other words (Roodenrys & Miller, 2008); it is possible that this may account, in part, for the unexpected findings related to\nitems 12 and 13. However, because evidence for the serial position effect was found, it is unlikely that these factors were sub-\nstantial confounders. It is possible, however, that the somewhat unexpected findings related to the fit of items 12 and 13 are an\nartifact of the current sample. Although a random-halving of the current data yielded results consistent with the findings in the\nfull sample, replication using an independent sample can help to ensure that the current findings are not an artifact of sampling\nTable 2. IRT parameter estimates and item fit statistics for the 11-item model\np-value\nNotes: IRT \u00bc item response theory; CI \u00bc confidence interval.\nanomalies. The unusual finding relating to items 12 and 13 may also suggest that the parameter estimates have been impacted\nby a small degree of local dependence, despite the fact that the removal of the four items seemingly reduced local dependence\nby a significant degree. Another potential limitation of this study is that participants were administered a single-trial list learn-\ning task, which is not the most common paradigm in clinical neuropsychology. Instead, most list learning tests present multiple\nimmediate recall trials. It is unclear whether the problems related to construct validity can be attenuated by the use of scores\nthat summarize multiple recall trials instead of a single trial. Extension of the current methods to include in-person, multitrial\nlist learning tests is likely to yield results that are more clinically meaningful and, ideally, more precise. Finally, these results\nare based on a sample of cognitively healthy adults, so external validation in a known clinical group was not possible using the\nMIDUS-II data. The extent to which these results are clinically meaningful will depend in part upon replication and extension\nin a memory disordered sample (e.g., AD). However, valid assessment of memory and other constructs in clinical samples is\ndependent on construct validity and the assumption of measurement invariance (Borsboom, 2006; Meredith & Teresi, 2006);\ntherefore, the current results point to areas in which the validity of memory measurement in clinical samples may be improved\nupon. Future studies should seek to determine whether there is evidence of item- and test-level measurement invariance based\non clinical and other groupings.\nIn conclusion, we tested the hypothesis that a single supraspan list learning test is affected by both memory and attention, as\nsuggested by research into the serial position effect. If true, the contributions of attention and memory to the total test score\ncannot be separated by traditional scoring methods (i.e., total score based on unit weighting); as a result, the recency effect may\nlead to artificially inflated test scores, especially in individuals with low episodic memory ability (e.g., caused by AD). The\ncurrent results suggest that removing four items from the RAVLT, under an IRT model, does not reduce the psychometric prop-\nerties of the test as a measure of latent memory skills, but does reduce the confounding effects of attention on this test. These\nfindings suggest a potential method for improving the assessment of episodic memory ability in adults; applying these findings\nto clinical groups should be a focus of future research.\nFunding\nthe National Operating Committee on Standards for Athletic Equipment; and by an unrestricted gift from the National Football\nLeague. The funding sources were not involved in the preparation, review, or approval of this article.\nConflict of Interest\nNone declared.\nReferences\nAtkinson, R. C., & Shiffrin, R. M. (1968). Human memory: A proposed system and its control processes. In K. W. Spence, & J. T. Spence (Eds.), The psych-\nology of learning and motivation (Vol. 2, pp. 89\u00ad195). New York: Academic Press.\nBaddeley, A. D. (1986). Working memory. London: Oxford University Press.\nBazarian, J. J., Wong, T., Harris, M., Leahey, N., Mookerjee, S., & Dombovy, M. (1999). Epidemiology and predictors of post-concussive syndrome after\nBrim, O., Ryff, C., & Kessler, R. (2004). How healthy are we?: A national study of well-being at midlife. Chicago: University of Chicago Press.\nBuschke, H., Sliwinski, M. J., Kuslansky, G., Katz, M., Verghese, J., & Lipton, R. B. (2006). Retention weighted recall improves discrimination of Alzheimer's\nCraik, F. I. M., & Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11,\nDeese, J., & Kaufman, R. (1957). Serial effects in recall of unorganized and sequentially organized verbal material. Journal of Experimental Psychology, 54,\nDelis, D. C., Kramer, J. H., Kaplan, E., & Ober, B. A. (2000). The California Verbal Learning Test (2nd ed.). San Antonio, TX: Psychological Corporation.\nDrasgow, F., & Lissak, R. (1983). Modified parallel analysis: A procedure for examining the latent dimensionality of dichotomously scored item responses.\nEdwards, M. C. (2009). An introduction to item response theory using the Need for Cognition Scale. Social and Personality Psychology Compass, 3/4,\nEmbretson, S., & Gorin, J. (2001). Improving construct validity with cognitive psychology principles. Journal of Educational Measurement, 38, 343\u00ad368.\nEmbretson, S. E., & Reise, S. P. (2000). Item response theory for psychologists. Mahwah, NJ: Lawrence Erlbaum Associates.\nGainotti, G., & Marra, C. (1994). Some aspects of memory disorders clearly distinguish dementia of the Alzheimer's type from depressive pseudo-dementia.\nJournal of Clinical and Experimental Neuropsychology, 16, 65\u00ad78.\nGavett, B. E., Poon, S. J., Ozonoff, A., Jefferson, A. L., Nair, A. K., Green, R. C., et al. (2009). Diagnostic utility of the NAB List Learning test in Alzheimer's\ndisease and amnestic mild cognitive impairment. Journal of the International Neuropsychological Society, 15, 121\u00ad129.\nGrammaldo, L. G., Giampa, T., Quataro, P. P., Picardi, A., Mascia, A., Sparano, A., et al. (2006). Lateralizing value of memory tests in drug-resistant temporal\nLinn, R. T., Wolf, P. A., Bachman, D. L., Knoefel, J. E., Cobb, J. L., Belanger, A. J., et al. (1995). The `preclinical phase' of probable Alzheimer's disease.\nLord, F. M., & Novick, M. R. (1968). Statistical theories of mental test scores. Reading, MA: Addison-Wesley.\nMeredith, W., & Teresi, J. A. (2006). An essay on measurement and factorial invariance. Medical Care, 44, S69\u00adS77.\nMungas, D., Beckett, L., Harvey, D., Farias, S., Reed, B., Carmichael, O., et al. (2010). Heterogeneity of cognitive trajectories in diverse older persons.\nMungas, D., & Reed, B. R. (2000). Application of item response theory for development of a global functioning measure of dementia with linear measurement\nMungas, D., Reed, B., & Kramer, J. (2003). Psychometrically matched measures of global cognition, memory, and executive function for assessment of\nNunnally, J. C., & Bernstein, I. H. (1994). Psychometric theory (3rd ed.). New York: McGraw Hill.\nR Development Core Team. (2010). R: A language and environment for statistical computing [computer software]. Vienna, Austria: R Foundation for\nStatistical Computing. Retrieved from http://www.R-project.org.\nRabin, L. A., Barr, W. B., & Burton, L. A. (2005). Assessment practices of clinical neuropsychologists in the United States and Canada: A survey of INS, NAN,\nReise, S. P. (1990). A comparison of item-and person-fit methods of assessing model-data fit in IRT. Applied Psychological Measurement, 14, 127\u00ad137.\nReise, S. P., & Waller, N. G. (1990). Fitting the two-parameter model to personality data. Applied Psychological Measurement, 14, 45\u00ad58.\nRey, A. (1964). L'examen clinique en psychologie. Paris: Presses Universitaires de France.\nRizopoulos, D. (2006). ltm: An R package for latent variable modelling and item response theory analyses. Journal of Statistical Software, 17, 1\u00ad25.\nRoodenrys, S., & Miller, L. M. (2008). A constrained Rasch model of trace redintegration in serial recall. Memory and Cognition, 36, 578\u00ad587.\nRundus, D. (1971). Analysis of rehearsal processes in free recall. Journal of Experimental Psychology, 89, 63\u00ad77.\nRyff, C. D., & Lachman, M. E. (2007). National Survey of Midlife Development in the United States (MIDUS II): Cognitive Project, 2004\u00ad2006 [Data file].\nICPSR25281-v1. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 13-07-2010. Retrieved from http://www.icpsr.\nSperling, R. A., Dickerson, B. C., Pihlajamaki, M., Vannini, P., LaViolette, P. S., Vitolo, O. V., et al. (2010). Functional alterations in memory networks in early\nSpinnler, H., Della Sala, S., Bandera, R., & Baddeley, A. (1988). Dementia, ageing, and the structure of human memory. Cognitive Neuropsychology,\nTaylor, E. M. (1959). Psychological appraisal of children with cerebral deficits. Cambridge, MA: Harvard University Press.\nThorndike, E. L., & Lorge, I. (1944). The Teacher's word book of 30,000 words. New York: Teachers College, Columbia University.\nTun, P. A., & Lachman, M. E. (2006). Telephone assessment of cognitive function in adulthood: The Brief Test of Adult Cognition by Telephone. Age and\nVallar, G., & Papagano, C. (1986). Phonological short-term store and the nature of the recency effect: Evidence from neuropsychology. Brain and Cognition, 5,\nYen, W. M. (1981). Using simulation results to choose a latent trait model. Applied Psychological Measurement, 5, 245\u00ad262.\nZickar, M. J., & Broadfoot, A. A. (2009). The partial revival of a dead horse? Comparing classical test theory and item response theory. In C. E. Lance, & R. J.\nVandenberg (Eds.), Statistical and methodological myths and urban legends: Doctrine, verity and fable in organizational and social sciences (pp. 37\u00ad59).\nNew York: Routledge."
}