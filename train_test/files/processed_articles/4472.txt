{
    "abstract": "Abstract\nTwo multi-instrument investigations in a university clinic/lab provided an opportunity to explore the impact of reducing the\nnumber of response alternatives in a scale measuring vocational personality traits. In a simulation study, a standard computer-\nbased administration provided a numeric scale for each item ranging from 0 to 10. The tests were then rescored to simulate\nthe effect of only three choices. For the follow-up study, two versions of the scale were created, one with two response\noptions and the other with six response options, and were randomly assigned to participants. Typical relationships were\nevident between the vocational personality traits and scores on standard measures of core personality traits with negligible\nimpact from reducing the number of response alternatives. Neither the ability of the participants nor the self-reported\ndistress had a measurable impact on the utility of the results with the reduction in response alternatives.\n",
    "reduced_content": "sgo.sagepub.com\nArticle\nThe optimal number of response options to maximize reli-\nability and validity in affective assessment scales is a long-\nstanding debate. Bloom, Fischer, and Orme (2003), for\nexample, noted that available research suggests five to seven\nresponse categories but add that a 9-point scale may be\nadvantageous if the user is capable of making the distinc-\ntions. Krieg (1999) described statistical adjustments to com-\npensate for problems with coarse measurement scales,\nconcluding that increasing the number of response options\nwould be a better choice.\nIn a general summary of the literature in questionnaire\nwere most commonly used and that enhanced psychometric\nqualities, including reliability and validity, were suggested as\nthe justification for including multiple response categories.\nRodgers,Andrews, and Herzog (1992) investigated the effect\nof using 2 to 10 response categories and concluded that\nexpected values of validity coefficients increased by approx-\nimately .04 with each additional response option. In contrast,\neffect of using response categories ranging from 2 to 9 and\nfound negligible impact on reliability when the number of\nresponse categories was increased.\nWhile Cook, Heath, Thompson, and Thompson (2001)\nnoted that most studies appear to support the desirability of a\nlarger number of response categories, they also cautioned\nthat when more score intervals are available, there is an\nimplicit assumption that the participant has the cognitive\ncapability to process the meaning of the intervals. When that\nassumption is untenable, the outcome, in addition to a longer\nadministration time, can be a reduction in score reliability. In\nthe background of discussions about the optimal number of\nresponse categories there are two other questions. One is the\nlong-standing debate about whether these scales produce\nonly ordinal data that do not support parametric statistical\nanalysis. Jamieson (2004), for example, succinctly argued\nfor ordinal interpretation, noting that the average of \"fair\"\nand \"good\" is not \"fair-and-a-half.\" It is, however, a common\npractice for researchers to assume that Likert categories are\n\"close enough\" to interval-level measurement, consistent\nwith Blaikie's (2003) position that interval-level statistics\ncan be justified if used with caution. Knapp (1990) summa-\nrized the pros and cons of ordinal and interval assumptions in\nscales using Likert categories, including challenging a belief\nthat more response categories automatically enhance the\nappropriateness of parametric analysis.\n1University of Nevada, Las Vegas, USA\nCorresponding Author:\nW. Paul Jones, Department of Educational Psychology and Higher\nEducation, University of Nevada, 4505 S. Maryland Parkway, Las Vegas,\nEmail: jones@unlv.nevada.edu\nOptimal Number of Questionnaire\nResponse Categories: More May\nNot Be Better\nW. Paul Jones1 and Scott A. Loe1\n Keywords\nbehavioral sciences, educational research, education, social sciences, careers, educational measurement & assessment,\neducational psychology & counseling, measurement and scaling methods, research methods\n2 SAGE Open\nIn addition to questions about whether Likert response\ncategories represent equal intervals, there is an underlying\nissue of inconsistency in category interpretation between\nindividuals contributing to misleading results when responses\nfrom research participants are aggregated for analysis. To\nillustrate, on a 4-point scale ranging from 1 = highly unsatis-\nfied to 4 = highly satisfied, Participant A marks 3 (satisfied)\nand Participant B marks 4 (highly satisfied). Arguably, the\ntwo participants could in fact be expressing exactly the same\ndegree of experience or product satisfaction with the differ-\nence in their responses reflecting an idiosyncratic interpreta-\ntion of the response category. It would be reasonable to\nassume that what it takes for Participant B to feel \"highly\nsatisfied\" would be consistent on other questions or on the\nsame question over time. But combining the responses of\nParticipant A and Participant B as a part of a group measure\nof central tendency is inherently flawed regardless of whether\nthe central tendency is reported as a median (ordinal) or as a\nmean (interval).\nLittle attention has been given to the potential for flawed\naggregation in the research literature. It would, in fact, be\nquite difficult to design a study to address this concern with-\nout some direct, perhaps physiological, indicator of underly-\ning satisfaction. While it is generally assumed that increasing\nthe number of response categories in Likert items enhances\nthe psychometric properties of a scale, it is possible that\ndoing so simply increases the likelihood that psychologically\nhomogeneous response categories are arbitrarily divided.\nStated in the opposite direction, reducing the number of\nresponse categories could have a positive effect on validity\nof aggregated data by reducing the number of opportunities\nfor inappropriately treating different perceptions of response\ncategories as if they were the same.\nData obtained in two multi-instrument investigations in a\nuniversity clinic/lab provided the opportunity for further\ninvestigation of the effect on psychometric characteristics of\na scale when the number of alternative responses is varied.\nFirst, a simulation study was conducted comparing the effect\nof using 3 and 10 response categories on a visual analog\nscale of vocational personality traits. This was followed by a\nstudy in which participants were randomly assigned to com-\nplete one of two versions of the same vocational personality\ntrait measure, one version with only 2 response categories or\nthe other version with 6 response categories. The studies\nwere designed to address two primary research questions:\nResearch Question 1: Is there a significant impact on\nmeasurement statistics contingent on the number of\nresponse options?\nResearch Question 2: Is there a significant difference in\nthe prediction of related external criteria contingent on\nthe number of response categories?\nThese questions were selected, in part, with the belief that\nunless there were marked differences in favor of the larger\nnumber of response categories, researchers might be better\nserved with questionnaires using fewer categories and thus\nreducing the influence of inter-individual inconsistencies in\ncategory interpretation on the resulting statistical estimates.\nSimulation Study\nMethod\nParticipants. Data for the simulation study were provided by\na total of 77 undergraduate students attending an urban,\nsouthwestern university in the United States. Participants\nwere volunteers who chose this study from several different\nprojects available to meet a research requirement for under-\ngraduate courses in educational psychology. The majority\nEthnic backgrounds were as follows: Caucasian (66%), Afri-\ncan American (10%), Hispanic American (6%), Asian\nAmerican (5%), and Other or no report (13%). The primary\nlanguage of the participants in the simulation study was Eng-\nlish (92%). The range of self-reported grade point average of\nInstrumentation. CogStyle, based on an artificial intelligence\nmodel postulated by Lowen (1982), is a scale designed to\nfacilitate equitable assessment of personality traits among\npersons with and without a disabling visual condition (Jones,\n1996). The items provide direct assessment of the six voca-\nRealistic, Investigative, Artistic, Social, Enterprising, and\nConventional. The item format uses paired comparisons of\nsix adjectives, one for each for the six Holland dimensions,\nand then paired comparisons of six action verbs, each also\ncorresponding to one of the Holland dimensions. Adjectives\nare practical, curious, flexible, sympathetic, ambitious, and\nefficient. The actions are fixing things, investigating things,\ndesigning things, assisting others, persuading others, and\norganizing things.\nIn addition to raw scores on each dimension, results on\nthe Holland scales are typically reported as two- or three-\nletter codes, indicating the strongest preferences. For exam-\nple, an IE code indicates that the person's highest score was\non the Investigative scale and the next highest score was on\nthe Enterprising scale, and this ranking then becomes the\nbasis for career exploration. The use of the ranking proce-\ndure enables some expected comparability among different\nvocational interest scales that differ in length and/or type of\nstimuli used for the assessment and does not require norma-\ntive transformation.\nThe NEO Five Factor Inventory (NEO-FFI; Costa &\nsuring each of the \"big-five\" trait dimensions of\nJones and Loe 3\nNeuroticism, Extraversion, Openness, Conscientiousness,\nand Agreeableness. Each scale consists of 12 items about\ntypical behaviors or reactions that are rated according to a\n5-point Likert scale, ranging from \"strongly disagree\" to\n\"strongly agree.\" The questionnaire yields T-scores for\neach of the five trait dimensions. Reliabilities reported in\nthe examiner's manual for the five dimensions range from\nThe Woodcock-Johnson Tests of Cognitive Ability\n(WJ-III; Woodcock, McGrew, & Mather, 2001) is an indi-\nvidually administered measure of general intellectual ability\nand specific cognitive abilities in children, adolescents, and\nadults, aged 2 years through 94 years. The battery yields a\nGeneral Intellectual Ability index (GIA) that is a measure of\ng along with cluster scores representing the broad Cattell-\nHorn-Carroll (CHC) abilities and individual subtest scores.\nProcedure.A series of studies conducted in a university\nputer-based test administration provided an adventitious\nopportunity to investigate the effect of the number of\nresponse options in a visual analog scale. The 30 items\nassessing the Holland dimension in the CogStyle scale\nwere adapted to conform with software requirements,\nresulting in a response mode in which the participant\nexpressed preference in the paired comparisons of adjec-\ntives and action verbs on a 0 to 10 visual analog scale. The\ntotal score for each dimension was weighted by the extent\nof preference.\nTo illustrate, one of the adjectives for the Realistic score\nis \"practical.\" One of the adjectives for the Investigative\nscore is \"curious.\" The item was presented on the computer\nscreen to participants with \"practical\" on the 0 end of the\nvisual analog scale and \"curious\" on the 10 end of the scale\nwith instructions for participants to indicate their preference.\nIf the participant clicked the 0, one point was added to the\nRealistic score; if the participant clicked the 10, one point\nwas added to the Investigative score. If the participant clicked\nthe 1, .9 was added to the Realistic score; if the participant\nclicked the 9, .9 was added to the Investigative score, and so\nforth. Participants who clicked the mid-point, 5, had .5 added\nto the Realistic and to the Investigative score.\nTo estimate the effect of a significant reduction in response\noptions, the scales were then rescored with two scenarios to\nsimulate responses if there had been only three options to\nexpress the degree of preference. For the first simulation sce-\nnario, using the \"practical\" or \"curious\" example in the pre-\nvious paragraph, clicking 0 to 4 resulted in one point added\nto the Realistic score; clicking 6 to 10 resulted in one point\nadded to the Investigative score. Clicking the mid-point of\nthe scale resulted in .5 point added to each of the two scores.\nRecognizing that it would be at least equally plausible to\nextend the range for the simulated middle score, a second\n3-option scenario used a different rubric. In the second sce-\nnario, when simulating the scoring of the \"practical\" or\n\"curious\" stimulus, responses from 0 to 3 added one point to\nthe Realistic score, responses from 7 to 10 added one point\nto the Investigative score, responses of from 4 to 6 added .5\nto each of the two scores, and so forth.\nAn appointment session for each participant began with\nthe completion of demographic information and review and\nsignature on human subjects consent forms. Individual\nadministration of selected cognitive scales was then pro-\nvided by school psychology graduate students, followed by\ncompletion of a series of computer-administered scales,\nincluding the vocational personality scale that is the focus of\nthis study.\nResults of the Simulation Study\nTable 1 displays the descriptive information for the 10-option\nand the two simulated 3-option scenarios for each of the six\nvocational personality scales. A perusal of Table 1 suggests\nthat the different response options did not appear to have a\nmarked effect on the obtained mean scores. Paired sample t\ntest analyses found three statistically significant differences\nin the mean scores obtained from the scoring alternatives\nwith Bonferroni correction for the 18 comparisons. Two of\nthe statistically significant differences were on the\nEnterprising scale: 10-option versus 3-option-scenario 1,\nsignificant difference on the Conventional scale: 10-option\nTable 1. Simulation Study: Descriptive Statistics for Vocational Personality Trait Scores With 10-Option and Simulated 3-Option\nRealistic Investigative Artistic Social Enterprising Conventional\n M (SD) M (SD) M (SD) M (SD) M (SD) M (SD)\nNote: Statistically significant difference between mean scores only on Enterprising scale. 3-Option: 1 is Scenario 1 with original scores of 5 coded as 2 in\n3-point simulation. 3-Option: 2 is Scenario 2 with original scores from 4 to 6 coded as 2 in 3-point simulation.\n4 SAGE Open\nsizes, Cohen's d with correction for repeated measures\n(Morris & DeShon, 2002), for these differences were mid-\nResults from a large-scale meta-analysis (Larson,\nRottinghaus, & Borgen, 2002) of tests of the six Holland\nvocational personality dimension and \"big-five\" personality\ntraits suggest a predictable relationship between several of\nthe scores. Their analysis suggested a consistent positive\nrelationship between the following:\n\u00b7\n\u00b7 Artistic vocational personality trait and big-five\nOpenness trait\n\u00b7\n\u00b7 Enterprising vocational personality trait and big-five\nExtraversion trait\n\u00b7\n\u00b7 Social vocational personality trait and big-five\nExtraversion trait\n\u00b7\n\u00b7 Social vocational personality trait and big-five\nAgreeableness trait\n\u00b7\n\u00b7 Investigative vocational personality trait and big-five\nOpenness trait\n\u00b7\n\u00b7 Conventional vocational personality trait and big-five\nConscientious trait.\nTable 2 displays the relationship between the 10-option\nand the two scenarios for simulated 3-option scoring of the\nvocational personality scales and the big-five personality\ntrait scores as measured by the NEO-FFI. A perusal of Table\n2 shows the expected pattern between vocational personality\ndimensions and big-five personality traits on four of the six\nrelationships, Enterprising\u00adExtraversion, Investigative\u00ad\nOpenness, Social\u00adAgreeableness, and Conventional\u00ad\nConscientiousness. Using the Fisher r to z transformation,\nnone of the differences in these four relationships between\nuse of the 10-option and the simulated 3-option scoring of\nvocational personality trait scales approached statistical\nsignificance.\nIt is typical for scores on instruments measuring the six\nvocational personality dimensions to be interpreted as codes\nor types based on ranking of the responses on the dimensions\n(Holland, 1997). In the simulation study, three separate anal-\nyses explored whether the number of response alternatives\nimpacted score rankings on the six scales: (a) analysis of the\ncongruence with different response option numbers, (b) anal-\nysis using a two-factor interpretation in which the two fac-\ntors are based on individual scale ranking, and (c) direct\ncomparison of the ranks obtained using the 10-option and the\ntwo 3-option scenarios.\nFor the congruence analysis, three Holland codes (two-\nletter) were generated for each participant based on 10-option\nscoring and each of the 3-option scoring scenarios. The two-\nletter code represents the two highest ranked scale scores for\nthe participant, for example, a participant whose highest\nscores were on the Realistic and Investigative scales would\nhave a code of RI. Three Iachan indices (Iachan, 1990) were\nthen calculated to assess the congruence of the codes from\nthe three scoring procedures.\nWith two-letter Holland codes, the Iachan congruence\nindex can range from zero to six, with six indicating that the\nTable 2. Simulation Study: Pearson Product\u00adMoment Correlation Coefficients Between NEO Personality Trait Scores and Vocational\nPersonality Traits With Three Scoring Conditions (N = 77).\nNote: N = Neuroticism; E = Extraversion; O = Openness; A = Agreeableness; C = Conscientious. 3-Option: 1 is scenario one with original scores of 5\ncoded as 2. 3-Option: 2 is scenario two with original scores from 4 to 6 coded as 2.\nJones and Loe 5\ntwo-letter codes are identical. Iachan index scores of five or\nsix are interpreted as highly congruent; index scores of three\nand four are interpreted as moderately congruent; index\nscores less than three are interpreted as incongruent (Cowger,\nBickham, Miller, & Springer, 1999). The mean congruence\nindex for the codes based on the 10-option scoring and codes\nbased on the simulated 3-option-scenario 1 scoring in this\nstudy was at the level identified as highly congruent (M =\n5.0, SD = 1.30). A highly congruent outcome was also evi-\ndent with 10-option scoring and the 3-option-scenario 2\nscoring (M = 5.1, SD = 1.40) and with both simulated\nData were available in the simulation study that enabled\nthe possible influence of ability on congruence of the\n10-option and 3-option scoring procedures to be tested. For\nthe independent variable, the Iachan congruence index scores\nwere divided into two categories: high congruence, defined\nas indices 5 or greater, and moderate/incongruent, defined as\nindices less than 5. Participant scores on the WJ-III GIA\nserved as the dependent variable.\nOf the 77 participants, 62 had a high level of congruence\nin codes from the 10-option and 3-option-scenario 1 rank-\nings. There was no apparent relationship between ability and\n(partial eta squared) = .003. Similar results were obtained for\ncongruence between 10-option scoring and 3-option-sce-\nnario 2 scoring (66 of the 77 participants had a high level of\ncongruence) with no statistically significant difference in\nwere also similar for congruence between the two simulation\nscenarios (n = 62 in the high congruence category) with no\nstatistically significant difference in scores on the ability test,\nrespectively.\nA comparable procedure was used to assess the possible\ninfluence of personal distress on the utility of the three scor-\ning procedures with the NEO Neuroticism scale serving as\nthe dependent variable. The difference in mean Neuroticism\nscores between participants in high congruence and moder-\nate/incongruent Iachan index categories using 10-option and\n3-option-scenario 1 scaling was not statistically significant,\nand 3-option-scenario 2 scoring to calculate the congruence\nindex, differences in mean Neuroticism scores were not sta-\n.019. Using the congruence index between the two simula-\ntion scenarios, the differences were also not statistically sig-\nPrediger, Swaney, and Mau (1993) proposed a two-factor\nsolution, devising a procedure that uses a weighted ranking\nof the six Holland dimensions to create two summary scales.\nOne creates a \"preference for working with things\" versus\n\"preference for working with people\" scale, and the other\ncreates a scale with a continuum of preference for working\nwith data or preference for working with ideas.\nDifferences in mean scores on the things-people scale for\nthe three scoring conditions in the simulation study were not\nstatistically significant. Using paired samples analysis, the\nresults were as follows: 10-option versus 3-option-scenario\nthe two 3-option scenarios was also not statistically signifi-\nrespectively.\nOn the data-ideas scale, there was a statistically signifi-\ncant difference when comparing the mean score using\n10-option scoring with the mean score using 3-option-sce-\nnario 2 scoring. Other comparisons were not statistically sig-\nnificant, and all effect sizes were small. The results on the\ndata-ideas scale were 10-option versus 3-option-scenario 1,\nThe Wilcoxon Signed Ranks Test for related samples was\nused for direct comparison of the ranks of the six Holland\nscales obtained with each of the three scoring conditions. Of\nthe 18 comparisons, without Bonferroni correction for mul-\ntiple comparisons, only 2 were statistically significant: rank\nof the Artistic scale in the 10-option scoring condition versus\nrank of the Artistic scale in the 3-option-scenario 1 simula-\nthe Artistic scale versus rank of the Artistic scale from\nand .320, were low, and neither of the differences would\nreach the .05 level of significance with the Bonferroni\ncorrection.\nSplit-half (alternating items) reliability coefficients were\ncalculated for each of the six scales in the 10-option scoring\nand the two 3-option simulation scenarios and corrected\nusing the Spearman-Brown prophecy formula. Results are\ndisplayed in Table 3. With the Bonferroni correction for mul-\ntiple comparisons, none of the differences in reliability esti-\nmates produced by the different number of response options\nwere statistically significant.\nResults of the simulation study challenged the belief that\nadditional response alternatives would have meaningful\nimpact on psychometric characteristics of a vocational per-\nsonality scale. But there was an obvious limitation with the\nvaried number of responses simulated rather than directly\ntested. It may, for example, be a reasonable assumption that\nparticipants who chose a middle response in the standard for-\nmat would also have chosen a middle response in a 3-point\nformat. And, it may be reasonable to assume that those who\nselected among the several choices on either side of the\n6 SAGE Open\nmid-point would have chosen the option in that direction if\nonly one had been available. But, these, particularly regard-\ning participants who did not choose a middle response, are\nonly assumptions that could not be directly addressed in a\nsimulation design.\nStudy 2 was a follow-up using the same core instrument\nbut with a design enabling direct comparison of the impact of\nvarying the number of alternative responses.\nStudy 2 Participants\nData for Study 2 were provided by 192 participants, volun-\nteers who chose this study from several different projects\navailable to meet a research participation requirement for\ncourses in educational psychology. The study was conducted\nwere female. The majority (59%) were in the age range of 18\ning an age range of 35 to 44, and 4% reporting an age range\nof 45 or older. Self-reported ethnic backgrounds were\nCaucasian (60%), Hispanic American (16%), African\nAmerican (8%), Asian American (6%), and Other (9%).\nEnglish was the primary language for all participants in\nStudy 2. The range of self-reported grade point average of\nStudy 2 Instrumentation\nCogStyle was again used as the core scale. In Study 2, two\nversions of the 30-item scale were used. In the first ver-\nsion, the comparison mode used the original binary form\nof the instrument. Participants, in forced-choice format,\nexpressed a preference (\"more likely to be\") for one of the\ntwo adjectives or actions (e.g., efficient or curious). In the\nalternate version, the extent of preference was reported in\na 6-point Likert format (e.g., much more likely to be effi-\ncient, somewhat more likely to be efficient, slightly more\nlikely to be efficient, slightly more likely to be curious,\nsomewhat more likely to be curious, much more likely to\nbe curious).\nInternational Personality Item Pool\u00ad50 (IPIP-50). In Study 2, the\nbig-five personality characteristics were measured using the\n50-item scale from the IPIP (http://ipip.ori.org/ipip/). The\nIPIP (Goldberg et al., 2006) is an open source resource with\nsample questionnaires for personality constructs including\nthe five-factor model of Extraversion, Neuroticism, Agree-\nableness, Conscientiousness, and Openness to Experience.\nReliability estimates (Cronbach's alpha) in this participant\nsample for the Extraversion, Neuroticism, Agreeableness,\nConscientiousness, and Openness to Experience scales were\nVocational Efficacy Scale. This subject pool study also included\na measure of vocational efficacy organized around the Hol-\nland six dimensions. Participants were instructed to respond\nto questions about the extent to which they felt confident in\ntheir ability to perform various tasks associated with each of\nthe dimensions. Reliability estimates, Cronbach's alpha, for\nthe efficacy dimensions of Realistic, Investigative, Artistic,\nStudy 2 Procedure\nAll scales were completed in an online format. Participants\nwere randomly assigned to complete the CogStyle scale in\nbinary format or the alternate version with preferences pre-\nsented in a 6-point Likert format. Typical for studies in sub-\nject pool online modalities, cases in which motivation was\nsuspect were identified and deleted before additional analy-\nsis. Suspect data were defined on the basis of overall time\nspent completing the several instruments. A total of 218 data\nsets were examined; 16 were eliminated on the basis of\ncompletion time too short to indicate other than random\nresponding. The analysis was thus based on a total of 192\nparticipants, 95 of whom completed the original binary form\nof the CogStyle scale and 97 who completed the Likert\nresponse form.\nThe random assignment appeared to identify equivalent\ngroups with no statistically significant difference in the\ndemographics of the two groups evident on gender,\nTable 3. Simulation Study: Split-Half Reliability Estimates With Spearman-Brown Prophecy Formula for Scoring Conditions With\nVocational Personality Traits.\nN = 77 10-response categories 3-response category simulation: Scenario 1 3-response category simulation: Scenario 2\nNote: In Scenario 1, scores of 5 were coded as 2 for a three-response category simulation. In Scenario 2, scores from 4 to 6 were coded as 2 for a three-\nresponse category simulation.\nJones and Loe 7\nThe randomly assigned groups were also compared on\nresults of the IPIP-50 scale used to test the big-five personal-\nity characteristics as a validity measure in Study 2. No statis-\ntically significant differences were found on the Neuroticism\nStudy 2 Results\nConsistent with the procedure described in the simulation\nstudy, the total score for each dimension was weighted by the\nextent of preference. For example, the adjective \"efficient\" is\nassociated with the Conventional scale in the Holland model;\n\"curious\" is associated with the Investigative scale. In the\nbinary format, the participant receives a score of 1 on one of\nthese two scales dependent on the selection. In the 6-point\nLikert format, the points are awarded based on the extent of\npreference (e.g., much more likely to be efficient was 1 point\non the Conventional scale, somewhat more likely to be effi-\ncient was .66 point on the Conventional scale, slightly more\nlikely to be efficient was .33 on the Conventional scale, and\nso forth.)\nTable 4 displays the descriptive information for the binary\nand the 6-point scoring for each of the six vocational person-\nality scales. As expected, the \"all or nothing\" response mode\nresulted in higher raw scores in the binary format on all\nscales. All mean differences were statistically significant.\nTable 5 displays the relationship between the binary and\n6-point scoring of the vocational personality scales and the\nbig-five personality trait scores. A perusal of Table 5 shows\nthe expected statistically significant positive relationship\nbetween the Artistic vocational personality trait and big-five\nOpenness trait (binary response mode), the Enterprising\nvocational personality trait and big-five Extraversion trait\n(both response modes), the Social vocational personality trait\nand big-five Extraversion trait (Likert response mode), and\nthe Social vocational personality trait and big-five\nAgreeableness trait (both response modes). Moreover, as\nTable 4. Study 2: Descriptive Statistics for Vocational Personality Trait Scores With Binary and 6-Point Likert-Type Response Options.\nRealistic Investigative Artistic Social Enterprising Conventional\n M (SD) M (SD) M (SD) M (SD) M (SD) M (SD)\nNote: Statistically significant differences (p < .01) were evident between all mean scores.\nTable 5. Study 2: Pearson Product-Moment Correlation Coefficients Between Big-Five Personality Trait Scores (IPIP) and Two Scoring\nConditions for Vocational Personality Traits.\nNote: IPIP = International Personality Item Pool. Big-Five: N = Neuroticism; E = Extraversion; O = Openness; A = Agreeableness; C = Conscientious.\nBinary-Likert: R = Realistic; I = Investigative; A = Artistic; S = Social; E = Enterprising; C = Conventional.\n8 SAGE Open\nexpected, there were statistically significant positive rela-\ntionships between the Investigative vocational personality\ntrait and big-five Openness trait (Likert mode) and the\nConventional vocational personality trait and big-five\nConscientious trait (both response modes).\nThere were only two instances where there appeared to be\na marked difference contingent on the response mode in the\npredicted relationships with big-five personality traits, one in\nfavor of the binary response mode and one in favor of the\nLikert response mode. Using the Fisher r to z transformation,\nnone of the differences in relationships with big-five person-\nality traits between the binary and the 6-point Likert scale\napproached statistical significance.\nFor exploration of the impact of the number of response\nalternatives on the rankings typically used for interpretation\nof scores on the six vocational personality traits, the Iachan\ncongruence index was not appropriate in Study 2 because\ndifferent participants completed the two versions of the\nCogStyle scale. The two-factor solution, described in the\nSimulation study, was used along with a direct comparison of\nthe ranks obtained in the binary and Likert formats.\nDifferences in the things-people scale and data-ideas\nscales for the two response modes in Study 2 were not statis-\nrespectively.\nFor a direct comparison of the ranks, the nonparametric\nMann-Whitney U procedure was used to compare the\nweighted rankings obtained from binary and Likert response\nmodes on each of the six Holland dimensions. The obtained\np values in comparing the rankings from the binary and\nLikert response modes on the Realistic, Investigative,\nArtistic, Social, Enterprising, and Conventional Scales were\ntively. Corresponding effect size estimates (Probability of\nrespectively. Grissom and Kim (2005) identifying equiva-\nlents of Cohen's d and the Probability of Superiority sug-\ngested that 56% is a small effect, 65% is a medium effect,\nand 71% is a large effect.\nOne additional scale, not available for participants in the\nSimulation study, was available for use as an external criteria\nmeasure in Study 2. The relationships between the binary\nand Likert scoring of the vocational personality traits and the\nscores on a vocational efficacy scale are displayed in Table 6.\nA perusal of Table 6 suggests that, in general, the expected\nrelationships between vocational personality trait and voca-\ntional efficacy scores were evident with no apparent advan-\ntage for the 6-point scoring. In fact, of the statistically\nsignificant relationships, most were in favor of the binary\nscoring. Using the Fisher r to z transformation, however,\nnone of the differences in relationships were statistically\nsignificant.\nSplit-half (alternating items) coefficients were calculated\nfor each of the six scales in the binary and in the Likert for-\nmats and corrected using the Spearman-Brown prophecy\nformula. Results are displayed inTable 7.With the Bonferroni\ncorrection for multiple comparisons, none of the differences\nbetween binary and 6-point Likert scoring were statistically\nsignificant. Results with this participant sample are consis-\ntent with other reliability data for the CogStyle instrument\nDiscussion of Two Studies\nCautions in generalizing to other participant samples and a\nneed for additional study are typically found in the discus-\nsion section of published research studies but are particularly\nimportant in interpreting the results reported here. One obvi-\nous limitation in generalizing the results of these studies is\nthe nature of the sample. All participants in both studies were\nuniversity students who were participating to meet a research\nTable 6. Study 2: Pearson Product-Moment Correlation Coefficients Between Vocational Efficacy Scores and Two Scoring Conditions\nfor Vocational Personality Traits.\nR-Efficacy I-Efficacy A-Efficacy S-Efficacy E-Efficacy C-Efficacy\nNote: R = Realistic; I = Investigative; A = Artistic; S = Social; E = Enterprising; C = Conventional.\nJones and Loe 9\nrequirement. The number of participants, particularly in the\nSimulation study, was relatively small. The Simulation study\nincluded only two of the many possible simulation scenarios\nfor the number of response options. In addition, the extent to\nwhich a simulation will reflect an actual response choice in a\nreal-life setting is always uncertain.\nIn reference to the first research question, the impact on\nmeasurement statistics, the only clearly evident effect in\neither of these studies was an increase in the mean scores\nwhen the binary (all or nothing) results were compared with\nsix response categories with partial credit. While no defini-\ntive conclusions could be drawn from the split-half reliabil-\nity estimates in the two studies, there was no apparent\nconsistent enhancement or decrement of the estimated reli-\nability with the different numbers of response options. In the\nSimulation study, most reliability estimates were low under\nall conditions, but there was no trend indicating advantage or\ndisadvantage of the number of options. Reliability estimates\nin Study 2 trended higher for the Likert response options\ncompared with the binary option, but the differences did not\nrise to a level of statistical significance.\nPossibly confounding interpretation of the measurement\nstatistics, including the reliability estimates, in these studies\nis that the forced-choice format in the core instrument results\nin ipsative scores, an artificial interdependence among the\nsix scales that may have an influence on psychometric prop-\nerties. While it has been suggested (Baron, 1996) that ipsa-\ntive scoring may actually control for bias that is inherent in\nLikert-type responses, further study using a core instrument\nwith normative scoring would be advantageous.\nIn reference to Research Question 2, neither study found\nevidence indicating increased utility in prediction of related\nexternal criteria associated with the number of response cat-\negories. In the Simulation study, the two scenarios with 3\nresponse categories appeared as effective as 10 response cat-\negories in predicting scores on a measure of core personality\ntraits and in creating ranks used for interpretation of voca-\ntional personality dimensions. In Study 2, 6 response catego-\nries appeared no more effective than 2 response categories in\npredicting scores on a different measure of core personality\ntraits, creating ranks used for interpretation of the vocational\npersonality dimensions, or in predicting scores on a self-\nreport of vocational efficacy.\nConsidering the limitations described above, these stud-\nies are not sufficient alone for conclusions about the opti-\nmal number of response categories. These findings,\nhowever, do appear sufficient for a \"that is correct\"\nresponse to the suggestion in the title of this article that\n\"more may not be better\" and to support a need for addi-\ntional studies comparable to Study 2 with random assign-\nment of the number of response options, more diverse\nparticipant samples, and perhaps a core instrument that\ndoes not use ipsative scoring.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nBaron, H. (1996). Strengths and limitations of ipsative measure-\nment. Journal of Occupational and Organizational Psychology,\nBendig, A. W. (1954). Reliability and the number of rating scale\nBlaikie, N. (2003). Analysing quantitative data. London, England:\nBloom, M., Fischer, J., & Orme, J. G. (2003). Evaluating practice:\nGuidelines for the accountable professional (4th ed.). Boston,\nMA: Allyn & Bacon.\nCook, C., Heath, F., Thompson, R. L., & Thompson, B. (2001).\nScorereliabilityinweb-orInternet-basedsurveys:Unnumbered\ngraphic rating scales versus Likert-type scales. Educational\nCorder, G. W., & Foreman, D. I. (2009). Nonparametric statistics\nfor the non-statistician. Hoboken, NJ: John Wiley.\nCosta, P. T., & McCrae, R. R. (1992). Revised NEO Personality\nInventory (NEO PI-R) and NEO Five-Factor Inventory\n(NEO-FFI): Professional manual. Lutz, FL: Psychological\nAssessment Resources.\nCowger, E. L., Bickham, P. J., Miller, M. J., & Springer, T. P.\n(1999). Accuracy of leisure activities finder among high school\nTable 7. Study 2: Split-Half Reliability Estimates With Spearman-Brown Prophecy Formula for Two Scoring Conditions on Vocational\nPersonality Traits.\nReliability estimate binary (n = 95) Reliability estimate Likert (n = 97)\nGoldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R.,\nAshton, M. C., Cloninger, C. R., & Gough, H. C. (2006).\nThe International Personality Item Pool and the future of\npublic-domain personality measures. Journal of Research in\nGrissom, R. J., & Kim, J. J. (2005). Effect sizes for research: A\nbroad practical approach. Mahwah, NJ: Lawrence Erlbaum.\nHolland, J. L. (1973). Making vocational choices. Englewood\nCliffs, NJ: Prentice Hall.\nHolland, J. L. (1997). Making vocational choices: A theory of voca-\ntional personalities and work environments (3rd ed.). Odessa,\nFL: Psychological Assessment Resources.\nIachan, R. (1990). Some extensions of the Iachan congruence index.\nJamieson, S. (2004). Likert scales: How to abuse them. Medical\nJones, W. P. (1996). Career assessment for patients with visual\ndisability: Consideration of instrument distortion. Journal of\nJones, W. P. (2009). CogStyle core scale reliability. Retrieved from\nhttp://faculty.unlv.edu/pjones/nl8_1.htm\nKnapp, T. R. (1990). Treating ordinal scales as interval scales:\nAn attempt to resolve the controversy. Nursing Research, 39,\nKrieg, E. F., Jr. (1999). Biases induced by coarse measurement\nscales. Educational and Psychological Measurement, 59,\nLarson, L. M., Rottinghaus, P. J., & Borgen, F. H. (2002). Meta-\nanalyses of big six interests and big five personality factors.\nLietz, P. (2010). Research into questionnaire design: A summary\nof the literature. International Journal of Market Research, 52,\nLowen, W. (1982). Dichotomies of the mind: A systems science\nmodel of the mind and personality. New York, NY: John\nWiley.\nMattell, M. S., & Jacoby, J. (1971). Is there an optimal number\nof alternatives for Likert scale items? Study I: Reliability and\nvalidity. Educational and Psychological Measurement, 31,\nMorris, S. B., & DeShon, R. P. (2002). Combining effect size\nestimates in meta-analysis with repeated measures and\nindependent-groups designs. Psychological Methods, 7,\nPrediger, D., Swaney, K., & Mau, W. (1993). Extending Holland's\nhexagon: Procedures, counseling applications, and research.\nRodgers, W., Andrews, F., & Herzog, R. (1992). Quality of survey\nmeasures: A structural modeling approach. Journal of Official\nWoodcock, R. W., McGrew, K. S., & Mather, N. (2001). Woodcock-\nJohnson Tests of Cognitive Abilities (WJ-III). Itasca, IL:\nRiverside.\nAuthor Biographies\nW. Paul Jones, Ed.D. is a Professor in the Department of\nEducational Psychology & Higher Education at the University of\nNevada, Las Vegas, where he directs doctoral studies in school psy-\nchology. His current research interests include adapting scales for\nonline administration and single-case statistical analysis.\nScott A. Loe, Ph.D., is an Associate Professor in the Department of\nEducational Psychology & Higher Education at the University of\nNevada, Las Vegas, where he directs clinical studies in school psy-\nchology. His current research interests include assessment training\nmodels and school wide positive behavior support."
}