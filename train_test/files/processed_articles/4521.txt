{
    "abstract": "Abstract\nEstimation by minimizing the sum of squared residuals is a common method for parameters of regression functions; however,\nregression functions are not always known or of interest. Maximizing the likelihood function is an alternative if a distribution\ncan be properly specified. However, cases can arise in which a regression function is not known, no additional moment\nconditions are indicated, and we have a distribution for the random quantities, but maximum likelihood estimation is difficult\nto implement. In this article, we present the least squared simulated errors (LSSE) estimator for such cases. The conditions\nfor consistency and asymptotic normality are given. Finite sample properties are investigated via Monte Carlo experiments\non two examples. Results suggest LSSE can perform well in finite samples. We discuss the estimator's limitations and\nconclude that the estimator is a viable option. We recommend Monte Carlo investigation of any given model to judge bias\nfor a particular finite sample size of interest and discern whether asymptotic approximations or resampling techniques are\npreferable for the construction of tests or confidence intervals.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nArticle\nIntroduction\nMinimizing the sum of squared residuals is a common esti-\nmation method for regression functions, but regression func-\ntions are not always known or of interest, particularly when\nmotivated by an underlying structural equation. If a distribu-\ntion for random terms is specified, maximizing the likeli-\nhood function is an alternative estimation method that does\nnot require an explicit regression function. However, cases\ncan arise in which the regression function is not known, no\nadditional moment conditions are indicated, and we have a\ndistribution for the random quantities, but maximum likeli-\nhood estimation is difficult to implement. This article pres-\nents a simulated moments estimation method for such cases\nsimilar to the simulation-based extensions of other common\nestimators such as maximum simulated likelihood and simu-\nlated scores (Gourieroux & Monfort, 1993; McFadden,\nConsider a simple example in which a response y to exog-\nenous variable x for each person w in a population of interest\nis modeled as y x\nw w\nw\n= + \n  \n (0,1). The model implies an idealized relation y =  + \u00b7x\nfrom which each person's response relationship is modified\nto a curve by a personal parameter \nw\n. Suppose the data gen-\nerating process implies a probability model for each data\nobservation i as y x\ni i\ni\n= + \n  \n( ) , with i\ndenoting a latent\n(i.e., unmeasured) variable (see Figure 1 for examples of this\nfunction). The regression equation is not known explicitly, so\nminimizing the sum of squared errors is not immediately\napplicable. For the same reason, the parameterization for the\nconditional likelihood function for y is not explicit. The least\nsquared simulated errors (LSSE) estimator presented below\nprovides a simple means of estimating such models.\nIn the following section, we present the estimator and its\nasymptotic properties. We then present a Monte Carlo inves-\ntigation of finite sample properties via two examples. In the\nfinal section, we discuss implications and limitations of the\nLSSE estimator.\nLSSE\nTo understand the LSSE estimator and its asymptotic proper-\nties, consider a response or dependent variable y, for an\nobservation i, expressed as a function g of observed\n1University of RochesterSchool of Medicine and Dentistry, NY, USA\nCorresponding Author:\nPeter J. Veazie, Department of Public Health Sciences, University of\nRochester School of Medicine and Dentistry, 265 Crittenden Blvd., CU\nEmail: peter_veazie@urmc.rochester.edu\nLeast Squared Simulated Errors: An\nEstimator for Structural Models With\nUnspecified Regression Functions\nPeter J. Veazie1 and Shubing Cai1\n Keywords\nleast squares analysis, nonlinear regression, Monte Carlo methods, simulation experiments\n2 SAGE Open\ncovariates x, unobserved quantities , and model parameters\n= (, ):\ny g\ni i i\n= ( )\nx , | ,\nwith\n \ni F\nThe function g represents the structural model under con-\nsideration (e.g., a theory-derived model of an underlying\nmechanism, not a regression function), and the unobserved\nquantity \ni\nhas distribution F\n, with parameters . Assuming\nthat the conditional expectation of y exists, the relationship\ny\ni\n= E(y\ni\n|x\ni\n) + \ni\ncan be expressed as\ny g x dF\ni i i i\n= ( ) +\n , | ,\n  \nwhere the integral represents the regression and \ni\nis the\nregression error term having expectation of 0.\nMonte Carlo integration provides an unbiased estimator,\nyi\n ( )\n , of the regression function E(y\ni\n|x\ni\n) (Robert & Casella,\ntion i is the average of the function g evaluated at x\ni\nacross\nthe R samples of  drawn from the distribution F\n:\ny\nR\ng x\ni i r\nr\nR\n    \n( ) = ( )\n( )\n\n=\nThe term \nr\nrepresents the rth random drawn from distribu-\ntion F\n.\nDenoting the simulated residual as\ne y y\ni i i\n \n \n( ) = - ( ), (5)\nand, for sample size N, denoting the corresponding N\u00d7 1\nsample vector of simulated residuals as e\nR ( ),\n then the defi-\nnition of the LSSE estimator is given as follows.\nFor N observations and N\u00d7N positive definite matrix M,\nthe LSSE estimator is\n  \n\n  \n= ( )   ( )\n\n\n\n\n\n\narg min .\ne e\nThe LSSE estimator is the value of  that minimizes the\nsample residual vector relative to the metric determined by\nM. For M, an identity matrix (i.e., a square matrix with ones\nin the diagonal elements and zeroes in the off-diagonal ele-\nments), the estimator simply minimizes the sum of squared\nsimulated errors:\n  \n\n  \n= ( )  ( )\n\n\n\n\n\n\narg min .\ne e\nThe large-sample properties of the LSSE estimator are\nestablished by showing that the LSSE estimator is asymp-\ntotically equivalent to the standard nonlinear least squares\nestimator (NLS). Essentially, for each property considered, it\ncan be shown that the key quantities of interest can be\nexpressed as the standard NLS estimator plus a simulation\nbias that is asymptotically zero.\nAs shown in Appendix A, we see that if the assumptions\nunderlying consistency and normality of the NLS estimator\nhold for the model under consideration, then for increasing\nnumber of Monte Carlo iterations used in the numeric integra-\ntion (R) and increasing sample size (N), the LSSE estimator is\nconsistent, and if R rises faster than the square root of N, the\nLSSE estimator has an asymptotic normal distribution.\nEssentially, as R and N go to infinity with R rising faster than\nthe square root of N, the LSSE estimator is asymptotically\nequivalent to the NLS (Seber & Wild, 2003). This is somewhat\ncomforting perhaps, but empirical researchers are doomed to\nfinite sample sizes and are therefore concerned about an esti-\nmator's performance with finite samples. Unfortunately, as\nwith the maximum likelihood estimator, the finite sample\nproperties of the LSSE estimator cannot be generally estab-\nlished--each model needs to be investigated to determine its\nown properties. In the next section, we present Monte Carlo\nexperiments investigating two models with different levels of\ncomplexity. Results suggest that the LSSE estimator can be a\nviable method of estimation in finite samples.\nSimulation Experiments\nIn this section, we present two Monte Carlo experiments to\nshow how the LSSE estimator works. The first experiment\npresents the estimation of parameters for a simple model,\nFigure 1. Graphs of the equation y = 1 + (0.2\u00b7x) for three\nvalues of .\nVeazie and Cai 3\nand the second experiment presents the estimation of the\nparameters for a more complex model. We then extend the\nsecond experiment by including 10 regressors using a large\nsample size to show how the LSSE estimator can function in\nthe multiple variable setting and to consider computational\nspeed. As with any estimator for which only the asymptotic\nproperties are generally known (e.g., maximum likelihood\nand generalized method of moments), the finite sample prop-\nerties of the estimator will depend on the function being con-\nsidered. Consequently, these examples of the LSSE estimator\ndo not reflect general convergence rates of other models; like\nthe maximum likelihood estimator, each model needs to be\ninvestigated separately.\nBecause the LSSE estimator optimizes nonlinear func-\ntions, some randomly generated data sets can produce degen-\nerate estimates (variance estimates near zero) or simply do\nnot converge from the automatically generated initial values.\nAlthough in a real-world analysis we would carefully con-\nsider the model, parameterization, and initial values to\nachieve proper estimates, such an approach is not practical\nwhen running a Monte Carlo experiment across thousands of\nsamples. Consequently, we drop such data sets and sample\nnew ones; we base the results only on such data sets that\nprovide reasonable convergence.\nIn this section, we discuss the general process of obtain-\ning the LSSE estimates. However, we do not present pro-\ngrammingdetailsastheyareidiosyncratictotheprogramming\nlanguage used, which in our case is that of the STATA soft-\nware. For those familiar with STATA, we present inAppendix\nB the details of our programs used with STATA's nl com-\nmand (nonlinear regression) to produce LSSE estimates.\nThe Simple Model\nOur first example is the estimation of parameters for the\nmodel y =  + (\u00b7x) in which  has a log-normal distribution.\nThe purpose of this article is to show the functioning of the\nLSSE estimator and not to address specific scientific prob-\nlems: We cannot foresee the structural models that scientists\nwill create in the future, and consequently, we focus on\nmathematical forms that challenge estimation rather than\nwith their current scientific motivation. Nonetheless, to pro-\nvide some context, we could motivate this model by consid-\nering it as representing response curves in which each\nindividual in a population has a response y to values of x\ngoverned by his or her own  value. The model allows  to be\ngreater than 1, reflecting an increasing rate of change, and\nalso allows  to be less than 1, reflecting a decreasing rate of\nchange (see Figure 1 for examples). Perhaps our interest is in\nestimating a curve representing the response for the average\n. Regardless of the purpose, however, we must estimate the\nmodel parameters. There is not an evident closed-form\nexpression for the expected value of y given x, so the com-\nmon least squares and maximum likelihood estimators are\nnot applicable. Fortunately, we can use LSSE. Figure 2\nprovides a heuristic outline of how to obtain the estimates for\nthis problem. In practice, it is easiest to use software that\nminimizes the sum of squared errors based on a user-sup-\nplied program that calculates Step 2b; such software will\nthen likely return the estimates, standard errors, confidence\nintervals, and various fit statistics--for this example, we\nused STATA's nl command for nonlinear regression (see\nAppendix B for STATA code).\nData samples were drawn from the preceding model with\nx~ Uniform(0,10), ~ lnNormal(\u00b5, ). The parameters were\nset to  = 1,  = 2, \u00b5 = -0.1, and ln() = -2. To consider stan-\ndard error estimates and yet facilitate the experiment using\nthousands of data sets, we automatically account for poten-\ntial heteroscedasticity in the regression error by using a het-\neroscedasticity-consistent robust standard error estimator.\nWe use the LSSE estimator on 3,000 Monte Carlo data sets\nIn Step 2a of Figure 2, we draw a sample of values from\nthe specified distribution of  for each individual in the data\nset: It is computationally more effective to draw a systematic\nsample from the distribution rather than a random sample (or\nthe pseudo-random sample that a computer's \"random\"\nnumber generator would obtain). For example, suppose we\nwant to represent the distribution of a variable X that has a\nstandard normal distribution. We could draw a random sam-\nple u from a uniform(0,1) distribution and then obtain ran-\ndom values x from the inverse cumulative distribution\nfunction (CDF; that is, x = -1(u)). Alternatively, we would\nselect an equally spaced grid of values u* from the uni-\nform(0,1) and obtain the random values x associated with the\ninverse CDF of these x = -1(u*). Figure 3 shows the histo-\ngrams of 200 draws from each method and shows lines rep-\nresenting a kernel density estimator of the histogram: Notice\nthat the grid sample fills out a normal shape better than the\nrandom sample, and the kernel estimator for the grid sample\nHeuristic algorithm for the LSSE estimation of the simple model:\nStep 1. Set parameters values ^\n \n= , ^\n \n= , ^\n \n= , and ln  ln\n.\nThese are arbitrary initial values at the first iteration and values\ncalculated by the optimization algorithm thereafter.\nStep 2. For each observation i in the data set:\na) Draw a sample of R values\n^ ^\n{ , }\ni i R\n \n... from the\ndistribution LnNormal( ^\n ,\n)\nb) Calculate\n( )\n^\n,\n^\n^ ( )\nR\ni r\ni i\nr\ny x\nR\n\n \n=\n= + \n\nStep 3. Calculate the sum or squared residuals:\n^\n( )\nN\ni i\ni\nSSR y y\n=\n= -\n\nStep 4. Use an appropriate numerical optimization algorithm (e.g.\nNewton-Raphson) to iterate Steps 1 through 3 and find the parameter\nvalues that minimize the SSR.\nFigure 2. LSSE procedure for the simple model.\nNote. LSSE = least squared simulated errors; SSR = sum or squared\nresiduals.\n4 SAGE Open\nis closer to a normal curve. For the purpose of Monte Carlo esti-\nmation of an integral over a variable's full domain, the grid sam-\nple will provide a more accurate estimate because it fills out the\ndistribution better. The benefit to the systematic sample is that,\nfor a given precision, one can use fewer draws from the distribu-\ntion to evaluate the integral--This provides greater computa-\ntional efficiency thereby decreasing the estimation time.\nFor computational efficiency then, we use a 200-element\n(i.e., R = 200) Modified Latin Hypercube Sampling (a type\nof grid sampling) for the Monte Carlo integration estimates\nof the regression equation (Hess, Train, & Polak, 2006). This\nmethod allows each observation to have a different grid by\nshifting it a small randomly selected amount. Specifically, to\ndraw R values of a variable  from its distribution F (using\nspecified parameter values for \u00b5 and ), we first randomly\ndraw a uniform value un\nfrom a uniform distribution for each\nobservation n (this will provide the random shift in the grid\nfor each observation). Then we generate R values across a\nuniform distribution as u\nn,r\n= (r- 1) / R + u\nn\n/ R for which the\nindex r takes on integer values of 1 to R. We get values \nn,r\n=\nn,r\n) to obtain the sample {n,1\n, . . ., n,R\n} from F for each\nobservation n of our sample with size N (this is Step 2a in\nFigure 2). The estimated expected value of y given x and\nparameter values  and  is then calculated as\ny\nR\nx\ni i\nr\nR\ni r\n = + \n( )\n( )\n\n=\n   , , (7)\nfor each observation i in the data set. The optimization pro-\ngram will find the values of \u00b5, , , and  that minimize the\nsum of the weighted squared errors (Equation 6) based on\ny\ni\n .\nTable 1 presents the results of our Monte Carlo experi-\nment. In conformance with the asymptotic properties, the\nmean of the parameter estimates approaches the true values\nfor all parameters as sample size increases (the absolute bias\nsample sizes of 5,000 where absolute bias is the absolute\nvalue of the difference between the estimate and the true\nvalue). However, the variance parameter ln() has a slower\nconvergence rate and therefore exhibits a larger bias for the\nsample sizes used here (absolute bias of 0.28 as compared\nwith the next largest absolute bias of 0.03 for the \u00b5 parame-\nter). Also, as expected, the standard deviation of the estima-\ntor's sampling distribution decreases with sample size (e.g.,\nthe standard deviation for the  estimator is 1.154 using a\n5,000). Moreover, as the histograms and both skewness and\nkurtosis indicate, the distribution of parameter estimates is\nconverging toward normality as sample size increases (a nor-\nmal distribution has a skewness of 0 and a kurtosis of 3). The\naverages of the robust standard error estimators are approxi-\nmating the standard deviation of the parameter sampling dis-\ntributions in all cases except for the variance parameter\nN = 5,000 experiment). The variance parameter has not yet\nGrid sample Random sample\nx\nFigure 3. Comparison between a grid sample and a random sample of a normal variable.\nVeazie and Cai 5\nTable 1. Simulation Results for the y =  + (\u00b7x) Model Using 3,000 Monte Carlo Samples.\nSample size\nTrue ln() = -2\nNote. Histograms are of the LSSE estimates. LSSE = least squared simulated errors.\n6 SAGE Open\nconverged sufficiently close to normal for this model and\nsample sizes; consequently, this standard error estimator is\nnot yet accurate.\nThe Complex Model\nVeazie and Cai (2007) proposed a model of the relationship\nbetween a person's sense of uniqueness  (i.e., how dissimi-\nlar from others a person believes herself to be, which can be\nexpressed as a function of other variables w), a stated statisti-\ncal proposition x (e.g., \"x% of people who purchase Product\nA report not being satisfied with the purchase\" or \"x% of\npeople who take Medication A get Side Effect B\"), personal\nexperience  with the context of the proposition (e.g., the\nperson's experience with similar products or the person's\nexperience with medication side effects in general), and the\nperson's believed likelihood y that she is subject to the claim\nof the proposition (e.g., her believed probability she will not\nbe satisfied with the purchase of Product A or her believed\nprobability she would have Side Effect B from taking\nMedication A). In essence, they model how a person's sense\nof being unique () impacts her belief of how likely she will\nexperience an effect (y) given information about how often\nthe effect is experienced in a population (x): a model of risk\nperception given population risk information. Examples of\nthe relationship are shown in Figure 4. For the current pur-\npose, however, the importance of this example is not the\nlogic of its derivation (see reference Veazie & Cai, 2007) but\nrather that the model clearly provides a complex estimation\nproblem. A simplified version of the model, which we use, is\ny\ne e\ne e\nu u du\ni\ne e\nx i i i\ni i i\ni i i\ni\n=\n+\n( )\n( ) ( ) -\n( ) \n+\n+\n- -\n+\n\n \n  \n  \n  \n\nfor\n  \ni i\nw\n= + \nand\ni F\nwith y, x, and w as measured variables, and  an unmeasured\nquantity with distribution F. Not only does the conditional\nexpectation of y not have a closed-form solution, but the inte-\ngral equation itself that models y does not have a closed-form\nsolution (Equation 8). Moreover, there are explanatory vari-\nables in the integrand (x) as well as in the parameterization of\nthe model (w). The standard methods do not apply to this\nmodel, but as the Monte Carlo results show below, the LSSE\ncan achieve reasonable estimates with little bias in finite\nsamples. Figure 5 provides a heuristic outline of how to\nobtain the estimates for this problem.\nData samples were drawn from this model, with x~\nUniform(0,1), w~ Normal(0,1), and ~ Normal(0, ). The\nparameters were set to \n= 2, and ln() = 0 yielding the\nvariance 2 = e0 = 1. Again, to automatically account for poten-\ntial heteroscedasticity in the regression error, we use a robust\nstandard error estimator. We use the LSSE estimator on 3,000\nMonte Carlo data sets for each of sample sizes 300, 500, and\nModified Latin Hypercube Sampling for the Monte Carlo inte-\ngration estimates of the regression equation (Hess et al., 2006).\nHeuristic algorithm for the LSSE estimation of the complex model:\nStep 1. Set parameters values\n^\n \n= ,\n^\n \n= , and ln  ln\n . These are\narbitrary initial values at the first iteration and values calculated by the optimization\nalgorithm thereafter.\nStep 2. For each observation i in the data set:\na) Draw a sample of R values\n^ ^\n{ , }\ni i R\n \n... from the distribution Normal\n( ^\n ,\n )\nb) Calculate\n^ ^ ^\ni i\nw\n  \n= + \nc) Calculate\n^ ^\n^ , ^ ^\n^ ,\n^ ^\n^ ,\n( ) ( )\ni i r\ni\nx\nR i i i r\ni\ne e\ni\ni i r\ni\nr\ne e\ny u u du\nR e e\n \n\n \n\n \n\n+\n+\n- -\n+\n=\n \n +\n=  - \n \n \n \n \n \n(note the integral is the CDF of a Beta distribution that many statistical\nsoftware will be able to calculate)\nStep 3. Calculate the sum or squared residuals:\n^\n( )\nN\ni i\ni\nSSR y y\n=\n= -\n\nStep 4. Use an appropriate numerical optimization algorithm (e.g. Newton-Raphson)\nto iterate Steps 1 through 3 and find the parameter values that minimize the SSR.\nFigure 5. LSSE procedure for the complex model.\nNote. LSSE = least squared simulated errors; CDF = cumulative distribution\nfunction; SSR = sum or squared residuals.\nFigure 4. Graphs of Equation 8 with  = 0 (an inexperienced\nperson).\nNote. The dashed line represents a model of perceived risks, given\ninformation on a population risk, for individuals with a strong sense of\nuniqueness (i.e., < 1); in this case, individuals believe they are different\nfrom the majority and are less likely to experience what the majority\nexperiences. The solid line represents perceived risks, given information\non a population risk, for individuals with a weak sense of uniqueness (i.e.,\n> 1); in this case, individuals will believe themselves to be more like the\nmajority and will have a greater chance of experiencing what the majority\nexperiences.\nVeazie and Cai 7\nTable 2 shows the results for estimates of \n, \n, and ln().\nThe estimator converges rapidly to the true parameter values\nin the N = 5,000 experiment) as well as to normality for \nand \n(as indicated by the skewness and kurtosis parameters\napproaching 0 and 3, respectively). As in the preceding\nexample, convergence to normality for the variance parame-\nter is slower. Again, the averages of the robust standard error\nestimates approximate the standard deviations of the param-\neter sample distributions. Notice, unlike the preceding\nmodel, at the sample size of 5,000, the ln() parameter is\nnear normal and consequently the average standard error is\ncloser to the sample standard deviation (absolute bias of\nTo show that the estimator applies to a multivariable set-\nting, we expanded the preceding model such that the param-\neter \ni\nis specified as a combination of 10 variables:\n  \ni k i k\nk\nw\n= + \n=\n\nTable 3 shows the average parameter estimates across\ntions, and again using the Modified Latin Hypercube to\nobtain 200 samples for numeric integration. In this experi-\nment, we used a larger sample size along with more variables\nto provide a sense of computational cost: Analysis was done\nusing desktop computer with a 32-bit operating system, 3.2\nGHz quad-processor (although STATA only used two\nTable 2. Simulation Results for the Complex Model (3,000 Monte Carlo Samples).\nSample size\nTrue \nTrue \nTrue ln() = 0\nNote. Histograms are of the LSSE estimates. LSSE = least squared simulated errors.\n8 SAGE Open\nprocessors)--Each estimate took approximately 1.5 min to\nobtain. Results indicate that the LSSE estimator does well\nwhen the model includes more explanatory variables (i.e.,\nthe true values and mean of the estimates are similar--the\nlargest bias being 0.009 for the variance parameter).\nMoreover, as in the preceding examples, the average of the\nrobust standard error estimates closely approximates the\nstandard deviations of the parameter sample distributions\nDiscussion\nThe LSSE estimator is consistent in sample size and number\nof simulation draws, and if the number of simulation draws\nrises faster than the square root of the samples size, it is\nasymptotically normal. This suggests that the LSSE is a\npromising estimator for structural models that do not have\nexplicit regression functions but for which we have a proba-\nbility model of unmeasured quantities. The two example\nand 5,000 indicate that the estimator indeed converges\ntoward a normal distribution with diminishing bias and\nincreasing precision.\nTo automate the Monte Carlo experiments across thou-\nsands of samples, and to focus on the main properties of the\nLSSE estimator, we did not directly address the potential for\nheteroscedasticity in each model but used a robust standard\nerror estimator instead, which uses a diagonal matrix for M\nin which the diagonal elements are a function of a consistent\nestimator of the variance for each observation. In practice, if\nstudy design or inspection of the residuals indicates that\nhomoscedasticity is plausible, then setting M to be an iden-\ntity matrix is advisable (or equivalently, ignoring M and\nusing Equation 7). However, the LSSE estimator is amenable\nto addressing heteroscedasticity and autocorrelation directly\nthrough the specification of M as is done with the usual fea-\nsible generalized least squared error estimators. It should be\nkept in mind, however, that homoscedasticity in the specified\nrandom term (, on our examples) does not imply homosce-\ndasticity in the regression error: The assumption of homosce-\ndasticity in the regression error needs to be assessed\nregardless of the specification of the random component of\nthe structural model (or a robust standard error estimator\nused, which would be less efficient). The assessment can be\nmade using the regression residuals from the LSSE estima-\ntor. Such an assessment would proceed as recommended in\nthe common linear and nonlinear regression modeling\nliterature.\nThe robust standard error estimator used in the Monte\nCarlo experiments performed well (as indicated by the simi-\nlarity between the standard deviation of estimates and the\nmean standard error estimates). However, the estimator was\nthat for the nonsimulated nonlinear least squares standard\nerrors typically reported by the statistical software, which\ndoes not account for noise due to the simulation process. It\nshould be noted that using a typical standard error estimator\n(i.e., a nonsimulated nonlinear least squared errors estimator)\nwill underestimate the true standard error. The LSSE estima-\ntor that uses direct random draws is approximately 1 1\n+\nR\ntimes the usual nonsimulated standard error estimator\n(McFadden & Ruud, 1994). Using the number of simulations\nin the hundreds (in our case, we used R = 200) makes this\nplace. Our use of a quasi\u00adMonte Carlo sequence of draws\n(i.e., our use of the Modified Latin Hypercube Sampling)\ndiminishes the bias further (Hess et al., 2006). Nonetheless,\nif a researcher is concerned with the underestimated standard\nerror, he or she can increase R, inflate the standard error\naccordingly, or, if otherwise appropriate, use a bootstrapped\nstandard error, which will automatically account for the sim-\nulation noise.\nThere are limitations to the LSSE method. First, the\nasymptotic properties depend on the regularity conditions\nand asymptotic properties of an unknown regression func-\ntion. A pragmatic solution is to engage a Monte Carlo inves-\ntigation of the finite sample properties of a proposed model\nTable 3. Parameters Estimates for the Complex Model With 10 Regressor Variables.\nParameter True value M estimate SD MSE\n\n\n\n\n\n\n\n\n\n\n\nVeazie and Cai 9\nprior to its estimation on real data. This is achieved by run-\nning a Monte Carlo experiment similar to those presented\nabove, only in this case based on the model being considered\nand the sample size of interest. If the model produces reason-\nable results for the specified sample size, then the use of\nLSSE would be indicated. If the LSSE cannot satisfactorily\nreproduce model parameters for the given sample size, then\neither the unknown regression function is not amenable to\nconsistent estimation by standard NLS or the rate of conver-\ngence is too slow for the model and sample size to be\nuseful.\nSecond, convergence to normality of some parameters\n(particularly the variance parameters) can be slower than\nothers, but it is clear that convergence toward normality is\nbeing achieved, as we expect from the estimator's asymp-\ntotic properties. If convergence to approximate normality is\nnot yet achieved (which often can be determined by inspect-\ning the estimator's bootstrapped distribution), then resam-\npling techniques such as the bootstrap or jackknife may be\nused to obtain standard errors, p values, and/or confidence\nintervals for these parameters.\nFinally, LSSE estimation employs the minimization of\nsquared errors, but unlike ordinary least squares and nonlin-\near least squares it is not asymptotically immune to distribu-\ntional assumptions. If the distribution of random quantities is\nmisspecified, then the simulated mean will not converge to\nthe proper expectation. In this respect, the LSSE estimator is\nsimilar to the maximum likelihood estimator because it\ndepends on the specification of a probability model. Hence,\nlike maximum likelihood estimation, care must be taken in\nspecifying the distribution.\nAs with maximum likelihood estimation, specification\ntests can be useful in identifying a statistically adequate\nmodel; however, the usefulness of such tests for the distribu-\ntion of latent variables will depend on the structural model's\nform and characteristics of the distributions being consid-\nered. For the simple Monte Carlo experiment presented\nabove, such tests worked reasonably well. Using a data set of\n1,000 observations, we compared the LSSE estimator based\non the correct log-normal distribution for  with one mis-\nspecified as  having a normal distribution and one misspeci-\nfied as  having a beta distribution on the unit interval.\nboth the normal and the beta distributions in favor of the cor-\nrespectively).\nSimilarly, specification of the model's functional form can\nalso be investigated using methods appropriate to linear and\nnonlinear regression. For example, using a sample of 1,000\nobservations from our simple Monte Carlo experiment, we\ncompared the correct specification of y x\ni i\ni\n= + \n  \n( ) with a\nmisspecified model y x\ni i\ni\n= + \n  \ntest rejected the misspecified model in favor of the correct\nGeneral advice for use of LSSE estimation follows that\nfor estimation of nonlinear models by any means. Because\nasymptotic properties of estimators for nonlinear models can\nbe of little comfort in finite samples, Monte Carlo investiga-\ntions for the given sample size ought to be used to determine\nwhether the potential bias is acceptable, to determine the\nproper standard error estimator and test statistic, and, for\nsimulation estimators such as this one, to select the number\nof simulations R for evaluating the integrals.\nThe applied researcher should find the LSSE estimator a\nuseful tool when other methods are not available. This is par-\nticularly true for those familiar with standard statistical soft-\nware that can estimate nonlinear least squares based on\nuser-specified functions. For example, we used the nonlinear\nleast squares command (i.e., \"nl\" command) in the common\nstatistical analysis software STATA Version 11 to implement\nthe Monte Carlo experiments presented above; the required\nuser-written program that calculates the conditional expecta-\ntions for each observation merely needs to embed a loop,\nacross observations, that implements a Monte Carlo integra-\ntion routine. The main pragmatic trade-off is that better\nresults accrue to larger data sets, but larger data sets require\ngreater computational time. However, rapid increase in com-\nputational speed available in today's computers makes this\ntrade-off a diminishing concern.\nAppendix A\nIn this appendix, we provide the proofs of the least squared\nsimulated errors (LSSE) properties of consistency and\nasymptotic normality.\nConsistency: Denoting SN R R R\n, ( ) ( ) ( )\n  \n=  \ne e\n \nM and\ngiven standard regularity assumptions for nonlinear least\nsquares estimators (NLS; see Amemiya, 1985, and Seber &\nWild, 2003, for necessary conditions), the LSSE is consistent\nif\nplim\nN\nS\nN\nS\n,\n, , * ,\n \n\n( )- ( )\n\n\n\n\n\nwhere * represents the true parameter value. Although the\nMonte Carlo estimator y\ni R\n ,\n( )\n is unbiased for m\ni\n(), S\n()\nis a nonlinear function of the residuals and is not unbiased\nwith respect to  for its nonsimulated true counterpart.\nBecause SN,R\n() is the sum of observation specific compo-\nnents Si,R\n(), the bias is the sum of individual biases associ-\nated with each component. Each component Si,R\n() is a\nsquared residual and can be expressed as a function of the\nmean for which we have an unbiased estimator. Using the\nnotation of dx\nk to indicate the kth derivative with respect to x\n(and further simplifying by denoting the first derivative as\nd\nx\n), a Taylor's series expansion about the true regression\nm\ni\n() and evaluating at y\ni R\n ,\n( )\n , yields\nS S y S m\nS m y m\ni R i R i R i i\nm i i i R i\ni\n, , ,\n,\n  \n  \n( ) = ( )\n( )= ( )\n( )+\n( )\n( ) ( )-\n\n\nd (\n( )\n( )+\n ( )\n( ) ( )- ( )\n( )\n=\n\np m\np\ni i i R i\np\np\ni\nS m y m\n! ,\n.\nd   \n\nTherefore, for a given R, the expectation with respect to \nof S\ni,R\n() is\nE S S m\nS m E y m\ni R i i\nA\nm i i i R i\ni\n\n\n \n  \n,\n,\n( )\n( )= ( )\n( )+\n( )\n( ) ( )- ( )\n(\nd ))+\n ( )\n( ) ( )- ( )\n( )\n=\nB\np m\np\ni i i R i\np\np\ni\nS m E y m\n! ,\nd   \n\n\n\nC\n.\nNotice that part B is zero, and because y\ni R\n ,\n( )\n is based on the\nsum of the function g evaluated at R independent draws from F\n, the\nexpectation in part C can be restated as\nE y m\nR\nE g x m\ni r i\np\np\ni r i\np\n\n\n \n   \n ,\n, | .\n( )- ( )\n( ) = \n( )\n( )- ( )\n( )\n-\nIf we define\nQ S m E g x m\ni\np\np m\np\ni i i r i\np\ni\n( ) ( ( )) ( ( , ( ) | ) ( )) ,\n!\n     \n\n   -\nthen\nN\nS\nN\nS m\nQ\nN R i i\ni\nN\nA\np i\np\ni\nN\nB\n,  \n\n( ) =  ( )\n( )\n+   ( )\n=\n-\n=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n=\n\n\np\nC\n.\nPart A of this equation is the sample average of the least\nsquared residuals associated with the NLS estimator, which\nis consistent for increasing N (Seber & Wild, 2003); part B\nare sample averages of Qi\np ( )\n that with respect to increasing\nN are finite for well-behaved data; therefore, part C is the\nsum of components that converge to zero as R increases.\nConsequently, with increasing R, the impact of Monte Carlo\nsimulation error disappears, and\nN\nS\nN\nS\n, ( ) ( )\n \n . It\nfollows that regarding increasing N and R,\nplim\nplim\nN\nN\nS\nN\nS\nN\nS\nN\nS\n,\n, , *\n*\n \n \n\n\n( )- ( )\n\n\n\n\n\n =\n( )- ( )\n\n\n\n\n\n\nProof that the right-hand side of this equation is zero fol-\nlows the well-established proof of consistency for the NLS\n(Seber & Wild, 2003). Hence, the LSSE estimator is consis-\ntent with respect to increasing N and R if the standard NLS\nestimator is consistent.\nAsymptotic Normality: For the consistent LSSE estimator\n\n , d )\n, ( converges to 0 and by the mean-value theo-\nrem we can write\nd d\nd\n \n\n = \n \nS\n, ,\n, ,\n( ) ( )+\n( ) -\n( )=\n\n\n\nfor some   \n( , *) . This implies\nS\nA\n  \n\n\n\n-\n( )= - ( )\n\n\n\n\n\n\n( )\n\n\n\n\n\n-\n*\n,\n,\n*\nN\nd\nN\nN\nd\n\n\n\n\nB\n.\nAsymptotic normality holds if part A of this equation con-\nverges to a constant and part B converges to a normal\ndistribution.\nConsider part B first. Note that d )\n, ( is the sum of\nindividualcomponents, d d\n \n \nN R i R\ni\nN\n, ,\n( ) ( )\n=\n=\n, and each\ncomponent can be expressed as a function of the simulated\nregressionfunction y\ni R\n ,\n( )\n suchthat d d\n \n \nS S y\ni R i i R\n, ,\n( ) ( ( )).\n= \nExpanding about the regression function m\ni\n(), we get\nd d\nd d\n \n\n \n \nS y S m\nS m y\ni i R i i\nA\nm i i i R\ni\n,\n,\n( )\n( )= ( )\n( )\n+ ( )\n( ) ( )-\n- ( )\n( )+\n ( )\n( ) ( )-\nm\nS m y m\ni\nB\np m\np\ni i i R i\ni\n\n  \n\n! ,\nd d (\n( )\n( )\n=\n\n p\np\nC\n.\nRegarding the expectation of this term with respect to , note that\ntheexpectedvalueofpartA withrespecttodrawsof\nr\nisjustA,the\nexpectedvalueofpartB is0because,again, E y m\ni R i\n  \n ,\n( ) ( )\n= ,\nandtheexpectedvalueofpartCisthesumof\nR\nW\np i\np\n-\n ( )\n ,where\nW S m E g x m\ni\np\np m\np\ni i i r i\np\ni\n( ) ( ( )) ( ( , ( ) | ) ( )) .\n!\n     \n \n=   -\nTherefore,\nlimit\nN\nN\nd limit\nd\ni\ni\nN\nA\nS\nN\nS\n\n\n\n\n,\n*\n*\n( )\n\n\n\n\n\n\n\n\n=\n( )\n=\n\n+  ( )\n( )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-\n=\n=\n\n\n N\nW\np i\ni\nN\np\nB\n*\n\n\n\n\n\n.\nThe limit of part A of this equation is the limit for the\nstandard NLS, which is asymptotically normal, (Seber &\n/ goes to 0, that\nis if R rises faster than the square root of N. Consequently,\nN\nS\nd \n, ( ) converges to a normally distributed variable\nif R rises faster than the square root of N and does not con-\nverge otherwise.\nRegarding part A of Equation A7, the limit of\n( / ) (\n,\nd\n\n)\n is addressed similarly to that of part B.\nExpressing d\n\n, ( )\n in terms of the unbiased Monte Carlo\nestimator for the mean function and expanding around the\ntrue mean yields\nd d\nd d\n \n\n \n\nS y S m\nS m y\ni i R i i\nA\nm i i i R\ni\n,\n,\n( )\n( )= ( )\n( )+\n( )\n( ) \n \n \n\n( )\n( )- ( )+\n ( )\n( ) ( )\nm\nS m y\ni\nB\np m\np\ni i i R\ni\n! ,\n- ( )\n( )\n=\n\n mi\np\np\nC\n\n.\nAs before, the expected value of part B of this equation,\nwith respect to the random draws of , is 0; therefore, the\nlimit of ( / ) (\n,\nd\n\n)\n reduces to the sum of the limits for\nparts A and C. The expected value of Part C, again asum over\n/ Rp- times functions of the pth centered moments\nof the simulated estimator for the regression mean, converges\nto zero as R goes to infinity. Hence, as R goes to infinity\nplim d plim d\n \n \n, ( ) ( )\n \n= , which is the limit of\npart A: This is the standard NLS that converges to a constant\ngiven the standard assumptions underlying nonlinear least\nAppendix B\nIn this appendix, we present the STATA 11 code used to esti-\nmate the models in the Monte Carlo experiments.\nFigure B1 presents the STATA program and the nl com-\nmand used to estimate the model of example 1. Lines 1\nthrough 25 specify the program that calculates the expected\nvalue for each observation. Lines 30 and 31 specify user-\nspecified parameters (not model parameters) used by the pro-\ngram--in this case, the number of Monte Carlo samples to\nuse for numeric integration (i.e., the quantity indicated as R\nin the manuscript). And, line 34 is the STATA nl command\nline that calls the program defined above and applies it to the\ndata (in this case, to variables y and x). See the STATA user\nmanuals for the details of the nl command.\nLines 1 through 10 of the program are STATA-specific\ncommands that name the program (line 1), variables (lines 4\nand 5), and model parameters (lines 7 through 10). Line 2 is\nused to specify the allowable syntax of the nl command (see\nthe STATA user manual for details). Line 12 sets the seed for\nthe STATA's random number generator to a user-supplied\nconstant determined at line 31 (we used the computer time\nclock to specify the seed). It is important to set the seed to the\nsame constant each time the program is called so that the\ncalculations yield the same result when STATAgives the pro-\ngram the same parameter values, and only differ when the\nprogram is given different parameter values. Lines 17\nthrough 22 calculate the Monte Carlo integration for the\nexpected values. In this example, we set the number of Monte\nCarlo samples for the integration to 200 (line 30), so a loop\nis set to iterate 200 times (line 17). Notice that rather than\ncalculating each observation's integral separately (i.e., loop-\ning over observation), which can be done, we are simultane-\nously calculating all integrals at once by generating and\nadding results from random vectors for each of the 200 itera-\ntions. Once the sum of these draws across the 200 iterations\nis obtained, we divide the sum by the number of iterations to\nobtain the vector of expected values for all observations (line\n23). This is the result that STATA uses in constructing the\nresiduals used to find parameter values that minimize the\nsum of the squared residuals. Line 34 contains the STATA nl\ncommand that uses the program nllnnorm to estimate model\nparameters (notice the program name has a prefix of \"nl\" in\nline 1, but the nl command in line 34 requires reference to the\nprogram name without the \"nl\" prefix). See the nl command\nin the STATA user manual for an explanation of the proper\nspecification of this command.\nFigure B2 presents the STATA program and the nl com-\nmand used to estimate the model of example 2. Lines 1\nthrough 28 specify the program that calculates the expected\nvalue for each observation. Lines 31 and 32 specify user-\nspecified parameters used by the program. Line 35 is the\nSTATA nl command line that calls the program defined\nabove and applies it to the data (in this case to variables\ny, x, w).\nLines 1 through 10 of the program are STATA-specific\ncommands that name the program (line 1), variables (lines 5\nthrough 7), and model parameters (lines 10 through 12). Line\n14 sets the seed for the STATA's random number generator to\na constant determined at line 32. Lines 19 through 26 calcu-\nlate the Monte Carlo integration for the expected values. As\nFigure B1. STATA code for estimation of Model 1.\nin the preceding example, we set the number of Monte Carlo\nsamples for the integration to 200 (line 31), so a loop is set to\niterate 200 times (line 19). Once the sum of the calculations\nacross the 200 iterations is obtained, we divide the sum by\nthe number of iterations to obtain the vector of expected val-\nues for all observations (line 26). As before, this is the result\nthat STATA uses in constructing the residuals used to find\nparameter values that minimize the sum of the squared resid-\nuals. Line 35 contains the STATA nl command that uses the\nprogram nlcomplex to estimate model parameters. It should\nbe noted that it only takes 28 lines of code to write the pro-\ngram required to estimate the complex model, much of which\nare STATA required standard lines.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nAmemiya, T. (1985). Advanced econometrics. Cambridge, MA:\nHarvardUniversity Press.\nClarke, K. A. (2003). Nonparametric model discrimination in inter-\nnational relations. Journal of Conflict Resolution, 47, 72-93.\nClarke, K. A. (2007). A simple distribution-free test for nonnested\nGourieroux, C., & Monfort, A. (1993). Simulation-based inference:\nA survey with special reference to panel data models. Journal\nHess, S., Train, K. E., & Polak, J. W. (2006). On the use of a\nModified Latin Hypercube Sampling (MLHS) method in\nthe estimation of a Mixed Logit Model for vehicle choice.\nTransportation Research Part B: Methodological, 40,\nMcFadden, D. (1989). A method of simulated moments for estima-\ntion of discrete response models without numerical integration.\nMcFadden, D., & Ruud, P. A. (1994). Estimation by simula-\nRobert, C. P., & Casella, G. (2004). Monte Carlo statistical meth-\nods (2nd ed.). New York, NY: Springer.\nSeber, G. A. F., & Wild, C. J. (2003). Nonlinear regression.\nHoboken, NJ: John Wiley.\nStern, S. (1997). Simulation-based estimation. Journal of Economic\nTrain, K. (2003). Discrete choice methods with simulation. New\nYork, NY: CambridgeUniversity Press.\nVeazie, P. J., & Cai, S. (2007). A connection between medication\nadherence, patient sense of uniqueness, and the personalization\nAuthor Biographies\nPeter J. Veazie, PhD, is associate professor in the Department of\nPublic Health Sciences, Chief of the Division of Health Policy and\nOutcomes Research, and Director of the Health Services Research\nand Policy Doctoral Program at the University of Rochester. His\nresearch focuses on medical and healthcare decision making, health\nand quality of life outcomes, and research methods.\nShubing Cai received her PhD in Health Services Research and\ncurrently is an assistant professor in the Department of Public\nHealth Sciences at the University of Rochester. Her main research\ninterests are focused on quality of care received by the elderly.\nShe is also interested in statistical modeling and causal\ninference.\nFigure B2. STATA code for estimation of Model 2."
}