{
    "abstract": "Abstract\nThe purpose of this study is to evaluate two methodological perspectives of test fairness using a national Secondary School\nCertificate (SSC) examinations. SSC is a suit of multi-subject national qualification tests at Grade 10 level in South Asian\ncountries, such as Bangladesh, India, and Pakistan. Because it is a high-stakes test, the fairness of SSC tests is a major concern\namong public and educational policy planners. This study is a first attempt to investigate test fairness of the national SSC\nexamination of Pakistan using two independent differential item functioning (DIF) and differential bundle functioning (DBF)\nprocedures. The SSC was evaluated for possible gender bias using multiple-choice tests in three core subjects, namely,\nEnglish, Mathematics, and Physics. The study was conducted in two phases using explanatory item response model (EIRM)\nand Simultaneous Item Bias Test (SIBTEST). In Phase 1, test items were studied for DIF, and items with severe DIF were\nflagged in each subject. In Phase 2, the item bundles were analyzed for DBF. Three items were detected with large DIF, one\nfor each subject, and one item bundle was detected with a negligible DBF. Taken together, the results demonstrate that there\nis no major threat to the validity of the interpretation of examinees' test scores on the SSC examination. The outcome from\nthis study provided evidence for test fairness, which will enhance test development practices at the national examination\nauthorities.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nDifferential item functioning (DIF) occurs when examinees\nwho have the same ability but belong to different groups\nhave a different probability of answering a test item cor-\nrectly, after being controlled for overall ability on the con-\nstruct measured by the test. DIF could be attributed as item\nbias or item impact. Item bias is a systematic error or invalid-\nity in how a test item measures a construct for the members\nof a particular group (Gierl, Rogers, & Klinger, 1999). When\nthe test item unfairly favors one group of examinees over\nanother, the item is considered biased. Alternatively, group\ndisparity on item performance due to actual knowledge and\nexperience of a group, on the construct of interest, is called\nitem impact. For example, the inclusion of map as part of the\nitem stem may not require prior knowledge of content on a\nmap but it could make an item easier or difficult for certain\nexaminees, based on their prior knowledge about the places\non the map (Ercikan, 2002). Differential bundle functioning\n(DBF) is a concept built upon DIF, in which a subset of items\nor testlets within a test are organized to form a group of two\nor more items. These testlets or item bundles are then ana-\nlyzed for differential performance among the groups, after\ncontrolling for their overall ability. Specific organizing prin-\nciples need to be followed while creating a bundle of items,\nsuch as grouping the items based on their content areas.\nResearchers from different geographical contexts have\npresented explanations as to why DIF or DBF occurs. For\nexample, a study from England (Ong, Williams, &\nas a function of mental capability of processing, storing, and\n1University of Alberta, Edmonton, Canada\n2Adam Smith Institute, Manchester, UK\n3Aga Khan University Examination Board, Karachi, Pakistan\nCorresponding Author:\nSyed Latifi, Center for Research in Applied Measurement and Evaluation,\nUniversity of Alberta, 6-110 Education North, Edmonton, Alberta, Canada\nEmail: syed.latifi@ualberta.ca\nDifferential Performance on National\nExams: Evaluating Item and Bundle\nFunctioning Methods using English,\nMathematics, and Science Assessments\nSyed Latifi1, Okan Bulut1, Mark Gierl1, Thomas Christie2, and\nShehzad Jeeva3\n Keywords\nexplanatory item response modeling, differential item functioning, differential bundle functioning, SIBTEST, validity, test\nfairness, national examinations\n2 SAGE Open\nretrieving the item solution. A study from Turkey\n(Kalaycioglu & Berberoglu, 2011) explained that the DIF in\nitems can be due to item format characteristics, subject mat-\nter\u00adrelated factors, and cognitive skills measured on the test.\nA study from China (Wei et al., 2012) has attributed gender\nDIF due to latent advantage of processing language and\nnumbers. A study from Pakistan (Abida et al., 2011) has\nattributed DIF as a function of weak instructional and assess-\nment practices.\nSecondary school leaving examinations (e.g., Grade 10,\nhigh school diploma) are high-stakes assessments, and ensur-\ning that these assessments are free from bias (DIF) is impor-\ntant from at least three ways. First, a fair test ensures that the\nexaminees have attained a prescribed level of achievement.\nSecond, a fair test enables the valid classification of examin-\nees with reference to their ability on the test. Third, a fair test\nprovides empirical evidence which could facilitate the criti-\ncal evaluation of educational objectives, examination poli-\ncies, and subject content, and, more broadly, could improve\nthe methods of instructions. However, there exists a gap in\nliterature about the possible differential performance in high-\nstakes South Asian context of Secondary School Certificate\n(SSC) examination. SSC is a suit of multi-subject national\nqualification tests at Grade 10 level in South Asian countries,\nsuch as Bangladesh, India, and Pakistan, which is equivalent\nto General Certificate of Secondary Education (GCSE)\nexamination in England. In Pakistan, SSC is considered high\nstakes and, to qualify in SSC, each examinee has to pass at\nleast eight subjects. If an examinee fails, then 2 years of aca-\ndemic training will be lost and the chances to continue edu-\ncation at the college and university level get reduced (Aly,\neducators working in different geographical and educational\nBisanz, Bisanz, Boughton, & Khaliq, 2001; Gierl et al.,\nMeasurement Models for DIF and DBF\nDifferent measurement methods could be used for studying\nDIF and DBF. These methods could use item response theory\n(IRT) or classical test theory (CTT). Methods based on IRT\nuse the property of parameter invariance for identifying the\nDIF and DBF between the group of interests (e.g., gender,\nsocial background, native language, etc.), whereas methods\nbased on CTT use the nonparametric techniques for identify-\ning DIF and DBF between the group of interests. Different\nIRT-based methods could be used to detect item and person\nparameter invariance, for example, hierarchical generalized\nlinear modeling (Kamata, 2001), extended structural equa-\ntion modeling (B. O. Muth\u00e9n, Kao, & Burstein, 1991), and\nexplanatory item response modeling (EIRM; De Boeck &\nThese methods evaluate the invariance in item and person\nparameters as well as the interactions between item and\nperson parameters that form the basis for DIF and DBF\nwithin the IRT framework. The DIF/DBF methods could be\nclassified based on the procedure they used for matching the\ngroups and on the assumptions they made for the item\nresponse function (IRF). The groups could be matched using\nobserved score or using the estimates of the latent variable.\nWhen the parametric assumptions are made for a functional\nform of IRF, the procedure is called parametric, and non-\nparametric otherwise. The Mantel\u00adHaenszel procedure\n(Holland & Thayer, 1988) and standardization method\n(Dorans & Kulick, 1986) are examples of observed-score\nnonparametric procedures, whereas the Simultaneous Item\nBias Test (SIBTEST; Shealy & Stout, 1993) is a latent-vari-\nable nonparametric procedure. Although many DIF detection\nprocedures are available, a relatively small number of these\nmethods are preferred based on their practicality as well as\non their theoretical and empirical strengths (Gierl, Gotzmann,\nDifferent DIF detection methods flag DIF differently, and\ntheir results could complement each other or may refute oth-\nerwise. Ideally, multiple methods should be employed for\nsimultaneous detection of DIF at the item and item\u00adbundle\nlevel. Both EIRM and SIBTEST are capable of detecting\nDIF at item and bundle level; however, their agreement of\ndetecting DIF was never evaluated in a high-stakes assess-\nment settings. Grade 10 school exiting examination, such as\nSSC, is a high-stakes assessment in almost all geographical\ncontexts of the world, and such examination often attracts\nhigh public scrutiny and accountability. To date, no attempt\nhas been made to study the fairness of SSC examination\nusing two statistically different differential functioning\nmethods. Hence, the purpose of this study was to contrast\nEIRM and SIBTEST procedures for differential item and\nbundle functioning using high-stakes secondary English,\nMathematics, and science assessments. Specifically, we\nevaluated the national SSC English, Mathematics, and\nPhysics exams for gender DIF and tested whether any of the\ntestlets created using subdomains within each subject show\ngender DBF. The outcomes from this methodological study\ncould reveal the detection concordance between the EIRM\nand SIBTEST methods, as well as it will disclose whether\nthe three national SSC assessments are free from fairness\nobjections.\nTheoretical Background\nDIF Analysis Framework\nThe DIF analysis framework employs the concepts of pri-\nmary and secondary dimensions to explain why DIF occurs.\nDimension refers to a substantive characteristic of an item\nthat can affect the probability of obtaining the correct\nresponse. Each item in a test is intended to measure the main\nconstruct called the primary dimension. DIF items measure\nat least one secondary dimension in addition to the primary\nLatifi et al. 3\ndimension (Ackerman, 1992; Boughton, Gierl, & Khaliq,\nsecondary dimension is auxiliary if it is intentionally assessed\nor nuisance if there is no intended reason for its existence.\nDIF caused by auxiliary dimensions is benign and reflects\nimpact, whereas DIF caused by nuisance dimensions is\nadverse and reflects bias. DIF is typically based on the com-\nparison of two groups: the reference group, which is usually\nthe majority group, and the focal group, which is usually a\nminority group. The focal group is compared with the refer-\nence group to detect bias in the items and item bundles.\nWhile examining the item\u00adbundle interaction between\nfocal and reference groups, uniform and nonuniform DIF/\nDBF could occur. The uniform DIF exists when the amount\nof DIF between focal and reference groups is always con-\nstant. That is, the probability of answering an item correctly\nis greater for one group uniformly across all ability levels.\nOn the contrary, nonuniform DIF occurs when there is an\ninteraction between ability level and group membership.\nThat is, the probability of answering an item correctly is non-\nuniform for examinee groups across levels of ability. In IRT\nterminology, nonuniform DIF is indicated by the crossing\nbetween the item characteristic curves of focal and reference\ngroups. SIBTEST is equally powerful for detecting uniform\nand nonuniform DIF/DBF, whereas EIRM only allows for\ntesting uniform DIF/DBF (French & Finch, 2015; Kan &\nFor this study, we have used the explanatory item\nresponse models (EIRMs; De Boeck & Wilson, 2004) and\nSIBTEST (Shealy & Stout, 1993). Both procedures are pre-\nferred methods based on their theoretical and empirical\nticularly advantageous in the detection of DIF/DBF because\nit allows the estimation of both fixed and random DIF/DBF\nmodels. The fixed DIF/DBF model assumes that the amount\nof DIF/DBF is fixed across persons, while the amount of\nDIF/DBF is allowed to vary across persons in the random\nDIF/DBF model. The random DIF/DBF model is also\nknown as random-weights DIF/DBF model (De Boeck &\nBy comparison, SIBTEST is particularly advantageous in\nsituations when examinees' prior knowledge of content\n(impact) is present in the data (Klockars & Lee, 2008), and\nits statistics could be compared against the well-established\ncriteria of DIF classification. Moreover, SIBTEST has been\nfound capable for adaptations to the multilevel data struc-\ntures (French & Finch, 2015), and its DIF statistics have\nbeen found robust, compared with other nonparametric DIF\ndetection procedures, when sample sizes for reference and\nfocal groups are small (Klockars & Lee, 2008; Roussos &\nStout, 1996b). Nevertheless, both EIRM and SIBTEST could\naccount for differences in ability between the focal and refer-\nence groups, have a well-established statistical foundation,\nthey are robust to different sample sizes, and could be used to\nevaluate items and item\u00adbundles (Briggs, 2008; De Boeck\nconditions that are common to most high-stakes assessment\nsettings.\nExplanatory Item Response Modeling\nAll item response models (IRMs) contain at least one param-\neter to describe the item and at least one parameter to describe\nthe person (Hambleton, Swaminathan, & Rogers, 1991),\nwhich could then be used for the measurement of item and\nperson properties. EIRM is an extension of IRM that employs\nthe generalized linear mixed modeling framework to explain\nthe item properties, person properties, and the interaction\nbetween item and person properties, thereby providing a\nbroader measurement framework than IRM (Briggs, 2008;\nWilson et al., 2008). The term \"explanatory\" refers to the con-\ntent and contextual variables that could be used to group the\nitem and/or the person based on common characteristics.\nIn the EIRM framework, dichotomously scored responses\nare denoted as Ypi = 0 or 1, where p is an index of person and\ni is an index for items. The expected value of Ypi is repre-\nsented by pi\n, which follows a binomial distribution. The\nlogit function is used to put the probability vales of pi\ninto\na continuous scale between - and + . Mathematically,\nthe logit function for the probability of responding an item\ncorrectly could be represented as   \npi pi pi\n= -\nln( )\n/( )\nUsing the notation from De Boeck et al. (2011), the one-\nparameter logistic (1PL) model can be formulated as\n  \npi p i\nk\nK\ni ik\nX\n= +\n=\n\n= for all items; Xik\nis a diagonal matrix with\nXik\n= 1, if i = k ( k K\n ; index k has the same range as\nindex i), and 0 otherwise; and \np is the ability parameter\nfor person p with a multivariate normal distribution\n( ~ ( , )),\n \n\np\n, and \ni\nrefers to the item easiness as\nopposed to item difficulty in the traditional IRT models. De\nBoeck et al. (2011) also suggested writing the model in a\nsimpler form as\n  \npi p i\nto highlight the plus sign as item easiness instead of item dif-\nficulty. Also, it should be noted that although the \ni\nparam-\neter represents \"item easiness,\" it in fact represents item\ndifficulty. The higher the item difficulty parameter is, the\neasier the item becomes to respond correctly. Furthermore,\nthis simple model can be extended by incorporating the\ngroup membership parameters Z, the group-by-item interac-\ntion parameter  \n( ), and Wpi to represent the product of item\nand group indicators ( ).\npi i\n= \u00d7 The extended version\nof EIRM is written as\n     \npi p i i pi\n= + + + ( )\nfocal\n4 SAGE Open\nHere, \nfocal\nrepresents the main effect of focal group in\ncomparison with the reference group, and  \ni\n( )\nis the DIF\nparameter. If an item is administered to two groups and they\nperformed equally, then the equation would become\n  \npi p i\n= + because Z Z\nfocal reference\n= = 0 . Conversely, if,\nfor item, the group-by-item interaction is found to be signifi-\ncantly different from zero, then the item would be flagged as\nexhibiting DIF.\nTesting DBF in EIRM\nA similar statistical model could be used for testing DBF\nwithin the EIRM framework, which involves testing the\ninteraction between the item and person properties. The term\n\"bundle\" indicates a set of items grouped together because\nthey share a common content dimension, cognitive similar-\nity, or a common item structure. However, in the EIRM\nframework, the DBF is a more parsimonious model for\ndetecting bias using the common characteristics of items (De\nBoeck & Wilson, 2004). Equation 3 could be extended for\nDBF by incorporating the paired values of (p, i), where the\npair (p, i) has a value of 1 on predictor h if person p belongs\nto the focal group and item i belongs to the bundle, and a\nvalue of 0 otherwise. Both DIF and DBF could be combined\nin one model as follows:\n     \npi p i p i h p i h\nh\nH\n= + + +\n=\n\nfocal focal\n( , )\n( )\n( , )\n.\nHere, \np\n, \ni\n, and \nfocal\nhave the same representation as\nbefore. However, Z p i\n( , )focal\n= 1 for the focal group and 0 for\nthe reference group with W p i h\n( , )\nas the person-by-item pre-\ndictor h, defined in such a way that W p i h\n( , )\n=1 if both\nZ p i\n( , )focal\n= 1 and either X p i k i\n( , ) =\n=1 (for item-specific DIF)\nor X p i k\n( , )\n=1 (for item bundle/subset DIF), and W p i h\n( , )\notherwise, and  \nh\n( ) as the corresponding DIF parameter (De\nTo identify items and bundles that function differentially\nfor male and female examinees, the lme4 package (Bates\nas the reference group and females as the focal group. A sta-\ntistically significant value that is positive indicates DIF/DBF\nagainst the focal group whereas a negative value indicates\nDIF/DBF against the reference group. Thus, the sign can be\nused to determine which group is favored. However, to date,\nthere is no guidelines that could be used to interpret and clas-\nsify the magnitude of DIF/DBF in the EIRM framework, and\nDIF/DBF is identified using statistical test of significance.\nSIBTEST\nSIBTEST provides a measure of effect size, Beta-uni (\n\nUNI\n), and a statistical test for each item or bundle. In this\napproach, the complete latent space is viewed as multidi-\nmensional, (, ), where  is the primary dimension and  is\nthe secondary dimension. The statistical hypothesis tested by\n: \n\nUNI\n: \n\nUNI\nis not\nrejected, then  = 0 and there is no DIF.\n\n\nUNI\ncan also be used to estimate the magnitude of DIF in\nterms of an effect size. To operationalize this approach, items\non the test are divided into the suspect subtest and the match-\ning or valid subtest. The suspect subtest contains the items or\nbundle of items that are believed to measure both primary\nand secondary dimensions, whereas the matching subtest\ncontains the items or bundle of items that are believed to\nmeasure only the primary dimension. The matching subtest\nplaces the reference and focal group examinees into sub-\ngroups at each score level so their performances on items\nfrom the suspect subtest can be compared. To estimate \n\nUNI\n,\nthe weighted mean difference between the reference and\nfocal groups on the suspect subtest item or bundle across the\nK subgroups is calculated by\n\n\nUNI\n=\n=\n\nk\nK\nk k\np d\npk\nis the proportion of focal group examinees in sub-\ngroup k, and dk\nis the difference in the adjusted means of\nreference and focal groups on the studied subtest items, or\nbundle, in each subgroup k (see also Jiang & Stout, 1998). A\nstatistically significant value of \n\nUNI\nthat is positive indi-\ncates DIF against the focal group whereas a negative value\nindicates DIF against the reference group. Thus, the sign can\nbe used to determine which group is favored.\nTo classify the SIBTEST effect size \n\nUNI\n, guidelines rec-\nommended by Dorans (1989) were used in this study. The\nabsolute values of the \n\nUNI\nmoderate DIF (Level B), and 0.100 and above indicate large\nDIF (Level C). Based on these guidelines, items are classi-\nfied as \"A\" (negligible DIF), \"B\" (moderate DIF), or \"C\"\n(large DIF). Also, to determine statistical significance, an\nalpha level of .05 was used. DIF literature also suggested\nother interpretation guidelines. For example, Jodoin and\nGierl (2001) suggested a slightly different guideline for clas-\nsifying DIF using logistic regression. However, the guide-\nlines suggested by Dorans are also widely used and they are\nappropriate for the current study (M. Gierl, personal com-\nmunication, January 6, 2016). More recently, Dorans's\nguidelines for interpreting the SIBTEST effect size were\nused by Puhan, Boughton, and Kim (2007) for analyzing the\ndifferential performance for test delivery mode (i.e., paper-\nand-pencil vs. computer-based testing) in a high-stakes\nassessment situation.\nAnalogous to the EIRM framework, DBF in SIBTEST\ncould be conceptualized as several DIF items acting in con-\ncert to produce an item bundle favoring one group over\nanother, as judged by a bundle score. DBF analysis requires\nthat the items be organized using certain organizing princi-\nples. Four organizing principles have been suggested in the\nspecification, content analysis, psychological analysis, and\nLatifi et al. 5\nempirical analysis. For the purpose of the present study, the\nbundles were created using the test specifications. The test\nspecifications and content-wise item details for each exami-\nnation are presented in Appendix A. In some cases, DIF at\nthe item level may not be statistically significant but can be\neasily detectable using the bundle approach (Douglas,\nacross the set of items can amplify the group difference.\nHence, DBF analysis is effective at identifying groups of\nitems that function together to generate a group difference.\nMethod\nData Source\nThe sample of this study represents Grade 10 students from\nthe affiliated schools of a national examination board in\nPakistan. The present study used test data from 103 randomly\nselected affiliated schools of the examination board, from\nannual SSC examination in year 2011. The question-level\ndata for three subjects were extracted from an electronic\ndatabase system, and the students' identifiable information\n(e.g., name, school name, and geographical details) was\nremoved before the data were released to the authors.\nThe subjects of English, Mathematics, and Physics exam-\ninations were chosen, not only because of their relative\nimportance of the results obtained from these tests in deter-\nmining career paths of the examinees but also because these\nsubjects are generally preferred by students (Iqbal, Shahzad,\n& Sohail, 2010). Each test item was developed by the con-\ntent specialists using item development guidelines and was\nreviewed for content representation and sensitivity by at\nleast three subject experts. Each subject has two paper\ncomponents: multiple-choice question (MCQ, Paper I) and\nconstructed-response question (CRQ, Paper II). For the pur-\npose of this study, only the MCQ portion of the exam was\nstudied. The MCQ test for English is composed of 25 items,\nthe Mathematics test includes 30 items, and the Physics test\ncontains 25 items. All items were dichotomously scored, and\nthe sample size details appeared in Table 1. All tests are\nbased on National Curriculum guidelines (Government of\nwhich describe the Competencies, Standards, and\nBenchmarks for SSC assessments. The test of English com-\nprises of two subcontent areas, whereas the tests of\nMathematics and Physics comprise of three subcontent areas.\nThe details related to the content area in each subject are\npresented in Appendix A.\nPsychometric Characteristics\nTo study the psychometric characteristics of the three test\nforms, the classical test score theory indices were computed\nusing BILOG-MG (Zimowski, Muraki, Mislevy, & Bock,\n2003). As shown in Table 1, the number of females outnum-\nbered the males by approximately 100 across the three sub-\nt test, the mean scores of male and female examinees differed\nsignificantly only for English (p < .05), whereas there was no\nmean difference in Mathematics and Physics. However, the\nweak Cohen's d statistic of 0.21 for English suggested com-\nparability among males and females, and thus overall the\nperformance of males and females was comparable across\nthree subjects.\nThe reliability of Mathematics examination is the highest\n(.89 for both male and females) while the reliabilities of the\nEnglish and Physics examinations were relatively lower but\nTable 1. Psychometric Characteristics for the SSC English, Mathematics, and Physics Examinations.\nCharacteristics\nEnglish Mathematics Physics\nMale Female Male Female Male Female\nNote. SSC = Secondary School Certificate.\naItem-to-total Pearson correlation (point-biserial).\nbAlpha reliability coefficient.\n6 SAGE Open\ncomparable (.79 and .81, respectively). The higher value of\nCronbach`s alpha for Mathematics reflects the higher dis-\ncrimination for the Mathematics items. This is likely due to\nthe test length and/or due to more structured nature of subject\nMathematics, compared with English and Physics.\nThe results presented in Table 1 indicate that the test devel-\nopers were successful in minimizing gender difference at the\noverall observed-score level, and there was no difference in\nmean performance between reference (male) and focal (female)\ngroups for English, Mathematics, and Physics examinations.\nSimilarly, the conformance on test difficulty and discrimination\nstatistics between male and female examinee groups confirms\nthe comparability of performance across three subjects. These\nfindings are important because they suggested that the gender\ngroups are equivalent and there is no prior mean differences\namong groups on the trait being tested, and the overall perfor-\nmance is comparable at the test score level.\nNext, the examinees' ability estimates were computed\nusing the ranef function within the lme4 package (De Boeck\net al., 2011). The accuracy of ability estimation using ranef\nwas found near perfect in other studies (e.g., Lamprianou,\n2013). Figure 1 represents the density plot of ability estimates\nfor male and female examinees in three subjects. The differ-\nence between the ability distributions of male and female\nexaminees in Mathematics and Physics tests was negligible.\nSimilarly, for English, the difference is fairly small, but the\nability distribution of male examinees has two small peaks on\nthe ability scale. Taken together, the comparison of ability\ndistributions indicates that both male and female examinees\ntend to perform equally across three subjects.\nTesting the Dimensionality of Data\nConfirmatory factor analysis (CFA) was conducted using\nwhether English, Mathematics, and Physics subtests of the\nSSC hold a unidimensional latent structure. A one-factor CFA\nmodel was fit to each of the three subtests of the SSC.\nGoodness-of-fit criteria, including comparative fit index\n(CFI), Tucker\u00adLewis index (TLI), and root mean square error\nof approximation (RMSEA), were used to evaluate model-\ndata fit of the one-factor CFA model. CFI and TLI are incre-\nmental fit indices that range between 0 and 1, with values\ncloser to 1 indicating good fit. RMSEAis an absolute fit index\nthat is independent of sample size and thus performs well as\nan indicator of practical fit. For CFA models with categorical\nTLI > .90, and CFI > .90 indicate good fit. As shown in Table\n2, the dimensionality analysis indicated that the three subjects\nhad acceptable levels of model-data fit based on all three\nFigure 1. Distributions of abilities of male and female students in English, Mathematics, and Physics test forms.\nTable 2. Results From Single-Factor CFA Model for Testing Unidimensionality in the SSC Subjects.\nSubject n 2 df 2/df CFI TLI RMSEA\nNote. CFA = confirmatory factor analysis; SSC = Secondary School Certificate; CFI = comparative fit index; TLI = Tucker\u00adLewis index; RMSEA = root\nmean square error of approximation.\nLatifi et al. 7\nmodel-fit criteria, suggesting evidence for the unidimensional\nstructure of three SSC subjects. The results of the chi-square\nmodel-fit tests also support this finding. For each subtest, the\nscree test (Cattell, 1966) was also conducted (see Appendix\nB) and it also conforms with the findings of CFA.\nTesting the Item and Model Fit\nNext, the IRT item difficulty and chi-square item-fit statis-\ntics were computed using the BILOG-MG (du Toit, 2003;\nZimowski et al., 2003). The RMSEA was also computed as\na second measure of item fit as well as for easy interpreta-\ntion of chi-square indices. The RMSEA was computed\nusing the formula suggested by Tennant and Pallant (2012;\nsee also Steiger, 1998). To interpret the RMSEA values,\nthe guidelines suggested by MacCallum, Browne, and\nSugawara (1996) were used, which suggested that the\nfit and below 0.08 shows a good fit (Hooper, Coughlan, &\nTable 3 shows the item difficulties and fit indices for the\nitems across English, Mathematics, and Physics test forms.\nThe average difficulty of items on test forms of English,\nrespectively. The item-fit statistics suggested that, for each\nsubject, almost all items had a good model fit. The only items\nthat provide poor fit to the data were 14 and 21 in Physics\nAfter establishing the test reliability and comparability on\nthe psychometric characteristics, the IRT ability estimates,\nthe unidimensionality of data, and the model-data fit between\nmale and female examinees' groups on three tests, the next\nstep was to analyze the differential item and bundle\nfunctioning.\nEIRM Results\nUnder the EIRM framework, the item-level DIF analysis was\ninitiated by estimating using the R implementation of general-\nized linear mixed-effects models (glmer) within the lme4\nTable 3. Item Difficulties and Fit Statistics From the 1PL Model.\nItem\nEnglish Mathematics Physics\nDifficulty 2 df p RMSEA Difficulty 2 df p RMSEA Difficulty 2 df p RMSEA\nNote. 1PL = one-parameter logistic; RMSEA = root mean square error of approximation.\n8 SAGE Open\npackage (Bates et al., 2014). The glmer function consists of a\nrandom component, a linear component, and a linking compo-\nnent. The examinees' responses on items were considered as\nthe random component, the test items and their interactions\nwith the gender group form the linear component, and the\nBernoulli/binomial distribution (logit) was used as a linking\ncomponent. The glmer considers the first item functions as the\nreference item, and that all other item parameters are estimated\nas deviations from the first (De Boeck et al., 2011). Thus, to get\nthe  \ni\n( ) statistics for the first item, glmer was run twice. For\neach item, the sign of an estimate was used to determine the\ndirection for DIF. Negative sign represents that the item favors\nfemales, and positive sign suggests that the item favors males.\nBy comparison, the \n\nUNI\nwas estimated using the software\nprogram SIBTEST (Shealy & Stout, 1993). For this phase,\neach test item was considered as a suspect subtest whereas\nthe remaining items were considered as the matching sub-\ntest. This process was then repeated for all the test items,\none by one. The sign of \n\nUNI\ndetermines the direction of\nDIF, with a negative \n\nUNI\nvalue favoring females and a\npositive \n\nUNI\nvalue favoring males. The DIF results are\nshown in Table 4.\nAssessing the Level of DIF\nTo visualize, the DIF estimates from SIBTEST were also\nplotted. As shown in Figure 2, the region above/below the\nred dotted line represents the items with large DIF (Level C)\nin favor of male/female group, the items between the red and\norange dotted lines represent the items with moderate DIF\n(Level B), and items between the orange dotted lines repre-\nsent the negligible DIF (Level A).\nFirst, the evaluation of Figure 2 suggested the even spread\nof values. That is, the items are evenly distributed among gen-\nder groups for all three subjects. Next, it also reveals that one\nTable 4. Comparison of DIF Results using EIRM and SIBTEST for English, Mathematics, and Physics Test Items.\nItem\nEnglish Mathematics Physics\n\n SE Favors Beta-uni Favors \n SE Favors Beta-uni Favors \n SE Favors Beta-uni Favors\nNote. DIF = differential item functioning; EIRM = explanatory item response model; SIBTEST = Simultaneous Item Bias Test.\nrepresents p < .05 as well as B and C level of DIF. *represents p < .05 only.\nLatifi et al. 9\nitem in each subject had large DIF. Specifically, Item 14 in the\nEnglish test and Item 11 in the Mathematics test favored males,\nwhereas Item 4 in Physics test favored females.\nAs an alternate view, the items could be organized based on\nthe content areas in each of the three subjects. As shown in\nFigure 3, there are two content areas in English--Listening\nSkills and Reading Skills--three subareas in Mathematics--\nCoordinate Geometry, Trigonometry, and Theorems; Fraction,\nFunctions, and Algebraic Manipulation; and Linear and\nQuadratic Equations, Inequalities, and Graphs--and three\nsubareas in Physics--Electronics, Telecom, and Radioactivity;\nElectrostatics, Current, and Magnetism; and Waves, Sound,\nand Optics. Figure 3 also suggested that the DIF statistics\n(\n\nUNI\n) are evenly spread across subcontent areas, among each\nsubject.\nDBF\nAs Phase 2 of this study, the analysis for DBF was conducted.\nFor each test, the test specification was used as an organization\nprinciple for forming the bundles of items (Gierl, 2005; Gierl\net al., 2004). The test specification was used because it not\nonly guides the assessment of dimensionality in a subject but\nalso outlines the achievement domain associated with the con-\ntent areas and cognitive skills (Gierl, 2005). The test specifica-\ntion and content-wise item details of this study are presented\nin Appendix A. Eight item bundles were created: two for\nEnglish and three for Mathematics and Physics. Table 5\npresents the indices from EIRM and SIBTEST DBF analyses.\nAmong eight bundles across three subjects, EIRM did not\ndetect any DBF. However, one bundle from English was found\nstatistically significant (p < .05), in favor of females within the\ncontent area of Listening Skills, as flagged by the SIBTEST.\nDiscussion and Conclusion\nCertificate, diploma, and other high-stakes examinations such\nas the SSC examination in South Asia will continue to be used\nfor making important decisions about the examinees and thus\nwill affect an individual`s career path. Hence, it is imperative\nthat test items be free from any source of bias. The findings\nfrom this study suggested that, for the most part, the items and\nitem\u00adbundles from three core subjects did not display DIF and\nDBF. The psychometric characteristics of the tests were com-\nparable for males and females across English, Mathematics,\nand Physics. First, the classical test indices were computed\nincluding internal consistency measure for evaluating the reli-\nability of tests. Second, the score distribution was evaluated\nfor comparability using ability estimated from 1PL model.\nAlso, using the effect size measure (i.e., Cohen`s d statistic),\nthe groups were found to be comparable on Mathematics and\nPhysics. However, the weak effect size measure for English\nsuggested that at the test level, the male and female examinees\nwere essentially the same and that neither was favored. Before\napplying the IRT-based EIRM, the data were also checked and\nfound unidimensional.\nFigure 2. Phase 1: Gender differences for the items from the SSC English, Mathematics, and Physics examinations.\nNote. SSC = Secondary School Certificate; DIF = differential item functioning.\nBoth EIRM and SIBTEST detection procedures identi-\nfied nearly the same number of items as DIF items; hence,\na consistent pattern of DIF was displayed across both statis-\ntical procedures. However, if the guidelines for categoriz-\ning the DIF were also developed for EIRM, then this\nframework could be improved because it would include\nboth a statistical test and an effect size measure for identify-\ning DIF. EIRM identified one item in each subject area as\npossessing large (or Level C) DIF, with two of these items\nfavoring males and one favoring females. Given the total\nnumber of items with less than Level C DIF items (e.g., 24\nof the 25 English items did not have Level C DIF), no\nnoticeable DIF was found, an indication that the test devel-\nopment practices remained successful in ensuring the test\nfairness because the majority of test items were found free\nfrom gender bias. Furthermore, the DBF analysis was con-\nducted where the bundles of items were created using the\nTable of Specification for each examination. From the eight\nTable 5. Comparison of DBF using EIRM and SIBTEST, for English, Mathematics, and Physics Test Items.\nSubject Content area No. of items EIRM SIBTEST\nNote. DBF = differential bundle functioning; EIRM = explanatory item response model; SIBTEST = Simultaneous Item Bias Test.\n*Significant at p < .05.\nFigure 3. Phase 2: Gender difference for items from SSC English, Mathematics, and Physics, May 2011 examinations, organized into\nbundles using the test specification.\nNote. SSC = Secondary School Certificate; DIF = differential item functioning.\nitem bundles, only one was found statistically significant in\nfavor of females.\nThe effect of gender in differential performance on content\nand cognitive skills was studied by Gierl, Bisanz, Bisanz, and\nBoughton (2003). They concluded that males perform better\nthan females on items that require significant spatial process-\ning and that females perform better than males on items requir-\ning memorization (or memory recall). Furthermore, Kan and\nBulut (2014) reported that the items presented as word prob-\nlems were differentially easier for female examinees. As\nListening Skills were tested using a recorded dialogue (which\nwas played at the exam hall using CD/cassette players) and the\nexaminees are expected to recall dialogue and answer the test\nitems, the finding from the present study is consistent with\nother reported findings in the literature (Gierl et al., 2003; Kan\n& Bulut, 2014). In this case, the items may reflect impact, not\nbias. Moreover, a Level C item from Waves, Sounds, and\nOptic of Physics may be due to impact, as it is assessing the\nknowledge from ultrasound (see Appendix C), which female\nexaminees are more likely to comprehend. This is also consis-\ntent with the earlier studies in which females were found to\nhave higher attitude toward social implications of science than\nThe Level B DIF items, which are by definition produc-\ning moderate DIF, were distributed evenly between male\nand female examinee groups. Moderate DIF is of little con-\ncern at this stage as it would be more challenging to explain\nthan the large DIF. However, in large-scale testing pro-\ngrams, many DIF items are classified as moderate DIF\nonly found 16% Level B items, with negligible composite\neffect (i.e.,  \n\n\nUNI\n0.01). This finding also confirms that\nthe tests were only assessing the primacy dimension and\nthus were fair overall.\nThe gender-specific difficulty estimates (male, female) for\nsevere DIF items are also in concordance with the EIRM and\nneeded to interpret the reason for the DIF for these three\nitems. For example, for Physics, a plausible explanation may\nbe that females outperformed males because females may\nhave prior knowledge about the use of ultrasound, and thus\nfemales may be more proficient than males when answering\nItem 4; this DIF is due to systematic difference between\nactual performance of males and females and should be attrib-\nuted as impact. The details about these Level C questions can\nbe inspected in Appendix C. The Phase 1 DIF was repeated\nafter dropping these three items, and this time no item was\nflagged as Level C DIF in any of the three subject tests.\nTaken together, the results of this study indicate that there\nwere only three items with Level C DIF in the studied sub-\njects of English, Mathematics, and Physics examinations,\nand the item bundle for Listening Skills was found signifi-\ncantly favoring female. However, to date, no agreed guide-\nlines exist to categorize the DIF and DBF estimate from\nEIRM, and DBF estimate from SIBTEST, and thus research\nis needed to identify and evaluate the guidelines for inter-\npreting differential item and bundle functioning in EIRM\nframework. Furthermore, the results from the present study\nsuggested that the small amount of DIF found does not con-\nfound the validity of the interpretation of the examinees' test\nscores on three studied SSC subjects and, based on the statis-\ntical evidence, it is safe to assume that the test development\npractices produce items that are generally fair for both gen-\nder groups. However, the substantive review by the panel of\ncontent experts could further confirm these findings.\nFinally, in this study, we have presented and compared two\nmethods for assessing uniform and nonuniform DIF, by using\nthe data from high-stakes examination settings. The methods\npresented in the study have a direct implication for practice in\nat least three ways. First, although the data from SSC context\nwere used, the method presented in this study could be\nadopted in other high-stakes assessment situations. Second,\nassessing differential functioning at the level of item and bun-\ndle allows two layers of analyzing the test fairness, one at\nitem level and other at subtest level. Third, the item\u00adbundle\napproach could facilitate the discussion about the validity of\nconstruct because the bundle approach provides more cohe-\nsive unit of evidence compared with the differential perfor-\nmances at the item level. Thus, the explanations of DIF\nthrough DBF could facilitate test developer, test publishers,\nsubject teachers, and generally to all those who directly or\nindirectly use scores from high-stakes assessments for mak-\ning judgment about students, assessments, or both.\nAppendix A\nTest Specification for English, Mathematics, and Physics Test\nForms.\nSubject Subject content area\nItem No. in\ntest form\nTotal\nitems\nMathematics Coordinate\nGeometry,\nTrigonometry, and\nTheorems\nFraction, Functions,\nand Algebraic\nManipulation\nLinear and\nQuadratic Equations,\nInequalities, and\nGraphs\nPhysics Electronics, Telecom,\nand Radioactivity\nElectrostatics,\nCurrent, and\nMagnetism\nWaves, Sound, and\nOptics\nAppendix B\nScree plots of the SSC English (top), Mathematics (middle), and\nPhysics (bottom) tests.\nNote. SSC = Secondary School Certificate.\nAppendix C\nTest Items With Severe Bias.\nSubject Question Content area\nEnglish 14. According to the passage,\nthe construction of the Pisa\ntower began in\nReading Skills\nA. the middle of the Field of\nMiracles\nB. white marble\nD. the period of Mussolini\nMathematics 11. The pair of points which lie\non the straight line (AB)  is\nLinear and\nQuadratic\nEquations,\nInequalities,\nand Graphs\nPhysics 4. Ultrasound is used for\ndifferent purposes, which of the\nfollowing is not a current use of\nultrasound?\nWaves, Sound,\nand Optics\nA. Detection of fault in engine\nB. Measure the depth of an\nocean\nC. Diagnosis of different\ndiseases\nD. Ranging and detection of\nairplanes\n"
}