{
    "abstract": "Abstract\nIn information societies, operations, decisions and choices previously left to humans are increasingly delegated to\nalgorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken\nas a result. More and more often, algorithms mediate social processes, business transactions, governmental decisions,\nand how we perceive, understand, and interact among ourselves and with the environment. Gaps between the design and\noperation of algorithms and our understanding of their ethical implications can have severe consequences affecting\nindividuals as well as groups and whole societies. This paper makes three contributions to clarify the ethical importance\nof algorithmic mediation. It provides a prescriptive map to organise the debate. It reviews the current discussion of\nethical aspects of algorithms. And it assesses the available literature in order to identify areas requiring further work to\ndevelop the ethics of algorithms.\n",
    "reduced_content": "Original Research Article\nThe ethics of algorithms:\nMapping the debate\nBrent Daniel Mittelstadt1, Patrick Allo1, Mariarosaria Taddeo1,2,\nSandra Wachter2 and Luciano Floridi1,2\n Keywords\nAlgorithms, automation, Big Data, data analytics, data mining, ethics, machine learning\nIntroduction\nIn information societies, operations, decisions and\nchoices previously left to humans are increasingly dele-\ngated to algorithms, which may advise, if not decide,\nabout how data should be interpreted and what actions\nshould be taken as a result.1 Examples abound.\nProfiling and classification algorithms determine how\nindividuals and groups are shaped and managed\n(Floridi, 2012). Recommendation systems give users\ndirections about when and how to exercise, what to\nbuy, which route to take, and who to contact (Vries,\npromise in helping make sense of emerging streams of\nbehavioural data generated by the `Internet of Things'\n(Portmess and Tower, 2014: 1). Online service providers\ncontinue to mediate how information is accessed with\npersonalisation and filtering algorithms (Newell and\nlearning algorithms automatically identify misleading,\nbiased or inaccurate knowledge at the point of creation\n(e.g. Wikipedia's Objective Revision Evaluation\nService). As these examples suggest, how we perceive\nand understand our environments and interact with\nthem and each other is increasingly mediated by\nalgorithms.\nAlgorithms are inescapably value-laden (Brey and\nare specified by developers and configured by users with\ndesired outcomes in mind that privilege some values\nand interests over others (cf. Friedman and\nwithin accepted parameters does not guarantee ethic-\nally acceptable behaviour. This is shown, for example,\nby profiling algorithms that inadvertently discriminate\nagainst marginalised populations (Barocas and Selbst,\n1Oxford Internet Institute, University of Oxford, Oxford, UK\n2Alan Turing Institute, British Library, London, UK\nCorresponding author:\nBrent Daniel Mittelstadt, Oxford Internet Institute, University of Oxford,\n1 St Giles, Oxford OX1 3JS, UK.\nEmail: brent.mittelstadt@oii.ox.ac.uk\nBig Data & Society\nReprints and permissions:\nsagepub.com/journalsPermissions.nav\nbds.sagepub.com\nCreative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons Attribution-\nNonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use, reproduction\nand distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nadvertisements according to perceived ethnicity\nDetermining the potential and actual ethical impact\nof an algorithm is difficult for many reasons.\nIdentifying the influence of human subjectivity in algo-\nrithm design and configuration often requires investi-\ngation of long-term, multi-user development processes.\nEven with sufficient resources, problems and underlying\nvalues will often not be apparent until a problematic\nuse case arises. Learning algorithms, often quoted as\nthe `future' of algorithms and analytics (Tutt, 2016),\nintroduce uncertainty over how and why decisions are\nmade due to their capacity to tweak operational par-\nameters and decision-making rules `in the wild'\n(Burrell, 2016). Determining whether a particular prob-\nlematic decision is merely a one-off `bug' or evidence of\na systemic failure or bias may be impossible (or at least\nhighly difficult) with poorly interpretable and predict-\nable learning algorithms. Such challenges are set to\ngrow, as algorithms increase in complexity and interact\nwith each other's outputs to take decisions (Tutt, 2016).\nThe resulting gap between the design and operation of\nalgorithms and our understanding of their ethical impli-\ncations can have severe consequences affecting individ-\nuals, groups and whole segments of a society.\nIn this paper, we map the ethical problems prompted\nby algorithmic decision-making. The paper answers\ntwo questions: what kinds of ethical issues are raised\nby algorithms? And, how do these issues apply to algo-\nrithms themselves, as opposed to technologies built\nupon algorithms? We first propose a conceptual map\nbased on six kinds of concerns that are jointly sufficient\nfor a principled organisation of the field. We argue that\nthe map allows for a more rigorous diagnosis of ethical\nchallenges related to the use of algorithms. We then\nreview the scientific literature discussing ethical aspects\nof algorithms to assess the utility and accuracy of the\nproposed map. Seven themes emerged from the litera-\nture that demonstrate how the concerns defined in the\nproposed map arise in practice. Together, the map and\nreview provide a common structure for future discus-\nsion of the ethics of algorithms. In the final section of\nthe paper we assess the fit between the proposed map\nand themes currently raised in the reviewed literature to\nidentify areas of the `ethics of algorithms' requiring\nfurther research. The conceptual framework, review\nand critical analysis offered in this paper aim to\ninform future ethical inquiry, development, and gov-\nernance of algorithms.\nBackground\nTo map the ethics of algorithms, we must first define\nsome key terms. `Algorithm' has an array of meanings\nacross computer science, mathematics and public\ndiscourse. As Hill explains, ``we see evidence that any\nprocedure or decision process, however ill-defined,\ncan be called an `algorithm' in the press and in\npublic discourse. We hear, in the news, of `algorithms'\nthat suggest potential mates for single people and\nalgorithms that detect trends of financial benefit to\nmarketers, with the implication that these algorithms\nscholarly critiques also fail to specify technical cate-\ngories or a formal definition of `algorithm' (Burrell,\nnot in reference to the algorithm as a mathematical\nconstruct, but rather the implementation and inter-\naction of one or more algorithms in a particular pro-\ngram, software or information system. Any attempt to\nmap an `ethics of algorithms' must address this confla-\ntion between formal definitions and popular usage of\n`algorithm'.\nan algorithm as a mathematical construct with ``a finite,\nabstract, effective, compound control structure,\nimperatively given, accomplishing a given purpose\nunder given provisions.'' However, our investigation\nwill not be limited to algorithms as mathematical con-\nstructs. As suggested by the inclusion of `purpose' and\n`provisions' in Hill's definition, algorithms must be\nimplemented and executed to take action and have\neffects. The popular usage of the term becomes relevant\nhere. References to algorithms in public discourse do\nnot normally address algorithms as mathematical con-\nstructs, but rather particular implementations. Lay\nusage of `algorithm' also includes implementation of\nthe mathematical construct into a technology, and an\napplication of the technology configured for a particular\ntask.2 A fully configured algorithm will incorporate the\nabstract mathematical structure that has been imple-\nmented into a system for analysis of tasks in a particu-\nlar analytic domain. Given this clarification, the\nconfiguration of an algorithm to a specific task or data-\nset does not change its underlying mathematical repre-\nsentation or system implementation; it is rather a\nfurther tweaking of the algorithm's operation in rela-\ntion to a specific case or problem.\nAccordingly, it makes little sense to consider the\nethics of algorithms independent of how they are imple-\nmented and executed in computer programs, software\nand information systems. Our aim here is to map the\nethics of algorithms, with `algorithm' interpreted along\npublic discourse lines. Our map will include ethical\nissues arising from algorithms as mathematical con-\nstructs, implementations (technologies, programs) and\nconfigurations (applications).3 Where discussion\nfocuses on implementations or configurations (i.e. an\nartefact with an embedded algorithm), we limit our\n2 Big Data & Society\nfocus to issues relating to the algorithm's work, rather\nthan all issues related to the artefact.\nHowever, as noted by Hill above, a problem with the\npopular usage of `algorithm' is that it can describe ``any\nprocedure or decision process,'' resulting in a prohibi-\ntively large range of artefacts to account for in a map-\nping exercise. Public discourse is currently dominated\nby concerns with a particular class of algorithms that\nmake decisions, e.g. the best action to take in a given\nsituation, the best interpretation of data, and so on.\nSuch algorithms augment or replace analysis and deci-\nsion-making by humans, often due to the scope or scale\nof data and rules involved. Without offering a precise\ndefinition of the class, the algorithms we are interested\nin here are those that make generally reliable (but sub-\njective and not necessarily correct) decisions based\nupon complex rules that challenge or confound\nhuman capacities for action and comprehension.4 In\nother words, we are interested in algorithms whose\nactions are difficult for humans to predict or whose\ndecision-making logic is difficult to explain after the\nfact. Algorithms that automate mundane tasks, for\ninstance in manufacturing, are not our concern.\nDecision-making algorithms are used across a var-\niety of domains, from simplistic decision-making\nmodels (Levenson and Pettrey, 1994) to complex profil-\ning algorithms (Hildebrandt, 2008). Notable contem-\nporary examples include online software agents used\nby online service providers to carry out operations on\nthe behalf of users (Kim et al., 2014); online dispute\nresolution algorithms that replace human decision-\nmakers in dispute mediation (Raymond, 2014;\nShackelford and Raymond, 2014); recommendation\nand filtering systems that compare and group users to\nprovide personalised content (Barnet, 2009); clinical\ndecision support systems (CDSS) that recommend diag-\nnoses and treatments to physicians (Diamond et al.,\n\u00b4 , 1990); and predictive policing systems\nthat predict criminal activity hotspots.\nThe discipline of data analytics is a standout exam-\nple, defined here as the practice of using algorithms to\nmake sense of streams of data. Analytics informs imme-\ndiate responses to the needs and preferences of the users\nof a system, as well as longer term strategic planning\nand development by a platform or service provider\n(Grindrod, 2014). Analytics identifies relationships\nand small patterns across vast and distributed datasets\n(Floridi, 2012). New types of enquiry are enabled,\nincluding behavioural research on `scraped' data (e.g.\ngrained behaviours and preferences (e.g. sexual orien-\ntation or political opinions; (Mahajan et al., 2012);\nand prediction of future behaviour (as used in predict-\nive policing or credit, insurance and employment\nscreening; Zarsky, 2016). Actionable insights (more on\nthis later) are sought rather than causal relationships\nAnalytics demonstrates how algorithms can chal-\nlenge human decision-making and comprehension\neven for tasks previously performed by humans. In\nmaking a decision (for instance, which risk class a pur-\nchaser of insurance belongs to), analytics algorithms\nwork with high-dimension data to determine which fea-\ntures are relevant to a given decision. The number of\nfeatures considered in any such classification task can\nrun into the tens of thousands. This type of task is thus\na replication of work previously undertaken by human\nworkers (i.e. risk stratification), but involving a quali-\ntatively different decision-making logic applied to\ngreater inputs.\nAlgorithms are, however, ethically challenging not\nonly because of the scale of analysis and complexity\nof decision-making. The uncertainty and opacity of\nthe work being done by algorithms and its impact is\nalso increasingly problematic. Algorithms have trad-\nitionally required decision-making rules and weights\nto be individually defined and programmed `by hand'.\nWhile still true in many cases (Google's PageRank\nalgorithm is a standout example), algorithms increas-\ningly rely on learning capacities (Tutt, 2016).\nMachine learning is ``any methodology and set of\ntechniques that can employ data to come up with\nnovel patterns and knowledge, and generate models\nthat can be used for effective predictions about the\ndata'' (Van Otterlo, 2013). Machine learning is defined\nby the capacity to define or modify decision-making\nrules autonomously. A machine learning algorithm\napplied to classification tasks, for example, typically\nconsists of two components, a learner which produces\na classifier, with the intention to develop classes that\ncan generalise beyond the training data (Domingos,\n2012). The algorithm's work involves placing new\ninputs into a model or classification structure. Image\nrecognition technologies, for example, can decide what\ntypes of objects appear in a picture. The algorithm\n`learns' by defining rules to determine how new inputs\nwill be classified. The model can be taught to the algo-\nrithm via hand labelled inputs (supervised learning); in\nother cases the algorithm itself defines best-fit models to\nmake sense of a set of inputs (unsupervised learning)5\nalgorithm defines decision-making rules to handle new\ninputs. Critically, the human operator does not need to\nunderstand the rationale of decision-making rules\nAs this explanation suggests, learning capacities\ngrant algorithms some degree of autonomy.\nThe impact of this autonomy must remain uncertain\nto some degree. As a result, tasks performed by\nmachine learning are difficult to predict beforehand\nMittelstadt et al. 3\n(how a new input will be handled) or explain afterwards\n(how a particular decision was made). Uncertainty can\nthus inhibit the identification and redress of ethical\nchallenges in the design and operation of algorithms.\nMap of the ethics of algorithms\nUsing the key terms defined in the previous section, we\npropose a conceptual map (Figure 1) based on six types\nof concerns that are jointly sufficient for a principled\norganisation of the field, and conjecture that it allows\nfor a more rigorous diagnosis of ethical challenges\nrelated to the use of algorithms. The map is not pro-\nposed from a particular theoretical or methodological\napproach to ethics, but rather is intended as a prescrip-\ntive framework of types of issues arising from algo-\nrithms owing to three aspects of how algorithms\noperate. The map takes into account that the algo-\nrithms this paper is concerned with are used to (1)\nturn data into evidence for a given outcome (henceforth\nconclusion), and that this outcome is then used to (2)\ntrigger and motivate an action that (on its own, or when\ncombined with other actions) may not be ethically neu-\ntral. This work is performed in ways that are complex\nand (semi-)autonomous, which (3) complicates appor-\ntionment of responsibility for effects of actions driven\nby algorithms. The map is thus not intended as a tool to\nhelp solve ethical dilemmas arising from problematic\nactions driven by algorithms, but rather is posed as\nan organising structure based on how algorithms oper-\nate that can structure future discussion of ethical issues.\nThis leads us to posit three epistemic, and two norma-\ntive kinds of ethical concerns arising from the use of\nalgorithms, based on how algorithms process data to\nproduce evidence and motivate actions. These concerns\nare associated with potential failures that may involve\nmultiple actors, and therefore complicate the question\nof who should be held responsible and/or accountable\nfor such failures. Such difficulties motivate the addition\nof traceability as a final, overarching, concern.\nInconclusive evidence\nWhen algorithms draw conclusions from the data they\nprocess using inferential statistics and/or machine\nlearning techniques, they produce probable6 yet inevit-\nably uncertain knowledge. Statistical learning theory\n(James et al., 2013) and computational learning\ntheory (Valiant, 1984) are both concerned with the\ncharacterisation and quantification of this uncertainty.\nIn addition to this, and as often indicated, statistical\nmethods can help identify significant correlations, but\nthese are rarely considered to be sufficient to posit the\nexistence of a causal connection (Illari and Russo, 2014:\nChapter 8), and thus may be insufficient to motivate\naction on the basis of knowledge of such a connection.\nThe term actionable insight we mentioned earlier can be\nseen as an explicit recognition of these epistemic\nlimitations.\nAlgorithms are typically deployed in contexts where\nmore reliable techniques are either not available or too\ncostly to implement, and are thus rarely meant to be\ninfallible. Recognising this limitation is important, but\nshould be complemented with an assessment of how the\nrisk of being wrong affects one's epistemic responsibil-\nities (Miller and Record, 2013): for instance, by\nweakening the justification one has for a conclusion\nbeyond what would be deemed acceptable to justify\naction in the context at hand.\nInscrutable evidence\nWhen data are used as (or processed to produce) evi-\ndence for a conclusion, it is reasonable to expect that the\nconnection between the data and the conclusion should\nbe accessible (i.e. intelligible as well as open to scrutiny\nand perhaps even critique).7 When the connection is not\nobvious, this expectation can be satisfied by better access\nas well as by additional explanations. Given how algo-\nrithms operate, these requirements are not automatically\nsatisfied. A lack of knowledge regarding the data being\nused (e.g. relating to their scope, provenance and qual-\nity), but more importantly also the inherent difficulty in\nthe interpretation of how each of the many data-points\nused by a machine-learning algorithm contribute to the\nconclusion it generates, cause practical as well as prin-\ncipled limitations (Miller and Record, 2013).\nMisguided evidence\nAlgorithms process data and are therefore subject to a\nlimitation shared by all types of data-processing,\nnamely that the output can never exceed the input.\nFigure 1. Six types of ethical concerns raised by algorithms.\n4 Big Data & Society\nWhile Shannon's mathematical theory of communica-\ntion (Shannon and Weaver, 1998), and especially some\nof his information-inequalities, give a formally precise\naccount of this fact, the informal `garbage in, garbage\nout' principle clearly illustrates what is at stake here,\nnamely that conclusions can only be as reliable (but\nalso as neutral) as the data they are based on.\nEvaluations of the neutrality of the process, and by\nconnection whether the evidence produced is mis-\nguided, are of course observer-dependent.\nUnfair outcomes\nThe three epistemic concerns detailed thus far address\nthe quality of evidence produced by an algorithm that\nmotivates a particular action. However, ethical evalu-\nation of algorithms can also focus solely on the action\nitself. Actions driven by algorithms can be assessed\naccording to numerous ethical criteria and principles,\nwhich we generically refer to here as the observer-\ndependent `fairness' of the action and its effects. An\naction can be found discriminatory, for example,\nsolely from its effect on a protected class of people,\neven if made on the basis of conclusive, scrutable and\nwell-founded evidence.\nTransformative effects\nThe ethical challenges posed by the spreading use\nof algorithms cannot always be retraced to clear cases\nof epistemic or ethical failures, for some of the effects of\nthe reliance on algorithmic data-processing and (semi-)\nautonomous decision-making can be questionable and\nyet appear ethically neutral because they do not seem to\ncause any obvious harm. This is because algorithms can\naffect how we conceptualise the world, and modify its\nsocial and political organisation (cf. Floridi, 2014).\nAlgorithmic activities, like profiling, reontologise the\nworld by understanding and conceptualising it in new,\nunexpected ways, and triggering and motivating actions\nbased on the insights it generates.\nTraceability\nAlgorithms are software-artefacts used in data-proces-\nsing, and as such inherit the ethical challenges asso-\nciated with the design and availability of new\ntechnologies and those associated with the manipula-\ntion of large volumes of personal and other data. This\nimplies that harm caused by algorithmic activity is hard\nto debug (i.e. to detect the harm and find its cause), but\nalso that it is rarely straightforward to identify who\nshould be held responsible for the harm caused.8\nWhen a problem is identified addressing any or all of\nthe five preceding kinds, ethical assessment requires\nboth the cause and responsibility for the harm to be\ntraced.\nThanks to this map (Figure 1), we are now able to\ndistinguish epistemological, strictly ethical and traceabil-\nity types in descriptions of ethical problems with algo-\nrithms. The map is thus intended as a tool to organise a\nwidely dispersed academic discourse addressing a diver-\nsity of technologies united by their reliance on algo-\nrithms. To assess the utility of the map, and to observe\nhow each of these kinds of concerns manifests in ethical\nproblems already observed in algorithms, a systematic\nreview of academic literature was carried out.9 The fol-\nlowing sections (4 to 10) describe how ethical issues and\nconcepts are treated in the literature explicitly discussing\nthe ethical aspects of algorithms.\nInconclusive evidence leading to\nunjustified actions\nMuch algorithmic decision-making and data mining\nrelies on inductive knowledge and correlations identi-\nfied within a dataset. Causality is not established prior\nto acting upon the evidence produced by the algorithm.\nThe search for causal links is difficult, as correlations\nestablished in large, proprietary datasets are frequently\nnot reproducible or falsifiable (cf. Ioannidis, 2005;\nLazer et al., 2014). Despite this, correlations based on\na sufficient volume of data are increasingly seen as suf-\nficiently credible to direct action without first establish-\ning causality (Hildebrandt, 2011; Hildebrandt and\nZarsky, 2016). In this sense data mining and profiling\nalgorithms often need only establish a sufficiently reli-\nable evidence base to drive action, referred to here as\nactionable insights.\nActing on correlations can be doubly problematic.10\nSpurious correlations may be discovered rather than\ngenuine causal knowledge. In predictive analytics cor-\nrelations are doubly uncertain (Ananny, 2016). Even if\nstrong correlations or causal knowledge are found, this\nknowledge may only concern populations while actions\nare directed towards individuals (Illari and Russo,\ncategories . . . signal certainty, discourage alternative\nexplorations, and create coherence among disparate\nobjects,'' all of which contribute to individuals being\ndescribed (possibly inaccurately) via simplified models\nor classes (Barocas, 2014). Finally, even if both actions\nand knowledge are at the population-level, our actions\nmay spill over into the individual level. For example,\nthis happens when an insurance premium is set for a\nsub-population, and hence has to be paid by each\nmember. Actions taken on the basis of inductive cor-\nrelations have real impact on human interests inde-\npendent of their validity.\nMittelstadt et al. 5\nInscrutable evidence leading to opacity\nThe scrutability of evidence, evaluated in terms of the\ntransparency or opacity of algorithms, proved a major\nconcern in the reviewed literature. Transparency is gen-\nerally desired because algorithms that are poorly pre-\ndictable or explainable are difficult to control, monitor\nand correct (Tutt, 2016). As many critics have observed\ntransparency is often nai\u00a8vely treated as a panacea for\nethical issues arising from new technologies.\nTransparency is generally defined with respect to ``the\navailability of information, the conditions of accessibil-\nity and how the information . . . may pragmatically or\nepistemically support the user's decision-making pro-\nthis topic is not new. The literature in information\nand computer ethics, for example started to focus on\nit at the beginning of the 21st century, when issues con-\ncerning algorithmic information filtering by search\nThe primary components of transparency are acces-\nsibility and comprehensibility of information.\nInformation about the functionality of algorithms is\noften intentionally poorly accessible. Proprietary algo-\nrithms are kept secret for the sake of competitive\nor privacy. Transparency can thus run counter to other\nethical ideals, in particular the privacy of data subjects\nand autonomy of organisations.\nGranka (2010) notes a power struggle between data\nsubjects' interests in transparency and data processors'\ncommercial viability. Disclosing the structure of these\nalgorithms would facilitate ill-intentioned manipula-\ntions of search results (or `gaming the system'), while\nnot bringing any advantage to the average non-tech-\ncial viability of data processors in many industries (e.g.\ncredit reporting, high frequency trading) may be threa-\ntened by transparency. However, data subjects retain\nan interest in understanding how information about\nthem is created and influences decisions taken in data-\ndriven practices. This struggle is marked by informa-\ntion asymmetry and an ``imbalance in knowledge and\ndecision-making power'' favouring data processors\nBesides being accessible, information must be com-\nprehensible to be considered transparent (Turilli and\nFloridi, 2009). Efforts to make algorithms transparent\nface a significant challenge to render complex decision-\nmaking processes both accessible and comprehensible.\nThe longstanding problem of interpretability in\nmachine learning algorithms indicates the challenge of\ncreating and modifying rules to classify or cluster large\ndatasets. The algorithm modifies its behavioural struc-\nture during operation (Markowetz et al., 2014). This\nalteration of how the algorithm classifies new inputs\nis how it learns (Burrell, 2016: 5). Training produces a\nstructure (e.g. classes, clusters, ranks, weights, etc.) to\nclassify new inputs or predict unknown variables. Once\ntrained, new data can be processed and categorised\nautomatically without operator intervention (Leese,\n2014). The rationale of the algorithm is obscured, lend-\ning to the portrayal of machine learning algorithms as\n`black boxes'.\nopacity of machine learning algorithms inhibits over-\nsight. Algorithms ``are opaque in the sense that if one\nis a recipient of the output of the algorithm (the clas-\nsification decision), rarely does one have any concrete\nsense of how or why a particular classification has\nbeen arrived at from inputs'' (Burrell, 2016: 1). Both\nthe inputs (data about humans) and outputs (classifi-\ncations) can be unknown and unknowable. Opacity in\nmachine learning algorithms is a product of the high-\ndimensionality of data, complex code and changeable\n179) suggests that machine learning can produce out-\nputs for which ``the human trainer himself is unable to\nprovide an algorithmic representation.'' Algorithms\ncan only be considered explainable to the degree\nthat a human can articulate the trained model or\nrationale of a particular decision, for instance by\nexplaining the (quantified) influence of particular\ninputs or attributes (Datta et al., 2016). Meaningful\noversight and human intervention in algorithmic deci-\nsion-making ``is impossible when the machine has an\ninformational advantage over the operator . . . [or]\nwhen the machine cannot be controlled by a human\nin real-time due to its processing speed and the multi-\n183). This is, one again, the black box problem.\nHowever, a distinction should be drawn, between the\ntechnical infeasibility of oversight and practical bar-\nriers caused, for instance, by a lack of expertise,\naccess, or resources.\nBeyond machine learning, algorithms with `hand-\nwritten' decision-making rules can still be highly\ncomplex and practically inscrutable from a lay data\nsubject's perspective (Kitchin, 2016). Algorithmic deci-\nsion-making structures containing ``hundreds of rules\nare very hard to inspect visually, especially when their\npredictions are combined probabilistically in complex\nways'' (Van Otterlo, 2013). Further, algorithms are\noften developed by large teams of engineers over\ntime, with a holistic understanding of the development\nprocess and its embedded values, biases and\n6 Big Data & Society\ninterdependencies rendered infeasible (Sandvig et al.,\n2014). In both respects, algorithmic processing con-\ntrasts with traditional decision-making, where human\ndecision-makers can in principle articulate their ration-\nale when queried, limited only by their desire and\ncapacity to give an explanation, and the questioner's\ncapacity to understand it. The rationale of an algo-\nrithm can in contrast be incomprehensible to humans,\nrendering the legitimacy of decisions difficult to\nchallenge.\nUnder these conditions, decision-making is poorly\ntransparent. Rubel and Jones (2014) argue that the fail-\nure to render the processing logic comprehensible to\ndata subject's disrespects their agency (we shall return\nto this point in Section 8). Meaningful consent to data-\nprocessing is not possible when opacity precludes risk\nassessment (Schermer, 2011). Releasing information\nabout the algorithm's decision-making logic in a sim-\nplified format can help (Datta et al., 2016; Tene and\nPolonetsky, 2013a). However, complex decision-\nmaking structures can quickly exceed the human and\norganisational resources available for oversight\n(Kitchin, 2016). As a result, lay data subjects may\nlose trust in both algorithms and data processors\nEven if data processors and controllers disclose\noperational information, the net benefit for society is\nuncertain. A lack of public engagement with existing\ntransparency mechanisms reflects this uncertainty,\nseen for example in credit scoring (Zarsky, 2016).\nTransparency disclosures may prove more impactful if\ntailored towards trained third parties or regulators rep-\nresenting public interest as opposed to data subjects\nTransparency disclosures by data processors and\ncontrollers may prove crucial in the future to maintain\na trusting relationship with data subjects (Cohen et al.,\nRaymond, 2014). Trust implies the trustor's (the\nagent who trusts) expectations for the trustee (the\nagent who is trusted) to perform a task (Taddeo,\n2010), and acceptance of the risk that the trustee will\nbetray these expectations (Wiegel and Berg, 2009).\nTrust in data processors can, for instance, alleviate con-\ncerns with opaque personal data-processing (Mazoue\n\u00b4 ,\n1990). However, trust can also exist among artificial\nagents exclusively, seen for instance in the agents of a\ndistributed system working cooperatively to achieve a\nTaddeo, 2010). Furthermore, algorithms can be per-\nceived as trustworthy independently of (or perhaps\neven despite) any trust placed in the data processor;\nyet the question of when this may be appropriate\nMisguided evidence leading to bias\nThe automation of human decision-making is often\njustified by an alleged lack of bias in algorithms\nunsustainable, as shown by prior work demonstrating\nthe normativity of information technologies in general\nand algorithm development in particular14 (e.g.\nMuch of the reviewed literature addresses how bias\nmanifests in algorithms and the evidence they produce.\nAlgorithms inevitably make biased decisions. An\nalgorithm's design and functionality reflects the values\nof its designer and intended uses, if only to the extent\nthat a particular design is preferred as the best or most\nefficient option. Development is not a neutral, linear\npath; there is no objectively correct choice at any\ngiven stage of development, but many possible choices\n(Johnson, 2006). As a result, ``the values of the author\n[of an algorithm], wittingly or not, are frozen into the\ncode, effectively institutionalising those values''\nbias in algorithms and the models they produce when\nencountered in isolation of the algorithm's develop-\nment history (Friedman and Nissenbaum, 1996;\nFriedman and Nissenbaum (1996) argue that bias\ncan arise from (1) pre-existing social values found in\nthe ``social institutions, practices and attitudes'' from\nwhich the technology emerges, (2) technical constraints\nand (3) emergent aspects of a context of use. Social\nbiases can be embedded in system design purposefully\nby individual designers, seen for instance in manual\nadjustments to search engine indexes and ranking\ncriteria (Goldman, 2006). Social bias can also be unin-\ntentional, a subtle reflection of broader cultural\nor organisational values. For example, machine learn-\ning algorithms trained from human-tagged data inad-\nvertently learn to reflect biases of the taggers\nTechnical bias arises from technological constraints,\nerrors or design decisions, which favour particular\ngroups without an underlying driving value\n(Friedman and Nissenbaum, 1996). Examples include\nwhen an alphabetical listing of airline companies leads\nto increase business for those earlier in the alphabet, or\nan error in the design of a random number generator\nthat causes particular numbers to be favoured. Errors\ncan similarly manifest in the datasets processed by algo-\nrithms. Flaws in the data are inadvertently adopted\nby the algorithm and hidden in outputs and models\nproduced (Barocas and Selbst, 2015; Romei and\nMittelstadt et al. 7\nEmergent bias is linked with advances in knowledge\nor changes to the system's (intended) users and stake-\nholders (Friedman and Nissenbaum, 1996). For exam-\nple, CDSS are unavoidably biased towards treatments\nincluded in their decision architecture. Although emer-\ngent bias is linked to the user, it can emerge unexpect-\nedly from decisional rules developed by the algorithm,\nrather than any `hand-written' decision-making struc-\nture (Hajian and Domingo-Ferrer, 2013; Kamiran and\nCalders, 2010). Human monitoring may prevent some\nbiases from entering algorithmic decision-making in\nThe outputs of algorithms also require interpretation\n(i.e. what one should do based on what the algorithm\nindicates); for behavioural data, `objective' correlations\ncan come to reflect the interpreter's ``unconscious\nmotivations, particular emotions, deliberate choices,\nsocio-economic determinations, geographic or demo-\nExplaining the correlation in any of these terms\nrequires additional justification \u00ad meaning is not self-\nevident in statistical models. Different metrics ``make\nvisible aspects of individuals and groups that are not\ncannot be assumed that an observer's interpretation\nwill correctly reflect the perception of the actor rather\nthan the biases of the interpreter.\nUnfair outcomes leading to\ndiscrimination\nMuch of the reviewed literature also addresses how dis-\ncrimination results from biased evidence and decision-\nmaking.15 Profiling by algorithms, broadly defined ``as\nthe construction or inference of patterns by means of\ndata mining and . . . the application of the ensuing pro-\nfiles to people whose data match with them''\ncited as a source of discrimination. Profiling algorithms\nidentify correlations and make predictions about\nbehaviour at a group-level, albeit with groups (or pro-\nfiles) that are constantly changing and re-defined by the\nalgorithm (Zarsky, 2013). Whether dynamic or static,\nthe individual is comprehended based on connections\nwith others identified by the algorithm, rather than\nIndividuals' choices are structured according to infor-\nmation about the group (Danna and Gandy, 2002:\n382). Profiling can inadvertently create an evidence-\nbase that leads to discrimination (Vries, 2010).\nFor the affected parties, data-driven discriminatory\ntreatment is unlikely to be more palatable than discrim-\nination fuelled by prejudices or anecdotal evidence.\nThis much is implicit in Schermer's (2011) argument\nthat discriminatory treatment is not ethically problem-\natic in itself; rather, it is the effects of the treatment that\ndetermine its ethical acceptability. However, Schermer\nmuddles bias and discrimination into a single concept.\nWhat he terms discrimination can be described instead\nas mere bias, or the consistent and repeated expression\nof a particular preference, belief or value in decision-\nmaking (Friedman and Nissenbaum, 1996). In contrast,\nwhat he describes as problematic effects of discrimin-\natory treatment can be defined as discrimination tout\ncourt. So bias is a dimension of the decision-making\nitself, whereas discrimination describes the effects of a\ndecision, in terms of adverse disproportionate impact\nresulting from algorithmic decision-making. Barocas\nand Selbst (2015) show that precisely this definition\nguides `disparate impact detection', an enforcement\nmechanism for American anti-discrimination law in\nareas such as social housing and employment. They\nsuggest that disparate impact detection provides a\nmodel for the detection of bias and discrimination in\nalgorithmic decision-making which is sensitive to differ-\nential privacy.\nIt may be possible to direct algorithms not to consider\nsensitive attributes that contribute to discrimination\n(Barocas and Selbst, 2015), such as gender or ethnicity\nSchermer, 2011), based upon the emergence of discrim-\nination in a particular context. However, proxies for\nprotected attributes are not easy to predict or detect\nwhen algorithms access linked datasets (Barocas and\nSelbst, 2015). Profiles constructed from neutral charac-\nteristics such as postal code may inadvertently overlap\nwith other profiles related to ethnicity, gender, sexual\nEfforts are underway to avoid such `redlining' by\nsensitive attributes and proxies. Romei and Ruggieri\n(2014) observe four overlapping strategies for discrim-\nination prevention in analytics: (1) controlled distortion\nof training data; (2) integration of anti-discrimination\ncriteria into the classifier algorithm; (3) post-processing\nof classification models; (4) modification of predictions\nand decisions to maintain a fair proportion of effects\nbetween protected and unprotected groups. These\nstrategies are seen in the development of privacy-\npreserving, fairness- and discrimination-aware data\nFairness-aware data mining takes the broadest aim, as\nit gives attention not only to discrimination but fair-\nness, neutrality, and independence as well (Kamishima\net al., 2012). Various metrics of fairness are possible\nbased on statistical parity, differential privacy and\n8 Big Data & Society\nother relations between data subjects in classification\nThe related practice of personalisation is also\nfrequently discussed. Personalisation can segment\na population so that only some segments are worthy\nof receiving some opportunities or information,\nre-enforcing existing social (dis)advantages. Questions\nof the fairness and equitability of such practices are\noften raised (e.g. Cohen et al., 2014; Danna and\ncing, for example, can be ``an invitation to leave\nquietly'' issued to data subjects deemed to lack value\nor the capacity to pay.16\nReasons to consider discriminatory effects as adverse\nand thus ethically problematically are diverse.\nDiscriminatory analytics can contribute to self-fulfilling\nprophecies and stigmatisation in targeted groups,\nundermining their autonomy and participation in soci-\nPersonalisation through non-distributive profiling,\nseen for example in personalised pricing in insurance\npremiums (Hildebrandt and Koops, 2010; Van Wel\nand Royakkers, 2004), can be discriminatory by violat-\ning both ethical and legal principles of equal or fair\ntreatment of individuals (Newell and Marabelli,\n2015). Further, as described above the capacity of indi-\nviduals to investigate the personal relevance of factors\nused in decision-making is inhibited by opacity and\nTransformative effects leading\nto challenges for autonomy\nValue-laden decisions made by algorithms can also\npose a threat to the autonomy of data subjects. The\nreviewed literature in particular connects personalisa-\ntion algorithms to these threats. Personalisation can be\ndefined as the construction of choice architectures\nwhich are not the same across a sample (Tene and\nPolonetsky, 2013a). Similar to explicitly persuasive\ntechnologies, algorithms can nudge the behaviour of\ndata subjects and human decision-makers by filtering\ninformation (Ananny, 2016). Different content, infor-\nmation, prices, etc. are offered to groups or classes of\npeople within a population according to a particular\nattribute, e.g. the ability to pay.\nPersonalisation algorithms tread a fine line between\nsupporting and controlling decisions by filtering which\ninformation is presented to the user based upon in-\ndepth understanding of preferences, behaviours, and\nperhaps vulnerabilities to influence (Bozdag, 2013;\n2016). Classifications and streams of behavioural data\nare used to match information to the interests and attri-\nbutes of data subjects. The subject's autonomy in deci-\nsion-making is disrespected when the desired choice\nreflects third party interests above the individual's\nThis situation is somewhat paradoxical. In prin-\nciple, personalisation should improve decision-\nmaking by providing the subject with only relevant\ninformation when confronted with a potential infor-\nmation overload; however, deciding which informa-\ntion is relevant is inherently subjective. The subject\ncan be pushed to make the ``institutionally preferred\naction rather than their own preference'' (Johnson,\n2013); online consumers, for example, can be nudged\nto fit market needs by filtering how products are\ndisplayed (Coll, 2013). Lewis and Westlund\nneed to be taught to `act ethically' to strike a balance\nbetween coercing and supporting users' decisional\nautonomy.\nPersonalisation algorithms reduce the diversity of\ninformation users encounter by excluding content\ndeemed irrelevant or contradictory to the user's beliefs\nthus be considered an enabling condition for autonomy\n(van den Hoven and Rooksby, 2008). Filtering algo-\nrithms that create `echo chambers' devoid of contra-\ndictory information may impede decisional autonomy\n(Newell and Marabelli, 2015). Algorithms may be\nunable to replicate the ``spontaneous discovery of new\nthings, ideas and options'' which appear as anomalies\nagainst a subject's profiled interests (Raymond, 2014).\nWith near ubiquitous access to information now feas-\nible in the internet age, issues of access concern whether\nthe `right' information can be accessed, rather than any\ninformation at all. Control over personalisation and\nfiltering mechanisms can enhance user autonomy, but\npotentially at the cost of information diversity (Bozdag,\n2013). Personalisation algorithms, and the underlying\npractice of analytics, can thus both enhance and under-\nmine the agency of data subjects.\nTransformative effects leading to\nchallenges for informational privacy\nAlgorithms are also driving a transformation of notions\nof privacy. Responses to discrimination, de-individua-\nlisation and the threats of opaque decision-making for\ndata subjects' agency often appeal to informational\nprivacy (Schermer, 2011), or the right of data subjects\nto ``shield personal data from third parties.''\nInformational privacy concerns the capacity of an indi-\nvidual to control information about herself (Van Wel\nMittelstadt et al. 9\nand Royakkers, 2004), and the effort required by third\nparties to obtain this information.\nA right to identity derived from informational\nprivacy interests suggests that opaque or secretive pro-\nfiling is problematic.17 Opaque decision-making by\nalgorithms (see `Inconclusive evidence leading to unjus-\ntified actions' section) inhibits oversight and informed\ndecision-making concerning data sharing (Kim et al.,\n2014). Data subjects cannot define privacy norms to\ngovern all types of data generically because their\nvalue or insightfulness is only established through\nprocessing (Hildebrandt, 2011; Van Wel and\nBeyond opacity, privacy protections based upon\nidentifiability are poorly suited to limit external man-\nagement of identity via analytics. Identity is increas-\ningly influenced by knowledge produced through\nanalytics that makes sense of growing streams of behav-\nioural data. The `identifiable individual' is not necessar-\nily a part of these processes. Schermer (2011) argues\nthat informational privacy is an inadequate conceptual\nframework because profiling makes the identifiability of\ndata subjects irrelevant.\nProfiling seeks to assemble individuals into mean-\ningful groups, for which identity is irrelevant (Floridi,\nconstruction by algorithms is a type of de-individuali-\nsation, or a ``tendency of judging and treating people\non the basis of group characteristics instead of on\ntheir own individual characteristics and merit.''\nIndividuals need never be identified when the profile\nis assembled to be affected by the knowledge and\nThe individual's informational identity (Floridi,\n2011) is breached by meaning generated by algo-\nrithms that link the subject to others within a dataset\nCurrent regulatory protections similarly struggle to\naddress the informational privacy risks of analytics.\n`Personal data' is defined in European data protection\nlaw as data describing an identifiable person; anon-\nymised and aggregated data are not considered perso-\nnal data (European Commission, 2012). Privacy\npreserving data mining techniques which do not require\naccess to individual and identifiable records may miti-\ngate these risks (Agrawal and Srikant, 2000; Fule and\nRoddick, 2004). Others suggest a mechanism to `opt-\nout' of profiling for a particular purpose or context\nwould help protect data subjects' privacy interests\nrecourse mechanisms for data subjects to question the\nvalidity of algorithmic decisions further exacerbates the\nchallenges of controlling identity and data about one-\nself (Schermer, 2011). In response, Hildebrandt and\nKoops (2010) call for `smart transparency' by designing\nthe socio-technical infrastructures responsible for pro-\nfiling in a way that allows individuals to anticipate and\nrespond to how they are profiled.\nTraceability leading to moral\nresponsibility\nWhen a technology fails, blame and sanctions must be\napportioned. One or more of the technology's designer\n(or developer), manufacturer or user are typically\nheld accountable. Designers and users of algorithms\nare typically blamed when problems arise (Kraemer\nuted when the actor has some degree of control\n(Matthias, 2004) and intentionality in carrying out the\naction.\nTraditionally, computer programmers have had\n``control of the behaviour of the machine in every\ndetail'' insofar as they can explain its design and func-\ntion to a third party (Matthias, 2004). This traditional\nconception of responsibility in software design assumes\nthe programmer can reflect on the technology's likely\neffects and potential for malfunctioning (Floridi et al.,\n2014), and make design choices to choose the most\ndesirable outcomes according to the functional specifi-\ncation (Matthias, 2004). With that said, programmers\nmay only retain control in principle due to the complex-\nity and volume of code (Sandvig et al., 2014), and the\nuse of external libraries often treated by the program-\nmer as `black boxes' (cf. Note 7).\nSuperficially, the traditional, linear conception of\nresponsibility is suitable to non-learning algorithms.\nWhen decision-making rules are `hand-written', their\nauthors retain responsibility (Bozdag, 2013). Decision-\nmaking rules determine the relative weight given to the\nvariables or dimensions of the data considered by the\nalgorithm. A popular example is Facebook's EdgeRank\npersonalisation algorithm, which prioritises content\nbased on date of publication, frequency of interaction\nbetween author and reader, media type, and other\ndimensions. Altering the relative importance of each\nfactor changes the relationships users are encouraged\nto maintain. The party that sets confidence intervals for\nan algorithm's decision-making structure shares\nresponsibility for the effects of the resultant false posi-\ntives, false negatives and spurious correlations (Birrer,\nresponsibility to monitor for ethical impacts of deci-\nsion-making by algorithms because ``the sensitivity of\na rule may not be apparent to the miner . . . the ability\nto harm or to cause offense can often be inadvertent.''\nSchermer (2011) similarly suggests that data processors\nshould actively searching for errors and bias in their\n10 Big Data & Society\nalgorithms and models. Human oversight of complex\nsystems as an accountability mechanism may, however,\nbe impossible due to the challenges for transparency\nalready mentioned (see `Inscrutable evidence leading\nto opacity' section). Furthermore, humans kept `in\nthe loop' of automated decision-making may be\npoorly equipped to identify problems and take correct-\nParticular challenges arise for algorithms with\nlearning capacities, which defy the traditional concep-\ntion of designer responsibility. The model requires the\nsystem to be well-defined, comprehensible and predict-\nable; complex and fluid systems (i.e. one with count-\nless decision-making rules and lines of code) inhibit\nholistic oversight of decision-making pathways and\ndependencies. Machine learning algorithms are par-\nticularly challenging in this respect (Burrell, 2016;\ngenetic algorithms that program themselves. The trad-\nitional model of responsibility fails because ``nobody\nhas enough control over the machine's actions to be\nable to assume the responsibility for them'' (Matthias,\nfor `machine ethics': ``the modular design of systems\ncan mean that no single person or group can fully\ngrasp the manner in which the system will interact or\nrespond to a complex flow of new inputs.'' From trad-\nitional, linear programming through to autonomous\nalgorithms, behavioural control is gradually transferred\nfrom the programmer to the algorithm and its operat-\nbetween the designer's control and algorithm's behav-\niour creates an accountability gap (Cardona, 2008)\nwherein blame can potentially be assigned to several\nmoral agents simultaneously.\nRelated segments of the literature address the `ethics\nof automation', or the acceptability of replacing or aug-\nmenting human decision-making with algorithms (Naik\nassume that algorithms can replace skilled professionals\nin a like-for-like manner. Professionals have implicit\nknowledge and subtle skills (cf. Coeckelbergh, 2013;\nMacIntyre, 2007) that are difficult to make explicit\nand perhaps impossible to make computable (Morek,\n2006). When algorithmic and human decision-makers\nwork in tandem, norms are required to prescribe when\nand how human intervention is required, particularly in\ncases like high-frequency trading where real-time inter-\nvention is impossible before harms occur (Davis et al.,\nAlgorithms that make decisions can be considered\nblameworthy agents (Floridi and Sanders, 2004a;\nWiltshire, 2015). The moral standing and capacity for\nethical decision-making of algorithms remains a\nstandout question in machine ethics (e.g. Allen et al.,\nEthical decisions require agents to evaluate the desir-\nability of different courses of actions which present con-\nflicts between the interests of involved parties (Allen\nFor some, learning algorithms should be considered\nmoral agents with some degree of moral responsibility.\nRequirements for moral agency may differ between\nhumans and algorithms; Floridi and Sanders (2004b)\nand Sullins (2006) argue, for instance, that `machine\nagency' requires significant autonomy, interactive\nbehaviour, and a role with causal accountability, to\nbe distinguished from moral responsibility, which\nrequires intentionality. As suggested above, moral\nagency and accountability are linked. Assigning moral\nagency to artificial agents can allow human stake-\nholders to shift blame to algorithms (Crnkovic and\nC\n\u00b8 u\n\u00a8 ru\n\u00a8 klu\n\u00a8 , 2011). Denying agency to artificial agents\nmakes designers responsible for the unethical behaviour\nof their semi-autonomous creations; bad consequences\nreflect bad design (Anderson and Anderson, 2014;\nis entirely satisfactory due to the complexity of over-\nsight and the volatility of decision-making structures.\nBeyond the nature of moral agency in machines,\nwork in machine ethics also investigates how best to\ndesign moral reasoning and behaviours into autono-\nmous algorithms as artificial moral and ethical\nand C\n\u00b8 u\n\u00a8 ru\n\u00a8 klu\n2009). Research into this question remains highly rele-\nvant because algorithms can be required to make real-\ntime decisions involving ``difficult trade-offs . . . which\nmay include difficult ethical considerations'' without an\nAutomation of decision-making creates problems of\nethical consistency between humans and algorithms.\nTurilli (2007) argues algorithms should be constrained\n``by the same set of ethical principles'' as the former\nhuman worker to ensure consistency within an organ-\nisation's ethical standards. However, ethical principles\nas used by human decision-makers may prove difficult\nto define and rendered computable. Virtue ethics is also\nthought to provide rule sets for algorithmic decision-\nstructures which are easily computable. An ideal model\nfor artificial moral agents based on heroic virtues is\nsuggested by Wiltshire (2015), wherein algorithms are\ntrained to be heroic and thus, moral.19\nOther approaches do not require ethical principles to\nserve as pillars of algorithmic decision-making frame-\nworks. Bello and Bringsjord (2012) insist that moral rea-\nsoning in algorithms should not be structured around\nclassic ethical principles because it does not reflect how\nhumans actually engage in moral decision-making.\nMittelstadt et al. 11\nRather, computational cognitive architectures \u00ad which\nallow machines to `mind read', or attribute mental\nstates to other agents \u00ad are required. Anderson and\nAnderson (2007) suggest algorithms can be designed to\nmimic human ethical decision-making modelled on\nempirical research on how intuitions, principles and rea-\nsoning interact. At a minimum this debate reveals that a\nconsensus view does not yet exist for how to practically\nrelocate the social and ethical duties displaced by auto-\nRegardless of the design philosophy chosen,\nFriedman and Nissenbaum (1996) argue that developers\nhave a responsibility to design for diverse contexts ruled\nby different moral frameworks. Following this, Turilli\n(2007) proposes collaborative development of ethical\nrequirements for computational systems to ground an\noperational ethical protocol. Consistency can be con-\nfirmed between the protocol (consisting of a decision-\nmaking structure) and the designer's or organisation's\nexplicit ethical principles (Turilli and Floridi, 2009).\nPoints of further research\nAs the preceding discussion demonstrates, the proposed\nmap (see `Map of the ethics of algorithms' section) can\nbe used to organise current academic discourse describ-\ning ethical concerns with algorithms in a principled\nway, on purely epistemic and ethical grounds. To\nborrow a concept from software development, the\nmap can remain perpetually `in beta'. As new types of\nethical concerns with algorithms are identified, or if one\nof the six described types can be separated into two or\nmore types, the map can be revised. Our intention has\nbeen to describe the state of academic discourse around\nthe ethics of algorithms, and to propose an organising\ntool for future work in the field to bridge linguistic and\ndisciplinary gaps. Our hope is that the map will\nimprove the precision of how ethical concerns with\nalgorithms are described in the future, while also ser-\nving as a reminder of the limitations of merely meth-\nodological, technical or social solutions to challenges\nraised by algorithms. As the map indicates, ethical con-\ncerns with algorithms are multi-dimensional and thus\nrequire multi-dimensional solutions.\nWhile the map provides the bare conceptual struc-\nture we need, it still must be populated as deployment\nand critical studies of algorithms proliferate. The seven\nthemes identified in the preceding sections identify\nwhere the `ethics of algorithms' currently lies on the\nmap. With this in mind, in this section we raise a\nnumber of topics not yet receiving substantial attention\nin the reviewed literature related to the transformative\neffects and traceability of algorithms. These topics can\nbe considered future directions of travel for the ethics\nof algorithms as the field expands and matures.\nConcerning transformative effects, algorithms\nchange how identity is constructed, managed and pro-\ntected by privacy and data protection mechanisms\n(see `Transformative effects leading to challenges for\ninformational privacy' section). Informational privacy\nand identifiability are typically closely linked; an indi-\nvidual has privacy insofar as she has control over data\nand information about her. Insofar as algorithms trans-\nform privacy by rendering identifiability less important,\na theory of privacy responsive to the reduced import-\nance of identifiability and individuality is required.\nVan Wel and Royakkers (2004) urge a re-conceptua-\nlisation of personal data, where equivalent privacy pro-\ntections are afforded to `group characteristics' when\nused in place of `individual characteristics' in generat-\ning knowledge about or taking actions towards an indi-\nvidual. Further work is required to describe how\nprivacy operates at group level, absent of identifiability\nReal world mechanisms to enforce privacy in analytics\nare also required. One proposal mentioned in the\nreviewed literature is to develop privacy preserving\ndata mining techniques which do not require access to\nindividual and identifiable records (Agrawal and\nwork is already underway to detect discrimination in\ndata mining (e.g. Barocas, 2014; Calders and Verwer,\nillegal discrimination. Further harm detection mechan-\nisms are necessitated by the capacity of algorithms to\ndisadvantage users in indirect and non-obvious ways\nthat exceed legal definitions of discrimination\nassumed that algorithms will discriminate according\nto characteristics observable or comprehensible to\nhumans.\nConcerning traceability, two key challenges for\napportioning responsibility for algorithms remain\nunder-researched. First, despite a wealth of literature\naddressing the moral responsibility and agency of algo-\nrithms, insufficient attention has been given to distrib-\nuted responsibility, or responsibility as shared across a\nnetwork of human and algorithmic actors simultan-\neously (cf. Simon, 2015). The reviewed literature (see\n`Traceability leading to moral responsibility' section)\naddresses the potential moral agency of algorithms,\nbut does not describe methods and principles for\napportioning blame or responsibility across a mixed\nnetwork of human and algorithmic actors.\nSecond, substantial trust is already placed in algo-\nrithms, in some cases affecting a de-responsibilisation of\nhuman actors, or a tendency to `hide behind the com-\nputer' and assume automated processes are correct by\nmaking to algorithms can shift responsibility away\n12 Big Data & Society\nfrom human decision-makers. Similar effects can be\nobserved in mixed networks of human and information\nsystems as already studied in bureaucracies, charac-\nterised by reduced feelings of personal responsibility\nand the execution of otherwise unjustifiable actions\n(Arendt, 1971). Algorithms involving stakeholders\nfrom multiple disciplines can, for instance, lead to each\nparty assuming the others will shoulder ethical respon-\nsibility for the algorithm's actions (Davis et al., 2013).\nMachine learning adds an additional layer of complexity\nbetween designers and actions driven by the algorithm,\nwhich may justifiably weaken blame placed upon the\nformer. Additional research is needed to understand\nthe prevalence of these effects in algorithm driven deci-\nsion-making systems, and to discern how to minimise the\ninadvertent justification of harmful acts.\nA related problem concerns malfunctioning and\nresilience. The need to apportion responsibility is\nacutely felt when algorithms malfunction. Unethical\nalgorithms can be thought of as malfunctioning soft-\nware-artefacts that do not operate as intended. Useful\ndistinctions exists between errors of design (types) and\nerrors of operation (tokens), and between the failure to\noperate as intended (dysfunction) and the presence of\nunintended side-effects (misfunction) (Floridi et al.,\n2014). Misfunctioning is distinguished from mere nega-\ntive side effects by avoidability, or the extent to which\ncomparable extant algorithm types accomplish the\nintended function without the effects in question.\nThese distinctions clarify ethical aspects of algorithms\nthat are strictly related to their functioning, either in the\nabstract (for instance when we look at raw perform-\nance), or as part of a larger decision-making system,\nand reveals the multi-faceted interaction between\nintended and actual behaviour.\nBoth types of malfunctioning imply distinct respon-\nsibilities for algorithm and software developers, users\nand artefacts. Additional work is required to describe\nfair apportionment of responsibility for dysfunctioning\nand misfunctioning across large development teams and\ncomplex contexts of use. Further work is also required to\nspecify requirements for resilience to malfunctioning as\nan ethical ideal in algorithm design. Machine learning in\nparticular raises unique challenges, because achieving\nthe intended or ``correct'' behaviour does not imply\nactions and feedback loops. Algorithms, particularly\nthose embedded in robotics, can for instance be made\nsafely interruptible insofar as harmful actions can be\ndiscouraged without the algorithm being encouraged\nto deceive human users to avoid further interruptions\nFinally, while a degree of transparency is broadly\nrecognised as a requirement for traceability, how to\noperationalise transparency remains an open question,\nparticularly for machine learning. Merely rendering the\ncode of an algorithm transparent is insufficient to ensure\nethical behaviour. Regulatory or methodological\nrequirements for algorithms to be explainable or inter-\npretable demonstrate the challenge data controllers now\nface (Tutt, 2016). One possible path to explainability is\nalgorithmic auditing carried out by data processors\nreflexive ethnographic studies in development and test-\ning (Neyland, 2016), or reporting mechanisms designed\ninto the algorithm itself (Vellido et al., 2012). For all\ntypes of algorithms, auditing is a necessary precondition\nto verify correct functioning. For analytics algorithms\nwith foreseeable human impact, auditing can create an\nex post procedural record of complex algorithmic deci-\nsion-making to unpack problematic or inaccurate deci-\nsions, or to detect discrimination or similar harms.\nFurther work is required to design broadly applicable,\nlow impact auditing mechanisms for algorithms (cf.\ncurrent work in transparency and interpretability of\nAll of the challenges highlighted in this review are\naddressable in principle. As with any technological\nartefact, practical solutions will require cooperation\nbetween researchers, developers and policy-makers.\nA final but significant area requiring further work is\nthe translation of extant and forthcoming policy applic-\nable to algorithms into realistic regulatory mechanisms\nand standards. The forthcoming EU General Data\nProtection Regulation (GDPR) in particular is indica-\ntive of the challenges to be faced globally in regulating\nThe GDPR stipulates a number of responsibilities of\ndata controllers and rights of data subjects relevant to\ndecision-making algorithms. Concerning the former,\nwhen undertaking profiling controllers will be required\nto evaluate the potential consequences of their data-\nprocessing activities via a data protection impact\nassessment (Art. 35(3)(a)). In addition to assessing priv-\nacy hazards, data controllers also have to communicate\nthese risks to the persons concerned. According to Art.\ninform the data subjects about existing profiling meth-\nods, its significance and its envisaged consequences. Art.\n12(1) mandates that clear and plain language is used to\ninform about these risks.22 Further, Recital 71 states\nthe data controllers' obligation to explain the logic of\nhow the decision was reached. Finally, Art. 22(3) states\nthe data controller's duty to ``implement suitable\nmeasures to safeguard the data subject's rights and\nMittelstadt et al. 13\nfreedoms and legitimate interests'' when automated\ndecision-making is applied. This obligation is rather\nvague and opaque.\nOn the rights of data subjects, the GDPR generally\ntakes a self-determination approach. Data subjects are\ngranted a right to object to profiling methods (Art. 21)\nand a right not to be subject to solely automated pro-\ncessed individual decision-making23 (Art. 22). In these\nand similar cases the person concerned either has the\nright to object that such methods are used or should at\nleast have the right to ``obtain human intervention'' in\norder to express their views and to ``contest the deci-\nAt first glance these provisions defer control to the\ndata subjects and enable them to decide how their data\nare used. Notwithstanding that the GDPR bears great\npotential to improve data protection, a number of\nexemptions limit the rights of data subjects.24 The\nGDPR can be a toothless or a powerful mechanism\nto protect data subjects dependent upon its eventual\nlegal interpretation: the wording of the regulation\nallows either to be true. Supervisory authorities and\ntheir future judgments will determine the effectiveness\nof the new framework.25 However, additional work is\nrequired in parallel to provide normative guidelines and\npractical mechanisms for putting the new rights and\nresponsibilities into practice.\nThese are not mundane regulatory tasks. For exam-\nple, the provisions highlighted above can be interpreted\nto mean automated decisions must be explainable to data\nsubjects. Given the connectivity and dependencies of\nalgorithms and datasets in complex information systems,\nand the tendency of errors and biases in data and models\nto be hidden over time (see `Misguided evidence leading\nto bias' section), `explainability'26 may prove particularly\ndisruptive for data intensive industries. Practical require-\nments will need to be unpacked in the future that strike\nan appropriate balance between data subjects' rights to\nbe informed about the logic and consequences of profil-\ning, and the burden imposed on data controllers.\nAlternatively, it may be necessary to limit automation\nor particular analytic methods in particular contexts to\nmeet transparency requirements specified in the GDPR\nalready exist in the US Credit Reporting Act, which\neffectively prohibits machine learning in credit scoring\nbecause reasons for the denial of credit must be made\navailable to consumers on demand (Burrell, 2016).\nConclusion\nAlgorithms increasingly mediate digital life and deci-\nsion-making. The work described here has made three\ncontributions to clarify the ethical importance of this\nmediation: (1) a review of existing discussion of ethical\naspects of algorithms; (2) a prescriptive map to organise\ndiscussion; and (3) a critical assessment of the literature\nto identify areas requiring further work to develop the\nethics of algorithms.\nThe review undertaken here was primarily limited to\nliterature explicitly discussing algorithms. As a result,\nmuch relevant work performed in related fields is only\nbriefly touched upon, in areas such as ethics of artificial\nintelligence, surveillance studies, computer ethics and\nmachine ethics.27 While it would be ideal to summarise\nwork in all the fields represented in the reviewed litera-\nture, and thus in any domain where algorithms are in\nuse, the scope of such as an exercise is prohibitive. We\nmust therefore accept that there may be gaps in cover-\nage for topics discussed only in relation to specific types\nof algorithms, and not for algorithms themselves.\nDespite this limitation, the prescriptive map is purpose-\nfully broad and iterative to organise discussion around\nthe ethics of algorithms, both past and future.\nDiscussion of a concept as complex as `algorithm'\ninevitably encounters problems of abstraction or\n`talking past each other' due to a failure to specify a\nlevel of abstraction (LoA) for discussion, and thus limit\nthe relevant set of observables (Floridi, 2008). A mature\n`ethics of algorithms' does not yet exist, in part because\n`algorithm' as a concept describes a prohibitively broad\nrange of software and information systems. Despite this\nlimitation, several themes emerged from the literature\nthat indicate how ethics can coherently be discussed\nwhen focusing on algorithms, independently of\ndomain-specific work.\nMapping these themes onto the prescriptive frame-\nwork proposed here has proven helpful to distinguish\nbetween the kinds of ethical concerns generated by algo-\nrithms, which are often muddled in the literature.\nDistinct epistemic and normative concerns are often trea-\nted as a cluster. This is understandable, as the different\nconcerns are part of a web of interdependencies. Some of\nthese interdependencies are present in the literature we\nreviewed, like the connection between bias and discrim-\nination (see `Misguided evidence leading to bias' and\n`Unfair outcomes leading to discrimination' sections) or\nthe impact of opacity on the attribution of responsibility\n(see `Inscrutable evidence leading to opacity' and\n`Traceability leading to moral responsibility' sections).\nThe proposed map brings further dependencies into\nfocus, like the multi-faceted effect of the presence and\nabsence of epistemic deficiencies on the ethical ramifi-\ncations of algorithms. Further, the map demonstrates\nthat solving problems at one level does not address all\ntypes of concerns; a perfectly auditable algorithmic\ndecision, or one that is based on conclusive, scrutable\nand well-founded evidence, can nevertheless cause\nunfair and transformative effects, without obvious\nways to trace blame among the network of contributing\n14 Big Data & Society\nactors. Better methods to produce evidence for some\nactions need not rule out all forms of discrimination\nfor example, and can even be used to discriminate\nmore efficiently. Indeed, one may even conceive of situ-\nations where less discerning algorithms may have fewer\nobjectionable effects.\nMore importantly, as already repeatedly stressed in\nthe above overview, we cannot in principle avoid epi-\nstemic and ethical residues. Increasingly better algorith-\nmic tools can normally be expected to rule out many\nobvious epistemic deficiencies, and even help us to\ndetect well-understood ethical problems (e.g. discrimin-\nation). However, the full conceptual space of ethical\nchallenges posed by the use of algorithms cannot be\nreduced to problems related to easily identified epistemic\nand ethical shortcomings. Aided by the map drawn here,\nfuture work should strive to make explicit the many\nimplicit connections to algorithms in ethics and beyond.\nDeclaration of conflicting interests\nThe author(s) declared no potential conflicts of interest with\nrespect to the research, authorship, and/or publication of this\narticle.\nFunding\nThe author(s) disclosed receipt of the following financial sup-\nport for the research, authorship, and/or publication of this\narticle: This study was funded by the University of Oxford's\nJohn Fell Fund (Brent Mittelstadt), by the PETRAS IoT Hub\n\u00ad a EPSRC project (Brent Mittelstadt, Luciano Floridi,\nMariarosaria Taddeo), and the European Union's Horizon\n2020 research and innovation programme under the Marie\nSupplementary material\nThe supplementary files are available at http://bds.sagepub.\nNotes\n1. We would like to acknowledge valuable comments and\nfeedback of the reviewers at Big Data & Society.\n2. Compare with Turner (2016) on the ontology of programs.\n3. For the sake of simplicity, for the remainder of the paper\nwe will refer generically to `algorithms' rather than con-\nstructs, implementations and configurations.\n4. Tufekci seems to have a similar class of algorithms in mind\nin her exploration of detecting harms. She describes `gate-\nkeeping algorithms' as ``algorithms that do not result in\nsimple, `correct' answers-instead, I focus on those that are\n5. The distinction between supervised and unsupervised\nlearning can be mapped onto analytics to reveal different\nways humans are `made sense of' through data.\nDescriptive analytics based on unsupervised learning,\nseeks to identify unforeseen correlations between cases to\nlearn something about the entity or phenomenon. Here,\nanalysis is exploratory, meaning it lacks a specific\ntarget or hypothesis. In this way, new models and classi-\nfications can be defined. In contrast, predictive analytics\nbased on supervised learning seeks to match cases to pre-\nexisting classes to infer knowledge about the case.\nKnowledge about the assigned classes is used to make\npredictions about the case (Van Otterlo, 2013).\n6. The term `probable knowledge' is used here in the sense\nof Hacking (2006) where it is associated with the emer-\ngence of probability and the rise of statistical thinking\n(for instance in the context of insurance) that started in\n7. In mainstream analytic epistemology this issue is con-\nnected to the nature of justification, and the importance\nof having access to one's own justifications for a specific\nbelief (Kornblith, 2001). In the present context, however,\nwe are concerned with a more interactive kind of justifi-\ncation: human agents need to be able to understand how\na conclusion reached by an algorithm is justified in view\nof the data.\n8. The often blamed opacity of algorithms can only partially\nexplain why this is the case. Another aspect is more clo-\nsely related to the role of re-use in the development of\nalgorithms and software-artefacts; from the customary\nuse of existing libraries, to the repurposing of existing\ntools and methods for different purposes (e.g. the use of\nseismological models of aftershocks in predictive policing\n(Mohler et al., 2011), and the tailoring of general tools for\nspecific methods. Apart from the inevitable distribution\nof responsibilities, this highlights the complex relation\nbetween good design (the re-use philosophy promoted\nin Structured Programming) and the absence of malfunc-\ntion, and reveals that even the designers of software-\nartefacts regularly treat part of their work as black\n9. See Appendix 1 for information on the methodology,\nsearch terms and query results of the review.\n10. A distinction must be made, however, between the ethical\njustifiability of acting upon mere correlation and a\nbroader ethics of inductive reasoning which overlaps\nwith extant critiques of statistical and quantitative meth-\nods in research. The former concerns the thresholds of\nevidence required to justify actions with ethical impact.\nThe latter concerns a lack of reproducibility in analytics\nthat distinguishes it in practice from science (cf.\nand is better understood as an issue of epistemology.\nfirst publications on this topic. The article compares\nsearch engines to publishers and suggests that, like pub-\nlishers, search engines filter information according to\nmarket conditions, i.e. according to consumers' tastes\nand preferences, and favour powerful actors. Two cor-\nrective mechanisms are suggested: embedding the ``value\nof fairness as well as [a] suite of values represented by the\nideology of the Web as a public good'' (Introna and\nranking algorithms, and transparency of the algorithms\nused by search engines. More recently, Zarsky (2013) has\nMittelstadt et al. 15\nprovided a framework and in-depth legal examination of\ntransparency in predictive analytics.\n12. This is a contentious claim. Bozdag (2013) suggests that\nhuman comprehension has not increased in parallel to the\nexponential growth of social data in recent years due to\nbiological limitations on information processing capaci-\nties. However, this would appear to discount advances in\ndata visualization and sorting techniques to help humans\ncomprehend large datasets and information flows (cf.\nTurilli and Floridi, 2009). Biological capacities may not\nhave increased, but the same cannot be said for tool-\nassisted comprehension. One's position on this turns on\nwhether technology-assisted and human comprehension\nare categorically different.\n13. The context of autonomous weapon systems is particu-\nlarly relevant here; see Swiatek (2012).\n14. The argument that technology design unavoidably value-\nladen is not universally accepted. Kraemer et al. (2011)\nprovide a counterargument from the reviewed literature.\nFor them, algorithms are value-laden only ``if one cannot\nrationally choose between them without explicitly or\nimplicitly taking ethical concerns into account.'' In\nother words, designers make value-judgments that\nexpress views ``on how things ought to be or not to be,\nor what is good or bad, or desirable or undesirable''\nalgorithms that produce hypothetical value-judgments or\nrecommended courses of action, such as clinical decision\nsupport systems, can be value-neutral because the judg-\nments produced are hypothetical. This approach would\nsuggest that autonomous algorithms are value-laden by\ndefinition, but only because the judgments produced are\nput into action by the algorithm. This conception of value\nneutrality appears to suggest that algorithms are designed\nin value-neutral spaces, with the designer disconnected\nfrom a social and moral context and history that inevit-\nably influences her perceptions and decisions. It is diffi-\ncult to see how this could be the case (cf. Friedman and\n15. Clear sources of discrimination are not consistently iden-\ntified in the reviewed literature. Barocas (2014) helpfully\nclarifies five possible sources of discrimination related to\nbiased analytics: (1) inferring membership in a protected\nclass; (2) statistical bias; (3) faulty inference; (4) overly\nprecise inferences; and (5) shifting the sample frame.\nple in the Royal Bank of Canada which `nudged' cus-\ntomers on fee-for-service to flat-fee service packages\nafter discovering (through mining in-house data) that\ncustomers on the latter offered greater lifetime value to\nthe bank. Customers unwilling to move to flat-fee services\nfaced disincentives including higher prices. Through price\ndiscrimination customers were pushed towards options\nreflecting the bank's interests. Customers unwilling to\nmove were placed into a weak bargaining position in\nwhich they were `invited to leave': losing some customers\nin the process of shifting the majority to more profitable\nflat-fee packages meant the bank lacked incentive to\naccommodate minority interests despite the risk of\nlosing minority fee-for-service customers to competitors.\n17. Data subjects can be considered to have a right to iden-\ntity. Such a right can take many forms, but the existence\nof some right to identity is difficult to dispute. Floridi\n(2011) conceives of personal identity as constituted by\ninformation. Taken as such, any right to informational\nprivacy translates to a right to identity by default, under-\nstood as the right to manage information about the self\nthat constitutes one's identity. Hildebrandt and Koops\n(2010) similarly recognise a right to form identity without\nunreasonable external influence. Both approaches can be\nconnected to the right to personality derived from the\nEuropean Convention on Human Rights.\n18. A further distinction can be made between artificial\nmoral agents and artificial ethical agents. Artificial\nmoral agents lack true `artificial intelligence' or the capa-\ncity for reflection required to decide and justify an ethical\ncourse of action. Artificial ethical agents can ``calculate\nthe best action in ethical dilemmas using ethical prin-\nciples'' (Moor, 2006) or frameworks derived thereof. In\ncontrast, artificial morality requires only that machines\nact `as if' they are moral agents, and thus make ethically\njustified decisions according to pre-defined criteria\n(Moor, 2006). The construction of artificial morality\nis seen as the immediate and imminently achievable\nchallenge for machine ethics, as it does not first require\nartificial intelligence (Allen et al., 2006). With that said,\nthe question of whether ``it is possible to create artificial\nfull ethical agents'' continues to occupy machine ethicists\nwith virtue-based frameworks would find their creation\nethically impermissible due to the impoverished sense of\nvirtues a machine could actually develop. In short, the\ncharacter development of humans and machines are too\ndissimilar to compare. He predicts that unless autono-\nmous agents are treated as full moral agents comparable\nto humans, existing social injustices will be exacerbated as\nautonomous machines are denied the freedom to express\ntheir autonomy by being forced into service of the needs\nof the designer. This concern points to a broader issue in\nmachine ethics concerning whether algorithms and\nmachines with decision-making autonomy will continue\nto be treated as passive tools as opposed to active (moral)\n20. Except for trivial cases, the presence of false positives and\nfalse negatives in the work of algorithms, particularly\nmachine learning, is unavoidable.\n21. It is important to note that this regulation even applies to\ndata controllers or processors that are not established\nwithin the EU, if the monitoring (including predicting\nand profiling) of behaviour is focused on data subjects\nthat are located in the EU (Art 3(2)(b) and Recital 24).\n22. In cases where informed consent is required, Art. 7(2)\nstipulates that non-compliance with Art. 12(1) renders\ngiven consent not legally binding.\n23. Recital 71 explains that solely automated individual deci-\nsion-making has to be understood as a method ``which\nproduces legal effects concerning him or her or similarly\nsignificantly affects him or her, such as automatic refusal\nof an online credit application or e-recruiting practices\n16 Big Data & Society\nwithout any human intervention'' and includes profiling\nthat allows to ``predict aspects concerning the data sub-\nject's performance at work, economic situation, health,\npersonal preferences or interests, reliability or behaviour,\nlocation or movements.''\n24. Art. 21(1) explains that the right to object to profiling\nmethods can be restricted ``if the controller demonstrates\ncompelling legitimate grounds for the processing which\noverride the interests, rights and freedoms of the data\nsubject or for the establishment, exercise or defence of\nlegal claims.'' In addition, Art. 23(1) stipulates that the\nrights enshrined in Art. 12 to 22 \u00ad including the right to\nobject to automated decision-making \u00ad can be restricted\nin cases such as ``national security, defence; public secur-\nity;(. . .); other important objectives of general public\ninterest of the Union or of a Member State, in particular\nan important economic or financial interest of the Union\nor of a Member State, including monetary, budgetary and\ntaxation a matters, public health and social security; (. . .);\nthe prevention, investigation, detection and prosecution\nof breaches of ethics for regulated professions; (...);''. As\na result, these exemptions also apply to the right to access\n(Art. 15 \u00ad the right to obtain information if personal data\nare being processed) as well as the right to be forgotten\n25. Art. 83(5)(b) invests supervisory authorities with the\npower to impose fines up to 4% of the total worldwide\nannual turnover in cases where rights of the data subjects\n(Art. 12 to 22) have been infringed. This lever can be used\nto enforce compliance and to enhance data protection.\n26. `Explainability' is preferred here to `interpretability' to\nhighlight that the explanation of a decision must be com-\nprehensible not only to data scientists or controllers, but\nto the lay data subjects (or some proxy) affected by the\ndecision.\n27. The various domains of research and development\ndescribed here share a common characteristic: all make\nuse of computing algorithms. This is not, however, to\nsuggest that complex fields such as machine ethics\nand surveillance studies are subsumed by the `ethics of\nalgorithms' label. Rather, each domain has issues which\ndo not originate in the design and functionality of the\nalgorithms being used. These issues would thus not be\nconsidered part of an `ethics of algorithms', despite the\ninclusion of the parent field. `Ethics of algorithms' is thus\nnot meant to replace existing fields of enquiry, but rather\nto identify issues shared across a diverse number\nof domains stemming from the computing algorithms\nthey use.\nReferences\nAdler P, Falk C, Friedler SA, et al. (2016) Auditing black-box\nAgrawal R and Srikant R (2000) Privacy-preserving data\nAllen C, Wallach W and Smit I (2006) Why machine ethics?\nIntelligent Systems, IEEE 21(4) Available at: http://ieeex\nAnanny M (2016) Toward an ethics of algorithms convening,\nobservation, probability, and timeliness. Science,\nAnderson M and Anderson SL (2007) Machine ethics:\nCreating an ethical intelligent agent. AI Magazine 28(4): 15.\nAnderson M and Anderson SL (2014) Toward ethical intelli-\ngent autonomous healthcare agents: A case-supported\nprinciple-based behavior paradigm. Available at: http://\nAnderson SL (2008) Asimov's `Three Laws of Robotics' and\nApplin SA and Fischer MD (2015) New technologies and\nmixed-use convergence: How humans and algorithms are\nadapting to each other. In: 2015 IEEE international sym-\nposium on technology and society (ISTAS). Dublin,\nIreland: IEEE, pp. 1\u00ad6.\nArendt H (1971) Eichmann in Jerusalem: A Report on the\nBanality of Evil. New York: Viking Press.\nBarnet BA (2009) Idiomedia: The rise of personalized, aggre-\nBarocas S (2014) Data mining and the discourse on discrim-\nination. Available at: https://dataethics.github.io/proceed\nings/DataMiningandtheDiscourseOnDiscrimination.pdf\nBarocas S and Selbst AD (2015) Big data's disparate impact.\nSSRN Scholarly Paper, Rochester, NY: Social Science\nResearch Network. Available at: http://papers.ssrn.com/\nBello P and Bringsjord S (2012) On how to build a moral\nBirrer FAJ (2005) Data mining to combat terrorism and the\nroots of privacy concerns. Ethics and Information\nBozdag E (2013) Bias in algorithmic filtering and personal-\nBrey P and Soraker JH (2009) Philosophy of Computing and\nInformation Technology. Elsevier.\nBurrell J (2016) How the machine `thinks:' Understanding\nopacity in machine learning algorithms. Big Data &\nCalders T and Verwer S (2010) Three naive Bayes approaches\nfor discrimination-free classification. Data Mining and\nCalders T, Kamiran F and Pechenizkiy M (2009) Building\nclassifiers with independency constraints. In: Data mining\nCardona B (2008) `Healthy ageing' policies and anti-ageing\nideologies and practices: On the exercise of responsibility.\nCoeckelbergh M (2013) E-care as craftsmanship: Virtuous\nwork, skilled engagement, and information technology in\nhealth care. Medicine, Health Care and Philosophy 16(4):\nMittelstadt et al. 17\nCohen IG, Amarasingham R, Shah A, et al. (2014) The legal\nand ethical concerns that arise from using complex pre-\ndictive analytics in health care. Health Affairs 33(7):\nColl S (2013) Consumption as biopower: Governing bodies\nwith loyalty cards. Journal of Consumer Culture 13(3):\nCrawford K (2016) Can an algorithm be agonistic? Ten scenes\nfrom life in calculated publics. Science, Technology &\nCrnkovic GD and C\n\u00b8 u\n\u00a8 ru\n\u00a8 klu\nDanna A and Gandy OH Jr (2002) All that glitters is not\ngold: Digging beneath the surface of data mining.\nDatta A, Sen S and Zick Y (2016) Algorithmic transparency\nvia quantitative input influence. In: Proceedings of 37th\nIEEE symposium on security and privacy, San Jose, USA.\nAvailable at: http://www.ieee-security.org/TC/SP2016/\nDavis M, Kumiega A and Van Vliet B (2013) Ethics, finance,\nand automation: A preliminary survey of problems in high\nfrequency trading. Science and Engineering Ethics 19(3):\nde Vries K (2010) Identity, profiling algorithms and a world\nof ambient intelligence. Ethics and Information Technology\nDiakopoulos N (2015) Algorithmic accountability:\nJournalistic investigation of computational power struc-\nDiamond GA, Pollock BH and Work JW (1987) Clinician\ndecisions and computers. Journal of the American\nDomingos P (2012) A few useful things to know about\nmachine learning. Communications of the ACM 55(10):\nDwork C, Hardt M, Pitassi T, et al. (2011) Fairness through\nElish MC (2016) Moral crumple zones: Cautionary tales in\nhuman\u00adrobot interaction (WeRobot 2016). SSRN.\nAvailable at: http://papers.ssrn.com/sol3/\nEuropean Commission (2012) Regulation of the European\nParliament and of the Council on the Protection of\nIndividuals with regard to the processing of personal data\nand on the free movement of such data (General Data\nProtection Regulation). Brussels: European Commission.\nAvailable at: http://ec.europa.eu/justice/data-protection/\nFeynman R (1974) `Cargo cult science' \u00ad by Richard\nFeynman. Available at: http://neurotheory.columbia.edu/\nFloridi L (2008) The method of levels of abstraction. Minds\nFloridi L (2011) The informational nature of personal iden-\nFloridi L (2012) Big data and their epistemological challenge.\nFloridi L (2014) The Fourth Revolution: How the Infosphere is\nReshaping Human Reality. Oxford: OUP.\nFloridi L and Sanders JW (2004a) On the morality\nof artificial agents. Minds and Machines 14(3).\nAvailable at: http://dl.acm.org/cit-\nFloridi L and Sanders JW (2004b) On the morality of artifi-\ncial agents. Minds and Machines 14(3). Available at: http://\nFloridi L, Fresco N and Primiero G (2014) On malfunction-\nFriedman B and Nissenbaum H (1996) Bias in computer sys-\ntems. ACM Transactions on Information Systems (TOIS)\nFule P and Roddick JF (2004) Detecting privacy and ethical\nsensitivity in data mining results. In: Proceedings of the 27th\nAustralasian conference on computer science \u00ad Volume 26,\nDunedin, New Zealand, Australian Computer Society,\nGadamer HG (2004) Truth and Method. London: Continuum\nInternational Publishing Group.\nGlenn T and Monteith S (2014) New measures of mental state\nand behavior based on data collected from sensors, smart-\nphones, and the internet. Current Psychiatry Reports\nGoldman E (2006) Search engine bias and the demise of\nsearch engine utopianism. Yale Journal of Law &\nGranka LA (2010) The politics of search: A decade retro-\nGrindrod P (2014) Mathematical Underpinnings of Analytics:\nTheory and Applications. Oxford: OUP.\nGrodzinsky FS, Miller KW and Wolf MJ (2010) Developing\nartificial agents worthy of trust: `Would you buy a used\ncar from this artificial agent?' Ethics and Information\nHacking I (2006) The Emergence of Probability: A\nPhilosophical Study of Early Ideas about Probability,\nInduction and Statistical Inference. Cambridge:\nCambridge University Press.\nHajian S and Domingo-Ferrer J (2013) A methodology for\ndirect and indirect discrimination prevention in data\nmining. IEEE Transactions on Knowledge and Data\nHajian S, Monreale A, Pedreschi D, et al. (2012) Injecting\ndiscrimination and privacy awareness into pattern discov-\nery. In: Data mining workshops (ICDMW), 2012 IEEE\n12th international conference on, Brussels, Belgium,\nHildebrandt M (2008) Defining profiling: A new type of\nknowledge? In: Hildebrandt M and Gutwirth S (eds)\nProfiling the European Citizen the Netherlands:\nSpringer, pp. 17\u00ad45Available at: http://link.springer.-\n18 Big Data & Society\nHildebrandt M (2011) Who needs stories if you can get the\ndata? ISPs in the era of big number crunching. Philosophy\nHildebrandt M and Koops B-J (2010) The challenges of\nambient law and legal protection in the profiling era.\nHill RK (2015) What an algorithm is. Philosophy &\nIllari PM and Russo F (2014) Causality: Philosophical Theory\nMeets Scientific Practice. Oxford: Oxford University Press.\nIntrona LD and Nissenbaum H (2000) Shaping the Web:\nWhy the politics of search engines matters. The\nIoannidis JPA (2005) Why most published research findings\nJames G, Witten D, Hastie T, et al. (2013) An Introduction to\nStatistical Learning. Vol. 6, New York: Springer.\nJohnson JA (2006) Technology and pragmatism: From value\nneutrality to value criticality. SSRN Scholarly Paper,\nRochester, NY: Social Science Research Network.\nJohnson JA (2013) Ethics of data mining and predictive ana-\nlytics in higher education. SSRN Scholarly Paper,\nRochester, NY: Social Science Research Network.\nKamiran F and Calders T (2010) Classification with no dis-\ncrimination by preferential sampling. In: Proceedings of\nthe 19th machine learning conf. Belgium and the\nNetherlands, Leuven, Belgium. Available at: http://\nwwwis.win.tue.nl/tcalders/pubs/benelearn2010 (accessed\nKamishima T, Akaho S, Asoh H, et al. (2012) Considerations\non fairness-aware data mining. In: IEEE 12th International\nConference on Data Mining Workshops, Brussels, Belgium,\nKim B, Patel K, Rostamizadeh A, et al. (2015) Scalable and\ninterpretable data representation for high-dimensional,\nKim H, Giacomin J and Macredie R (2014) A qualitative\nstudy of stakeholders' perspectives on the social network\nservice environment. International Journal of Human\u00ad\nKitchin R (2016) Thinking critically about and researching algo-\nKornblith H (2001) Epistemology: Internalism and\nExternalism. Oxford: Blackwell.\nKraemer F, van Overveld K and Peterson M (2011) Is there\nan ethics of algorithms? Ethics and Information Technology\nLazer D, Kennedy R, King G, et al. (2014) The parable of\nLeese M (2014) The new profiling: Algorithms, black boxes,\nand the failure of anti-discriminatory safeguards in the\nLevenson JL and Pettrey L (1994) Controversial decisions\nregarding treatment and DNR: An algorithmic Guide\nfor the Uncertain in Decision-Making Ethics (GUIDE).\nAmerican Journal of Critical Care: An Official\nPublication, American Association of Critical-Care Nurses\nLewis SC and Westlund O (2015) Big data and journalism.\nLomborg S and Bechmann A (2014) Using APIs for data col-\nLou Y, Caruana R, Gehrke J, et al. (2013) Accurate intelli-\ngible models with pairwise interactions. In: Proceedings of\nthe 19th ACM SIGKDD international conference on know-\nledge discovery and data mining. Chicago, USA, ACM,\nLouch MO, Mainier MJ and Frketich DD (2010) An analysis\nof the ethics of data warehousing in the context of social\nnetworking applications and adolescents. In: 2010\nLupton D (2014) The commodification of patient opinion:\nThe digital patient experience economy in the age of big\nMacIntyre A (2007) After Virtue: A Study in Moral Theory,\n3rd ed. London: Gerald Duckworth & Co Ltd. Revised\nedition.\nMacnish K (2012) Unblinking eyes: The ethics of automating\nsurveillance. Ethics and Information Technology 14(2):\nMahajan RL, Reed J, Ramakrishnan N, et al. (2012)\nCultivating emerging and black swan technologies. ASME\n2012 International Mechanical Engineering Congress and\nMarkowetz A, Blaszkiewicz K, Montag C, et al. (2014)\nPsycho-informatics: Big data shaping modern psychomet-\nMatthias A (2004) The responsibility gap: Ascribing respon-\nsibility for the actions of learning automata. Ethics and\nMayer-Scho\n\u00a8 nberger V and Cukier K (2013) Big Data: A\nRevolution that will Transform How We Live, Work and\nThink. London: John Murray.\nMazoue\n\u00b4 JG (1990) Diagnosis without doctors. Journal of\nMiller B and Record I (2013) Justified belief in a digital age:\nOn the epistemic implications of secret Internet technolo-\nMittelstadt BD and Floridi L (2016) The ethics of big data:\nCurrent and foreseeable issues in biomedical contexts.\nMohler GO, Short MB, Brantingham PJ, et al. (2011) Self-\nexciting point process modeling of crime. Journal of the\nMoor JH (2006) The nature, importance, and difficulty of\nmachine ethics. Intelligent Systems, IEEE 21(4).\nAvailable at: http://ieeexplore.ieee.org/xpls/abs_all.jsp?ar-\nMorek R (2006) Regulatory framework for online dispute\nresolution: A critical view. The University of Toledo Law\nMittelstadt et al. 19\nNaik G and Bhide SS (2014) Will the future of knowledge work\nautomation transform personalized medicine? Applied &\nNakamura L (2013) Cybertypes: Race, Ethnicity, and Identity\non the Internet. New York: Routledge.\nNewell S and Marabelli M (2015) Strategic opportunities\n(and challenges) of algorithmic decision-making: A call\nfor action on the long-term societal effects of `datifica-\ntion'. The Journal of Strategic Information Systems 24(1):\nNeyland D (2016) Bearing accountable witness to the ethical\nalgorithmic system. Science, Technology & Human Values\nOrseau L and Armstrong S (2016) Safely interruptible agents.\nAvailable at: http://intelligence.org/files/Interruptibility.\nPariser E (2011) The Filter Bubble: What the Internet is Hiding\nfrom You. London: Viking.\nPasquale F (2015) The Black Box Society: The Secret\nAlgorithms that Control Money and Information.\nCambridge: Harvard University Press.\nPatterson ME and Williams DR (2002) Collecting and\nAnalyzing Qualitative Data: Hermeneutic Principles,\nMethods and Case Examples. Advances in tourism\nApplication Series, Champaign, IL, Champaign, USA:\nSagamore Publishing, Inc. Available at: http://www.tree-\nPortmess L and Tower S (2014) Data barns, ambient intelli-\ngence and cloud computing: The tacit epistemology and\nlinguistic representation of Big Data. Ethics and\nRaymond A (2014) The dilemma of private justice systems:\nBig Data sources, the cloud and predictive analytics.\nNorthwestern Journal of International Law & Business,\nForthcoming. Available at: http://papers.ssrn.com/sol3/\nRomei A and Ruggieri S (2014) A multidisciplinary survey on\ndiscrimination analysis. The Knowledge Engineering\nRubel A and Jones KML (2014) Student privacy in learning\nanalytics: An information ethics perspective. SSRN\nScholarly Paper, Rochester, NY: Social Science Research\nNetwork. Available at: http://papers.ssrn.com/\nSametinger J (1997) Software Engineering with Reusable\nComponents. Berlin: Springer Science & Business Media.\nSandvig C, Hamilton K, Karahalios K, et al. (2014) Auditing\nalgorithms: Research methods for detecting discrimination\non internet platforms. Data and Discrimination: Converting\nCritical Concerns into Productive Inquiry. Available at:\nhttp://social.cs.uiuc.edu/papers/pdfs/ICA2014-\nSchermer BW (2011) The limits of privacy in automated pro-\nfiling and data mining. Computer Law & Security Review\nShackelford SJ and Raymond AH (2014) Building the virtual\ncourthouse: Ethical considerations for design, implemen-\ntation, and regulation in the world of Odr. Wisconsin Law\nShannon CE and Weaver W (1998) The Mathematical Theory\nof Communication. Urbana: University of Illinois Press.\nSimon J (2010) The entanglement of trust and knowledge on\nSimon J (2015) Distributed epistemic responsibility in a hyper-\nconnected era. In: Floridi L (ed.) The Onlife Manifesto.\nStark M and Fins JJ (2013) Engineering medical decisions.\nSullins JP (2006) When is a robot a moral agent? Available at:\nhttp://scholarworks.calstate.edu/xmlui/bitstream/handle/\nSweeney L (2013) Discrimination in online ad delivery. Queue\nSwiatek MS (2012) Intending to err: The ethical challenge of\nlethal, autonomous systems. Ethics and Information\nTechnology14(4). Available at: https://www.scopus.com/\nTaddeo M (2010) Modelling trust in artificial agents, a first\nstep toward the analysis of e-trust. Minds and Machines\nTaddeo M and Floridi L (2015) The debate on the moral\nresponsibilities of online service providers. Science and\nTaylor L, Floridi L and van der Sloot B (eds) (2017) Group\nPrivacy: New Challenges of Data Technologies, 1st ed.\nNew York, NY: Springer.\nTene O and Polonetsky J (2013a) Big data for all: Privacy and\nuser control in the age of analytics. Available at: http://\nheinonlinebackup.com/hol-cgi-bin/get_pdf.cgi?han-\ndle=hein.journals/nwteintp11&section=20 (accessed 2\nTene O and Polonetsky J (2013b) Big Data for all: Privacy\nand user control in the age of analytics. Available at:\nhttp://heinonlinebackup.com/hol-cgi-bin/get_pdf.cgi?han-\ndle=hein.journals/nwteintp11&section=20 (accessed 2\nTonkens R (2012) Out of character: On the creation of virtu-\nous machines. Ethics and Information Technology 14(2):\nTufekci Z (2015) Algorithmic harms beyond Facebook and\nGoogle: Emergent challenges of computational agency.\nJournal on Telecommunications and High Technology Law\nTurilli M (2007) Ethical protocols design. Ethics and\nTurilli M and Floridi L (2009) The ethics of information\ntransparency. Ethics and Information Technology 11(2):\nTurner R (2016) The philosophy of computer science. Spring\n2016. In: Zalta EN (ed.) The Stanford Encyclopedia of\nPhilosophy. Available at: http://plato.stanford.edu/arch-\n20 Big Data & Society\nTutt A (2016) An FDA for algorithms. SSRN Scholarly Paper,\nRochester, NY: Social Science Research Network.\nValiant LG (1984) A theory of the learnable. Communications\nvan den Hoven J and Rooksby E (2008) Distributive justice\nand the value of information: A (broadly) Rawlsian\napproach. In: van den Hoven J and Weckert J (eds)\nInformation Technology and Moral Philosophy.\nVan Otterlo M (2013) A machine learning view on profiling.\nIn: Hildebrandt M and de Vries K (eds) Privacy, Due\nProcess and the Computational Turn-Philosophers of Law\nMeet Philosophers of Technology. Abingdon: Routledge,\nVan Wel L and Royakkers L (2004) Ethical issues in web data\nVasilevsky NA, Brush MH, Paddock H, et al. (2013) On the\nreproducibility of science: Unique identification of research\nresources in the biomedical literature. PeerJ 1: e148.\nVellido A, Marti\u00b4n-Guerrero JD and Lisboa PJ (2012) Making\nmachine learning models interpretable. In: ESANN 2012\nWiegel V and van den Berg J (2009) Combining moral theory,\nmodal logic and mas to create well-behaving artificial\nagents. International Journal of Social Robotics 1(3):\nWiener N (1988) The Human Use of Human Beings:\nCybernetics and Society. Da Capo Press.\nWiltshire TJ (2015) A prospective framework for the design\nof ideal artificial moral agents: Insights from the science of\nZarsky T (2013) Transparent predictions. University of\nIllinois Law Review 2013(4). Available at: http://\nZarsky T (2016) The trouble with algorithmic decisions an\nanalytic road map to examine efficiency and fairness in\nautomated and opaque decision making. Science,\nMittelstadt et al. 21"
}