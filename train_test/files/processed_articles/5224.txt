{
    "abstract": "Abstract\nThis article responds to recent debates in critical algorithm studies about the significance of the term ``algorithm.''\nWhere some have suggested that critical scholars should align their use of the term with its common definition in\nprofessional computer science, I argue that we should instead approach algorithms as ``multiples''--unstable objects that\nare enacted through the varied practices that people use to engage with them, including the practices of ``outsider''\nresearchers. This approach builds on the work of Laura Devendorf, Elizabeth Goodman, and Annemarie Mol. Different\nways of enacting algorithms foreground certain issues while occluding others: computer scientists enact algorithms as\nconceptual objects indifferent to implementation details, while calls for accountability enact algorithms as closed boxes\nto be opened. I propose that critical researchers might seek to enact algorithms ethnographically, seeing them as\nheterogeneous and diffuse sociotechnical systems, rather than rigidly constrained and procedural formulas. To do so,\nI suggest thinking of algorithms not ``in'' culture, as the event occasioning this essay was titled, but ``as'' culture: part of\nbroad patterns of meaning and practice that can be engaged with empirically. I offer a set of practical tactics for the\nethnographic enactment of algorithmic systems, which do not depend on pinning down a singular ``algorithm'' or\nachieving ``access,'' but which rather work from the partial and mobile position of an outsider.\n",
    "reduced_content": "Original Research Article\nAlgorithms as culture: Some tactics for\nthe ethnography of algorithmic systems\nNick Seaver\n Keywords\nAlgorithms, ethnography, multiplicity, tactics, methodology\nThis article is a part of special theme on Algorithms in Culture. To see a full list of all articles in this special\ntheme, please click here: http://journals.sagepub.com/page/bds/collections/algorithms-in-culture.\nTerminological anxiety\nAt a conference on the social study of algorithms in\n2013, a senior scholar stepped up to the audience\nmicrophone: ``With all this talk about algorithms,'' he\nsaid, ``I haven't heard anybody talk about an actual\nalgorithm. Bubble sort, anyone?''1 Over two days,\nspeakers had covered algorithmic topics from\nGoogle's autocomplete feature to credit scoring, but\nthe questioner was right: they had not examined any-\nthing like bubble sort, a simple algorithm for ordering a\nlist. Bubble sort, the questioner implied, was an actual\nalgorithm; autocomplete was something else.\nThe conference, Governing Algorithms, hosted at New\nYork University, was an early moment in the growing\ntransdisciplinary field of critical algorithm studies--\nbroadly speaking, the application of humanistic and\nsocial scientific approaches to algorithms.2 But already,\none of the field's central tensions was evident: Did these\nhumanists and social scientists, taking on objects that\nhad, until recently, been the domain of computer scien-\ntists, know what they were talking about?\nAs ``algorithm'' drifted out of computer science and\ninto popular and critical academic discourse, it seemed\nto signify a renewed concern for technical specificity.\nWhere ``Big Data'' was vague--originating in an over-\nheated marketing discourse--algorithms were precise.\nThey were the core stuff of computer science,\nTufts University, USA\nCorresponding author:\nNick Seaver, Tufts University, Medford, MA 02155, USA.\nEmail: nick.seaver@tufts.edu\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License (http://\nwww.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nBig Data & Society\njournals.sagepub.com/home/bds\ndefinitionally straightforward and, for many humanists,\nas distilled a case of rationalizing, quantifying, proced-\nural logics as it was possible to find (see, e.g., Totaro and\nNinno, 2014). The work to be done was clear: apply\nclassic critiques of rationality, quantification, and pro-\ncedure to these new objects and hit ``publish.''\nYet, just as critical scholars picked them up, algo-\nrithms seemed to break apart. They had become, in the\nwords of Governing Algorithms' organizers, ``somewhat\nof a modern myth'' (Barocas et al., 2013: 1), attributed\nwith great significance and power, but with ill-defined\nproperties. Where an algorithm like bubble sort was so\nsimple it could be described in a sentence,3 the charis-\nmatic mega-algorithms that had caught scholars'\nattention--Google's search, Facebook's newsfeed,\nNew York City's predictive policing system, Netflix's\nrecommender, and so on--were more complicated and\nharder to specify. Although these systems were called\n``algorithms'' by the public, academics, and even by\ntheir makers, the distance between them and their\nComputer Science 101 counterparts was unsettling.\nMaybe, in our inexpert enthusiasm, critical algorithm\nstudies scholars had made a mistake. Maybe we don't\nunderstand what algorithms are, after all.\nI take terminological anxiety to be one of critical\nalgorithm studies' defining features. But this is not\nbecause, as disciplinary outsiders, we are technically\ninept. Rather, it is because terminological anxieties\nare first and foremost anxieties about the boundaries\nof disciplinary jurisdiction, and critical algorithm stu-\ndies is, essentially, founded in a disciplinary transgres-\nsion. The boundaries of expert communities are\nmaintained by governing the circulation and proper\nusage of professional argot, demarcating those who\nhave the right to speak from those who do not\nand algorithms are no different. Our worry about\nwhat ``algorithm'' means has more to do with our pos-\nitions vis-a-vis other groups of experts than it has to do\nwith our ability to correctly match terms and referents.\nRather than offering a ``correct'' definition, this art-\nicle advances an approach to algorithms informed by\ntheir empirical profusion and practical existence in the\nwild--always at the boundaries of diverse communities\nof practice. It looks to anthropology for inspiration,\nboth because my own training is anthropological and\nbecause anthropology proves useful for thinking\nthrough encounters between disparate knowledge\ntraditions. It outlines an ethnographic approach to\nalgorithms because ethnographic methods are well\nsuited to the concerns that tend to occupy critical\nscholars--particularly concerns about how formalisms\nrelate to culture. Ethnography roots these concerns in\nempirical soil, resisting arguments that threaten to wash\naway ordinary experience in a flood of abstraction.\nRather than entering the field with a definition in\nhand, I propose using fieldwork to discover what algo-\nrithms are, in practice. After exploring two competing\nvisions of what an anthropology of algorithms might\nentail, I offer a set of practical tactics for the ethnog-\nraphy of algorithmic systems, derived from my own\nethnographic experience.\nAnthropology 1: Algorithms in culture\nA straightforward solution to our definitional crisis\nwould be to take some expert definition as decisive:\nlet computer scientists define ``algorithms'' and then\nexamine how those things interact with our own areas\nof expertise. Like many straightforward solutions, this\none has complications. As Paul Dourish has noted,\n``the limits of the term algorithm are determined by\nsocial engagements rather than by technological or\nmaterial constraints'' (2016: 3). That is to say, different\npeople, in different historical moments and social situ-\nations have defined algorithms, and their salient quali-\nties, differently. A data scientist working at Facebook in\n2017, a university mathematician working on a proof in\n1940, and a doctor establishing treatment procedures in\n1995 may all claim, correctly, to be working on ``algo-\nrithms,'' but this does not mean they are talking about\nthe same thing. An uncritical reliance on experts takes\ntheir coherence for granted and runs the risk of obscur-\ning a key interest of critical scholars: what happens at the\nedges of knowledge regimes.\nGiven this instability and diversity, Dourish\nadvances an anthropological case for taking on a\n``proper'' expert definition of algorithms: we should\ndo it not because this definition offers ``a foundational\ntruth of the nature of algorithms as natural occur-\nrences,'' but because it is what engineers do (2016: 2).\nIn anthropological parlance, ``algorithm'' is an emic\nterm, an actor's term, part of the linguistic architecture\nthrough which software engineers make sense of the\nworld and their work. Moreover, taking on such a def-\ninition facilitates critical algorithm scholars' political\nprojects: ``Critiques of algorithmic reasoning that set\ntheir own terms of reference for key terminology,'' he\nwrites, ``are unlikely to hit home'' (Dourish, 2016: 2).\nIf we want to understand engineers and get them to\nlisten to us, then we need to use terms as they do; it\nis a matter ``of ethnographic responsibility and prac-\nBut what exactly is the emic definition of\n``algorithm,'' and where should we find it? Dourish's\nargument hinges on a definition of group boundaries.\nHe writes:\nWhen technical people get together, the person who\nsays, ``I do algorithms'' is making a different statement\n2 Big Data & Society\nthan the person who says, ``I study software engineer-\ning'' or the one who says, ``I'm a data scientist'' and the\nnature of these differences matters to any understand-\ning of the relationship between data, algorithms, and\nThe boundaries of the ``algorithm'' are social bound-\naries: between algorithm people and other ``technical\npeople,'' and between technical people and their\nnon-technical others, who may not understand the\ndefinitions in play. Now imagine an ethnographer\nencountering these technical people; if she did not\nknow what an algorithm was supposed to be, she\nwould have to suss it out from how they speak and\nact. (``These people say `algorithm' whenever they\nwant those people to stop talking. . .'' ``Those people\ntalk about algorithms like precious intellectual prop-\nerty, while these people talk about them like\nplumbing. . .'') Dourish suggests that, if an ethnog-\nrapher does this, she will end up where he begins:\n``In computer science terms, an algorithm is an\nabstract, formalized description of a computational\nThis is, however, an empirical question: Do the\npeople who ``do'' algorithms today actually treat\nthem according to this ``proper'' definition?\nEthnography often throws analytic frameworks into\ndisarray, and this proved true in my own fieldwork\nwith US-based developers of algorithmic music recom-\nmender systems. After setting out to study engineers\nspecifically, I realized that many more actors shaped\nthe systems these companies built. Eventually, I inter-\nviewed half the employees of a recommender company\nthat at the time employed roughly 80 people. These\npeople's jobs ranged from summer intern to CEO,\nand from systems engineer to front-end web developer.\nAll of them, whether ``technical people'' or not, were\nparty to the production of algorithms, but in the office,\nthe ``algorithm'' seemed to be nowhere in particular.\nSo I sought it out, asking people to identify the algo-\nrithms they worked on. They typically balked at this\nquestion, and even people in the most ``algorithmic''\nroles at the company, working on machine learning\ninfrastructure or playlist personalization, located\n``the algorithm'' just outside the scope of their work,\nsomewhere in the company's code. One, a senior\nsoftware engineer with a prestigious undergraduate\ndegree in computer science told me that her training\non algorithms in theory was irrelevant to her work on\nalgorithms in practice, because algorithms in practice\nwere harder to precisely locate: ``It's very much black\nmagic that goes on in there; even if you code a lot of it\nup, a lot of that stuff is lost on you.'' The ``algorithm''\nhere was a collective product, and consequently every-\none felt like an outsider to it.\nWhen my interlocutors talked about algorithms,\nit was usually as part of a popular critical discourse\nthat pitted algorithmic recommendation against\nhuman curators, claiming that ``algorithms'' could not\nunderstand music well enough to recommend it (e.g.\nTitlow, 2013). These engineers, humans within a\nsystem described as inhuman, resisted this framing:\n``algorithms are humans too,'' one of my interlocutors\nput it, drawing the boundary of the algorithm around\nhimself and his co-workers. Sitting in on ordinary prac-\ntical work--whiteboarding sessions, group trouble-\nshooting, and hackathon coding--I saw ``algorithms''\nmutate, at times encompassing teams of people and\ntheir decisions, and at other times referring to emer-\ngent, surprisingly mysterious properties of the code-\nbase. Only rarely did ``algorithm'' refer to a specific\ntechnical object like bubble sort. When pressed, many\nof my interlocutors could recite a proper definition, but\nthese definitions were incidental to the everyday deci-\nsion making I observed in the field. In practice, ``algo-\nrithm'' had a vague, ``non-technical'' meaning,\nindicating various properties of a broader ``algorithmic\nsystem'' (Seaver, 2013), even in nominally ``technical''\nsettings.\nSo, while Dourish's argument for emic definition is\nsound, we cannot know those definitions in advance or\nassume that they will be precise and stable. If we look to\nthe places where algorithms are made, we may not find a\nsingular and correct sense of ``algorithm.'' Assuming\nthat we will reifies a vision of the algorithm that risks\nobscuring humanistic concerns and blinding us to diver-\nsity in the field. ``Technical people'' are not the only\npeople involved in producing the sociomaterial tangles\nwe call ``algorithms'' and in practice, even they do not\nmaintain the definitional hygiene that some critics have\ndemanded of each other. A diverse crowd of people,\nusing a wide array of techniques and understandings,\nproduce the ``algorithm'' in a loosely coordinated con-\nfusion. Neglecting this is especially problematic for the\nalgorithms that the public and most critics focus on:\nthese are distributed, probabilistic, secret, continuously\nupgraded, and corporately produced.\nMoreover, the ``correct'' definition of algorithms has\nbeen used precisely to isolate them from the concerns of\nsocial scientists and humanists, and it has been picked\nup by advocates and critics alike to set algorithmic\nprocesses apart from cultural ones. Dourish suggests\nthat clarifying what algorithms are facilitates a discus-\nsion of how they interact with what they are not, and he\nprovides a set of algorithmic ``others'' (things they are\noften confused with, but properly distinct from). This\nlist makes clear how the proper definition of algorithms\nserves to distinguish them from typical critical con-\ncerns: algorithms are not automation (thus excluding\nquestions of labor), they are not code (thus excluding\nquestions of texts), they are not architecture (thus\nexcluding questions of infrastructure), and they are\nnot their materializations (thus excluding questions\ndefinition carves out a non-social abstract space for\nalgorithms, artificially setting them apart from the vari-\nous concerns that they tangle with in practice. The tech-\nnologist who insists that his facial recognition\nalgorithm has no embedded politics and the critic\nwho argues that algorithmic music recommendation is\nan exogenous threat to culture both rely on an a priori\ndistinction between cultural and technical stuff.\nLet's call this the algorithms in culture approach. It\nhinges on the idea that algorithms are discrete objects\nthat may be located within cultural contexts or brought\ninto conversation with cultural concerns. Understood\nas such, algorithms themselves are not culture. They\nmay shape culture (by altering the flows of cultural\nmaterial), and they may be shaped by culture (by\nembodying the biases of their creators), but this rela-\ntionship is like that between a rock and the stream it is\nsitting in: the rock is not part of the stream, though the\nstream may jostle and erode it and the rock may pro-\nduce ripples and eddies in the stream. In this view,\nalgorithms can affect culture and culture can affect\nalgorithms because they are distinct.\nAnthropology 2: Algorithms as culture\nSomething like what has happened to computer scien-\ntists and the term ``algorithm'' happened earlier with\nanthropologists and ``culture'': a term of art for the\nfield drifted out of expert usage, and the experts lost\ncontrol. Through the 1980s, American anthropologists\nwere becoming generally skeptical of ``culture'' as an\nexplanatory concept or object of study (Abu-Lughod,\n1991). Its implicit holism and homogenizing, essential-\nist tendencies seemed politically problematic and ill-\nsuited to the conflictual, changing shape of everyday\nlife in anthropological field sites.\nBut while this skepticism grew, the culture concept\ngained purchase outside of anthropology: ``cultures''\nwere taken to mark diverse, bounded groups with time-\nless traditions, often synonymous with ethno-national\nidentities; companies that once described their employ-\nees as a ``family'' might now say they had a ``culture,''\nwhich designated an attitude toward work and, per-\nhaps, what food and games were in the break room\ngists debated the usefulness or even existence of culture,\nit became a matter of concern among people with no\nobligation to anthropological definitions.\nAs anthropologists increasingly studied people with\nthe power to resist outside explanations (Gusterson,\nnot just an epistemological one. While anthropologists\ncould critique the use of ``culture'' by fellow field-\nworkers, these new users of ``culture'' were often influ-\nential parts of the social scene anthropologists wanted\nto describe. Groups of people linked by employer or\nethnicity might take up ``culture'' as a project,\nmaking their understandings influential, even if anthro-\npologists disagreed with them; vernacular theories of\nculture could shape social action in their image.\nConsequently, many anthropologists turned from a\nvision of cultures as coherent symbolic orders to prac-\ntice as the stuff of cultural life (Bourdieu, 1972; Ortner,\n1984). As Lila Abu-Lughod put it, the practice\napproach to culture ``is built around problems of\ncontradiction, misunderstanding, and misrecognition,\nand favors strategies, interests, and improvisations\nover the more static and homogenizing cultural tropes\nsetting for actions, culture might be something people\ndo--an outcome of actions. This multiplies culture rad-\nically: instead of unified ``cultures,'' we have an array of\nloosely coordinated practices that compete and collab-\norate, sometimes for ``cultural'' goals and sometimes\nfor other goals--for instance, ``technical'' ones.\nAnnemarie Mol has advanced a radical version of\nthis focus on practice through her ethnographic and\nphilosophical work, which she calls ``praxiography''\n(2002). For Mol, reality itself is not prior to practices\nbut rather a consequence of them; in Mol's ``practical\nontology'' (Gad et al., 2015), actors do not act on\npre-given objects, but rather bring them into being--a\nprocess she calls ``enactment.'' Consequently, objects\nacted on in many different ways become ``multiples'':\nA ``culture,'' for instance, is not one coherent thing, nor\nis it a set of disparate things, such that every person\nenacts and imagines their own in isolation.\nFollowing Laura Devendorf and Elizabeth\nGoodman, I find this a useful way to approach algo-\nrithms--not as stable objects interacted with from\nmany perspectives, but as the manifold consequences\nof a variety of human practices. In their study of an\nonline dating site (2014), Devendorf and Goodman\nfound various actors enacted the site's algorithm differ-\nently: engineers tweaked their code to mediate between\nthe distinctive behaviors of male and female users; some\nusers tried to game the algorithm as they understood it,\nto generate more desirable matches; other users took\nthe algorithm's matches as oracular pronouncements,\nregardless of how they had been produced. No inner\ntruth of the algorithm determined these interactions,\nand non-technical outsiders changed the algorithm's\nfunction: machine learning systems changed in response\nto user activity, and engineers accommodated user pro-\nclivities in their code.\n4 Big Data & Society\nWe can call this the algorithms as culture position:\nalgorithms are not technical rocks in a cultural stream,\nbut are rather just more water. Like other aspects of\nculture, algorithms are enacted by practices which do\nnot heed a strong distinction between technical and\nnon-technical concerns, but rather blend them together.\nIn this view, algorithms are not singular technical\nobjects that enter into many different cultural interac-\ntions, but are rather unstable objects, culturally enacted\nby the practices people use to engage with them.\nThis vision of algorithms as culture differs from the\nnotion of ``algorithmic culture'' (Striphas, 2015), which\nposits algorithms as an transformative force, exogenous\nto culture. In this view, a movie recommender is cul-\ntural because it shapes flows of cultural material, not\nbecause its algorithmic logics are themselves cultural\n(Hallinan and Striphas, 2016). Nor is it what Gillespie\ncalls ``algorithms becoming culture'' (2016), which\nhappens when algorithms become objects of popular\ndebate and targets of strategic action (e.g. fans launch-\ning a listening campaign to influence a music rec-\nommender). Rather, algorithms are cultural not\nbecause they work on things like movies or music, or\nbecause they become objects of popular concern,\nbut because they are composed of collective human\npractices. Algorithms are multiple, like culture, because\nthey are culture.\nMethodological enactments\nIf we understand algorithms as enacted by the practices\nused to engage with them, then the stakes of our own\nmethods change. We are not remote observers, but\nrather active enactors, producing algorithms as particu-\nlar kinds of objects through our research. Where a\ncomputer scientist might enact algorithms as abstract\nprocedures through mathematical analysis, an anthro-\npologist might use ethnographic methods to enact them\nas rangy sociotechnical systems constituted by human\npractices. A computer scientist may be concerned with\nmatters of efficiency or how an algorithm interacts with\ndata structures; an anthropologist may care instead\nabout how an algorithm materializes values and cul-\ntural meanings. This disparity does not mean one of\nus is wrong and the other right--rather, we are engaged\nin different projects with different goals, and just as my\ndiscipline's methods are poorly suited to determining\nthe efficiency of an algorithm in asymptotic time, so\nthe computer scientist's are poorly suited to under-\nstanding the cultural situations in which algorithms\nare built and implemented.\nThis recognition of algorithms' multiplicity may\nseem destabilizing, causing trouble for efforts to hold\nalgorithms accountable or even to identify their effects.\nHowever, I want to suggest that this approach\nfacilitates, rather than limits, critique, making our\naccounts more adequate to the practices they describe,\nand centering those practices as a site of dispute and\npotential regulation. As Marina Welker puts it in\nEnacting the Corporation: ``How we construct corpor-\nations as actors has crucial entailments for how we\nassign responsibilities to them, and vice versa'' (2014:\n4). We can replace ``corporations'' with ``algorithms'' in\nthat sentence, but we do not have to: the algorithms we\ncare about are very often corporate products, and they\nseem more similar to corporations themselves--in their\nheterogeneity, diffuseness, and legal architecture--than\nto textbook algorithms like the bubble sort that I began\nwith. Presuming that algorithms must have a stable and\ncoherent existence makes it harder, not easier, to grap-\nple with their production and ongoing maintenance.\nAlgorithmic auditing provides a useful case for\nexamining how critical methods enact algorithmic\nobjects. These projects, inspired by historical efforts\nto expose housing discrimination, treat algorithmic sys-\ntems as black-box functions: hidden operations that\nturn inputs into outputs, like mortgage applications\ninto home loans (Diakopoulos, 2013; Sandvig et al.,\n2014). By varying inputs and examining the corres-\nponding outputs (e.g. constructing personae that vary\nby apparent race), audit studies can demonstrate\n``disparate impact''--differences in outcome that\naffect legally protected classes of people and thus\ninvoke regulatory response. What they cannot do is\nexplain conclusively how that disparate impact came\nabout.\nBy treating the ``inside'' of the algorithm as unknow-\nable, these approaches participate in enacting an under-\nstanding of the algorithm as a black box, as knowable\nonly through the relation between inputs and outputs.\nThis is not to say that audit approaches are responsible\nfor algorithmic secrecy--they are clearly responding to\nother efforts to keep algorithmic operations hidden--\nbut they are part of a set of coordinated practices\nthrough which algorithms becomes understood as,\nand remain, secret.\nWhile one might presume secrecy to be a simple\nmatter of hiding facts that could be easily revealed,\nsecrecy in practice is not so clear; secrecy is a social\nprocess, enacted by both insiders and outsiders (this is\na longstanding trope in the anthropology of secrecy;\nging algorithmic secrecy enact the algorithm differently,\nsetting its boundaries in different places--at the inter-\nface, at a certain point in the code, or at the legal\nboundary of the corporation. If, as Frank Pasquale\n(2015) suggests, we take algorithmic secrecy as a legal\nproblem, then our efforts to understand the algorithm\nneed to involve legal reasoning. If, as Jenna Burrell\n(2016) suggests, one source of algorithmic opacity\nis the intrinsic complexity of methods like neural net-\nworks, then we might try to engineer ``explainable'' sys-\ntems (e.g. Aha, 2017). Thus, details of contract law may\nbe as salient to the cultural functioning of algorithms as\nthe ability of a neural network to ``explain'' its outputs.\nAs Daniel Neyland demonstrates through ethno-\ngraphic work on an algorithmic accountability project,\nmaking algorithms accountable often means literally\nchanging them--making them ``account-able,'' in\nethnomedological jargon (2016). To make something\naccount-able means giving it qualities that make it\nlegible to groups of people in specific contexts.\nAn accountable algorithm is thus literally different\nfrom an unaccountable one--transparency changes\nthe practices that constitute it. For some critics this is\nprecisely the point: the changes that transparency\nnecessitates are changes that we want to have. This is\na plain example of how different efforts to enact an\nobject are both coordinated with each other and poten-\ntially in conflict. Transparency is not a revealing of\nwhat was always there, but a methodical reconfigur-\nation of the social scene that changes it toward particu-\nlar ends (see e.g. Ananny and Crawford, 2016;\nEthnographic tactics\nThis discussion recasts the anxiety described at the\nbeginning of this article: concerns about the proper def-\ninition of ``algorithm'' are caught up not only in the\nboundary work that constitutes disciplines, but also in\nthe methods those disciplines use. As algorithms are\nenacted by a wider range of methods, these enactments\ncome into conflict with each other, and the work of\ncoordinating among them becomes more challenging.\nAny method for apprehending algorithms now takes\nplace amidst this confusion.\nIn this situation, I have found ethnography to be a\nuseful method for enacting algorithms. Given its his-\ntory in the study of cultural difference, it is well suited\nto life among plural methods--for engaging the various\nways that people go about their lives, rather than trying\nto displace them. Not everyone needs to become an\nethnographer, but ethnography as a method is distinct-\nively appropriate to understanding how diverse meth-\nods interact. Ethnography is also good for seeing\nalgorithms as, rather than in culture--for apprehending\nthe everyday practices that constitute them and keep\nthem working and changing, not just for observing\nhow they relate to a distinct and properly ``cultural''\ndomain. Ethnographic methods help us gain purchase\non topics that concern critical scholars: the interrelation\nof people and computers, the local production of\nabstract representations, and the human sensemaking\nthat pervades algorithmic operations, locating these\nwithin sociocultural contexts that are diverse and\nchanging.\nIn the remainder of this article, I offer a set of ethno-\ngraphic tactics following the tradition of the ``ethnog-\nwhich has directed researcher attention to the often-\nneglected cultural features of sociotechnical systems.\nThese tactics are not unique to algorithmic\nobjects--they have their origins in a variety of ethno-\ngraphic domains. Nor should we expect every new\nobject to engender brand new tactics; indeed, one\nappealing feature of seeing algorithms as, rather than\nin, culture is a rejection of the idea that they are new or\nunique at all. Where many methods for critically enga-\nging algorithms take secrecy as one of algorithms'\ndefining features, I try here to offer paths away from\nsecrecy, so that ethnographers might ask and answer\nother questions. Given traditional ethnographic con-\ncerns with access, this is a challenge, requiring openness\nto fieldwork that diverges from canonical imaginaries.\nI have found these tactics especially useful for making\nalgorithms ethnographically tractable. They are a prag-\nmatic counterpart to the conceptual argument\nadvanced thus far--a set of theoretically informed\nScavenge\nAlgorithms are not the only obscure objects ethnog-\nraphers have tried to study. Hugh Gusterson has stu-\ndied the culture of nuclear weapons scientists--an\nto access their workplaces, Gusterson developed an\nethnographic method he called ``polymorphous engage-\nment'': this meant ``interacting with informants across a\nnumber of dispersed sites, not just in local commu-\nnities, and sometimes in virtual form; and it mean[t]\ncollecting data eclectically from a disparate array\nGusterson, these sources included the local cafeteria,\nbirthday parties, newspaper articles, and a variety of\nother heterodox ``sites.''\nAlthough this heterogeneous and apparently undis-\nciplined approach to ethnographic data collection\nseems a departure from the idealized image of a field-\nworker embedded long-term in a bounded society, it\nretains what Gusterson describes as ``the pragmatic\namateurism that has characterized anthropological\nEthnographers have always gleaned information from\ndiverse sources, even when our objects of study appear\npublicly accessible. Moreover, the scavenger replicates\nthe partiality of ordinary conditions of knowing--\neveryone is figuring out their world by piecing together\nheterogeneous clues--but expands on them by tracing\n6 Big Data & Society\ncultural practices across multiple locations (Marcus,\n1995) and through loosely connected networks\n(Burrell, 2009). These are ``entry points, rather than\nsites,'' as Burrell has suggested the networked ethnog-\ndition for all anthropological knowledge; as Ulf\nHannerz writes, ``ethnography is an art of the possible''\nA great deal of information about algorithmic sys-\ntems is available to the critic who does not define her\nobject of interest as that which is off limits or intention-\nally hidden. If our interest is not in the specific config-\nuration of a particular algorithm at one moment in\ntime, but in the more persistent cultural worlds algo-\nrithms are part of, then useful evidence is not bounded\nby corporate secrecy. In my own research, I learned\nfrom off-the-record chats with engineers about industry\nscuttlebutt, triangulated with press releases and the\nsocial media updates of my interlocutors. Sometimes,\ninterviewees stuck resolutely to the company line; other\ntimes, often after several interviews, they spilled the\nbeans. In academic and industry conference hallways,\npeople working in diverse sites talked across their dif-\nferences and around their various obligations to\nsecrecy, providing a rich source of information about\nhow algorithms and their meanings vary. On mailing\nlists, in patent applications, and at hackathons, I found\narguments, technical visions, and pragmatic bricolage.\n``Algorithms'' manifest across these sites differently: a\nconference presentation that evaluates an algorithm\n``properly'' for its accuracy in predicting user ratings\nis followed by a hallway conversation about how that\nmetric is useless in practice, or how the algorithm is too\ncomplex to be worth implementing at scale. There is\nmuch to be scavenged if we do not let ourselves be\ndistracted by conspicuous barriers to access.\nAttend to the texture of access\nNor is access as straightforward as it might seem.\nAchieving access remains a dream and challenge\nfor would-be fieldworkers, granting the right to say\n``I was there,'' but it may be more important for assert-\ning one's anthropological bonafides than it is for doing\ngood ethnography--interpreting the ordinary cultural\npatterns and practices that make up human life, with or\nwithout algorithms. This is largely because the\nMalinowskian imaginary of fieldwork access, in which\nit happens suddenly and thoroughly--``Imagine your-\nself suddenly set down surrounded by all your gear,\nalone on a tropical beach close to a native village''\n(Malinowski, 1922: 4)--is just that: imaginary. In prac-\ntice, ``access'' is a protracted, textured practice that\nnever really ends, and no social scene becomes simply\navailable to an ethnographer because she has shown up.\nRather, ordinary social interaction is marbled with\nI supplemented my own scavenging ethnography\nwith a summer internship at a music recommendation\ncompany, where I was free to roam around the office\nand interview employees. But even in the open plan\noffice, there were always further barriers to access.\nConversations in email threads or chat rooms I\nwasn't privy to, closed meetings, and coordination\nwith companies external to the office meant that\naccess was not a one-time achievement, but rather a\ncontinuous (and exhausting) process. Knowledge\nmight be hidden behind non-disclosure agreements,\ntaciturn interviewees, or inside jokes.\nAlthough it often felt like I was being excluded as an\noutsider ethnographer, this situation was not unique to\nme. For people all over the company and the industry\nmore broadly, everyday work was marked by varying\nlevels of access and obscurity. Casper Bruun Jensen has\ndescribed these ``asymmetries of knowledge'' (2010),\narguing that challenges to access--hidden meetings,\nreluctant interlocutors, non-disclosure agreements--are\npart of the field, not simply barriers around it. The field\nis ``a partially existing object emerging from multiple\nsites of activity that are partly visible, partly opaque to\nall involved actors, including the ethnographer''\nknow everything that is going on, both because algo-\nrithms can be quite complex and distributed (Seaver,\n2013) and because that is how human social life works\nmore generally. The field site is not a black box that can\nbe simply opened.\nRather than thinking of access as a perimeter around\nlegitimate fieldwork, the scavenging ethnographer can\nattend to access as a kind of texture, a resistance to\nknowledge that is omnipresent and not always the\nsame. These challenges are data themselves--about\nthe cultural life of algorithmic systems, how their\nsecrecy is constituted in practice, what kinds of infor-\nmation are so important that they must be kept secret,\nand what kinds of information are so important that\nthey must be widely known. Ethnographic projects, like\nthe algorithmic systems we want to study, ``are charac-\nterized by limited presence, partial information and\npaying attention to the texture of access, the ethnog-\nrapher learns about how knowledge circulates, infor-\nmation that is practically useful but also a research\noutcome in its own right: an algorithm's edges are\nenacted by the various efforts made to keep it secret.\nTreat interviews as fieldwork\nBecause the scavenging ethnographer is highly mobile,\nher fieldwork is likely to be interview-centric; with\nless time spent in any given location, it is challenging to\nsettle into idealized participant observation (Hannerz,\n2003). Multi-sited ethnographers are often anxious that\nthis reliance on interviews renders their work less\nethnographic, because interviews are commonly under-\nstood as artificial situations created by researchers. In\nanthropological methods talk, interviews are cast as the\nabject other of participant observation: they merely\nreflect what people say, not what they do. We all\nknow that talkers dissimulate, misremember, or select-\nively highlight features of their lives. Interviews are thus\nframed as unreliable--primarily a source for statements\nto be falsified by proper fieldwork. Ironically, this fram-\ning values participant observation much like how cor-\nporations value behavioral data analytics: people say\none thing (as indicated by their favorite artist on\nFacebook), but they do another (as indicated by their\nlistening history on Spotify).\nHowever, it is worth considering interviews as a\nform of cultural action themselves--not an artificial\nsituation constructed by researchers, but part of the\nworld in which research subjects live and make mean-\ning. The people who work in and around algorithmic\nsystems live in what Jenny Hockey calls an ``interview\nculture''--they know what interviews are, they witness\nthem conducted regularly in a variety of media, they've\nlikely been interviewed before, and often, they've\nconducted interviews themselves (Hockey, 2002; and\norganizes an interview, she is setting up a known kind\nof interaction with its own tacit rules and meanings.\nInterviews do not extract people from the flow of every-\nday life, but are rather part of it.\nHere is an partial list of interviews I conducted\nduring fieldwork: meeting for coffee with an engineer\nin a San Francisco coffeeshop; setting up a Skype con-\nversation with an interlocutor who insisted that I send\nhim a Google Calendar invite so that our meeting\nwould show up on his work calendar; interviewing a\nteam of research scientists over lunch in the restaurant\nnext door to their office; chatting in a bar with a former\nemployee of a music streaming service that was slowly\ngoing out of business; strolling in a park with a long\nterm informant who had become a friend; walking with\nan academic lab director who insisted we could only\ntalk while he was running his on-campus errands.\nNone of these interactions were unusual for my\ninterviewees. They fit me into existing patterns in\ntheir lives--often in ways that made my work difficult,\ngiving me only 30 distracted minutes or making it hard\nto take notes or use my audio recorder. They treated\nme like a prospective hire, a supervisor, an advisee,\na journalist, a friend, or a therapist. In these variously\nformatted conversations, algorithmic concerns mani-\nfested in many ways: as technical puzzles worked out\nwith colleagues over lunch, as sources of anxiety or\npower, as marketing tools, or even as irrelevant to the\nreal business of a company. Treating interviews as field-\nwork does not just reduce the ethnographer's anxiety\nabout relying on them--it broadens her attention,\nturning the mundane mechanics of arranging and con-\nducting conversations into material for analysis.\nParse corporate heteroglossia\nWhile I was in the field in January 2014, a new music\nstreaming service named Beats Music launched.\nAn extended commercial featured a manifesto, which\nlauded human musicality and criticized algorithmic\nprocessing, read over scratchy, organic animations: sil-\nhouettes kissing, turntables spinning, a sailboat tossed\non stormy water made of 1s and 0s.\nWhat if you could always have the perfect music for\neach moment, effortlessly? Drives would be shorter.\nKisses, deeper. Inspiration would flow, memories\nwould flood. You'd fall in love every night. [. . .] And\nto do that you'll need more inside your skull than a cir-\ncuit board. [...] We've created an elegant, fun solution\nthat integrates the best technology with friendly, trust-\nworthy humanity--that understands music is emotion,\nand joy, culture . . . and life.\nThe scavenging ethnographer will at some point\nencounter a document like this--an advertisement or\npress release or blog post from a company executive,\nwhich, in spite of claiming an author, seems certainly\nwritten by committee. Beats' manifesto evinces what\nMikhail Bakhtin called ``heteroglossia,'' the quality of\na text that speaks with many voices at once (1982). In\nthe short extract above, at least two voices are evident:\nthe flowery evocation of kisses, inspiration, memories,\nand love and the business language of ``elegant, fun\nsolutions.'' The ad was a paean to the human curators\nBeats claimed distinguished it from competitors, a jere-\nmiad against algorithms from a software company, and\nan awkward corporate effort to integrate technology\nand humanity.\nCorporate speech is often heteroglot, but for Bakhtin\nand the linguistic anthropologists who follow him, het-\neroglossia is not just a consequence of corporate author-\nship; it is an ordinary feature of language: ``In the reality\nof everyday life [. . .] the speech of any one person is filled\nby many different voices or linguistically constructed\nglossia requires long-term familiarity--without famil-\niarity, people tend to assume that voices are\nnecessarily singular and that variation is evidence of\nincoherence and contradiction rather than multiplicity\n8 Big Data & Society\nLinguistic anthropologists have made this case\nfor people, but for corporations its necessity is even\nclearer: outsiders often attribute singular agency and\nvoice to corporations composed of hundreds or thou-\nsands of employees, working in scores of internal\ngroups. Thus, we hear that ``Facebook'' has claimed\nor done something or that ``Spotify'' has a particular\npoint of view. But while managers may try to coordin-\nate their work, nothing intrinsically binds an engineer-\ning team to a social media intern or a company\nfounder. Especially in young companies or those in\ntransformation, the institutionalizing forces that\nwork to align these various voices are weak, and obvi-\nous heteroglossia in public statements is one notable\nconsequence.\nAs one of my interlocutors--an engineer with\nanother company--put it on social media, Beat's\nmanifesto was ``bullshit'': while it aired, Beats was\nadvertising for algorithm engineering positions, to\nwork on the system that would recommend its curators'\nplaylists to users. At a conference the previous year,\nI had even met one of those engineers, who bragged\nabout the company's technical sophistication.\nSpeaking through advertisements, job postings, and\nits engineers, the company said many things at once.\nAcross these various channels, and between the many\nvoices within them, ``algorithms'' were different things:\nprecious intellectual property, incompetent calculators,\nor the heralds of a new age of technical sophistication.\nHeteroglossia is a resource and a hazard for the eth-\nnographer: catching corporate messages as they move\nin and out of phase with each other can reveal the\ninterplay of practices within the corporation; it is not\nmerely evidence of corporate dissimulation. The eth-\nnographer needs to take care to resist interpretations\nthat cast corporations as singular actors with plain\nintentions expressed in public statements. Relying on\nonly one channel, or trying to find a presumed latent\ncoherence in it, oversimplifies corporate action. Some\nincoherence is to be expected.\nBeware irony\nIf careful parsing is important for making sense of cor-\nporate speech, it is doubly important for interpreting\nthe speech of computer programmers. As ethnog-\nraphers of computing cultures have noted (e.g.\nColeman, 2015), programmers are especially inclined\ntoward irony and jokes, making the ability to parse\nlayered meanings crucial for understanding what they\nsay. Recall that the central purpose of Clifford Geertz's\n``thick description'' was to detect irony--the difference\nbetween a blink, a wink, ``fake-winking, burlesque-\nfake-winking, rehearsed-burlesque-fake-winking'' and\nthe combinatorial elaboration of layered meanings\nwhich subtend activities which, on the surface, may\nengagement and richly contextual description could the\nethnographer distinguish such variety--or, in other\nwords, be in on the joke. Superficial accounts risk\ntaking ironic statements literally or missing the con-\nflicted experience of programmers negotiating between\ndifferent sets of values.\nIn my own fieldwork, I met many commercially\nemployed programmers who were deeply ambivalent\nabout their own work or their industry: some had pre-\nviously been academics or musicians and felt like they\nhad sold out; others felt a moral charge to pursue their\nown teams' work but felt guilty about broader industry\ndynamics. This ambivalence often manifested in\nambiguous claims or jokes about things like data,\nmarkets, or algorithms. One of my interlocutors often\njoked that data had ``forced'' him to make a design\ndecision that was, in context, clearly a matter of per-\nsonal preference. Out of context, his remarks were\ninterpreted as evidence of what Kate Crawford has\ncalled ``data fundamentalism''--``the notion that\ncorrelation always indicates causation, and that mas-\nsive data sets and predictive analytics always reflect\nobjective truth'' (2013). In context, it seemed evidence\nmore of parody or resigned irony than of enthusiastic\nbelief.\nThe attribution of fundamentalisms--technological\ndeterminism, naive economism, or hyper-rationalism--\nto computer programmers may, in some cases, indicate\nmore about critics' inability to parse ironic speech\nthan it does about technologists' simplistic beliefs.\nMy technical interlocutors read critical literature\n(I encountered the Crawford piece cited above\nshared by my interlocutors on social media), and\nthey criticized thoughtless uses of algorithmic process-\ning; but they also knew how to work in that language,\neither to persuade believers or to crack jokes at their\nexpense. Describing their statements as ``fundamental-\nism'' fails to capture the complex ways that data is\ntactically used in practice, and it risks obscuring local\ncritiques. As Judith Nagata has argued, ``The funda-\nmentalist epithet is often a form of verbal ammunition\nagainst ideological and political enemies and `Others,'\nrecalling old orientalisms, and usually imposed unilat-\nerally, whether by academics, popular opinion, or the\nirony, long-term engagement is crucial for ethnog-\nraphers ``to understand, not only the substance and\ncontent of beliefs, but also their context, source, and\nare not necessarily populated by data fundamentalists,\nbut rather by diverse and ambivalent characters,\nworking in contexts and with meanings that cannot\nbe simply read or guessed at, but which must be\nfound empirically. It is too easy (and too common) to\ntry reading programmers' motivations off of algorith-\nmic systems and to conclude that the programmers\nthemselves must be algorithmic--limited by naive\nand rigid assumptions about human life. Ironically,\nthis mistake echoes a common humanistic critique of\nBig Data: that it reduces people to decontextualized\nformalisms. As Clifford Geertz put it 45 years ago,\nboth of these approaches suffer from trying ``to under-\nPace Dourish, I would argue that avoiding such read-\nings is a serious matter ``of ethnographic responsibility\nand practical politics,'' allowing us to grasp the social\ndynamics within algorithmic systems and to commu-\nnicate with the people there about those systems' con-\nConclusion\nI have argued for the merits of understanding algo-\nrithms as intrinsically cultural--as enacted by diverse\npractices, including those of ``outside'' researchers.\nApproaching algorithms ethnographically enacts them\nas part of culture, constituted not only by rational\nprocedures, but by institutions, people, intersecting\ncontexts, and the rough-and-ready sensemaking that\nobtains in ordinary cultural life. This enactment differs\nfrom canonical expert enactments, which hold algo-\nrithms to be essentially abstract procedures, and it\ndiffers from other cultural approaches to algorithms\nthat try to locate them as forces on culture's bound-\naries. Ethnography helps us to attend to empirical situ-\nations which are not necessarily stable or coherent.\nEthnography provides a useful orientation for enter-\ning and understanding worlds of meaning-laden prac-\ntice, but conventional understandings of algorithms as\ndefined by secret procedure suggest that ethnographic\napproaches are infeasible without a level of access that\ncannot realistically be obtained. The tactics I have laid\nout here are techniques for routing around that chal-\nlenge; they work to enact algorithms not as inaccessible\nblack boxes, but as heterogeneous and diffuse socio-\ntechnical systems, with entanglements beyond the\nboundaries of proprietary software.\nWhile I insist on imagining alternatives to visions of\nalgorithms as essentially secret, this style of ethno-\ngraphic enactment does not answer all the questions\nthat people might want to ask of them. Questions\nabout the particular workings of particular algorithms\nat particular moments in time remain broadly\nunanswerable so long as corporations are able to hide\nbehind legal and technical secrecy. Nonetheless, a sense\nof the algorithm as multiple, and of ethnography as a\npractice for producing and participating in plural\nenactments of algorithms, offers fruitful avenues for\nproducing actionable knowledge in spite of such\nsecrecy.\n"
}