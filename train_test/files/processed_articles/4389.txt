{
    "abstract": "Abstract\nAlthough the results of the final examination in Indonesia were the dominant factor in determining high school\ngraduation, the public still did not know how the examination was administered nor how results were used to determine\nstudent graduation. Despite its importance, no comprehensive studies about its implementation have been conducted.\nThe present study investigates the implementation of school-based assessment (SBA) in upper secondary schools\nfocusing on the development and administration of the English writing test. This particular test was not covered in\nthe English national examination. Twenty-one schools were surveyed, selected through stratified random sampling. In-\ndepth case studies were conducted in three selected schools representing fully implementing school (FIS), moderately\nimplementing school (MIS), and partially implementing school (PIS). The majority were categorized as PIS. This study\nsuggests that the way in which examinations were implemented needs serious consideration, especially in light of the\nnew regulation that student graduations are based on the result of the school examinations and no longer on the result\nof the national examination.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nIntroduction\nThe recent government regulation in Indonesia has changed\nthe status of school examinations from being low stakes to\nbeing high stakes (Regulation No. 13/2015). In the Indonesian\ncontext, school-based assessment (SBA) in upper secondary\nschool is intended to measure whether students have attained\ncompetence standards for graduation, and SBA can now\ndetermine whether or not students graduate. The competen-\ncies cover subject mastery, knowledge, good character, and\nattitude and skills necessary to become independent individ-\nuals and to continue their education.\nDespite the absence of a comprehensive study of the\nresults of Indonesian school examinations, some writers\noutside Indonesia have reported positive aspects of the\nimplementation of SBA. For example, Chong (2009); Talib,\nKamsah, Naim, and Latif (2014); and Tong and Adamson\n(2015) mentioned that SBA can promote a favorable educa-\ntion process that is oriented to learning. As such, schools\nare no longer preoccupied with teaching to test practices\ndue to negative washback effects of the National\nExamination (Andrew, Fullilove, & Wong, 2002; Cheng &\n2005). They allow teachers to get involved in making\nassessment decisions and improving teaching methods in\nresponse to students'needs (Maxwell & Cumming, 2011, in\nSBA challenges teachers' creativity in monitoring stu-\ndents' learning progress and designing appropriate tests\naligned with the curriculum content (Sulistyo, 2009; Talib\net al., 2014). In the writing test in Indonesia's National\nExamination, for example, teachers needed to become more\ncreative in designing more relevant performance assess-\nments, especially as they still use indirect testing. Indirect\ntesting of performance reduces its validity (Cohen, 1998;\ndifficult to find any correlation between the micro aspects of\nlinguistics (knowledge of grammar and vocabulary) with\nwriting ability.\nOther studies were also in doubt. In Hong Kong, for\nexample, skepticism about the appropriateness of SBA was\nwidespread when it was first introduced (Cheng, Andrews, &\n1Ma Chung University, Malang, Indonesia\n2Universitas Negeri Malang, Indonesia\nCorresponding Author:\nDaniel Ginting, Ma Chung University, Villa Puncak Tidar N-1 Malang, East\nEmail: daniel.ginting@machung.ac.id\nTests of Writing in the School\nExamination in Upper Secondary Schools\nDaniel Ginting1 and Ali Saukah2\n Keywords\nschool examination, writing tests, Government Regulation No. 13/2015\n2 SAGE Open\nYu, 2011). In Malaysia, a similar issue had revolved around\nthe technical aspects of the implementation and teacher\u00adstu-\ndent readiness for change. Majid (2011) mentioned that\nteachers in Malaysia still had some uncertainties about the\ndemands of SBA. In particular, the teachers were concerned\nabout their ability to meet its demands and their role, and\nexpected difficulties in implementation. The same findings\nwere also reported by Talib et al. (2014), who mentioned that\nalmost 80% of Malaysian teachers' SBA was within the\nrange of unsatisfactory to basic. This finding implies that the\nteachers'knowledge of language testing was still low. Similar\nstories have also been reported by Sulistyo (2009) when\ninterviewing a number of Indonesian teachers long before\nSBA was made a high-stakes test. The teachers were not\nfully ready to conduct SBA and still needed expertise in lan-\nguage testing.\nAlthough the prevailing laws have made all schools in\nIndonesia implement SBA as a criterion for student gradua-\ntion, no comprehensive study has been conducted. This study\nseeks to fill the gap in the literature by focusing on the imple-\nmentation of writing skills as one of the subjects tested in the\nexamination at the end of upper secondary school. It begins\nby presenting the results of the survey in general upper sec-\nondary schools in the city of Malang (East Java Province,\nIndonesia), and then reports on a study carried out in three of\nthem. The article concludes with recommendations for\nappropriate treatment of writing tests. These improvements\ncontribute more widely to the effective development of SBA\nexaminations in Indonesia.\nMethod\nThe target population of this present study included all of\nthe general upper secondary schools in Malang conducting\nSBA writing tests as the examination at the end of upper\nsecondary school. Referring to the reports of the National\nExamination from the Education National Standard Body\ngeneral upper secondary schools in Malang were reported to\nhave participated in the National Examination in 2010 (see\nThis population reflected an imbalance; students in pri-\nvate and public schools had different levels of achievement.\nFor example, the score distribution varied considerably, with\nFor that reason, a stratified random sampling technique\nwas used with the following procedure. First, the desired\nsample size was 50% of the total population (43 schools), giv-\ning 22 schools to be subjects. Next, samples were stratified on\nthe basis of whether schools were state or private and on the\nbasis of their achievement. This was done by establishing the\nquantitative categories of school achievement and the interval\nvalues for each category. The range (2.59) was obtained\nby subtracting the lowest score (5.68) from the highest\nscore (8.27). Next, the interval score (0.9) for each school\nachievement category was established by dividing the range\nscore of 2.59 by 3. This resulted in three categories of school:\nhigh achieving (7.30). The summary of the stratified random\nsampling of this present study can be seen in Table 2.\nFollowing the survey, three different schools were stud-\nied, representing full, moderate, and partial implementation.\nThese three schools were then categorized according to the\nextent to which they had implemented the writing test as a\nschool examination, determined by a checklist of about 14\nyes/no questions (a dichotomous closed question format). A\nyes answer was scored 1, a no answer was scored 0, and a\nblank answer was also scored 0. Checklist items were\nweighted because each question had a different degree of\nimportance: less important (0.10), important (0.30), and very\nimportant (0.60). Of the 14 questions, two were considered\nas less important (14%), five were viewed as important\n(36%), and seven were regarded as very important (50%).\nThis resulted in three categories of schools for their imple-\nmentation of the test (see Table 3):\nExploratory factor analysis (EFA) was carried out to assess\nthe construct validity of the instrument (Walt & Steyn, 2008;\nWeigle, 2002). The qualitative data were obtained from docu-\nment study and interview with semistructured format. The\ndata analysis obtained from the checklist went through three\nstages. First, it dealt with the descriptive statistics. Second,\nthis present study used the chi-square test of a distribution to\nobtain the evidence of the significant differences in the fre-\nquency distribution of different categories. Third, following\nTable 1. Descriptive Statistics of the 2010 National Examination\nof the General Upper Secondary Schools in Malang.\nThe average score of national examination\nTable 2. School Samples of the Present Study.\nNo. Category\nState\nschools\nPrivate\nschools Total\n1 High-achieving schools\n2 Middle-achieving schools\n3 Low-achieving schools\nGinting and Saukah 3\nthe chi-square computation, the data were then analyzed by\nmeans of cross-tabulation statistical technique.\nStatistical Validation\nThe chi-square technique was used to check for significant\ndifferences among the variables under investigation: the\nimplementation of the writing test in the state and private\nsecondary schools. This study found that the value of the\nasymptotic significance (two-sided) Pearson chi-square was\n.024, which was smaller than the significance alpha () .05.\nAs such, the approximately significant (.024) <.05 indicated\nthat the null hypothesis (H0) was rejected. Hence, it was con-\ncluded that there was a significant difference in terms of the\nimplementation of the writing test in the state and the private\nupper secondary schools.\nNext, cross-tabulation was done to indicate the frequency\nwith which the corresponding categories of the categorical\nvariables co-occur. Referring to Table 4, the majority of the\nupper secondary schools (48%) were categorized as partially\nimplementing school (PIS) category, followed by 42% as a\nmoderately implementing school (MIS) category and 10% as\nfully implementing school (FIS) category.\nFindings of Categorization of Writing\nTests\nRegarding the ratio of private to state schools, this study cat-\negorized 56% of private schools and about 20% of the state\nschools as partially implementing. In the MIS category, 44%\nwere private schools and 40% were state schools.About 40%\nof state schools belonged to the FIS category, but no private\nschools.\nThe clustering indicated that the policy on the writing\ntests in school examinations had not yet operated as expected;\nnearly all subjects were clustered in moderate and partial\nimplementation categories.\nImplementation Description\nThe following is a detailed description about how schools\nimplemented the writing tests.\nIn the checklist, the respondents were asked whether\nthey had made some alternative writing materials (the sec-\nondary tests and the makeup tests). Quite surprisingly,\n95.2% of all schools in the sample preferred not to prepare\nother writing materials (see Table 5). According to the\nteachers in the majority of schools (MIS and PIS), their\nheavy teaching loads kept them from preparing alternative\ntests. They reported that they usually had to handle from\nfour to six classes, representing approximately 24 teach-\ning hours per week. Besides, they also believed that stu-\ndents could not cheat in the writing test because it was\nsubjective.\nThe next question in the checklist asked respondents\nwhether they had involved experts in validating assessment\nconstructs. Table 6 shows that both MIS and PIS, as the\nmajority (76.2%), did not consult experts either inside or out-\nside their schools. Schools in both categories viewed the\ninvolvement of experts outside their schools as complicated\nbecause of administrative procedures. They were not sure\nwhether the schools supported the idea of involving external\nexperts in test development. Instead, the teachers were asked\nto work on their own test designs. Even if there was\nMusyawarah Guru Mata Pelajaran (MGMP) (a teacher asso-\nciation teaching in the same subjects), they never contacted\nthem due to their heavy teaching loads. This implies that\nmost schools did not see the involvement of experts as an\nurgent need.\nMeanwhile, only FIS (23.8%) had involved experts in test\ndesign. These experts were usually assigned to check whether\nTable 3. Categorization of Schools Based on Writing Test\nImplementation.\nTotal score\nPercentage of all\nchecklist items Category\nNote. PIS = partially implementing school; MIS = moderately implementing\nschool; FIS = fully implementing school.\nTable 4. Cross-Tabulation Analysis of the Implementation of the\nWriting Tests.\nCategory of the\nimplementation of\nthe writing test\nSectors\nTotal %\nState % Private %\nNote. PIS = partially implementing school; MIS = moderately implementing\nschool; FIS = fully implementing school.\nTable 5. Preparation for Other Writing Test Materials.\nResponse Frequency % Valid %\nTable 6. Consultation With the Experts.\nResponse Frequency % Valid %\n4 SAGE Open\nthe test design had fit the requirements of the construct rep-\nresentation and construct relevance.\nThe next checklist question asked respondents whether\ntheir schools had followed the regulation for seat arrange-\nment in the test room, moving the seats of test takers 1 meter\naway from each other (see Figure 1). Table 7 shows that\nnearly all schools (FIS and MIS; 90.5%) adhered to this reg-\nulation. By contrast, PIS (9.5%) that did not follow the pro-\ncedure had permitted the students to sit as in a regular class.\nThe PIS teachers believed that fair administration of the\nschool examination was not always primarily indicated by\nsuch a seat arrangement. Instead, they relied on thorough\nsupervision of the teachers.\nThe next question asked respondents whether their\nschools had informed both the students and proctors about\nthe procedures for penalties for cheating during the examina-\ntion. About 90.5% of schools (FIS and MIS) reported that\nthey had informed both students and the proctors of those\nprocedures through oral delivery in meetings and in printed\ndocuments (see Table 8). The majority of schools (FIS and\nMIS) adopted the rules of the test administration from the\nlocal National Education Department and disseminated them\nprior to the exam. Moreover, teachers in FIS and MIS\nreported that the schools posted the rules on the walls of all\ntesting rooms so that everyone could see them.\nBy contrast, PIS (9.5%) reported the rule was only dis-\nseminated orally prior to the examination. Teachers in PIS\nmentioned that it was not necessary to inform proctors of\npenalty procedures; violating the norms of the test adminis-\ntration would never make any sense to proctors as they were\ngood people tied to ethical codes.\nThe next item in the checklist asked respondents whether\nthey as assessors had shared their scores to obtain the aver-\nage as the final score. While MIS and PIS, as the majority\nused this strategy in the scoring procedure (see Table 9). FIS\nheadmasters assigned a teacher to carry out cross-scoring.\nAll assessors took turns to read all students' work indepen-\ndently and then entered the scores in the forms. The students'\nfinal scores were obtained by dividing all scores. This\nachieved greater objectivity in scoring.\nMIS and PIS, the majority of schools (85.7%), did not have\na cross-scoring procedure and never applied average scores\nto obtain final scores. For them, cross-scoring was time-\nconsuming and was impossible due to the limited number of\nassessors (three to five teachers). Some reported that more\nthan 400 students took the tests. For practicality, teachers\ndecided to divide students'writings into equal piles. For exam-\nple, if there were 100 items and two assessors, then, each\nassessor would get 50 items to read and score. They only read\nand scored students' writings of their own, and never shared\nthose pieces with other assessors to read and score. The school\nadministrators often demanded that they had to submit the scores\nin short time frames, with some teachers even reporting that\nthey were required to finish only 1 day after the examination.\nAs another alternative, MIS conducted a cross-prompt\nassessment procedure. Different teachers were assigned par-\nticular prompts in the test. For example, if there were four\nwriting prompts in the writing test, teachers would only read\nand score the prompts to which they had been assigned. As\nTeacher A had been assigned to read Prompt 1, he or she only\nscored that part and left other prompts to other teachers. In the\nsame way, Teacher B was assigned to read Prompt 2, and he\nor she only checked and scored that part. The same procedure\nwas also true with Teachers C and D. Hence, the final score\nwas obtained by adding all scores from Teachers A, B, C, and\nD. These teachers believed that this procedure potentially\nincreased the fairness of the scoring. The assumption was that\nFigure 1. Seating arrangement in the school examination.\nTable 7. Seating Arrangement.\nResponse Frequency % Valid %\nTable 9. The Final Score.\nResponse Frequency % Valid %\nTable 8. Sanction for the Students and Proctors.\nResponse Frequency % Valid %\nGinting and Saukah 5\nteachers easily tend to give unfairly higher scores to their own\nstudents.\nThe next item on the checklist was to ask respondents\nwhether they had involved third assessors to resolve discrep-\nancies in scores. MIS and PIS, as the majority (90.5%), never\ninvolved a third assessor to mediate score discrepancies (see\nTable 10). For them, this procedure was unnecessary and\nthey preferred to score the students' work individually. In\naddition to the limited number of assessors, scoring the stu-\ndents' work individually was more practical as they could\nsubmit the score list in due time.\nMeanwhile, FIS, as the minority (9.5%), that had used a\ncross-scoring procedure viewed the involvement of the\nthird assessors as necessary to solve score discrepancy\nproblems. According to FIS teachers, besides mediating\nthe score discrepancy, the third assessors were also\nassigned to check the assessors' lists of scores. When dis-\ncrepancies inevitably occurred, the third assessor usually\nasked all assessors to meet to discuss them. If they came to\nagreement when reexamining their previous grades, the\ntwo assessors simply modified the scores. However, if they\ncould not get agreement, then the third assessor was\nassigned to read the students' work and give their own\nscores. In these cases, students' final scores were the mean\nof the scores from all assessors, including the third\nassessors.\nAnother item in the checklist asked whether respondents\nhad prepared scoring rubrics to score the students' essay\n(see Table 11). In practice, MIS and PIS as the majority\n(85.7%) had used mixed scoring formats both for essays\nand objective tests. For objective tests, they prepared the\nanswer keys to such test prompts: filling in the empty\nblanks with the correct words and arranging the jumbled\nsentences into good paragraphs (see Figures 2 and 3).\nMeanwhile, for essays, the scoring rubrics were designed in\nan analytical format that included such aspects as grammar,\nmechanics, vocabulary, and content. Hence, the students'\nfinal scores were the combination of the results from both\nobjective tests and essays.\nTable 10. The Involvement of the Third Rater.\nResponse Frequency % Valid %\nFigure 2. The sample of objective writing prompts in MIS.\nNote. MIS = moderately implementing school.\nMeanwhile, FIS, as the minority (14.3%), had used a holis-\ntic scoring rubric. Some descriptors even described certain\nalso trialed their writing tests prior to implementation as the\nreal test (see Figure 4). A small number of the students were\ninvited to take the test, and the results were gathered for analy-\nsis. One teacher with extensive experience in language testing\ntook roles of both expert and chairperson of the teacher panel\nthat was responsible for validating the test.\nThe next question asked respondents whether they had\ntrialed the writing test before using it. This step collected\nevidence on whether the particular test models and scoring\nrubrics had been properly designed. Table 12 shows MIS and\nPIS, the majority of schools (95.2%), did not trial the test.\nInstead, the teacher panel discussed the test quality. PIS that\nhad a shortage of teaching staff entrusted the teacher to\nassess the quality of their own tests.\nBy contrast, the teachers in FIS (4.8%) stated that the trial\nwas part of a series of activities in test development. Teachers\nobserved the test administration and analyzed the results of\nthe writing tests. During the trial, about three or four volun-\ntary students did the writing tests. The teachers gathered the\nresponses of the sample students and reviewed them. They\nTable 11. The Scoring Rubrics.\nResponse Frequency % Valid %\nArrange the jumbled sentences below into a good paragraph\n 1. Then, whisk the eggs with a fork until smooth\n 2. Add some milk and whisk well\n 3. Heat the oil in a frying pan\n 4. Crack the eggs into a bowl and stir\n 5. Grate the cheese into the bowl and stir. Then\n 6. Pour the mixture into the frying pan\nTurn the mixture with a spatula when it browns. Cook\nboth sides\n 8. The cheese omelet is ready to be served\n 9. Place it on a plate and season it with salt and pepper\n10. After the omelet is cooked\nFigure 3. The sample of objective writing prompts in PIS.\nNote. PIS = partially implementing school.\n6 SAGE Open\nClass: XII LanguageTime: 120 minutes\nDo the writing test clearly!\nCreate an Hortatory Exposition Text with a theme of \"Clean and green is our School\"\n (Persuade the reader to support the school program to get the environment of the school clean and green) Start your writing with:\n Thesis : An introduction of the issue\n Arguments : Arguments to lead the readers believe\n Recommendation : Strengthen the writer's point of view to ask the reader to follow\n Use : Simple present tense and passive voice\n Connectors , examples: then, after that, etc.\n Words that link arguments, examples: firstly, on the other hand, therefore, etc.\nFigure 4. The sample of writing prompts in FIS.\nNote. FIS = fully implementing school.\nwho had been appointed as convenor. The convenor usually\nannounced the technical aspects of the scoring procedure\nwith which assessors had to comply, such as reading the stu-\ndents' writing attentively, applying the scoring rubrics to the\nstudents' writing, using the students' writing samples to rep-\nresent different levels of performance or the aspects being\nassessed, putting the students' score on the list of the grades\nprovided, obtaining the final scores, and involving the third\nassessor in case of a discrepancy in scores.\nDiscussion\nThis study found the writing test had not been satisfactorily\nimplemented. In practice, nearly all schools were clustered\ninto PIS (48%) and MIS (42%), with very few categorized as\nFIS (10%). Many schools neglected substantial aspects of\nlanguage testing, such as validity and reliability (Bachman &\n2002). For example, the threat to validity is obviously appar-\nent for most schools (MIS and PIS), which still used indirect\ntesting for assessing writing skills. Besides the absence of\nexperts and test trialing, many teachers still believed that an\nindirect testing approach, as used in the National Exam, is\ncredible. In line with Sulistyo's (2009) findings, most teach-\ners readily adopt indirect testing to develop writing tests in\nthe SBA because none of them doubted the validity of the\nNational Examination. Moreover, the threat to reliability is\nquite observable; most schools never scored under controlled\nreading, never used cross-scoring, and never involved a third\nassessor to resolve score discrepancies between assessors.\nAlthough the School Examination test has high stakes for\nstudents, it did not affect teachers'professional performance.\nThe present study has shown how teachers were reluctant to\nbe creative in developing a more relevant assessment to test\nperformance skills like writing. Saukah and Cahyono (2015)\nhave reported that teachers in mostly low-achieving schools\nTable 12. The Tryout of the Tests.\nResponse Frequency % Valid %\nTable 13. The Scoring Under Controlled Reading.\nResponse Frequency % Valid %\nfocused on noting areas of students' confusion, vague or\nincomplete responses, and unanticipated responses. If stu-\ndents' responses indicated ineffectiveness, considerable\nrestructuring of the testing tool would be necessary.\nThe next question in the checklist asked respondents\nwhether they had scored the students' writing at the same\ntime and place in a group work. Table 13 shows that MIS and\nPIS, the majority of schools (85.7%), had disregarded this\nprocedure in the writing test administration. They did not see\nsuch a procedure as important. To them, this requirement\nwas impractical due to possible distractions from other mem-\nbers of the group and the limited number of assessors, so\nthey preferred to score students' work individually. Thus, the\nteachers shared the students' writing in equal number and\ntook them home for further scoring.\nBy contrast, FIS regarded the controlled reading proce-\ndure and scoring as a necessity. The headmasters in FIS\n(14.3%) officially required that the reading of the students'\nwriting took place at the school within the scheduled time. It\nwas done at a certain date and place and led by one teacher\nGinting and Saukah 7\nhave given their attention to the preparation of the National\nExamination, although it is no longer the sole basis for stu-\ndent graduations. They mention heavy workloads and the\nabsence of institutional support as reasons for not developing\ntheir professional skills. These findings were similar to the\nstudy by Talib et al. (2014). Teachers were unprepared for\nthe change and found the new system challenging. About\n79.66% of Malaysian teachers were not fully engaged in\npracticing SBA. Similarly, overall results revealed that\nMalaysian teachers' SBA practice classrooms are within the\nrange of unsatisfactory to basic levels of almost 80%.\nConclusion\nThis study finds that the practices of the writing test in the final\nexamination of secondary school have resulted in different\npatterns of implementation, with a strong tendency for the par-\ntial implementation of policy. Hence, this study recommends\nsome action points. First, teachers should be aware that the\nschool examination, as a high-stakes test, demands significant\nresponsibility on the part of teachers, and this should affect\ntheir professional performance. The decisions that teachers\nmake in the school examination will affect the future of their\nstudents. Second, although the government has so far provided\nthe teachers with technical guidance for administering the\nexaminations (Pedoman Teknis Pelaksanaan Ujian Sekolah/\nMadrasah/The Technical Guidance of the Implementation of\nMadrasah/School Examination), teachers clearly need profes-\nsional development to improve their skills. Short training\ncourses given by experts would help teachers to be able to\ndesign better tests for school examinations.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nAndrew, S. J., Fullilove, J., & Wong, Y. (2002). Targeting wash-\nBachman, L. F., & Palmer, A. S. (1996). Language testing in prac-\ntice. Oxford, UK: Oxford University Press.\nBrown, H. D. (2007). Principles of language learning and teaching.\nNew York, NY: Pearson Education.\nBSNP (2010). Laporan Hasil Ujian Nasional Tahun Pelajaran\nPendidikan Badan Penelitian dan Pengembangan Kementerian\nPendidikan Nasional.\nCheng, L., Andrews, S., & Yu, Y. (2011). Impact and consequences\nof school-based assessment (SBA): Students' and parents'\nCheng, L., & Watanabe, Y. (2004). Washback in language test-\ning: Research contexts and methods. Thousand Oaks, CA:\nChong, K. K. (2009). Whither school-based coursework assess-\nment in Singapore. In 35th IAEA Conference\u00adAssessment for\na Creative World, Brisbane, Australia. Retrieved from http://\nwww.iaea.info/documents/paper_4d73afd.pdf\nCohen, A. (1998). Strategies in learning and using a second lan-\nguage. Harlow, UK: Longman.\nFuraidah, F., Saukah, A., & Widiati, U. (2015). Washback of\nEnglish national examination in the Indonesian context.\nHamp-Lyons, L. (1991). Assessing second tests. Harlow, UK:\nPearson Education.\nHughes, A. (2003). Testing for language teachers. Cambridge, UK:\nCambridge University Press.\nMajid, F. A. (2011). School-based assessment in Malaysian schools:\nThe concerns of the English teachers. Journal of US-China\nMaxwell, G. S., & Cumming, J. J. (2011). Managing without exam-\nination: Successful and sustained curriculum and assessment\nreform in Queensland. Australia's Curriculum Dilemmas:\nMcNamara, T. (1996). Measuring second language performance.\nLondon, England: Longman.\nQi, L. (2005). Stakeholders' conflicting aims undermine the wash-\nback function of a high-stakes test. Language Testing, 22,\nSaukah, A., & Cahyono, A. E. (2015). National exam in Indonesia\nand its implications to the teaching and learning of English.\nSulistyo, G. H. (2009). English as a measurement standard in\nthe national examination: Some grassroots' voice. TEFLIN\nTalib, R., Kamsah, M. K., Naim, H. A., & Latif, A. A. (2014). From\nprinciple to practice: Assessment for learning in Malaysian\nschool-based assessment classroom. International Journal of\nTong, S. A., & Adamson, B. (2015). Student voices in school-based\nassessment. Australian Journal of Teacher Education, 40(2),\nWalt, J. L., & Steyn, F. (2008). The validation of language tests.\nWeigle, S. C. (2002). Assessing writing. Cambridge, UK:\nCambridge University Press.\nWeir, C. (1993). Understanding and developing language tests.\nNew York, NY: Prentice Hall.\nAuthor Biographies\nDaniel Ginting is currently teaching at the English Letter Study\nProgram, Ma Chung University (Indonesia). He obtained his\nDoctor's degree in ELT from State University of Malang.\nAli Saukah has been a full-time faculty member of the English\nDepartment of Universitas Negeri Malang (Indonesia) since\n1980, and involved in the development of English Language\nCurriculum and in other related activities at the national levels"
}