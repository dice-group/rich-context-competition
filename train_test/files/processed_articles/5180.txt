{
    "abstract": "Abstract\nKitaoka's Tomato is a color illusion in which a semitransparent blue-green field is placed on top of a\nred object (a tomato). The tomato appears red even though the pixels would appear green if\nviewed in isolation. We show that this phenomenon can be explained by a high-pass filter and by\nhistogram equalization. The results suggest that this illusion does not require complex inferences\nabout color constancy; rather, the tomato's red is available in the physical stimulus at the\nappropriate spatial scale and dynamic range.\n",
    "reduced_content": "Special Issue: Seeing Colors\nKitaoka's Tomato: Two\nSimple Explanations Based on\nInformation in the Stimulus\nArthur Shapiro\nDepartment of Psychology, American University, Washington, DC, USA;\nDepartment of Computer Science, American University, Washington,\nDC, USA; Program in Behavior, Cognition, and Neuroscience, American\nUniversity, Washington, DC, USA\nLaysa Hedjar and Erica Dixon\nProgram in Behavior, Cognition, and Neuroscience, American University,\nWashington, DC, USA\nAkiyoshi Kitaoka\nDepartment of Psychology, Ritsumeikan University, Kyoto, Japan\n Keywords\nadaptation/constancy, color, lightness/brightness, natural image statistics\nIntroduction\nA color illusion created by Akiyoshi Kitaoka recently went viral on the Internet (see\nFigure 1). The image, reminiscent of demonstrations by Land (1959), consists of an object\n(a strawberry or a tomato) behind a veiling transparent layer. The image is considered an\n``illusion'' because the tomato appears red, but the pixels that make up the tomato have\ngreater values for B and G than for R. So, if a small patch of the tomato is viewed in\nisolation, the patch will appear blue-green.\nThe standard account for recent color phenomena such as the color-changing dress is\nbased on color constancy (Gegenfurtner, Bloj, & Toscani, 2015; Hesslinger & Carbon,\nCorresponding author:\nArthur Shapiro, American University, 4400 Massachusetts Ave NW, Washington, DC 20016, USA.\nEmail: arthur.shapiro@american.edu\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License\n(http://www.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sage-\npub.com/en-us/nam/open-access-at-sage).\ni-Perception\njournals.sagepub.com/home/ipe\nO'Regan, 2017) and has also been applied to Kitaoka's Tomato. Color constancy refers\nto the observation that objects maintain a relatively stable color appearance across a wide\nrange of illuminants. For example, a red tomato viewed under a greenish light and then a\nyellowish light appears to be red under both illuminants, but the light reflected from the\ntomato under the greenish illuminant to the eye is wildly different from the light reflected\nfrom the tomato under the yellowish illuminant to the eye. According to the standard\naccount of color constancy, the change in illuminant tends not to affect our perception of\nthe object because our perceptual system forms a representation of the tomato's material (i.e.,\nthe distal object), not a representation of the light reaching the eye (i.e., the proximal\nstimulus).\nThe problem is that the visual system does not have direct access to the material and\ntherefore must construct a representation from responses to the light reaching the eye. Most\ncurrent color constancy theories propose that the visual system forms a representation of the\nmaterial's surface based on cues in the image about the reflectance of the material and the\nnature of the illuminant (Foster, 2011; Lee & Smithson, 2016; Radonjic, Cottaris, &\nsystem assigns these cues probabilistic weights based on prior experiences with the object,\nthe illumination, and other information. So, in Figure 1(a), the standard color constancy\nFigure 1. The original Kitaoka Tomato Illusion. (a) The image of the tomato is composed of green pixels.\nThe image is decomposed into low-pass (b) and high-pass (c) images. The low-pass image contains the\ninformation of the overlay/illuminant and the high-pass image contains information from the object. The\nvalues of the pixels are shown in squares and were taken using the 1 \u00c2 1 pixel grabber in Adobe Photoshop at\nthe same location in both images.\n2 i-Perception\naccount suggests that the tomato appears red because the observer has previous experience\nwith tomatoes and with greenish illumination; the visual system automatically discounts\nthe veiling illumination so that the observer can infer the likely color of the\nmaterial--hence, the tomato appears the color of the surface (i.e., red) and not the color\nof the light reaching the eye (i.e., blue-green).\nWhile inference-based approaches have their appeal, there is a long history of other\napproaches to such phenomena that are based on early-stage filters and on visual\nadaptation (Blakeslee, Cope, & McCourt, 2016; Blakeslee & McCourt, 2011; Ekroll &\nMcCann, Parraman, & Rizzi, 2014; Ratliff, Knight, & Graham, 1969). Indeed, many well-\nknown illusions, like Adelson's Checker-Shadow and Lotto and Purves' Rubik's cube, can\neasily be explained by simple computations on the image (Dixon & Shapiro, 2017; A. Shapiro\n& Lu, 2011) but are still given as examples of processes based on inferences, experience, and\ncognitive strategies.\nHere, we illustrate how two low-level approaches can account for Kitaoka's Tomato and\ncan possibly give insight into early visual processes. The results suggest that many aspects of\ncolor and brightness illusions arise because of information physically available in the image,\nand that this information could potentially be extracted by processes in the early visual\nsystem.\nDemonstrations\nWe will demonstrate two image-processing algorithms on Kitaoka's Tomato: One procedure\nis based on separating the image into components with different spatial responses (Dixon &\nShapiro, 2017; A. Shapiro & Lu, 2011) and the other is based on optimal tuning with\nhistogram equalization (see Barlow & Foldiak, 1989; A. Shapiro & Lu, 2011; A. G.\nShapiro, 1993). We present the demonstrations using commercial filters available in Adobe\nPhotoshop to illustrate the simplicity of the approach and so that other investigators can test\nthe techniques with minimal effort.\nDemonstration 1: High Spatial Frequency and Low Spatial Frequency Color Vision\nDemonstration 1 is based on that idea that visual images carry information at a variety of\nspatial scales (Graham, 1989). Following Dixon and Shapiro (2017), we divide Kitaoka's\nTomato into low and high spatial frequency component images. Figure 1(a) shows a\nreproduction of the original Kitaoka Tomato created by placing an image of a tomato on\nLayer 1 and a semitransparent blue-green on the layer above (see Dixon & Shapiro, 2017).\nWhen these layers are combined, a target pixel on the tomato has an R, G, and B value of\n133, 168, and 164, respectively (the values of B and G are higher than R); these values can be\nseen in the colored square next to Figure 1(a).\nThe low-pass and high-pass versions of Figure 1(a) are shown in Figure 1(b) and (c),\nrespectively. To create Figure 1(b), the image in Figure 1(a) is blurred (low-pass filter)\nwith a radius equal to 200 pixels. To create Figure 1(c), Figure 1(a) is filtered with Adobe\nPhotoshop's high-pass filter with a cutoff of 200.\nThe low-pass component (Figure 1(b)) shows a solid field with a chromaticity that\napproximately equals the chromaticity of the overlay (an R, G, and B value of 101, 178,\nand 168, respectively). The tomato is not discernable in the image; a visual system that has\nonly a low spatial frequency response would encode global changes but would be blind to\nShapiro et al. 3\nvisual objects in the scene. A high-pass component (Figure 1(c)) shows the tomato as if the\nveiling blue-green layer has been removed. The tomato in Figure 1(c) has a stronger R value\nrespectively)--corresponding more directly to an observer's reports. A high-pass filter,\ntherefore, is equivalent to subtracting the blurred image from the original and adding a\nconstant. In effect, Figure 1(c) ``discounts'' the information contained in the blurred image\n(Figure 1(b)) from the original image (Figure 1(a)).\nDemonstration 2: Histogram Equalization\nThe visual system continually adapts to chromatic and luminance information in the\nenvironment. Historically, and in many current Perception textbooks, adaptation is\ndiscussed in terms of ``fatigue'': a process in which a cell lowers its response rate to steady\nstimulation. However, as a general rule, visual adaptation can be considered a process for\nmaximizing the response range available to the visual system (see Barlow & Foldiak, 1989;\nCraik, 1938; Webster, 2015). For example, when looking at a field of green grass, the visual\nsystem should adjust its response so that it can discriminate the maximum number of shades\nof green, at the expense of discrimination of shades of red. So, if an image has a statistical\ndistribution along a particular dimension, the visual system should adjust its response so that\nit can maximize the number of levels that can be discriminated along that dimension. One\nway of encapsulating this principle is with histogram equalization, a standard image-\nprocessing technique.\nHere, we will apply histogram equalization to Kitaoka's Tomato. Figure 2(a) shows the\noriginal image along with the histogram of the R, G, and B values. Figure 2(b) shows the\nimage after a rough equalization correction. The histogram equalization was performed\nmanually using a level operator and adjusting the maximum and minimum of each\nchannel independently. A simple histogram equalization technique eliminates the effect of\nthe overlay and returns an image close to the original image. For Figure 2(b), the readjusted\nThe R, G, and B values of the test patch are now 234, 81, and 115, respectively.\nFigure 2. The original Kitaoka Tomato Illusion showing (a) the distribution of the R, G, and B pixels in the\nimage and (b) the image adjusted to equalize the distribution of each of the pixels.\n4 i-Perception\nThe result shows that at the appropriate dynamic range, the tomato is red--a result that\nshould be expected given the results from Figure 1(b), since the histogram equalization\nprocedure recenters the average value of the image, thereby eliminating the blue-green\noffset. In effect, the procedure recenters each channel, a principle otherwise referred to as\nConclusion\nKitaoka's Tomato is a strong color illusion, and variants of the illusion have become popular\non the Internet. We have shown two methods for eliminating the illuminant based purely on\nthe stimulus and without any consideration of the material properties of the object.\nNumerous filter-based models would handily account for Kitaoka's Tomato. For instance,\nthe Milano Retinex Family (Rizzi & Bonanomi, 2017) shows variations of Land's (1983,\n1986) Retinex algorithms. Almost all of these algorithms could account for the illusions, as\nZeman, Brooks, and Ghebreab (2015), and Buchsbaum (1980). Indeed, even von Kries'\nThe methods presented here differ from other filtering or adaptation models only in\nsimplicity. Following A. Shapiro and Lu (2011), we are suggesting that the filters ``work''\nin part because the information for color constancy (and for estimating the illuminant) exists\nin the stimulus at the appropriate spatial/intensity scale. That is, the overlay changes the\nchromaticity of the tomato at the level of the pixel, but not for high spatial frequency content.\nSimilarly, as demonstrated by Dixon and Shapiro (2017), global changes in illumination are\nprimarily carried in the low spatial frequency content. Most filtering or adaptation models\ntherefore will be successful or differ from each other in how they adjust to extract that\ninformation from the image and how they equalize the dynamic range of the responding\ncolor channels.\nDemonstrations of the simplicity and efficacy of simple filter approaches are necessary\nbecause recent publications, expert discussion on the internet and news, and discussions on\nthe CVNet mailing list seem to ignore the power of these basic approaches in favor of models\nbased on Bayesian priors and sophisticated estimates concerning our knowledge about the\nmaterial properties and illumination. In the standard color constancy model, the light\nreaching the eye is almost incidental to our perception since our perception is assumed to\ncare primarily about understanding the properties of a distal object. It therefore needs to be\nreemphasized that very simple computational operations can give a first approximation to\nthese distal properties under many circumstances.\nLow-level processes for color constancy are often dismissed because they seem to lack a\nfunctional purpose, because they cannot account for all brightness/lightness phenomena\n(such as assimilation), and because they are thought to produce ``scalloped'' artifacts that\nare not typically perceived. We are not suggesting that the simple operations capture exactly\nwhat the visual system does, nor do we think that this is a complete model of the visual\nsystem. Furthermore, the operations in Demonstrations 1 and 2 are done to the whole image,\nand such operations (if they exist) are almost certainly accomplished by local processes. Also,\nas we have noted elsewhere, our one parameter filter models have difficulty with Cornsweet\nmany versions of White's effect and assimilation.\nHowever, it would be surprising if the principles underlying these filters did not exist in\nsome analogous physiological form. An easy way to implement a tunable spatial filter (as in\nShapiro et al. 5\nFigure 1(c)) would be with an array of Difference of Gaussian filters, where the radii of\nthe center and surround Gaussians can adapt independently of each other. The size of the\ninhibitory surround controls the amount of low spatial frequency content passed by the filter,\nand the size of an excitatory center affects the amount of high spatial frequency content\npassed by the filter. Indeed, the simplicity of such a tunable system suggests a purpose for\nwhy center\u00adsurround receptive fields are found in retinal cells and are ubiquitous in nearly all\nsensory systems.\nThe filter, however, does not have to occur at an early retinal stage since, as was emphasized\nby A. Shapiro and Lu (2011), a cortical representation of an object is itself a form of high-pass\nfilter. Presumably, the early visual system samples the retinal image at a range of spatial scales;\nthe later visual system builds representations of objects by selectively pooling from these\nfiltered responses. The pooling processes would give a higher weight to filters that respond\nmaximally to regions that are about the same size of the object and would give a lower weight to\nfilters that respond maximally to areas larger than the object. This process diminishes the\nimportance of the low spatial content and therefore acts something like the processes that\ncreate Figures 1(c) and 2(b). More than that, since global illumination is primarily contained in\nthe low spatial frequency content, any representation of an object will not encode information\nabout the illuminant and will intrinsically behave with some level of color constancy.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or\npublication of this article.\nFunding\nThe author(s) received no financial support for the research, authorship, and/or publication of this\narticle.\nReferences\nBarlow, H. B., & Foldiak, P. (1989). Adaptation and decorrelation in the cortex. In C. Miall, R.\nM. Durbin, & G. J. Mitchison (Eds.), The Computing Neuron (pp. 54\u00ad72). Wokingham, England:\nAddison Wesley.\nBlakeslee, B., Cope, D., & McCourt, M. E. (2016). The Oriented Difference of Gaussians (ODOG)\nmodel of brightness perception: Overview and executable Mathematica notebooks. Behavior\nBlakeslee, B., & McCourt, M. E. (1999). A multiscale spatial filtering account of the White effect,\nBlakeslee, B., & McCourt, M. E. (2001). A multiscale spatial filtering account of the Wertheimer\u00ad\nBlakeslee, B., & McCourt, M. E. (2004). A unified theory of brightness contrast and assimilation\nincorporating oriented multiscale spatial filtering and contrast normalization. Vision Research, 44,\nBlakeslee, B., & McCourt, M. E. (2011). Spatiotemporal analysis of brightness induction. Vision\nBuchsbaum, G. (1980). A spatial processor model for object colour perception. Journal of the Franklin\n6 i-Perception\nCraik, K. J. (1938). The effect of adaptation on differential brightness discrimination. The Journal of\nDakin, S. C., & Bex, P. J. (2003). Natural image statistics mediate brightness `filling in.'. Proceedings of\nDixon, E. L., & Shapiro, A. G. (2017). Spatial filtering, color constancy, and the color-changing dress.\nEkroll, V., & Faul, F. (2009). A simple model describes large individual differences in simultaneous\nGegenfurtner, K. R., Bloj, M., & Toscani, M. (2015). The many colours of `the dress'. Current Biology,\nGraham, N. V. S. (1989). Visual pattern analyzers. New York, NY: Oxford University Press.\nHering, E. (1905/1964). Outline of a theory of light sense (L. M. Hurvich & D. Jameson, Trans.).\nCambridge, MA: Harvard University Press.\nHesslinger, V. M., & Carbon, C. C. (2016). #TheDress: The role of illumination information and\nindividual differences in the psychophysics of perceiving white-blue ambiguities. i-Perception, 7,\nJobson, D. J., Rahman, Z., & Woodell, G. A. (1997). A multiscale retinex for bridging the gap between\ncolor images and the human observation of scenes. IEEE Transactions on Image Processing, 6,\nKingdom, F. A., McCourt, M. E., & Blakeslee, B. (1997). In defence of ``lateral inhibition'' as the\nunderlying cause of induced brightness phenomena: A reply to Spehar, Gilchrist and Arend. Vision\nLand, E. H. (1983). Recent advances in Retinex theory and some implications for cortical\ncomputations: Color vision and the natural image. Proceedings of the National Academy of\nLee, R. J., & Smithson, H. E. (2016). Low levels of specularity support operational color constancy,\nparticularly when surface and illumination geometry can be inferred. Journal of the Optical\nMcCann, J. J., Parraman, P., & Rizzi, A. (2014). Reflectance, illumination, and appearance in color\nRadonjic, A., Cottaris, N. P., & Brainard, D. H. (2015). Color constancy in a naturalistic, goal-directed\nRatliff, F., Knight, B. W., & Graham, N. (1969). On tuning and amplification by lateral inhibition.\nProceedings of the National Academy of Sciences of the United States of America, 62, 733\u00ad740.\nRizzi, A., & Bonanomi, C. (2017). Milano Retinex family. Journal of Electronic Imaging, 26, 1\u00ad7.\nRobinson, A. E., Hammon, P. S., & de Sa, V. R. (2007). Explaining brightness illusions using spatial\nShapiro, A., & Lu, Z. L. (2011). Relative brightness in natural images can be accounted for by removing\nShapiro, A. G., Beere, J. L., & Zaidi, Q. (2003). Time-course of S-cone system adaptation to simple and\nToscani, M., Gegenfurtner, K. R., & Doerschner, K. (2017). Differences in illumination estimation in\nvon der Twer, T., & MacLeod, D. I. (2001). Optimal nonlinear codes for the perception of natural\nShapiro et al. 7\nvon Kries, J. (1905/1970). Chromatic adaptation. In D. L. MacAdam (Ed.), Sources of color science\nWallisch, P. (2017). Illumination assumptions account for individual differences in the perceptual\ninterpretation of a profoundly ambiguous stimulus in the color domain: ``The dress.''. Journal of\nWitzel, C., Racey, C., & O'Regan, J. K. (2017). The most reasonable explanation of ``the dress'':\nWitzel, C., van Alphen, C., Godau, C., & O'Regan, J. K. (2016). Uncertainty of sensory signal explains\nXiao, B. (2016). Color constancy. In R. Luo (Ed.), Encyclopedia of color science and technology\n(pp. 1\u00ad10). New York, NY: Springer.\nZaidi, Q., & Shapiro, A. G. (1993). Adaptive orthogonalization of opponent-color signals. Biological\nZeman, A., Brooks, K. R., & Ghebreab, S. (2015). An exponential filter model predicts lightness\nAuthor Biographies\nArthur Shapiro is Professor of Psychology and is currently the Chair\nof Computer Science at American University in Washington, D.C.\nHe is also affiliated with American University's Program in\nBehavior, Cognition, and Neuroscience and Center for Behavioral\nNeuroscience. He is co-editor of the Oxford Compendium of Visual\nIllusions (2017) and has won international awards for visual\nillusions he created. His research focuses primarily on color and\ncolor contrast perception, motion perception, perceptual\norganization, and low-light-level vision, but he is interested in any\nquestion that relates to how the brain constructs our perceptual\nworld.\nLaysa Hedjar is currently a PhD student in the Behavior,\nCognition, and Neuroscience Program at American University.\nShe obtained a bachelor's degree in biology from Old Dominion\nUniversity. Her research interests include color vision and\nluminance and contrast perception.\n8 i-Perception\nErica Dixon is a research project manager with the Center for\nHealth Incentives and Behavioral Economics at Penn Medicine,\nwhere she works on research in health policy and medication\nadherence. She has a PhD in Behavior, Cognition, and\nNeuroscience from American University, where she studied how\nspatial filtering in the visual system underlies brightness\nphenomena and color constancy. Erica's research interests lie in\nhow and why people perceive the world the way they do and how\ndecision making impacts perception (and vice versa).\nAkiyoshi Kitaoka is a professor of Psychology at Ritsumeikan\nUniversity in Japan. He studies visual illusions and produces a\nvariety of illusion works.\nShapiro et al. 9"
}