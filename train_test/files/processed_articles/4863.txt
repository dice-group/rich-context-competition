{
    "abstract": "Abstract. We investigated the contribution of binocular disparity to the rapid recognition of\nscenes and simpler spatial patterns using a paradigm combining backward masked stimulus\npresentation and short-term match-to-sample recognition. First, we showed that binocular\ndisparity did not contribute significantly to the recognition of briefly presented natural and\nartificial scenes, even when the availability of monocular cues was reduced. Subsequently,\nusing dense random dot stereograms as stimuli, we showed that observers were in principle\nable to extract spatial patterns defined only by disparity under brief, masked presentations.\nComparing our results with the predictions from a cue-summation model, we showed that\ncombining disparity with luminance did not per se disrupt the processing of disparity. Our\nresults suggest that the rapid recognition of scenes is mediated mostly by a monocular\ncomparison of the images, although we can rely on stereo in fast pattern recognition.\n",
    "reduced_content": "a Pion publication\nMatteo Valsecchi\nAbteilung Allgemeine Psychologie, Justus-Liebig-Universit\u00e4t, Otto-Behaghel-Str. 10F, D-35394 Giessen, Germany;\ne-mail: matteo.valsecchi@psychol.uni-giessen.de\nBaptiste Caziot\nGraduate Center for Vision Research, SUNY College of Optometry, 33 W 42nd St., New York, NY 10036, USA; SUNY Eye\nBenjamin T. Backus\nGraduate Center for Vision Research, SUNY College of Optometry, 33 W 42nd St., New York, NY 10036, USA; SUNY Eye\nKarl R. Gegenfurtner\nAbteilung Allgemeine Psychologie, Justus-Liebig-Universit\u00e4t, Otto-Behaghel-Str. 10F, D-35394 Giessen, Germany;\ne-mail: Karl.R.Gegenfurtner@psychol.uni-giessen.de\n Keywords: stereo vision, natural image, scene recognition, random-dot stereogram, visual masking.\n1 Introduction\nBinocular disparity, along with luminance, color and their changes over space and time, is one of the\nelements that define our visual world (Adelson & Bergen, 1991). Yet the purpose, extent, and condi-\ntions under which our visual systems use disparity are not fully understood.\nValsecchi and Gegenfurtner (2012) demonstrated that binocular stereo can improve the long-term\nmemory for natural scenes, although this is the case only under very specific conditions, i.e. only for\nscenes where a semantic encoding could not support recognition (forest scenes). Another situation in\nwhich binocular disparity could be of advantage is when scenes have to be encoded quickly for imme-\ndiate recognition. Binocular disparity could contribute to the fast encoding of the scene by enhancing\nthe segmentation of the single objects (Caziot, Valsecchi, Gegenfurtner, & Backus, 2011). Moreover, it\ncannot be excluded that the disparity of at least some of the objects and surfaces in the scene is directly\nencoded and contributes to recognition.\nIt has been shown that disparity can enhance the recognition of objects (Edelman & B\u00fclthoff,\n1992) particularly if the viewpoint changes between the encoding and recognition phases (Burke,\nmovements within the peri-personal space (McKee, Levi, & Bowne, 1990; Sheedy, Bailey, Buri, &\nBass, 1986) and to break camouflage (Julesz, 1971; McKee, Watamaniuk, Harris, Smallman, & Taylor,\n1997; Wardle, Cass, Brooks, & Alais, 2010). More recent evidence indicates that stereopsis can be\nhelpful in determining the depth arrangement of objects embedded in scenes and beyond peri-personal\nThe role of binocular disparity in rapid scene and\npattern recognition\n123 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\nAlthough disparity might contribute to the visual processing of scenes, it might do so with a\nslower time course as compared with luminance-based shape extraction (e.g. McKee et al., 1990; also\nsee Valsecchi & Gegenfurtner, 2012; Westheimer, 2011). The first question we asked in the present\nstudy is thus whether the visual system uses disparity in order to encode scenes for short-term recogni-\ntion. The second question we asked is whether the visual system relies on binocular disparity when\nother visual dimension(s) defining the visual scene are greatly impoverished.After finding no evidence\nfor a stereo enhancement of recognition with images of natural and simulated scenes we asked a third\nquestion: is disparity actually available to the visual system, but simply not used for encoding scenes\nwhen other cues are available?\nResearchers have dealt extensively with the combination of disparity and other visual cues in the\nestimation of depth (e.g. see Landy, Maloney, Johnston, & Young, 1995), in the estimation of slant\n(e.g. Backus & Banks, 1999; Backus, Banks, van Ee, Crowell, & Crowell, 1999; Banks & Backus,\nDoorschot, Kappers, & Koenderink, 2001; Liu, Collin, & Chaudhuri, 2000). These studies were typi-\ncally conducted by having participants judge stimuli along one dimension (depth, slant, shape), often\nbuilding psychometric curves and estimating points of subjective equality. Tasks requiring a yes/no\nresponse, such as detection at threshold were less frequent (but see Ichikawa, Saida, Osa, & Mune-\nchika, 2003; Meese & Holmes, 2004). The role of binocular disparity in visual recognition has been\nlargely neglected, except for the study by Liu and colleagues (2000) who found a relatively weak\nenhancement in recognition rate when faces were defined by both stereo and luminance as compared\nto luminance alone.\nWe first tested the relative contribution of binocular disparity to scene recognition by using natural\nand artificial scenes and manipulating the presence of monocularly available cues, such as the chroma-\nticity of object surfaces. Then, using random dot stereograms (RDS) we investigated the recognition\nof patterns defined solely in stereo and luminance.\n2 Experiment 1: natural scenes\nThe first question we addressed in the present study is thus whether and how fast disparity contributes\nto the visual encoding of scenes. To this aim, we applied a modified version of the paradigm which\nGegenfurtner and Rieger (2000) used in order to demonstrate that color plays a role in the early pro-\ncessing of visual scenes. In this paradigm, the processing of the scene image was interrupted by the use\nof a pattern mask. So far, dynamics of disparity processing have been studied with short presentations\nchronous masking has rarely been used (but see Lehmkuhle & Fox, 1980 for a metacontrast masking\nexample), and we know of only one study that used a post-mask whose 2D location coincided with\nthe one of the targets (Ritter, 1980). Little direct evidence was thus available guiding our choice of the\nmask to use. Nonetheless, the patterns of interference that have been observed are compatible with the\nnotion that the visual system processes stereo-defined depth effectively as a third spatial dimension\nto use masks overlapping with the targets also in depth, i.e. nonstereo masks for nonstereo targets and\nstereo masks for stereo targets. Furthermore, we constructed our mask images with a stereo structure\nthat had depth relations consistent with the overlap (occlusions) in the 2D pattern. As targets we used\nforest images, a category of images for which Valsecchi and Gegenfurtner (2012) showed that binocu-\nlar disparity enhances long-term memorization.\nMethods\nObservers\nOne group of 13 students from the Justus-Liebig University of Giessen (11 females, mean age:\n25.1 years) participated in the study in exchange for payment. Participants in this and the following\nexperiments provided written informed consent in agreement with the Declaration of Helsinki. Meth-\nods and procedures were approved by the local ethics committee LEK FB06 at Giessen University\n(proposal number 2009-0008). All observers were naive as to the aim of the study.\nStimuli\nStimuli were 216 color pictures of forest scenes. The pictures were taken in the Schiffenberger\nWald, in the vicinity of Giessen, using a Fujifilm Finepix W1 3D digital camera (Fujifilm Holdings\nCorporation, Tokyo, Japan). A subset of the images was used by Valsecchi and Gegenfurtner (2012). In\nthe present and in the following experiments, the images were displayed in a mirror stereoscope. The\npictures were rescaled to 656  492 pixels and shown on a pair of identical 19-inch Dell UltraSharp\n1907FP LCD monitors (Dell, Inc., Round Rock, TX) with a 75-Hz frame rate. The monitors were\nviewed through two orthogonal first surface mirrors (169  194 mm). From the effective viewing\ndistance of 55.5 cm, the pictures subtended 19.9\u00b0  14.8\u00b0. The vergence demand of the fixation mark\nspecified a distance equal to the viewing distance.\nFor each trial, a picture was randomly chosen from the database to be used as a target, and another\nto be used as the distractor. A mask, composed of a superimposition of 50 irregular polygons whose\ncolors were randomly sampled from the target and distractor images, was created for each trial. From\ndirect inspection of a subset of images, we estimated that the largest (uncrossed) disparities were 4.5%\nof the horizontal size of the image for the farthermost elements that could be individuated, whereas\ngenerally no element had crossed disparity relative to the image frame. We thus created our masks\nby distributing the polygons evenly from the maximum disparity (54.3 arcmin, i.e. 4.5% of 19.9\u00b0) to\n0 disparity. The furthest polygons were occluded by the nearest ones and their projected area varied\nalong the disparity gradient (from around 64 deg2 for the furthest polygons to around 2.6 deg2 for the\nnearest ones). Mask image generation and the stimulus presentation were carried out using Matlab\n(MathWorks, Inc., Natick, MA) and the PsychToolbox (Brainard, 1997; Pelli, 1997).\nProcedure\nThe trial event sequence is depicted in Figure 1. First, the fixation square alone was displayed for 1 s\nin order to obtain proper binocular alignment. Then, the target image was shown for a variable time\ndistractor pictures were shown in two successive 500-ms intervals. Participants indicated the target\ninterval with a key press. After the key press, if the answer was incorrect, the fixation point turned red\nThe images could be shown in one of two modalities: stereo and nonstereo (randomly choosing\nthe left- or right-eye view and displaying it to both eyes). In each trial, the same modality of presenta-\ntion was applied coherently to the target, mask, and choice images. Each combination of modality of\npresentation and image duration was tested in 48 trials. Twenty extra pictures were used for practice\ntrials at the beginning of the experiment. The whole experimental session lasted approximately 1 hr.\nData analysis\nThroughout the current paper, the proportions of correct responses were transformed to z-scores before\nbeing analyzed with ANOVAS and t-tests. The same transformation was applied to the data before\naggregating over observers and calculating confidence intervals. Confidence intervals were calculated\nas percentiles after bootstrapping the original sample 10,000 times. The aggregated values and confi-\ndence intervals were transformed back to proportion correct for plotting.\nFigure 1. Trial event sequence in Experiment 1 (the left-eye views of the images are shown). Observers indicated\nwhich of the scenes in the last two intervals had been presented at the beginning of the trial.\n125 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\nResults\nAccuracy in Experiment 1 (forest scenes) is depicted in Figure 2. Once again, performance increased\nas a function of exposure time from chance level at 13 ms (stereoscope setup) to quite good recognition\nat 80-ms exposure time. It is quite evident that stereo presentation does not constitute an advantage\ncompared to nonstereo presentation. In order to confirm this finding, we performed a repeated-measure\nANOVA with Image Duration and Presentation Modality as factors on the accuracy.\np\np\np\nDiscussion\nThe results of Experiment 1 seem to indicate that binocular disparity does not contribute to the fast\nrecognition of visual scenes, if anything, a nonsignificant trend for worse recognition of stereo pictures\nemerged, a result which is strikingly different from the observation of Liu and colleagues (2000), who\nfound that stereo produced a small (2.5%) enhancement in the recognition rate of faces with a viewing\ntime of 1.5 s and the observation of Valsecchi and Gegenfurtner (2012), who found a significant im-\nprovement in the long-term memorization rate when observers viewed the same forest pictures for 7 s.\nFor the sake of brevity, we only report here one experiment conducted using forest images,\nwhere, based on the finding by Valsecchi and Gegenfurtner (2012), we expected a stereo advantage\nfor scene recognition. Notice that in multiple additional experiments we replicated the current find-\nings with urban scenes, indoor scenes, and when the target and the distractor scenes were presented\nsimultaneously.\nIn the following two experiments, we test two possible accounts for this finding. In Experiment\n2, we test whether observers relied only on objects defined by contours and chromaticity in order to\nrecognize the whole scene. In Experiment 3, we test whether the access to binocular disparity was\nlimited by the fast presentation time.\n3 Experiment 2: artificial scenes\nThe aim of Experiment 2 was to test whether impoverishing the monocular cues to scene identity could\nincrease reliance on binocular disparity. For this purpose, we turned to artificial scenes, for which\nscene parameters could be better controlled. We created scenes exclusively populated by cubes sus-\npended in space, while manipulating the presence of color on the surface of the cubes. In this context,\nonce surface information is removed, the only aspect distinguishing scenes is their spatial layout, in\nprinciple maximizing the relevance of binocular disparity. Furthermore, we chose to simulate small\nscenes, entirely contained within the peri-personal space, where the relevance of disparity is supposed\nFigure 2. Recognition accuracy in Experiment 1 (forest scenes) as a function of image duration and modality of\npresentation. No stereo indicates binocular presentation without disparity. Error bars are between-subject 95%\nconfidence intervals of the mean. The points have been displaced horizontally to increase visibility. The presence\nof binocular disparity did not enhance recognition performance.\nto be maximal (McKee et al., 1990; Sheedy et al., 1986). Finally, in order to increase the opportu-\nnity for our observers to adjust to the presence of disparity, we tested stereo and 2D performance in\nseparate sessions. In this and the next experiments, we use rendered random arrangements of surfaces\nin depth as masks. These mimicked the properties of the scene images, including, besides disparity,\nresidual monocular cues to 3D structure such as illumination.\nMethods\nObservers\nSixteen observers (13 females, mean age: 24.9 years) participated in Experiment 2 (artificial scenes).\nAll observers were naive as to the aim of the study.\nStimuli\nStimuli were scenes containing 50 cubes rendered with OpenGL. The same stereoscope setup was\nused as in Experiment 1 (forest scenes), with a 55.5-cm viewing distance. In the simulated environ-\nvisual angle at fixation distance). Each cube was randomly rotated around its center. The center of one\ncube was always placed directly behind the fixation point (its center was displaced beyond the fixation\npoint by half the size of the cube, so that participants would be fixating near the surface of the central\ncube during stimulus presentation). The centers of the cubes were distributed within a cube centered\non the fixation point and whose side was 153.7 mm (15.7\u00b0), with the constraint that the minimum\ndistance between the centers of two cubes should be at least 25.6 mm (2.64\u00b0). The centers of the cubes\nwere not allowed to be nearer than 25.6 mm (2.64\u00b0) to the cyclopean line of sight in order to avoid the\nocclusion of the central cube.\nFigure 3. Trial event sequence in Experiment 2. Examples of grayscale (A) and colored (B) artificial scene pictures\nare shown. Observers indicated which of the two intervals contained the target scene which had been presented\nat the beginning of the trial. Cyclopean viewpoint versions of the images (as used for nonstereo presentation) are\nshown. Notice that the 2D layouts of the choice stimuli differ due to their perspective projection.\n127 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\nFor each scene, we created two versions by re-seeding only the z-coordinate of the cube centers\n(meaning the position along a line perpendicular to the display screen) and keeping the orientation of\neach cube constant, one to be used as target and the other to be used as the corresponding distractor.\nMoreover, for each scene we rendered three views, one from the left eye, one from the right eye and a\ncyclopean view (the latter to be used in 2D presentation).\nWe created 220 scenes with homogeneous gray cubes (as depicted in Figure 3) and 220 scenes\nwhere each cube was colored (the colors were randomly drawn from the whole RGB 32-bit color\nspace). The colors of the cubes (except for the central one) differed between the two versions of each\nscene, and thus constituted a cue for the recognition of the target. The scenes were rendered with both\nambient light and a point-light positioned 85.4 mm below the line of sight and 256.2 mm in front of\nthe screen.\nWe also created two sets of mask images (grayscale and colored). The mask images were rendered\nscenes containing 1,500 irregular quadrilateral surfaces. The surfaces were located within a rectangu-\nz-axis. The surfaces nearer to the observer had smaller size and a higher probability of being rotated\nfrom the fronto-parallel plane. This ensured that the mask completely occluded the space behind the\nconfiguration and allowed for surfaces with intermediate disparities to be visible from the observer's\npoint of view. As we did with the scene images, we created left-eye, right-eye, and cyclopean views\nfor masks.\nProcedure\nThe trial event sequence is depicted in Figure 3. The test image was shown for a variable interval (40,\nshown in two successive intervals in order to maximize display size while keeping size constant from\ntarget to choice presentation. Participants indicated the target interval with a key press. After the key\npress, if the answer was incorrect, the fixation point turned red for 1 s.\nThe images could be shown in two modalities (stereo or nonstereo). In each trial, the same modal-\nity of presentation was applied coherently to the target, mask and choice images. Each combination\nof Color and Presentation Modality was tested in a separate session including 220 trials. There were\n44 trials for each combination of Color, Presentation Modality and Image Duration. Ten practice tri-\nals using extra scenes were run at the beginning of each session. Each session lasted approximately\nResults\nAccuracy in Experiment 2 (artificial scenes) is depicted in Figure 4. Recognition performance was\nclose to chance level with 40-ms exposure time and did not reach 90% even when the scenes were\nexposed for 147 ms. In most of the observations, removing the color information from the surface of\nFigure 4. Proportion of correctly recognized pictures in Experiment 2 (artificial scenes) as a function of image\nduration. Stereo and No Stereo indicate binocular viewing with and without disparity, respectively. Error bars are\nbetween-subject 95% confidence intervals of the mean. The points have been displaced horizontally to increase\nvisibility. Color greatly increased recognition performance but performance did not improve when binocular\ndisparity was present.\nthe cubes reduced the correct recognition rate by about half (discounting the 50% chance rate). Once\nagain, there is no evident advantage for stereo viewing of the scene images over nonstereo viewing. A\n(stereo vs. no stereo) and Color (vs. grayscale) as factors was performed. The main effect of Color (F(1,\np\n\np\np\nsignificant. All remaining effects and interactions were not significant (main effect of presentation\np\np\np\np\nDiscussion\nSimilar to what we observed when observers were faced with real-world scenes in Experiment 1 (for-\nest scenes), and despite the limited availability of monocular cues, our observers still did not benefit\nfrom disparity while recognized artificial scenes in Experiment 2. Contrary to Experiment 1 (forest\nscenes), in this case the lack of advantage given by the addition of binocular disparity to the display\ncannot be due to the fact that the images depicted objects outside of the peri-personal space. This\nfinding is all the more striking because the stereo vs. no-stereo factor was fixed within a given session\nin Experiment 2.\nOur synthetic scenes were deprived of a number of monocular cues to scene identity, i.e. the\npresence of recognizable objects in the scene, their number and, depending on the condition, color.\nEvidently, the scene images still contained a number of monocular cues which the observers could\nuse in order to recover the spatial arrangement of the objects, including occlusions and illumination\nand objects' size gradients. Moreover, due to perspective, the different position of the cubes along the\ndepth axis produced different 2D projections between the target and distractor images. We think, how-\never, that any display lacking these residual monocular cues would not qualify as a scene, so we feel\nconfident in stating that binocular stereo does not produce appreciable improvements in the recogni-\ntion of scenes when fast encoding is required.\nAs stated above, the second possible explanation for our results could be that the fast presentation\nprevented the extraction of disparity structures from the display. In order to tackle this question in the\nnext experiment, we used dense RDS images, where disparity can be completely decoupled from any\n4 Experiment 3: RDS patterns\nThe rationale for Experiment 3 was to determine whether binocular disparity was available to observ-\ners in the \"recognition after masked presentation\" paradigm of Experiments 1 and 2. In other words,\ndid observers fail to use disparity because it was not available to them, or because this signal was\nmeasured by the visual system but did not participate when they did the task? As a proxy for scene\nrecognition, which does not allow for the decoupling of stereo from monocular cues, we used an RDS\npattern-recognition task. As a benchmark, we also investigated the effect of masked presentation on\nthe recognition of the corresponding luminance patterns of matched difficulty.\nWithin the same experiment, we also tested the performance on the recognition of patterns\ndefined by both disparity and luminance. This approach has not yet been applied to the domain\nof binocular disparity, given that the only study directly dealing with the effect of stereo on scene\nrecognition/object recognition (as opposed to spatial layout) lacked a stereo-only condition (Liu et\nal., 2000). However, researchers did investigate the combination of other visual dimensions in rec-\nognition tasks, such as spatial frequency (Olds & Engel, 1998), color and luminance (Syrkin & Gur,\n1997), orientation, contrast, and spatial frequency (Thomas & Olzak, 1990). Furthermore, the ques-\ntion of how disparity and luminance contribute to pattern recognition is formally related to the ques-\ntion of how different sources of information contribute to the detection of threshold stimuli, and\nspecifically of whether subthreshold summation takes place (e.g. Meinhardt, Persike, Mesenholl, &\nHagemann, 2006), and to the question of how changes in individual dimensions contribute to the\nappearance of complex stimuli (Landy et al., 1995; To, Baddeley, Troscianko, & Tolhurst, 2011; To,\nLovell, Troscianko, & Tolhurst, 2008). In all of these contexts, it is possible to compare the perfor-\nmance which is observed when combined stimuli are presented with the performance which can be\npredicted based on the results of the single-modality stimuli assuming independent processing of\nthe two dimensions and linear combination of the responses. Following Dosher, Sperling, and Wurst\n129 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\n(1986) and Backus (2009), we decided to implement a probit model in which the strength of each\nsignal (Stereo and Luminance) is derived from the recognition accuracy for the stimuli in the single\nmodality conditions. The situation in which both signals are present can be modeled assuming that\neach signal (cue) is analyzed by an independent Bernoulli Expert (Backus, 2009) that contributes\na subjective reliability to the observer's overall belief about a binary property of the world. Within\nthis context, the predicted combined strength of the two signals is simply given by the sum of the\nindividual strengths.\nMethods\nObservers\nTwelve observers (nine females, mean age: 25.5 years) participated in Experiment 3 (RDS patterns).\nOne observer had already participated in Experiment 2 (artificial scenes). All observers were naive as\nto the aim of the study.\nStimuli\nThe stimuli were configurations of eight squares defined by disparity, luminance or both (Figure 5).\nBoth the squares and the background were textured by a grayscale white noise pattern composed of\nlarge \"display pixels\" that ranged in luminance from 0 to 96 cd/m2. The background had a size of\nplay pixels (area 256 display pixels) and subtended 1.96\u00b0. The centers of the squares on the cardinal\naxes were located 2.81\u00b0 from fixation.\nIn each configuration, when the pattern was defined in luminance and/or binocular disparity, the\ncontrast polarity of the squares and the sign of the disparity of the squares were evenly distributed,\ni.e. four squares were darker than the background and four were brighter, four squares had crossed\ndisparity and four had uncrossed disparity, i.e. they appeared in front of and behind the background,\nrespectively. When present, the disparity was 7.3 arcmin. The visibility of the configurations was\nmanipulated by varying the contrast or the disparity coherence (from 0 to 256 pixels) of the squares.\nThe same mask images as in the Grayscale condition of Experiment 2 (artificial scenes) were used.\nProcedure\nThe trial event sequence is depicted in Figure 5. The test image was shown for a variable interval (40,\nshown in two successive intervals. The target image and the distractor image differed only in the polar-\nity of four out of the eight squares; the reason for this was to push observers to process the configura-\ntions in a global fashion. Participants indicated the target interval with a key press. Between trials, a\n1-s interval where the fixation point turned red was inserted if the answer was incorrect, otherwise the\nnext trial started immediately after key press.\nThe experiment was composed of four sessions. The first was a pre-test session whose aim was\nto titrate the visibility of the stimuli for the subsequent three experimental sessions. The pre-test ses-\nFigure 5. Trial event sequence in the Luminance condition of Experiments 3 (RDS patterns). Observers indicated\nwhich of the two intervals contained the target pattern which had been presented at the beginning of the trial.\nsion was preceded by a training session of 35 trials in which the image exposure duration decreased\nlinearly from 2 s to 120 ms. During the training and pre-test sessions only single-modality trials were\npresented, i.e. the stimuli were defined either by disparity or by luminance, and the modalities were\nrandomly interleaved. The luminance contrast (i.e. the difference between the average luminance\nof each square and the average luminance of the background) and the disparity coherence (i.e. the\nproportion of pixels within each square displaced between the left and right eye images) were cho-\nsen randomly from a wide range of values in the training trials. In the subsequent pre-test session\ntheir values were chosen in order to target the 75% visibility range for each participant and stimulus\nmodality. The pre-test session consisted of 250 trials in which the stimulus exposure time was fixed\nat 120 ms. In the first 30 trials of each modality and in 25% of the subsequent trials, the luminance\ncontrast and disparity coherence values were drawn from two Gaussian distributions ( = 0 cd/m2, \n= 37.2 cd/m2 for luminance difference;  = 0%,  = 39% for disparity coherence) derived from pilot\ndata, trimming values below 0 for both modalities and values above 100% for coherence. After the\n30th trial, in 75% of the cases the participants' responses from the current modality were fitted with a\ncumulative Gaussian model using the psignifit toolbox version 2.5.41 for Matlab (see http://bootstrap-\nsoftware.org/psignifit/), which implements the maximum likelihood method described by Wichmann\nand Hill (2001). The value for the subsequent trial of the same modality was chosen from the Gauss-\nian distribution with the parameter values obtained from the fit (trimming  values below 1.42 cd/m2\nand 1.9%), so as to adapt to the observer's performance. The same fitting procedure was used at the\nend of the training session in order to calculate the 75% accuracy thresholds for each participant and\nmodality, which were then used for the subsequent sessions. In the remaining 25% of the trials, the\nstimulus values were drawn from the original broad distributions so as to allow further exploration of\nthe stimulus space. Three participants failed to demonstrate 75% accuracy in at least one modality and\ndid not continue with the experiment. The first session lasted approximately 75 min.\nEach of the three subsequent experimental sessions consisted of 270 trials and lasted approxi-\nmately 75 min. Three modality conditions were tested: Disparity, Luminance, and Combined. In the\nDisparity and Luminance conditions, the stimuli were defined in only one modality, as in the pre-test\nsession. In the Combined condition, both Disparity and Luminance defined the difference between\nthe target and the distractor. The polarity of the squares in the two modalities was matched coherently\nthrough each session. Pooling all three sessions, observers were exposed to 54 trials in each combina-\ntion of Modality and Image Duration.\nResults\nRecognition accuracy is reported in Figure 6. The rate of correct recognition increases with a remark-\nably similar slope as a function of exposure time for both stereo- and luminance-defined patterns. In\nboth cases, performance is at chance level with 40-ms exposure and increases to around 80% correct\nFigure 6. Observed and predicted proportion of correctly recognized trials in Experiment 3 (RDS patterns). Error\nbars are between-subject 95% confidence intervals of the mean. The points have been displaced horizontally\nto increase visibility. Recognition performance was equally affected by brief image presentation for binocular\ndisparity and luminance patterns. Performance increased in combined trials to the level predicted by the strength\nsummation model for longer image durations and beyond the predicted level for the shorter image durations.\n131 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\nrecognition if the image is presented for 147 ms. Combining the two dimensions increases the correct\nrecognition rate by around 15%, independent of exposure time.\nAccuracy results were analyzed with a two-way repeated-measure ANOVA with Image Duration\np\np\n\np\n2 = 0.071). Planned comparisons indicate that the Combined trials yielded higher performance than\np\n\np\nThis result indicates that the information from both modalities is pooled when participants rec-\nognize patterns which differ in both luminance and disparity. Nonetheless, one can still ask whether\nthe performance in combined trials is higher or lower than the prediction from linear summation. One\nconvenient way of handling 2AFC accuracies in order to compute predictions is by converting them to\nSi\nwhere Ai\nis the accuracy in trials where a given modality is presented in isolation and -1 is the inverse\nof the standard normal cumulative probability density function. Similarly, Equation (2) can be used to\nconvert strength scores to accuracy:1\nAi\n=  (Si\nEquation (1) maps 100% correct performance to infinite strength. This inflates the variability of\nstrength estimates when accuracy values are high. We thus postulated an error rate of 1/2n, where n is\nthe number of trials, in all design cells where all answers were correct.\nWhen both single-modality recognition rates are above chance, the predicted sensitivity for the\ncombined trials, assuming linear summation is simply given by\nSC\nwhere SD\nis the strength of the disparity signal and SL\nis the strength of the Luminance signal. We per-\nform our calculations assuming that the two modalities contribute the same strength to the recogniz-\nability of the combined stimulus as they did when tested in isolation.\nThe prediction is plotted along with the results in Figure 6. Notice that our model handles negative\nsensitivities correctly and is unbiased. Indeed, at a 40-ms exposure, the prediction overlaps with the\ntwo single-modality performances which are recognized at chance level. Models which cannot handle\nnegative sensitivities such as the one used by Meinhardt and colleagues (2006) would produce a posi-\ntively biased prediction in this situation. We performed a repeated-measure ANOVA with Prediction\nobserved performance with the one predicted by the cue strength summation model. The difference\np\np\n= 0.48). Obviously, the analysis also yielded a significant main effect of Image Duration (F(4, 32) =\np\n2 = 0.84). The interaction seems to be driven by the fact that the observed per-\nformance was higher than the predicted one at short image durations, although post-hoc paired t-tests\nwith Bonferroni correction failed to show a significant difference between observed and predicted\n1 Notice that the strength formulas differ from the d calculation in 2AFC tasks (as used in Meinhardt et al., 2006) only by the\n2 factor. The d and strength values are thus linearly related and our prediction model could be applied to either score yielding\nthe same expected accuracies.\nFurthermore, the linear summation model we apply corresponds to a cue summation with a Minkowski exponent equal to 1\n(also see Macmillan & Creelman, 2005; e.g. To et al., 2011). Other models such as Euclidean summation (Minkowski exponent\nequal to 2) or MAX rule summation (infinite Minkowski exponent) predict a lesser increase of performance when both stimuli\nare presented at the same time. Our model thus constitutes a conservative test for superadditivity.\nDiscussion\nA few results emerged from Experiment 3 (RDS patterns). First, the fact that the recognition of dis-\nparity and luminance patterns was affected in a remarkably similar fashion by our masking procedure\nindicates that the lack of any performance improvement in the presence of binocular disparity with\nreal-world and artificial scenes in Experiments 1 and 2 was not due to a failure in ability to measure\ndisparity, such as might have been caused by general \"slowness\" of stereo. Indeed, partially degraded\nstereo patterns could be recognized with above chance performance at 67-ms exposure, and with\nan accuracy superior to 75% at 93-ms exposure, whereas in Experiment 2 we failed to observe any\nimprovement of artificial scene recognition even with 147-ms presentation. Second, observers could\nrecognize combined patterns better than they could recognize either single-modality pattern.\nThe comparison between the observers' responses in combined trials and the predictions by our\nprobability summation model showed superadditivity at short presentation times and subadditivity\nat longer presentation times. One possible interpretation of the superadditivity could be that limited\nchanges in luminance within a patch can enhance its segmentation and its binocular fusion.\n5 General discussion\nIn three experiments, we investigated how human observers make use of binocular disparity when\nthey had to recognize briefly presented images of natural scenes (Experiment 1), images of artificial\nscenes (Experiment 2), and patterns within random-dot stereograms (Experiment 3).\nThe results of Experiments 1 and 2 indicate that binocular disparity does not contribute in any sig-\nnificant way to the recognition of briefly presented scenes, even when monocular cues are artificially\nreduced and recognition is largely impaired. The results of Experiments 3 show that, in principle, if\nobservers are forced to process binocular disparity by the use of displays completely deprived of any\nmonocular cues, they can recognize patterns defined by disparity presented briefly.\nOverall, we suggest that human observers strategically ignore the information provided by bin-\nocular disparity as soon as monocular cues provide enough information to support even a poor rec-\nognition of rapidly encoded scenes. This is true regardless of whether the scenes represent objects\noutside of or within the peri-personal space and even when the amount of monocular cues is reduced\nto a minimum consistent with still being a \"scene.\"\nWe believe that two factors contribute to the primacy of luminance in scene recognition tasks with\nbrief presentation. First, an encoding capacity limit is consistent with this result. When faced with rela-\ntively simple patterns in Experiment 3 (RDS patterns), our observers combined the information com-\ning from luminance and disparity, but when faced with the complex patterns typical of scenes, such as\nin Experiments 1 (forest scenes) and 2 (artificial scenes), they did not. Using disparity to recognize a\ncomplex scene may require cognitive resources (attention and/or memory) that were allocated instead\nto the processing of luminance and color. Notice that the fact that Valsecchi and Gegenfurtner (2012)\nfound improved recognition from long-term memory for stereo pictures of the same forest scenes rules\nout the hypothesis that a general storage capacity limits the using of disparity in the encoding phase.\nThe factor limiting the contribution of binocular disparity to scene recognition must be related to the\nrate at which visual information can be processed before encoding is terminated by the presentation\nof the mask.\nThe results from Experiments 1 and 2, using real-world and artificial scenes, have implications\nfor our understanding of the scene recognition process itself. The first straightforward conclusion is\nthat even if human observers use binocular disparity to segment objects in briefly presented scenes,\nthe results from that process are not used for scene recognition. Stereo differs from chromaticity in this\nregard. Gegenfurtner and Rieger (2000) found enhanced fast encoding for colored natural images and\nattributed this advantage to the use of color-defined edges, which occur frequently in natural scenes\n(Hansen & Gegenfurtner, 2009), for segmentation. Within this framework, the present results indicate\nthat, despite our previous report that binocular disparity can speed the detection of isolated targets\nagainst a zero-disparity background (Caziot et al., 2011), disparity-based segmentation does not con-\ntribute when the visual system must recognize a briefly presented complex visual scene. Consider again\nour forest scene experiment. Forest scenes lack the sharp luminance boundaries that characterize man-\nmade objects and our artificial scene objects. Any factor enhancing the segmentation of the single ele-\nments should have an immediate impact on performance in this case, but stereo did not have that effect.\nSecond, our results indicate that human observers do not base the recognition of briefly presented\nscenes on the binocular disparity associated with elements or surfaces. The presence of salient objects\n133 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\nmarked by a strong disparity discontinuity, such as a near tree in front of further-off trees, should be\nable to support the recognition of the target. Although the encoding of salient objects could contribute\nto the advantage of color in previous work, such as that of Gegenfurtner and Rieger (2000), we found\nno evidence that stereo labels objects as salient for the purpose of recognizing whole scenes.\nThird, given that binocular disparity can enhance the perception of the depth arrangement of\nobjects in scenes (McKee & Taylor, 2010), our results suggest that humans do not encode a detailed\nmodel of the spatial structure of rapidly presented scenes for their short-term recognition. Rapid\nshort-term scene recognition has not been investigated extensively, since most of the studies on rapid\nscene perception have dealt with scene categorization (e.g. Greene & Oliva, 2009) or with object/\nanimal detection within scenes (e.g. Drewes, Trommershauser, & Gegenfurtner, 2011; Rousselet,\nFabre-Thorpe, & Thorpe, 2002). For these tasks, observers use low-level properties of the image (e.g.\nCrouzet & Serre, 2011) not a detailed representation of the 3D structure. It is thus quite possible that\nthe representation used for the fast encoding of images in our experiments does not go beyond their\n2D properties.\nOur finding that the immediate recognition of briefly presented scenes is not enhanced by binocu-\nlar disparity is in contrast with the finding by Valsecchi and Gegenfurtner (2012) that the same stereo\npictures could be better recognized from long-term memory. Evidently, the tasks induced two quali-\ntatively different encoding strategies. In the current study, where performance was largely determined\nby the encoding speed, observers relied on the straightforward matching of the 2D layout of the image,\nor possibly of a portion of it. Conversely, in the study by Valsecchi and Gegenfurtner (2012), where the\nmain factor limiting long-term memory for scenes was the interference from other memorized items,\nobservers did rely on binocular disparity in order to maximize performance. Anyway, it was quite evi-\ndent from the result of Valsecchi and Gegenfurtner (2012) that reliance on binocular stereo was limited\nto the case where scenes had to be encoded visually. Binocular stereo produced no advantage when\nscenes could be encoded semantically, for instance based on the artifacts they contained.\nOur findings with RDS displays are broadly consistent with studies in which binocular disparity\nwas processed for presentation times as fast as 150 ms. Our observers recognized patterns defined\nby disparity (7.3 arcmin crossed or uncrossed) with 120-ms masked exposure. This is compatible in\nparticular with the better-than-chance discrimination of 6-arcmin disparity between adjacent segments\nreported by Foley and Tyler (1976), and with the lack of any considerable increase in stereo thresholds\nfound that both simple and relative disparity judgments were above chance with 120-ms masked pres-\nentations. Our findings extend the previous results by showing that even masks containing similar dis-\nparity as the target do not completely disrupt the extraction of stereo patterns. Our experimental ques-\ntion originated in the study of natural images. Object boundaries in natural images are marked both\nby luminance and stereo, so we used superimposed stereo and luminance patterns in our experiments.\nFuture research could address how human observers combine binocular disparity and luminance when\nthe patterns are not co-localized, a manipulation that influences how signals from different visual\ndimensions are combined (e.g. Krummenacher, Muller, & Heller, 2002).\nWe found that observers do not take into account stereo when they recognize briefly presented\npictures of scenes if they have to choose between an identical presentation of the same picture and a\ndistractor. This finding does not necessarily extend to other tasks. The RDS recognition task results\nshow that stereo can in fact be processed. We chose RDS recognition when we tested stereo reliance in\na different task because it cannot be performed monocularly. Other tasks might also benefit from ste-\nreo. For example, viewpoint-independent recognition of objects is facilitated by binocular stereo (Lee\n& Saunders, 2011). It is possible that this facilitation intervenes also in the case of the more complex\npictures depicting scenes, even if little time is available to encode them.\nIn conclusion, even though binocular disparity is an omnipresent quality of our visual experience,\nand even though it can be extracted quickly from brief visual displays, human observers do not make\nuse of it to rapidly encode the various parts of a scene, possibly because it requires additional resources\nto do so. We suggest that the recognition of briefly presented scene images is largely mediated by the\ncomparison of their 2D structure.\nReferences\nAdams, W. J., & Mamassian, P. (2004). Bayesian combination of ambiguous shape cues. Journal of Vision, 4,\nAdelson, E. H., & Bergen, J. R. (1991). The plenoptic function and the elements of early vision. In M. S. Landy\n& J. A. Movshon (Eds.), Computational models of visual processing (pp. 3\u00ad20). Cambridge, MA: MIT\nPress.\nAllison, R. S., Gillam, B. J., & Vecellio, E. (2009). Binocular depth discrimination and estimation beyond\nBackus, B. T. (2009). The mixture of Bernoulli Experts: A theory to quantify reliance on cues in dichotomous\nBackus, B. T., & Banks, M. S. (1999). Estimator reliability and distance scaling in stereoscopic slant perception.\nBackus, B. T., Banks, M. S., van Ee, R., Crowell, J. A., & Crowell, D. (1999). Horizontal and vertical disparity,\nBanks, M. S., & Backus, B. T. (1998). Extra-retinal and perspective cues cause the small range of the induced\nBurke, D. (2005). Combining disparate views of objects: Viewpoint costs are reduced by stereopsis. Visual\nBurke, D., Taubert, J., & Higman, T. (2007). Are face representations viewpoint dependent? A stereo advantage\nButler, T. W., & Westheimer, G. (1978). Interference with stereoscopic acuity: Spatial, temporal, and disparity\nCaziot, B., Valsecchi, M., Gegenfurtner, K. R., & Backus, B. T. (2011). Role of binocular vision during image\nCrouzet, S. M., & Serre, T. (2011). What are the visual features underlying rapid object recognition? Frontiers\nDoorschot, P. C. A., Kappers, A. M. L., & Koenderink, J. J. (2001). The combined influence of binocular\nDosher, B. A., Sperling, G., & Wurst, S. A. (1986). Tradeoffs between stereopsis and proximity luminance\nDrewes, J., Trommershauser, J., & Gegenfurtner, K. R. (2011). Parallel visual search and rapid animal detection\nEdelman, S., & B\u00fclthoff, H. H. (1992). Orientation dependence in the recognition of familiar and novel views of\nFoley, J. M., & Tyler, C. W. (1976). Effect of stimulus-duration on stereo and vernier displacement thresholds.\nGegenfurtner, K. R., & Rieger, J. (2000). Sensory and cognitive contributions of color to the recognition of\nGirshick, A. R., & Banks, M. S. (2009). Probabilistic combination of slant information: Weighted averaging and\nGreene, M. R., & Oliva, A. (2009). Recognition of natural scenes from global properties: Seeing the forest\nHansen, T., & Gegenfurtner, K. F. (2009). Independence of color and luminance edges in natural scenes. Visual\nHarwerth, R. S., Fredenburg, P. M., & Smith, E. L. (2003). Temporal integration for stereoscopic vision. Vision\nHillis, J. M., Ernst, M. O., Banks, M. S., & Landy, M. S. (2002). Combining sensory information: Mandatory\nHillis, J. M., Watt, S. J., Landy, M. S., & Banks, M. S. (2004). Slant from texture and disparity cues: Optimal\nIchikawa, M., Saida, S., Osa, A., & Munechika, K. (2003). Integration of binocular disparity and monocular\nJulesz, B. (1971). Foundations of cyclopean perception. Chicago, IL: University of Chicago Press.\nKnill, D. C., & Saunders, J. A. (2003). Do humans optimally integrate stereo and texture information for\nKrummenacher, J., Muller, H. J., & Heller, D. (2002). Visual search for dimensionally redundant pop-out\ntargets: Parallel-coactive processing of dimensions is location specific. Journal of Experimental\n135 Valsecchi M, Caziot B, Backus B T, Gegenfurtner K R\nLandy, M. S., Maloney, L. T., Johnston, E. B., & Young, M. (1995). Measurement and modeling of depth cue\nLee, Y. L., & Saunders, J. A. (2011). Stereo improves 3D shape discrimination even when rich monocular shape\nLehmkuhle, S., & Fox, R. (1980). Effect of depth separation on metacontrast masking. Journal of Experimental\nLiu, C. H., Collin, C. A., & Chaudhuri, A. (2000). Does face recognition rely on encoding of 3-D surface?\nLong, N., & Over, R. (1974). Stereospatial masking and aftereffect with normal and transformed random-dot\nMacmillan, N. A., & Creelman, C. D. (2005). Detection theory: A user's guide (2nd ed.). Mahwah, NJ: Erlbaum.\nMcKee, S. P., Levi, D. M., & Bowne, S. F. (1990). The imprecision of stereopsis. Vision Research, 30,\nMcKee, S. P., & Taylor, D. G. (2010). The precision of binocular and monocular depth judgments in natural\nMcKee, S. P., Watamaniuk, S. N. J., Harris, J. M., Smallman, H. S., & Taylor, D. G. (1997). Is stereopsis\nMeese, T. S., & Holmes, D. J. (2004). Performance data indicate summation for pictorial depth-cues in slanted\nMeinhardt, G., Persike, M., Mesenholl, B., & Hagemann, C. (2006). Cue combination in a combined feature\nOlds, E. S., & Engel, S. A. (1998). Linearity across spatial frequency in object recognition. Vision Research, 38,\nPelli, D. G. (1997). The VideoToolbox software for visual psychophysics: Transforming numbers into movies.\nRitter, M. (1980). Perception of depth: Different processing times for simple and relative positional disparity.\nRousselet, G. A., Fabre-Thorpe, M., & Thorpe, S. J. (2002). Parallel processing in high-level categorization of\nSheedy, J. E., Bailey, I. L., Buri, M., & Bass, E. (1986). Binocular Vs monocular task-performance. American\nSyrkin, G., & Gur, M. (1997). Colour and luminance interact to improve pattern recognition. Perception, 26,\nThomas, J. P., & Olzak, L. A. (1990). Cue summation in spatial discriminations. Vision Research, 30,\nTo, M. P. S., Baddeley, R. J., Troscianko, T., & Tolhurst, D. J. (2011). A general rule for sensory cue summation:\nevidence from photographic, musical, phonetic and cross-modal stimuli. Proceedings of the Royal\nTo, M. P. S., Lovell, P. G., Troscianko, T., & Tolhurst, D. J. (2008). Summation of perceptual cues in natural\nTyler, C. W., & Kontsevich, L. L. (2005). The structure of stereoscopic masking: Position, disparity, and size\nValsecchi, M., & Gegenfurtner, K. R. (2012). On the contribution of binocular disparity to the long-term\nWardle, S. G., Cass, J., Brooks, K. R., & Alais, D. (2010). Breaking camouflage: Binocular disparity reduces\nWestheimer, G. (2011). Three-dimensional displays and stereo vision. Proceedings of the Royal Society\nWichmann, F. A., & Hill, N. J. (2001). The psychometric function: I. Fitting, sampling, and goodness of fit.\nCopyright 2013 M Valsecchi, B Caziot, B T Backus, K R Gegenfurtner\nPublished under a Creative Commons Licence a Pion publication\nBenjamin T. Backus studied mathematics at Swarthmore College (BA), vi-\nsion science at UC Berkeley (PhD), and neuroscience at Stanford University\n(postdoctoral). He is currently Empire Innovation Associate Professor in the\nGraduate Center for Vision Research at SUNY College of Optometry. His\ninterests include binocular vision and stereopsis, perceptual learning, neural\nplasticity, amblyopia, and strabismus.\nKarl R. Gegenfurtner studied psychology in Regensburg (Germany) and\nthen did a PhD in Experimental Psychology at New York University. After\nspending time as a PostDoc in New York and T\u00fcbingen, he became Professor\nof Psychology in Magdeburg. Since 2001, he has been at Giessen University\n(http://www.allpsych.uni-giessen.de/karl/).\nBaptiste Caziot studied psychology at the Universit\u00e9 Paris Descartes and\nelectronics at the CNED. He then received a research master's degree in\ncognitive science jointly delivered by the ENS, EHESS, and Universit\u00e9 Paris\nDescartes. He is now a PhD student at the Graduate Center for Vision Re-\nsearch, SUNY College of Optometry.\nMatteo Valsecchi studied Psychology at the Vita-Salute San Raffaele\nUniversity in Milan (Italy). He subsequently got a PhD in Cognitive Sciences\nand Education from the University of Trento (Italy). He is currently a Humboldt\npostdoctoral fellow at the Department of General Psychology of the Justus-\nLiebig University of Giessen (Germany)."
}