{
    "abstract": "Abstract\nIn a recent issue of Political Analysis, Grant and Lebo authored two articles that forcefully argue against the use of the\ngeneral error correction model (GECM) in nearly all time series applications of political data. We reconsider Grant and\nLebo's simulation results based on five common time series data scenarios. We show that Grant and Lebo's simulations\n(as well as our own additional simulations) suggest the GECM performs quite well across these five data scenarios\ncommon in political science. The evidence shows that the problems Grant and Lebo highlight are almost exclusively the\nresult of either incorrect applications of the GECM or the incorrect interpretation of results. Based on the prevailing\nevidence, we contend the GECM will often be a suitable model choice if implemented properly, and we offer practical\nadvice on its use in applied settings.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nIn a recent issue of Political Analysis, Taylor Grant and\nMatthew Lebo author the lead and concluding articles of a\nsymposium on time series analysis. These two articles\nargue forcefully against the use of the general error correc-\ntion model (GECM). In their lead article, Grant and Lebo\ndeclare: \"we recommend the GECM in only one rare situa-\ntion: when all of the variables are strictly unit-root series,\nYt\nis unbounded, Yt\nand Xt\nare cointegrated, and the\nMacKinnon critical values are used... A careful look at the\napplied literature in political science will not find any\nexamples that meet all those criteria\" (p.27). They reiterate\nthis point in their concluding article, stating: \"we remain\nskeptical that the GECM is a reliable model except in the\nvery rare case where one has unbounded unit-root variables\nthat are cointegrated with each other\" (p.80). Given the\npopularity of the GECM for time series analysis (e.g. Beck\ninsistence that the GECM is inappropriate for political sci-\nence applications would seem to hold major implications\nfor time series practitioners.1\nGrant and Lebo identify two primary concerns with the\nGECM. First, when time series are stationary, the GECM\ncannot be used as a test of cointegration. This is a useful\nand often under-appreciated point.2 However, if this were\nGrant and Lebo's only concern, their analysis would not\nfundamentally alter the conclusions of past research and\ncould be easily dealt with in future studies by not using the\nGECM to test for cointegration with stationary time series.\nGrant and Lebo's second concern is much more troubling.\nThey argue that across most (and perhaps all) political sci-\nence time series, the GECM will produce \"an alarming rate\nof Type I errors\" (p.4). This threat of spurious findings is\nDon't jettison the general error correction\nmodel just yet: A practical guide to\navoiding spurious regression with\nthe GECM\nPeter K. Enns1, Nathan J. Kelly2, Takaaki Masaki3 and\nPatrick C. Wohlfarth4\n Keywords\ntime series, ecm, error correction model, spurious regression, methodology, Monte Carlo simulations\n1Cornell University, USA\n2University of Tennessee, USA\n3College of William & Mary, USA\n4University of Maryland, College Park, USA\nCorresponding author:\nNathan J. Kelly, University of Tennessee, 1001 McClung Tower,\nEmail: nathan.j.kelly@gmail.com\nResearch Article\n2 Research and Politics \nthe primary reason Grant and Lebo advocate abandoning\nthe GECM.\nIf the GECM regularly produces spurious results, schol-\nars would indeed be well-advised to abandon this approach.\nHowever, the problems Grant and Lebo highlight follow\nalmost entirely from either incorrect applications of the\nGECM or incorrect interpretation of results. Indeed, a care-\nful examination of Grant and Lebo's results, as well as the\nother contributions to the Political Analysis symposium,\nshows the GECM performs quite well across a variety of\ncommon data scenarios in political science. In this article,\nwe examine five of the scenarios that Grant and Lebo con-\nsider and we find that for four of the data scenarios, if\napplied correctly the GECM can be estimated without con-\ncern for spurious relationships. With the fifth data type\n(fractionally integrated series), the GECM sometimes\noffers a suitable approach.3 Our analysis pays particularly\nclose attention to the cases of bounded unit roots and near-\nintegrated time series. We devote extra attention to these\ntypes of time series because they are common in political\nscience and because none of the other symposium articles\nreconsider Grant and Lebo's claims about these types of\ndata.4 We show that when applied correctly, there is no\ninherent problem when using the GECM with bounded unit\nroots or near-integrated data.\nAlthough our conclusions differ greatly from Grant and\nLebo's recommendations, we do not expect our findings to\nbe controversial. Most of our evidence comes directly from\nGrant and Lebo's own simulations. We also support our\nclaims with additional simulation results. Although our\nfindings are straightforward, our conclusions about the\nGECM are important for multiple reasons. First, a correct\nunderstanding of the GECM holds implications for how we\nunderstand existing research. Grant and Lebo identified\nfive prominent articles and in each case they critiqued the\nauthors'use of the GECM. Grant and Lebo also pointed out\nthat none of the other symposium articles provide \"a\ndefense of any GECM results published by a political sci-\nentist\" (p.70). Our findings show that Grant and Lebo were\ntoo quick to criticize these researchers' use of the GECM.\nIndeed, we reconsider two of the articles that Grant and\nand we demonstrate that a correct understanding of the\nGECM indicates that the methods and findings of these two\narticles are sound.\nUnderstanding the GECM also holds implications for\nfuture research. For time series analysis, Grant and Lebo\nrecommend fractional integration (FI) methods. Although\nFI methods are certainly an important statistical approach\n(e.g. Box-Steffensmeier et al., 1998; Box-Steffensmeier\ndisagreement exists regarding their utility for political sci-\nence data (Box-Steffensmeier and Helgason, 2016).5 Given\nthe debate about the utility of FI methods (especially with\nshort time series), it is important for researchers to know\nwhen the GECM avoids the errors that Grant and Lebo\nascribe to it. Until alternate methods are shown to perform\nbetter, based on our findings we recommend that research-\ners use the GECM for bounded unit roots (when statistical\ntests indicate the dependent variable contains a unit root\nand cointegration is present) and with near-integrated data\n(again, cointegration must be established when statistical\ntests suggest a unit root).6 We also remind readers that the\nGECM is appropriate with FI data in some contexts (Esarey,\ntime series (although we point out that the mathematically\nequivalent autodistributed lag model (ADL) is less likely to\nproduce errors of researcher interpretation with stationary\ndata).\nWe conclude with a detailed summary of our recommen-\ndations for time series practitioners, highlighting where we\nagree with Grant and Lebo and where our recommenda-\ntions differ. The conclusion also discusses avenues for\nfuture research. These include studying the performance of\nthe GECM with other types of time series. Wlezien (2000),\nfor example, discusses \"combined\" time series (which con-\ntain both integrated and stationary processes) and he shows\nthey can be modeled with a GECM. Future research also\nneeds to continue to evaluate the performance of FI tech-\nniques. Of particular interest is resolving the debate\nregarding how long a time series must be to reliably esti-\nmate the FI parameter d and demonstrating whether Grant\nand Lebo's proposed fractional error correction model\n(FECM) is able to identify true relationships in short time\nseries.\nReconsidering five of Grant and Lebo's\ndata scenarios\nIn this section, we revisit Grant and Lebo's first five data\nscenarios. We find that across all five scenarios, the\nGECM typically avoids spurious relationships. We\nexplain why our results differ from Grant and Lebo's\nconclusions and highlight when we agree with their\nrecommendations.\nCase 1: The dependent variable contains a unit\nGrant and Lebo begin with a very important, and often for-\ngotten, point. They show that if the dependent and inde-\npendent variables contain a unit root, cointegration must be\nestablished prior to interpreting the results of a GECM (see\nalso Enns et al., 2014).7 Grant and Lebo explain, \"Without\ncointegration, however, the model is unbalanced and the\npractitioner should set aside the estimates and choose a\ndifferent specification\" (p.7).8 Grant and Lebo also correctly\nemphasize that when Yt\ncontains a unit root and the GECM\nis used to test for cointegration, \"non-standard critical\nEnns et al. 3\nvalues must be used (Ericsson and MacKinnon, 2002)\nto evaluate cointegration\" (p.8). Specifically, the t -statistic\nassociated with 1\n(i.e. the error correction rate) in the GECM\n \nt t t t t\n= ,\n    \n+ + + +\n- -\nmust be less than the corresponding critical value in\nEricsson and MacKinnon (2002). Grant and Lebo's Table 2\n(second row) confirms that when an analyst uses the appro-\npriate critical values, this cointegration test performs well\neven when T = 60 and regardless of the number of predic-\ntors specified in the model. Since Grant and Lebo only find\n(incorrect) evidence of cointegration about 5 per cent of the\ntime, if researchers follow Grant and Lebo's recommenda-\ntion to set aside estimates when there is no evidence of\ncointegration, incorrect evidence of a long-run relationship\n(i.e. \nin Equation (1)) could never exceed 5 per cent with\nintegrated data (because 95% of potential analyses have\nbeen set aside).\nAt first glance, the bottom row of Grant and Lebo's\nTable 2 might appear to contradict this statement, because\nthese results show that Grant and Lebo incorrectly rejected\nthe null hypothesis that \n= 0 after testing for cointegra-\ntion between 14 and 47 per cent of the time. However, these\nresults are based on cointegration tests with incorrect criti-\ncal values.9 Thus, the bottom row of Grant and Lebo's\nTable 2 should be read as evidence of the importance of\nusing the correct MacKinnon critical values when testing\nfor cointegration, not evidence of spurious relationships\nwith the GECM. Their Table 2 also shows that although \nprovides accurate tests of cointegration (when researchers\nuse the correct critical values), \n(which typically ranges\nbetween 0 and -1.0 ) is biased downward (i.e. more nega-\ntive). Typically, this bias will not pose a problem for\nresearchers. First, the bias does not affect the estimates of\n\nor \n(because these estimated short- and long-run\neffects do not depend on \n). Second, the magnitude of the\nbias is small.10 Third, the bias influences the estimate of the\ntotal effect of Xt\non Yt\n(i.e. the long-run multiplier (LRM))\nin a conservative direction. The conservative bias emerges\nbecause a smaller (more negative) \nwill decrease the\nestimated LRM (because the LRM equals\n\n\n), leading\nresearchers to conclude a smaller total effect of Xt\non Yt\n.\nBecause \nindicates how quickly the total effect of Xt\non\nYt\noccurs through future time periods, researchers must\nalso be aware that if \nis biased downward, the true rate of\nerror correction likely takes longer than \nimplies. When\ndiscussing the LRM or the rate of error correction, research-\ners must acknowledge how the bias in \nthat Grant and\nLebo identified could affect their estimates. However, as\nlong as researchers acknowledge this bias in \n, we agree\nwith Grant and Lebo that the GECM can be used with inte-\ngrated data that are cointegrated.\nCase 2: The dependent variable is a bounded\nunit root\nGrant and Lebo's second case considers bounded unit roots,\nwhich are time series that \"can exhibit the perfect-memory of\nintegrated data\" but are bound between an upper and lower\nlimit (p.10). Grant and Lebo cite the public's policy mood\nas an example (Stimson, 1991). They find that policy mood\ncontains a unit root, but because it is based on survey per-\ncentages, it is clearly bound between 0 and 100. According\nto Grant and Lebo, bounded unit roots should not be ana-\nlyzed with a GECM. They write, \"Even if we find series that\nare strictly unit-roots and we use MacKinnon CVs, mistakes\nare still rampant if our dependent variable is one of the vast\nmajority of political times series that is bounded\" (p.12).\nA reconsideration of Grant and Lebo's simulations dem-\nonstrates, however, that the GECM performs no worse with\nbounded unit roots than it does with the integrated time series\ndiscussed in Case 1 (where Grant and Lebo recommend the\nuse of the GECM). First, consider Grant and Lebo's finding\nthat, \"Boundedness does not seem to affect the estimation of\n\nor \n\" (pp.11\u00ad12). This is an important result because\nthese parameters correspond with the long-run and immedi-\nate relationships between Xt\nand Yt\n, which are typically\nwhat researchers are most interested in testing. Grant and\nLebo's concern is that the GECM yields incorrect inferences\nabout cointegration with bounded unit roots. Consistent with\nthis claim, their Table 3 shows that the GECM incorrectly\nshows evidence of cointegration between unrelated series\nmore than 5 per cent of the time. However, the inflated rate\nof Type I errors associated with the cointegration test results\nbecause of an incorrect application of the GECM.11\nTo understand why the results in Grant and Lebo's Table\n3 are problematic, recall that Grant and Lebo's key point\nabout bounded unit roots is that despite containing a unit\nroot, bounded unit roots behave differently than pure I(1)\nseries. Specifically, \"as the series nears its upper and lower\nthresholds it tends towards mean reversion\" (Grant and\nLebo, Supplementary Materials, p.13). This mean reversion\nat set bounds can produce a constant mean with a constant\nvariance, which means the series would behave as if station-\nary. If the series behaves as if it is stationary, even if the data\ngenerating process (DGP) contains a unit root, these sample\nproperties must inform the statistical analysis. As Grant and\nLebo write, \"Analysts should deal with the properties of the\ndata sample they have and not make arguments about asymp-\ntotics\" (p.72).12 Thus, even if we know the DGP to contain a\nunit root, if the bounded nature of the data produces a series\nthat mimics a stationary time series, we must take this into\naccount.\nAs noted above (also see Case 3 below), Grant and Lebo\nshow that tests of cointegration based on a GECM will\nincorrectly find evidence of cointegration too often if the\ndependent variable is stationary. This realization provides a\n4 Research and Politics \nrather simple modeling strategy for bounded unit roots.\nWhen a bounded unit root behaves as if it is stationary, the\nGECM should not be used to test for cointegration. When a\nbounded unit root behaves as if it contains a unit root, the\nGECM should be appropriate. To test whether this strategy\navoids Type I errors, we replicate Grant and Lebo's bounded\nunit root simulations, adding a test for the time series prop-\nerties of Yt\n. Specifically, we use Grant and Lebo's simulation\ncode to generate dependent series to be bounded unit roots.\nThese series depend on three parameters: the range of the\nthe disturbance term, which is set to 1, 2, or 3, and the series\nand Lebo, the predictors are integrated time series I(1) .\nAfter generating these series, we use an augmented\nDickey Fuller (ADF) test to evaluate whether we reject the\nnull hypothesis of a unit root in the dependent series. At\nfirst, our use of an ADF test may seem surprising. It is well\nknown that ADF tests are underpowered against the alter-\nnative hypothesis of stationarity (Blough, 1992; Cochrane,\n1992). Thus, the ADF may incorrectly conclude that a\nseries that behaves as if stationary follows a unit root pro-\ncess in the observed data. However, this means we are bias-\ning our simulations against support for the GECM since we\nare more likely to incorrectly conclude the series contains a\nunit root and thus inappropriately utilize the GECM as a\ntest of cointegration (thereby inflating the rate of Type I\nerrors with those cointegration tests). If the ADF rejects the\nnull of a unit root, we do not use the GECM to test for coin-\ntegration. Even though the true DGP in our simulations is a\nbounded unit root, if the series behave as if stationary\n(because the bounds and mean reversion generate a series\nwith a constant variance and mean), the GECM should not\nbe used to test for cointegration. Not only would the cointe-\ngration test be wrong if the dependent series behaves as if\nstationary, but we have no reason to expect a cointegrating\nrelationship between an integrated predictor and an out-\ncome variable that appears stationary (Keele et al., 2016).\nTable 1 reproduces Grant and Lebo's original results\n(from their Table 3, p.11) and the results from our simula-\ntions. Recall that our simulations are identical to Grant and\nLebo's except we do not use the GECM to test for cointe-\ngration if an ADF test on Yt\nrejects the null of a unit root.\nWe add this step because a stationary time series cannot be\ncointegrated with an integrated time series (Keele et al.,\n2016), rendering it unnecessary to test for cointegration,\nand because tests for cointegration with a stationary time\nseries will produce biased results (Grant and Lebo, 2016).\nFirst, note that Grant and Lebo's results show that naively\nestimating a GECM will produce incorrect evidence of\ncointegration more than 5 per cent of the time, and these\nerrors will increase as T increases, the variance increases,\nand with the more limited bounds of 49 to 71. These results\nare not surprising because as discussed above, each of these\nconditions (i.e. longer T, greater variance, and narrower\nbounds) would increase the mean reversion of the series.\nOur primary interest is evaluating whether the GECM\nyields correct inferences if we first test the sample proper-\nties of the bounded unit root. To test this expectation, the\n\"With Unit Root Test\" rows only use the GECM to test for\ncointegration if theADF test does not reject the null hypoth-\nesis of a unit root in the dependent series. Across all param-\neters, the Type I error rate is about 5 per cent. By first\ndiagnosing the time series properties of the dependent\nseries (which is standard practice in time series analysis),\nwe avoid the Type I errors in the cointegration tests. These\nresults show that there is no inherent problem with using\nthe GECM with bounded unit roots.14 This is an important\nresult. Grant and Lebo state, \"Even if we find series that are\nstrictly unit-roots and we use MacKinnon CVs, mistakes\nare still rampant if our dependent variable is one of the vast\nmajority of political times series that is bounded\" (p.12).\nYet, the results in Table 1 show that if we follow these\nguidelines (i.e. find evidence of unit\u00adroots and use\nMacKinnon critical values), the mistakes Grant and Lebo\nfound essentially disappear.\nIn addition to offering guidance about the appropriate use\nof the GECM, these simulation results hold implications for\nGrant and Lebo's analysis of Kelly and Enns (2010). Grant\nand Lebo use Kelly and Enns' analysis of the relationship\nbetween income inequality and policy mood to illustrate the\npitfalls of analyzing a bounded unit root with a GECM.\nSpecifically, based on Kelly and Enns' analysis, Grant and\nLebo conclude that there is \"No cointegration\" and thus the\n\"GECM model [is] inappropriate\" (p.26). Yet, looking at\nKelly and Enns' most parsimonious analysis (Table 1,\nColumn 2) we find clear evidence of cointegration.15 We\nselect the most parsimonious specification because in Keele,\nLinn, and Webb's first contribution to the symposium, they\nsuggested that Kelly and Enns over-fit their model. By\nfocusing on this parsimonious model (which Keele, Linn,\nand Webb did not consider) we mitigate concerns that the\nresults are due to over-fitting the model. The evidence of\ncointegration in Kelly and Enns'analysis combined with the\nsimulation results above in Table 1 support the use of the\nGECM. Grant and Lebo also conclude that there is \"No sup-\nport for short- or long-term effect of income inequality on\npublic mood\" (p.26). This conclusion is surprising because,\nas noted above, Grant and Lebo conclude that \"Boundedness\ndoes not seem to affect estimation of \nor \nand \nand \nreflect Kelly and Enns' estimates of the\nshort- and long-term relationships between income inequal-\nity and public mood (see Grant and Lebo's Equation (5)).\nGrant and Lebo's conclusion that the estimates of \nand\n\nare not affected by boundedness further validates Kelly\nand Enns' estimates of these parameters.\nSince the various simulations validate Kelly and Enns'\nestimates and conclusions about cointegration, we won-\ndered why Grant and Lebo's replication of Kelly and Enns\nwith nonsense regressions produced spurious results. In\ntheir nonsense regressions, Grant and Lebo replicated Kelly\nand Enns' analysis substituting the key predictor variables\nEnns et al. 5\nfor variables that are surely unrelated to the public's policy\nmood: beef consumption, coal emissions, tornado fatalities,\nand onion acreage (see Grant and Lebo's Table E.13). Grant\nand Lebo write, \"Based on our past replications and simula-\ntions we expect spurious regressions, and that is what we\nfind\" (Grant and Lebo, supplementary materials, p.43).\nThis conclusion depends, however, on an incorrect applica-\ntion of the GECM. Across the eight nonsense regressions,\nnone show evidence of cointegration.16 If Grant and Lebo\nfollowed their own advice to \"set aside the estimates\" with-\nout cointegration, they would never have reported these\nresults from these nonsense regressions.17 The spurious\nresults in their nonsense regressions result because they did\nnot test for cointegration. We should also note that even if\nthe GECM was implemented correctly, we do not recom-\nmend the use of atheoretical variables to demonstrate the\npossibility of spurious findings. Instead, we recommend the\nstandard approach of simulating data and conducting Monte\nCarlo experiments.18\nGrant and Lebo's final critique of Kelly and Enns comes\nfrom their Tables E.16 and E.17, where they re\u00adanalyze\nKelly and Enns' data with a FECM and find no significant\nrelationships in the data. What Grant and Lebo fail to con-\nsider is the possibility that FECMs under-identify true rela-\ntionships in small samples. Since Grant and Lebo's fractional\ndifferencing analyses failed to fully replicate four out of five\ninfluential articles, this is a critical consideration. In all of\ntheir simulations, Grant and Lebo never report how often\nfractional differencing methods identify true relationships.\nHelgason (2016) considered prediction error and found that\nthe performance of the FECM depends heavily on whether\nshort-term dynamics are present and the sample size, but he\ndid not test how often FECMs correctly identify true rela-\ntionships. We also do not know how the FECM performs if\napplied to data that are not fractionally integrated. If the\nFECM is overly conservative, Grant and Lebo's re-analysis\nand critiques of the other articles would also be highly prob-\nlematic. This is an important area for future research.\nCase 3: The dependent variable and all\nindependent variables are stationary\nAs we noted earlier, Grant and Lebo make an important\ncontribution by highlighting the fact that \nin a GECM\nTable 1. The percentage of simulations that provide incorrect evidence of cointegration based on Grant and Lebo's Table 3 and\nbased on first testing for a unit root.\nBivariate\nMultivariate (2 IVs)\nTable entries report Type I error rates when testing for cointegration with simulated data, depending on the specified sample size, measurement\nbounds, and variance. \"Grant & Lebo reproduction\" presents values from Grant and Lebo, Table 3. \"With unit root test\" presents the corresponding\nvalues after first testing for a unit root in Yt .\n6 Research and Politics \ncannot be used as a test of cointegration when the depend-\nent variable is stationary. Grant and Lebo also claim that\nthe GECM is inappropriate with a stationary dependent\nvariable. In their concluding article, they emphasize, \"the\nincreased risk of Type I errors for X when using the\nGECM rather than the ADL\" (p.70). In their response to\nGrant and Lebo, Keele, Linn, and Webb challenge this con-\nclusion by stating the equivalence of the ADL and GECM\nis \"mathematical fact\" (p.32).19 Indeed, from a mathemati-\ncal standpoint, since the ADL is appropriate with a station-\nary dependent variable, the GECM must also be appropriate\nin this scenario.\nHere we wish to clarify that the increased risk of Type I\nerrors that Grant and Lebo refer to is due entirely to the\npotential for incorrect interpretation of GECM results.\nAlthough the GECM and ADL contain the exact same\ninformation, the two models present this information dif-\nferently. When researchers fail to realize that the two mod-\nels present information differently, errors of interpretation\ncan emerge. Thus, we agree with Grant and Lebo that when\nthe dependent variable is stationary, the parameterization of\nthe GECM is more likely than the ADL to lead to errors of\ninterpretation. Specifically, when estimating a GECM with\na stationary Yt\n, researchers must remember not to interpret\n\nas a test of cointegration and researchers cannot directly\nconsider the estimate for \n. Instead, to obtain the esti-\nmated lagged effect of Xt-1\non Yt\n, \nmust be subtracted\nfrom \n(and the corresponding standard error must be cal-\nculated). Estimating an ADL avoids these considerations.\nAlthough the true Type I error rate for ADLs and GECMs is\nidentical, we agree with Grant and Lebo that with a station-\nary dependent variable the ADL is less likely to lead to\nerrors of interpretation.\nCase 4: The dependent variable is strongly\nautoregressive/near-integrated\nGrant and Lebo's fourth case focuses on near-integrated\ndata, which are time series with a root close to, but not quite,\nunity (Phillips, 1988). Grant and Lebo again draw a confus-\ning distinction between the ADL and GECM. In their first\narticle, they write: \"Our findings for theADL match those of\nDe Boef and Granato (1997), who find that the model has\nacceptable spurious regression rates with near\u00adintegrated\ndata. But we also find that this does not translate for the\nsame data in the GECM\" (p.15). They present additional\nsimulation results in support of this claim in their conclud-\ning article, stating that when estimating a GECM, \"With\nsixty observations there is a significant threat of Type I\nerrors\" (p.75). Because the ADL and GECM are the same\nmodel, as with the stationary dependent variable example\nabove, the threat of errors stems entirely from potential\nresearcher errors. Because the ADL and GECM are mathe-\nmatically equivalent, and since Grant and Lebo found evi-\ndence that the ADL avoids spurious correlations (see their\nTable 6), the same must be true for the GECM. There is no\ninherent problem with the GECM and near-integrated data.\nIf the GECM is implemented correctly, no problems emerge.\nHere we review Grant and Lebo's simulation results to\nillustrate potential errors that researchers need to avoid.\nOne potential error is evaluating \nin the GECM without\nfirst testing for cointegration. This inflated Type I error rate\nis evident in Table 1 of Grant and Lebo's concluding article,\nwhere they report false rejection rates as high as 24 per cent\nfor the null hypothesis that the long run multiplier (i.e.\n\n\n)\nequals zero for all simulations. Because the long-run multi-\nplier assumes a cointegrating relationship between Xt\nand\nYt\n, researchers should not evaluate whether the LRM is sig-\nnificant unless first finding evidence of cointegration. As\nGrant and Lebo's lead article explained, estimates should\nbe \"set aside\" if there is no evidence of cointegration. It is\nthe failure to first test for cointegration, not the GECM, that\nleads to the inflated false rejection rate in Table 1 of Grant\nand Lebo's concluding article.20\nA second error can result from using the incorrect criti-\ncal values when testing for cointegration with \n. For\nexample, the \"GECM Model\" results that Grant and Lebo\nreport in Table 6 (of their lead article) are based on a test of\ncointegration that uses standard (one\u00adtailed) critical values\ninstead of the appropriate MacKinnon critical values. To\nillustrate the consequences of this error, we consider the\nresults in Grant and Lebo's Tables G.1 through G.5 (in their\nsupplementary materials). Although the results Grant and\nLebo reported in the text of their article did not use the\nappropriate critical values, Tables G.1 through G.5 in their\nsupplementary materials did include results based on the\ncorrect MacKinnon critical values. Thus, we can rely on\nthese tables from their supplementary materials to evaluate\nthe performance of the GECM when analyzed correctly. In\nTable 2 we reproduce a summary of the results from Grant\nand Lebo's Tables G.1\u00adG.5. To clarify their findings, we\nmake two changes to how they reported their results. First,\ninstead of reporting the number of false rejections, we\nreport the per cent of false rejections. Second, we account\nfor the fact that each additional parameter estimated\nincreases the likelihood of falsely rejecting the true null\nhypothesis. This step is necessary because Grant and Lebo\nreport how many times one or more significant relation-\nships emerge. Since k predictors increases the likelihood\nof finding a significant relationship k times, we divide the\nnumber of false rejections reported by Grant and Lebo\nby k . Again, although we make these changes for ease of\ninterpretation, all values in our Table 2 are based directly on\nGrant and Lebo's Tables G.1\u00adG.5.\nThe left half of our Table 2 reports the false rejection\nrates for \n, which represents the immediate effect of Xt\non Yt\n. Across all scenarios, the false rejection rate is\nclose to the expected 5 per cent. This is an important result.\nEven without testing for cointegration, the immediate\nEnns et al. 7\nrelationship is estimated correctly. Of course, this must be\nthe case. In the GECM \ncorresponds with the coefficient\non Xt\nin the ADL, which is what Grant and Lebo report in\nthe top half of their Table 6. Since they did not observe\nevidence of spurious regression with their ADL estimates,\nwe do not expect to observe spurious results for the identi-\ncal estimates based on the GECM.\nAs noted above, the bottom half of Grant and Lebo's\nTable 6 found false rejection rates above 5 per cent because\nthey did not use the appropriate MacKinnon critical values.\nThe right half of Table 2 reports the same information (i.e.\nthe false rejection rate for \n= 0 ) when the\ncorrect critical values are used. These results are based\nentirely on Grant and Lebo's Tables G.1\u00adG.5. When we use\nthe correct critical values (as Grant and Lebo did in their\nTables G.1\u00adG.5), the percentage of false rejections is\nGrant and Lebo's lead and concluding articles recom-\nmend that the GECM should not be estimated with near-\nintegrated data and short time series. We agree that if\nscholars implement the GECM incorrectly with near-inte-\ngrated data, incorrect results will emerge. However, Grant\nand Lebo's simulation results (based on the ADL as well as\nthe GECM) show that if the GECM is implemented and\ninterpreted correctly (i.e. long-run relationships in the\nGECM are only considered if there is evidence of cointegra-\ntion), it is completely appropriate with near-integrated data.\nCase 5: The dependent variable is fractionally\nGrant and Lebo's fifth case focuses on fractionally inte-\ngrated time series. Often researchers assume a time series is\nstationary ( d = 0 ) or integrated ( d = 1). Fractional inte-\ngration, by contrast, allows d to take on values between 0\n\"When d takes on fractional values, it allows the data to be\nmean reverting, yet still maintain its long memory.\"\nAlthough both fractionally integrated series and near-inte-\ngrated series are often characterized as long memory pro-\ncesses, fractionally integrated processes are associated with\nhyperbolically decaying autocorrelations while near-inte-\ngrated processes are associated with geometric or exponen-\nDespite Grant and Lebo's enthusiasm for FI methods,\ntwo other contributions to the Political Analysis sympo-\nsium demonstrate that even when data are fractionally inte-\ngrated, the GECM is often appropriate. Esarey (2016)\nexamined fractionally integrated time series where d\ntions, Esarey concludes, \"I find evidence that the simple\nautodistributed lag model (ADL) or equivalent error cor-\nrection model (ECM) can, without first testing or correct-\ning for fractional integration, provide a useful estimate of\nthe immediate and long-run effects of weakly exogenous\nvariables in fractionally integrated (but stationary) data\"\n(p.42). Grant and Lebo, however, also conduct simulations\nwith fractionally integrated series where d takes on these\nvalues. The results appear in Table 2 of their concluding\narticle and they seem to challenge Esarey's conclusions.23\nAgain we find, however, that the different conclusions can\nbe resolved by following Grant and Lebo's advice to test\nfor cointegration with the correct critical values.\nThe top half of Grant and Lebo's concluding Table 2\nreports the rate of false rejections for LRMs. The results\nseem to contradict Esarey's findings because in almost\nevery case, the false rejection rate was greater than 5 per\ncent. However, Grant and Lebo did not first test for cointe-\ngration, which means the rate of Type I errors is greatly\ninflated. As Grant and Lebo explained in the context of\nTable 2. The percentage of spurious relationships for near-integrated series, results from Grant and Lebo Tables G.1\u00adG.5 (T = 60).\ny\nx\nXt\nXt-\nNumber of predictors Number of predictors\nNotes: Long-term estimates (1\n) reflect the percentage of spurious relationships when the MacKinnon critical values provide evidence of\ncointegration. Because Grant and Lebo report the number of simulations with at least one significant predictor variable, we divide this number by\nthe number of predictors in the model to account for the fact that k additional predictors increase the number of parameter estimates that could be\nsignificant k times.\n8 Research and Politics \nnear-integrated data, \"relying on the significance of the\nLRM rather than the joint hypothesis test of the \n* and \n*\nparameters does lead to an increased rate of Type I errors\"\n(p.75). The bottom half of their Table 2 (which also shows\nfalse rejection rates above 5 per cent) does test for cointe-\ngration. However, the results reported in Table 2 do not rely\non the correct critical values. Grant and Lebo's replication\ncode indicates that they used standard critical values based\non a one-tailed test (1.645). Based on their lead article, we\nshould not be surprised that this critical value produced\ninflated Type I error rates. We should also not view this\nincorrect application of the GECM as evidence that the\nGECM is inappropriate in this context. Indeed, as Esarey's\narticle shows, when applied appropriately, the GECM (and\nits equivalent ADL) provide appropriate estimates.\nHelgason (2016) performs additional simulations with\nfractionally integrated time series and finds that the perfor-\nmance of the GECM and FECM depends on the length of\nthe time series and whether or not short-run dynamics are\npresent. When a cointegrating relationship exists between\nfractionally integrated variables, both models provide simi-\nlar results when T = 50 , but the FECM provides more\naccurate estimates as T increases. However, if short-run\ndynamics are present, the GECM performs better as T\nincreases. Importantly, despite Grant and Lebo's concerns,\nneither Esarey nor Helgason found evidence of increased\nType I error rates when the GECM was applied to fraction-\nally integrated series.\nThe fact that neither Esarey nor Helgason found evi-\ndence of increased Type I error rates when the GECM was\napplied to fractionally integrated series makes Grant and\nLebo's conclusions regarding Casillas et al. (2011) seem\nsurprising. Casillas, Enns, and Wohlfarth examined the\nrelationship between the public's policy mood and\nSupreme Court decisions. Grant and Lebo argue that the\ndependent variables analyzed by Casillas, Enns, and\nWohlfarth were fractionally integrated and they should not\nhave estimated a GECM. However, a closer look at the\ndata and Grant and Lebo's analysis suggests that the\nGECM was indeed appropriate.24\nFirst, Grant and Lebo's estimate of the FI parameter d is\nproblematic. To illustrate our concerns, we focus on the per\ncent of Supreme Court decisions decided in a liberal direc-\ntion (among all cases that reversed the lower court's ruling)\nand the per cent of liberal reversals among non-salient cases\n(two of Casillas, Enns, and Wohlfarth's dependent varia-\nbles). Grant and Lebo report that these variables are frac-\ntionally integrated ( d = 0.62 for both series).25 We focus on\nGrant and Lebo's FI estimates for these series because in\ntheir concluding article they refer to this analysis as a \"good\ndemonstration\" of the steps necessary to test for FI in a time\nseries (p.79). However, Grant and Lebo failed to test whether\nthe residuals from their autoregressive fractionally inte-\ngrated moving average (ARFIMA) model were white noise.\nIf researchers followed Grant and Lebo and ignored this\nstep, they would reach the wrong conclusions about the time\nseries properties of the variable of interest. Of course,\nresearchers must remember that debate exists regarding how\nlong a time series must be to yield accurate tests of FI (Keele\nare conducting FI tests, they need to be aware of the\nnecessary steps. When we follow the appropriate steps, the\nevidence that Casillas et al.'s (2011) dependent variables\nare fractionally integrated disappears.\nTable 3. Portmanteau (Q) test for autocorrelation in ARFIMA and ARIMA models of the per cent of liberal supreme court\ndecisions that reversed the lower court.\n All reversals Non-salient Reversals\nNotes: Table entries report the results of Ljung and Box (1978) Portmanteau (Q) white noise tests across different lag lengths. The maximum lag\nlength (20) is determined by\nn\n- . Columns 1 and 3 represent Grant and Lebo's ARFIMA model. Cell entries reflect p -values. Values less than\n0.05 indicate a significant autocorrelation at that lag length.\nEnns et al. 9\nTo understand the necessary steps when testing for FI,\nrecall that if the ARFIMA model that estimates d is speci-\nfied correctly, the resulting series should be white noise. As\nGrant and Lebo explain, \"no significant autocorrelations\nshould remain\" (p.79). Grant and Lebo's ARFIMA analysis\ndoes not satisfy this requirement. Columns 1 and 3 in Table\n(Portmanteau Q) test for autocorrelation among the residu-\nals following Grant and Lebo's ARFIMA model. The bot-\ntom row, which corresponds with the maximum lag length\n(\nn\n- ) represents the standard value reported with this\ntest. In both Columns 1 and 3, we reject the null hypothesis\nFurthermore, we see that the evidence of significant auto-\ncorrelation is not sensitive to the choice of lag length.\nAcross multiple lag lengths, we reject the null hypothesis of\nno autocorrelation. Given this result, we explored alternate\ndynamic processes and we found that autoregressive inte-\ngrated moving average (ARIMA) models [ (2,1,0) and\n(0,1,1) ] fit the data better (i.e. minimized the Akaike infor-\nmation criterion).26 Given their better model fit, not surpris-\ningly Columns 2 and 4 show that for these ARIMA models,\nnone of the autocorrelations are significant, indicating that\nthe residuals are white noise. Furthermore, because these\nARIMA models are I(1) , they provide evidence that these\nseries contain a unit root.27\nA second concern with Grant and Lebo's approach is\nthe model they use to estimate d. In their concluding arti-\ncle, Grant and Lebo rebuke Keele, Lynn, and Webb's use\nof the time-domain exact maximum likelihood (EML)\n(p.77) (for a similar critique, see Grant, 2015). Given their\ncriticism of using the EML estimator, it is surprising that\nthis is the estimator Grant and Lebo used to diagnose the\ntime series properties in Casillas et al.'s (2011) data.28 In\nSupplementary Appendix 5 we report estimates of d\nbased on three semi-parametric estimators, the Geweke/\nPorter\u00adHudak log periodogram estimator (Geweke and\nPorter-Hudak, 1983), the Phillips modified log perio-\ndogram estimator (Phillips, 1999a,b), and the Robinson\nlog periodogram estimator. In all cases, the results are con-\nsistent with the ARIMA models above suggesting that the\npercentage of liberal Supreme Court decisions (among all\nreversals and non-salient reversals) contain a unit root.\nThe results also demonstrate that, at least with small sam-\nples, FI techniques are highly sensitive to the estimator\nused and to how the models are specified (i.e. whether\nautoregressive and moving average parameters are mod-\neled with parametric approaches and the number of\nordinates included with semi-parametric methods). This is\nan important consideration that Grant and Lebo did not\ndiscuss. When testing series for FI, researchers must\nremember to report the choices they make and whether the\nresults are sensitive to these decisions.\nIt may be that with short time series, we cannot draw\nfirm conclusions about the time series properties of vari-\nables. Yet, the balance of evidence from the various tests\nsuggest that these series contain a unit root. Of course,\nbecause these series are percentages, they are clearly\nbounded. We saw above that when we cannot reject the\nnull of a unit root (as is the case here), as long as the\nMacKinnon critical values show evidence of cointegra-\ntion, researchers can model bounded unit roots with a\nGECM. In addition, the t-statistics associated with the\ncoefficient and standard error for the lagged dependent\nvariable in CEW's Table 1 (\n-\n- and Table 2\n(\n-\n- fall below the corresponding critical\nvalue in Ericsson and MacKinnon (2002). Thus, the\nGECM was an appropriate modeling decision.\nWhy then did Grant and Lebo's nonsense regressions,\nwhere they replaced the predictors in Casillas, Enns, and\nWohlfarth's analysis with the annual number of shark\nattacks, tornado fatalities, and beef consumption, show\nevidence of spurious relationships? We again emphasize\nthat simulations, not nonsense regressions, are the most\nappropriate way to test for the rate of spurious regression.\nHowever, even if we take these nonsense regressions at\nface value, once again we find that Grant and Lebo's con-\nclusions result because they interpreted the GECM results\ndespite no evidence that shark attacks, tornado fatalities,\nand beef consumption are cointegrated with the dependent\nvariables.29 If Grant and Lebo followed their own advice to\n\"set aside the estimates\" without cointegration, they would\nnever have reported these results from their shark attack/\ntornado/beef analysis. Instead, they conclude, \"our non-\nsense IVs are significant far too often\" (p.22). This is an\nerroneous conclusion that emerged because Grant and\nLebo failed to follow their own recommendations regard-\ning the GECM.\nConclusions and recommendations\nWe applaud Grant and Lebo for trying to clarify the\ntime series literature. Their lead and concluding articles\nto the recent Political Analysis symposium on time\nseries error correction methods make some important\ncontributions. Accordingly, we agree with the following\nrecommendations.\n1. When analyzing integrated time series, researchers\nmust establish cointegration with appropriate\nMacKinnon critical values prior to interpreting the\nresults of a GECM.\n2. When analyzing a dependent variable that is sta-\ntionary, researchers cannot use \nin the GECM as\na test of cointegration.\n10 Research and Politics \n3. Although the ADL and GECM produce the same\ninformation (in different formats), the ADL is less\nlikely to yield errors of interpretation when Y is\nstationary.\nHowever, we are not convinced that Grant and Lebo\nhave presented sufficient evidence to raise fundamental\nquestions about the applicability of the GECM to political\ntime series. In this article we have re-examined many of\nGrant and Lebo's own simulations and have supplemented\nthese with our own new analyses. The results here point to\nthe conclusion that, when executed properly, the GECM is\nan analytically appropriate model choice when a dependent\nvariable is:\n1. a bounded unit root (with cointegration);\n2. near-integrated (with cointegration).\nThese scenarios are common in social science applications,\nmeaning that Grant and Lebo's skepticism toward the\nGECM is largely misplaced.\nIn addition to making the broad point that the GECM\ncan be usefully applied to a variety of political time series,\nwe also showed that Grant and Lebo's critiques of Casillas\nTaking Grant and Lebo's critiques at face value, Keele,\nLinn, and Webb suggested that the problem with these anal-\nyses could be over-fitting. But in reality there was no prob-\nlem to solve.30 When analyzed appropriately, the core\nresults of these earlier studies remain intact, and it is incor-\nrect to conclude that the GECM was inappropriately applied\nin these cases.\nSome of our conclusions are echoed by other contribu-\ntors to the Political Analysis symposium. But none of the\nresponses directly question Grant and Lebo's core argu-\nment about the highly constrained set of circumstances in\nwhich a GECM would be appropriate. We have attempted\nto highlight substantial problems with key conclusions in\nGrant and Lebo's work. In sum, our results show that their\nmethodological concerns about the inappropriateness of the\nGECM for political science time series are far too broad.\nMoreover, the critiques of at least two of the substantive\nanalyses that Grant and Lebo replicate are not supported by\ntheir own evidence. We therefore believe that it would be a\nmistake for applied researchers to adopt Grant and Lebo's\nrecommendation to use \"the GECM in only one rare situa-\ntion\" (p.27). The evidence does not support this recommen-\ndation. The GECM should not be set aside. It should remain\na technique that is regularly applied alongside other tech-\nniques in the tool kit of social science research.\nBased on our reanalysis, there is still much to learn from\nGrant and Lebo's work. Most particularly, they have done a\ngreat service by drawing additional attention to FI tech-\nniques. Our primary purpose here was not to explore\nmodels of FI, and the papers in the symposium leave sev-\neral considerations about FI techniques unresolved. First,\ndebate exists regarding the ability of FI tests to accurately\nidentify FI. Second, as we demonstrated by reviewing\nGrant and Lebo's re-analysis of Casillas et al. (2011),\nresearchers have multiple parametric and semi-parametric\nmethods available to estimate factional integration, yet dif-\nferent assumptions made in these tests (e.g. presence and\nidentification of short-term dynamics) may lead to different\nconclusions about FI and the dynamic properties of a time\nseries. Third, it is not yet evident that FI modeling\napproaches can reliably identify true relationships in the\ndata, especially with short time series. Fourth, it is not clear\nhow FI techniques perform if incorrectly applied to series\nthat are not fractionally integrated. We look forward to see-\ning new contributions in political methodology that help to\nsort out these and other remaining issues regarding FI.\nAnother important avenue for future research is the consid-\neration of \"combined\" time series. Wlezien (2000) has\nshown that \"combined\" time series (where a process com-\nbines both integrated and stationary components) are likely\ncommon in political science data and that these series can\nbe modeled with a GECM. Combined time series are par-\nticularly important to consider because, as Wlezien (2000)\nexplains, they tend to look like FI series in finite samples,\nwhich further calls into question the ability of FI tests to\ncorrectly identify time series when T is small. We hope\nfuture research addresses these questions and remembers\nthat, if applied correctly, the GECM is appropriate in a vari-\nety of data scenarios common to political science research.\n"
}