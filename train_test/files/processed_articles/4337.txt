{
    "abstract": "Abstract\nWhen could it be safe to report ordinal scores instead of linear measures? In this study, preschool gains measured with\nordinal scores were compared with residualized gain scores, as well as Rasch model measures of linear change (logits) to\nclarify respective implications for objectivity, precision, validity, and meaningfulness. Results showed that ordinal scores\nand linear gains were highly correlated (~.90), and specific conditions were identified such as pre-test score distributions,\npre-test variability, and overall test targeting that determine complementarity of ordinal scores and linear scale values\nfor reporting achievement gains. Several properties of ordinal score gains were discussed, including negative correlation\nbetween gain and pre-test, unreliability of gains, and usefulness of residualized gains. This report concludes by supporting\ninterchangeability of ordinal scores and objective, linear measures when appraisal of complementarity is supervised by\nprinciples of mathematical logic.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nA perplexing question that has accompanied the rise of 20th-\ncentury social sciences is how to report gains after instruction\nor an intervention. Despite breath-taking advances in high-\nspeed computing, an enormous proliferation of statistical\nmodels, and integration of probability and measurement the-\nory, this simple question continues to confound social science\nresearchers of every stripe. In general, gain scores or mea-\nsurement of change present challenges, and not surprisingly,\nthere are few topics in social science methodology that have\nelicited as much confusion, misunderstanding, and anxiety as . . .\nThe unspeakable source of this confusion is lack of an\nexplicit, uniform interval measuring unit. Conventional social\nscience researchers insist on measuring change with ordinal\nmethods, hence without equal interval (linear) numerical units.\nUnfortunately, mathematical logic imposes explicit require-\nments on manipulation of numbers, which has direct implica-\ntions for measuring psychometric change. Social science\nresearchers would be wise to increase their understanding of\nthe mathematical foundations for measuring change.\nMuch confusion surrounding measurement of change\nresults directly from the rapid rise of social science meth-\nodology during past 100 years or so. In fact, confused\nnomenclatureaboundsthroughouttheliterature.Measurement\nprecision, for example, is commonly confused with psy-\nchometric reliability, just as ordinal scores are conflated\nwith linear measures. Even more confusing are linear mod-\nels, linear regression, and general linear models, which\ncompute correlations with ordinal scores to estimate a\nregression line. Regression lines, of course, do not have\nnumber properties or a measuring unit but are central to\ncontemporary psychometric ideas about scientific knowl-\nedge. Now, oddly, these statistical models are not carefully\ndistinguished from generalized linear models (GLMs;\nNelder & Wedderburn, 1972) or generalized additive mod-\nels (GAMs; Hastie & Tibshirani, 1990), which, in fact,\nimplement a mathematical transformation function and\nmeasure with a linear unit.\n1The Chicago School of Professional Psychology, IL, USA\n2DePaul University, Chicago, IL, USA\n3Chicago Public Schools, IL, USA\nCorresponding Author:\nNikolaus Bezruczko, The Chicago School of Professional Psychology,\nEmail: nbezruczko@thechicagoschool.edu\nThree Tales of Change: Ordinal Scores,\nResidualized Gains, and Rasch\nLogits--When Are They Interchangeable?\nNikolaus Bezruczko1, Serah S. Fatani2,3, and Noriko Magari3\n Keywords\ngain scores, measurement of change, Rasch measurement models, residualized gain scores, preschool assessment, ordinal\nmeasures, Likert-type scales, Stevens scale types, measurement and scaling methods, research methods, social sciences,\nresearch methodology and design\n2 SAGE Open\nPurpose\nThis research asks the question: Can communication with\neducational and psychological testing consumers be improved\nwith ordinal scores? Likewise, would description about\npatient-reported outcomes (PROs) benefit from ordinal\nscores? It is an unusual question, because scientific measure-\nment, in principle, is directed at eliminating ordinal score dis-\ntortions such as nonuniform units, unstable scale properties,\nand sample dependent parameter values (Hambleton, 1991).\nSo how can ordinal scores improve communication?\nThe reality is testing consumers such as parents, teachers,\nand patients typically prefer ordinal scores rather than objec-\ntive linear units. Linear units are generally opaque to layper-\nsons, especially when they are expressed on log scales or as\nprobabilistic measures. Even conventional standardized scores\nare more meaningful to laypersons than linear measures.\nLinear change, of course, requires mathematical transforma-\ntion of ordinal scores to equal interval units, which eliminates\nscore distortion in distribution tails--upper tail units are larger\nhence harder for students and patients than lower tail units\nwhich are smaller and easier. In general, reporting growth,\nlearning, or patient status with ordinal scores forces social sci-\nence researchers to confront an unresolved issue fundamental\nto science--measurement of \"change,\" which in psychomet-\nrics is frequently referred to as gain scores.\nIn addition to traditional educational and psychological\ntesting, ordinal scores and qualitative assessment are imple-\nmented during psychometric patient assessment in health care\n(see Uniform Data System for Medical Rehabilitation, 2014).\nPhysical rehabilitation clinicians, for example, may describe\npatients concretely such as \"walking with and without assis-\ntance\" or \"assisted\" versus \"unassisted transfers\" rather than\nrefer to probabilistic, log-odds (logit) units of linear gain\n(Granger, Hamilton, Keith, Zielezny, & Sherwin, 1986).\nTherefore, a useful goal here too would be to document cor-\nrespondence of linear change with passing a specific number\nof items for a particular assessment with some degree of\ngenerality.\nEducational, psychological, and health care researchers are\ndeeply divided over practical differences between ordinal and\nlinearized scores. Following sections present a brief overview\nof these differences measuring change that might otherwise\nseem enigmatic. This report then presents an empirical com-\nparison of three methods of measuring change: simple raw\nscore gains, residualized gains, and linear gains, and the inten-\ntion here is to clarify their comparability for reporting objec-\ntive change between pre- and post-assessments. Finally, a\ndiscussion follows about conditions that tend to support their\nqualified agreement and complementary use.\nBackground\nTo add confusion to this already difficult topic, some\nresearchers claim ordinal and linearized score differences are\nirrelevant to practical applications such as measuring change\ntionship between scale type and statistical techniques\" (p.\ncan be used as if they are linear (Spector, 1976). Some com-\nmentators assert ordinal scores already have interval proper-\nties and scoff at any suggestion of empirical justification\n(Borgatta & Bohrnstedt, 1980). Likewise, Carifio and Perla\ntype units to \"myths and urban legends\" (Carifio & Perla,\nto a generally capricious approach to statistical analysis and,\nnot surprisingly, scientifically unsound quantitative practices\nsurround ordinal analyses. Contemporary studies into Likert-\ntype ratings are finally providing much-needed insight about\ntheir unstable properties (Lantz, 2013). In general, ordinal\nscores and ratings remain treacherous and require caution.\nSeparate Branches\nIronically, psychometric gain scores have been studied so\nintensely that a reconciliation with a perspective emphasiz-\ning linear units is unlikely. In fact, this report points toward\ntwo sciences, one where gain measurement now defines a\nfundamental bifurcation of social science methods into ordi-\nnal and linear epistemologies. Indeed, practical divisions are\nnow so deep that certain empirical problems are only\naddressed with objective, linear methods, while others rely\non ordinal score methods. For example, professional licen-\nsure and certification are exclusively conducted with linear\nmeasurement models, while survey and opinion research\ntypically relies on ordinal methods. Large-scale educational\ntesting is unusual because international comparisons such as\nProgramme for International Student Assessment (PISA) is\nconducted with a linear measurement model, while large-\nscale American testing is conducted with item response the-\nory (IRT), which transforms scores to linear units, then\nchanges them back to an ordinal scale with additional proce-\ndures (2 and 3 parameters). Conventional psychological or\nbehavioral research continues to implement untransformed\nordinal scores.\nRather than alarm, this differentiation and specialization\nrepresents a long waited maturity of social science research.\nHistorically, similar branching has occurred in physical sci-\nences such as differentiation of high energy physics from stat-\nics, quantum from classical physics, and analytical from plane\ngeometry. Empirical psychology has long recognized two\ndisciplines, namely, experimental and correlational perspec-\nphilosophical differences between IRT and Rasch models.\nPresent research, however, goes much further and asserts that\ndifferences between ordinal and linear measures now consti-\ntute independent branches of social scientific research.\nLogically, this branching reflects the robust health of social\nscience methodology in 21st century after adaptations to\nBezruczko et al. 3\nnewer and more complex measurement problems.\nUnderstanding these differences should diminish the mystery\nof measuring gain.\nHistorical Antecedents\nAlthough statistical record keeping has ancient origins, con-\ntemporary social science research methods did not appear\nuntil well into 19th and early 20th centuries when several\nkey forces converged. Laplace's 19th-century contribution of\ncentral limit theorem together with Gauss's theory of errors\nestablished modern inferential statistical foundations (Stigler,\nods that modeled human judgments with an error distribu-\ntion. Later, Thurstone (1928) was inspired by Fechner's\nadvances and developed statistical methods, also based on\nGauss's mean error, for measuring opinions and attitudes.\nWhile these innovations offered objective foundations for\nmeasuring psychological change, a nagging question would\necho among traditional, physical scientists. Do these meth-\nods for measuring social observations actually constitute\nobjective scientific measurement?\nSocial science in the late 19th century was also significant\nfor monumental conceptual leaps. Simon and Binet's devel-\nopment of IQ finally moved scientific thinking beyond only\nphysical constructs. IQ was the first nonphysical quality\nparameterized with numerical operations and validated with\nIQ differs fundamentally from Fechner's perceptual judg-\nments and Galton's cognitive measures by representing a\nhypothetical qualitative hierarchy in a numerical framework.\nIn 1920, in collaboration with Simon, Jean Piaget proposed a\nqualitative intellectual stage theory based on IQ, which\nwould become the foundations for 20th-century develop-\nmental theory. Around this time, Spearman (1904) also con-\ntributed unidimensional factor theory to social scientific\nconceptualizations, which would dominate statistical and\nconceptual insights for several decades.\nIn addition to psychophysics and attitude measurement,\nsocial survey and mental testing methods would emerge in\nthe 19th century from Galton's fascination with eugenics.\nTogether with his student Cattell, Galton advanced mental\ntesting to identify hereditary cognitive differences (Cattell &\nGalton, 1890), and Galton would invent statistical correla-\ntion and regression to demonstrate generational transmission\nThese early methods and constructs paved the way for\nvirtually the entire 20th-century elaboration of social science\nresearch methods. Yet, despite their monumental signifi-\ncance, these early advances lacked a traditional measuring\nunit. Instead, their numerical values were not \"real\" numbers\nbut only rank orders described by mathematicians as count-\ning numbers, natural numbers, and positive integers. Spatial\ndistance between ranks is not uniform nor do they have uni-\nform material quality; hence, their addition and subtraction\nare not logically meaningful. Pearson (1894) introduced\nstandard deviation, a generalization of Gauss's mean error to\ndescribe ordinal score distributions, which arguably suc-\nceeded in obscuring this fundamental anomaly of mental\nscores.\nA further stigma of mental test scores is rank orders are\nconfounded with specific samples, because a person's mental\ntest score rank arbitrarily changes from sample to sample.\nSuccessive sampling, of course, contributes to \"true\" param-\neter values in the long run, but practical measurement is\ncommonly based on isolated observations. Substitution of\nconcrete population parameters for isolated samples has sta-\nbilized measures but sacrificed objectivity. By comparison,\nscientific units, which define abstract, universal constructs\nsuch as length and weight, are independent of specific sam-\nples or populations. It would be another 50 years before con-\nfounded items and persons and their sample dependency\nwould benefit from Rasch's (1968) separability theorem and\nDespite these limitations, early 19th-century mental testing\napplications quickly led to population norms and standard\nscores (Thorndike, 1904; Thorndike, Bregman, Cobb, &\nWoodyard, 1926), and those methods were generalized to\nparameterized Binet\u00adSimon's IQ items with a statistical unit\nof measure.\nSocial Undercurrents and Institutionalization\nWhile 19th- and 20th-century social science research is\ndeeply indebted to European intellectual traditions mainly in\nEngland, Germany, and France, phenomenal growth of men-\ntal testing and eventual institutionalization was uniquely\nimported into American public schools, then adapted during\nWorld War I to select officer candidates (Army Alpha and\nBeta Tests). Further adaptations such as Knox Cube Test in\n1920s would examine newly arrived immigrants (Boake,\nPsychological Corporation was established to promote com-\nmercial IQ testing interests, and by the 1930s widespread\nschool achievement testing was being conducted with Iowa\nTests of Basic Skills (ITBS). Nowhere else did mental testing\nrise so quickly or widely as in the United States. This early\nemphasis on testing for selection rather than growth or learn-\ning overshadowed most concerns about measurement of\nchange.\nHistorical accounts point to at least three powerful socio-\npolitical and economic forces that facilitated American insti-\ntutionalization of mental testing and the ordinal methods that\nsupported it. First, many Americans shared a common belief\nin \"progress,\" and scientific ideas to solve problems. Then,\nbetween mid-19th century and the Great Depression, mas-\nsive demographic migrations, literally millions of Europeans\nimmigrated to America, and mental testing was seen as\n4 SAGE Open\ninstrumental to addressing cultural effects and social prob-\nlems such as poverty, crime, and unemployment.\nConsequently, Americans listened closely to Galton's ideas\nabout mental testing and his active promotion of eugenics.\nThe following quote describes convergence of these forces:\n[American] culture . . . worries about the so-called \"menace of the\nfeebleminded\" and infatuation with eugenics was widespread . . .\nin which the turn to science as an important means of addressing\nsocial and industrial problems was embraced by many. . . . United\nStates may have been one of few in which a sufficient number of\ncultural, material, and institutional factors could come together to\nmake mental testing appear to be a technology worth pursuing on\nAmerican ability and intelligence testing was institution-\nfollowed by Scholastic Aptitude Test (SAT) in 1926. An\nemphasis on selection also appeared in cognitive aptitude\ntesting at General Electric conducted by Johnson O'Connor\nselecting students for college admission based on verbal and\nquantitative aptitude test scores. Throughout this rapid\nexpansion, a central motivation underlying mental testing\nwas ability selection. Practical mental testing was largely\nobsessed with efficiency and accuracy of discriminating\nbetween higher and lower ability, and the reproducibility of\nobtained rank order (reliability). In contrast, increased under-\nstanding of psychometric properties to support measurement\nof cognitive development and learning were not on the social\nresearch agenda.\nRejection of Scientific Foundations\nThe rapid 20th-century American assimilation of mental test-\ning brought attention to test scores, which were unusual by sci-\nentific standards. For example, measures in Fechner's\npsychophysics experiments, as well as Galton's studies of indi-\nvidual differences, lacked the property of unit additivity, which\nis instrumental for demonstrating consistency between empiri-\ncal quantities and mathematical logic.Additive units legitimize\narithmetic operations, which has extraordinary importance for\nadding and subtracting units on a number line--a chief purpose\nof scientific measurement. In general, objective measurement\nis critically dependent on valid numerical operations.\nPerplexity of gain measurement without a measuring unit\nwas frequently noted during this early period of theAmerican\ntesting movement. By 1920s, commentaries began appearing\nness of measuring change without an explicit unit became an\ncommented on measurement of change as follows:\nNumerous perplexing studies on the relation of [initial] status to\nchange in test scores . . . show greater gains for those subjects\noriginally making the [highest] scores. The more common\nresult, due . . . to unequal units or too low a ceiling . . . is a low\nnegative correlation. . . . It is usually assumed that pupils tested\nat the beginning . . . then later in the year . . . have shown a\nchange [that] have much the same meaning as scores on the\noriginal test . . . This assumption is certainly not true. (p. 187)\nA consequence of this fundamental discrepancy between\ntest scores and numerical properties is early 20th-century\nphysical scientists began raising questions about commensu-\nrability of social measures and logical number systems, as\nwell as meaningfulness of mental measures in mathematical\nequations. These concerns about test scores and psychologi-\ncal measurement finally came to a peak in 1932 when the\nBritish Association for Advancement of Science organized a\nspecial committee to investigate logical foundations of psy-\nchophysical measurement (Ferguson et al., 1940; see sum-\nmary in Michell, 1999). These meetings were conducted at\nCambridge University attended by eminent contemporary\nauthorities in physics and philosophy. Their conclusion after\n8 years of discussion and rigorous debate was to reject logi-\ncal foundations of psychological measurement. Central\nissues leading to this rejection were unit additivity and con-\ncatenation criteria, which are required for validating physical\nmeasuring units. An excerpt from that report follows below:\nAny law purporting to express a quantitative relation between\nsensation intensity and stimulus intensity is not merely false but\nis in fact meaningless unless and until a meaning can be given to\nObviously, Ferguson Committee results were a shocking set-\nback for social science research. In philosophical terms, the\nsocial sciences faced a hopeless measurability crisis (Michell,\nQualitative meaningfulness of physical units has been\ndefined for over 2,000 years by an explicit concatenation\nprocedure that demonstrates spatial extension or Euclidean\nstructure between perception and number. Mathematical uni-\nformity of this relation defines the logical foundations of sci-\nentific measurement. The simple empirical transformation of\nice to steam, for example, is a qualitative validation of tem-\nperature parameterization within a fairly restricted phenom-\nenological range. It provides legitimacy for measuring\ntemperature of observations outside human perceptual range.\nA more dramatic example would be qualitative high energy\ntransformations that occur across microscopic and celestial\nlevels of scale. In contrast, test score addition and subtraction\ndo not present sensory or qualitative change; hence, their\nnumerical operations lack meaningfulness. This issue is\nexacerbated by mental testing constructs that do not assert\nontological entities or objective reality, hence lack philo-\nsophical integrity. In other words, the dynamical cosmologi-\ncal narrative that underlies physical science and its measures\nis absent from mental test scores.\nBezruczko et al. 5\nLegitimation\nConsequences of the Ferguson Committee for social sci-\nence researchers were disastrous. Leading scientific author-\nities seemed to have settled a long-standing question, which\nlargely eliminated mathematical social sciences from more\nthan 2,000 years of scientific advancement; a judgment that\nseemed without appeal. Then S. S. Stevens, a psychologist\nwho had attended Ferguson committee meetings, offered\nthe social sciences a solution that would have tectonic\nconsequences.\nStevens, in a creative act that is still not widely appreci-\nated, proposed extending the standard scientific model by\naccommodating social observations with a hierarchy of scale\ntypes. Instead of expecting social observations to conform to\na narrow, monolithic ideal established by physical measure-\nment, he proposed a broader conceptualization that included\nordinal representations commonly obtained during psycho-\nlogical observations and mental testing. Stevens was not pro-\nposing an approximation to physical measurement\nidealizations but instead a conceptual leap to \"new\" data\nstructures. Stevens proposed extending the boundaries of\nmathematical logic to include ordinary nonphysical experi-\nence, which he organized in a quantitative hierarchy of nomi-\nnal, ordinal, interval, and ratio data types. Moreover, he\nprovided an empirical \"key\" for validating new data types by\ninvoking the principle of scale invariance. In his system,\ndata-type validity depended on preserving empirical order\nafter completing simple arithmetic operations. In one cre-\native sweep, Stevens demonstrated that a broader conception\nof science could easily accommodate the uniqueness of\nsocial observations without sacrificing traditional scientific\nrigor. Highest levels of his scale hierarchy, namely interval\nor ratio data types, demonstrated axiomatic operations logi-\ncally consistent with the real number system, which, in fact,\nemulated foundational physical measurement. Lower levels\nsuch as ordinal scores were less comprehensive but still logi-\ncally consistent with computing medians and percentiles.\nStevens's hierarchy of scale types was immediately\nabsorbed by authoritative professional literature and text-\nbooks, which offered discipline to social researchers by\nimposing a simple comprehensive logic on both physical and\nnonphysical measurements. Over the decades, Stevens's sys-\ntem stimulated challenges and rebukes, which has led to dis-\ncoveries of additional levels (Cicchetti, 2014; Mosteller &\ntypes are now in fact understood to provide a logical basis for\nmeasuring change.\nDiscontent Among Social Science Researchers\nAlthough Stevens's (1946) model was supported by physical\nscientists, many social science researchers expressed derision\nand disdain for his proposed trade-off between scale levels\nWhile Stevens seemed to have found the magic key to legiti-\nmize psychological measurement, the cost to social science\nresearchers was unacceptable. Those rejections are periodi-\ncally reasserted in contemporary literature, and now, more\nthan 60 years since Stevens's introduction of scale types, dis-\nsension and bitterness continue to ferment, which has ulti-\nmately forged the separate branches that now predominate.\nSupporters of data types and their restrictive statistical\nimplications questioned validity of computing means and\nstandard deviations with only ordinal, Likert-type ratings\ndemonstrated that population mean differences based on\nordinal scores can lead to incorrect results. Other commenta-\ntors raised concerns about untransformed Likert-type ratings\nproposed alternative scale taxonomies and recommended\nrescaling ordinal scores. Thomas (2014) warned that num-\nbers may have properties that influence statistical analyses,\nand Stine (1989) objected to statistics dissociated from scale\ntype. Other researchers were disparaging of parameter esti-\nmation based on only vague interval assumptions (Kuzon,\nIn contrast, those who objected to restrictions conducted\nempirical studies that suggested parameter estimation under\ncertain conditions was robust to data-type violations\nonstrated robustness of F test to data-type violations (see\nalso Davison & Sharma, 1988). Empirical investigation also\nfound Pearson's product\u00admoment correlations robust to data\nviolations (Havlicek & Peterson, 1976), and others empha-\nsized that arbitrary scale-type restrictions inevitably lead to\nmisleading conclusions (Velleman & Wilkinson, 1993).\nHowever, other efforts at demonstrating robustness of ordi-\nnal scores were confounded by complexities (Brennan, Yin,\nWhile studies cited above have shown support for weaker\nordinal scores, nowhere with exception of Davison and\nSharma (1988) did they delineate the limits or clarify boundar-\nies of valid ordinal inference. Consequently, qualified robust-\nness revealed by some studies has gained \"blind faith\"\nallegiance among social science researchers, which has led to\noverzealous if not mindless implementation and wide abuse.\nIn fact, robustness of significance testing with ordinal scores is\nimportant, but it does not automatically confer equal intervals\non ordinal scores, a widely held misconception. Further cloud-\ning validity of much contemporary ordinal analyses is confla-\ntion of group and person parameters, which has consequences\nfor interpreting effects of change on individuals.\nContemporary Status\nControversy over miss-inference and measuring change has\nnot prevented social science researchers from adapting ordi-\nnal scores for (a) personality and cognitive abilities\n6 SAGE Open\nmeasurement, (b) survey research, (c) educational testing\nand accountability, and (d) program evaluation. In addition,\na vast majority of psychological research remains committed\nto ordinal scores sometimes referred to as classical test the-\nNunnally & Bernstein, 1994). However, resistance to ordinal\nscore restrictions has not been without costs, which are\ndescribed in the following sections.\nLimitations, Constraints, and Warnings\nAmid decades of defiance, the literature now shows a grow-\ning chorus of warnings about hazards of ordinal scores. For\nexample, Embretson (1996) warned of spurious ANOVA\ninteractions (see also Kang & Waller, 2005; Romanoski &\nDouglas, 2002), while Morse, Johanson, and Griffeth\n(2012) described spurious interactions during multiple\nregression. Berk, Brown, Buja, and George (2013; see also\nBerk, 2011) pointed to distortions from computing ordinal\nmeans during regression, while Woody and Costanzo\n(1990) described statistical artifacts. Others described dis-\ntortions computing effect sizes (Hobart, Cano, Zajicek, &\nfound individual treatment effects distorted by ordinal\ngroup means. Townsend and Ashby (1984) described\nuniqueness theorem violations, while Prieler emphasized\ndistorted measures of patient gain (Prieler, 2007; Prieler &\nRaven, 2008). Kahler, Rogausch, Brunner, and Himmel\n(2008) described inconsistent parametric estimation, and\nWright & Linacre, 1989) have long emphasized miss-\ninference implications of ordinal scores.\nIn addition to statistical operations described above, a gen-\neral problem associated with ordinal scores is theoretical frag-\nmentation and proliferation of redundant scales. For example,\nStreiner and Norman commented on depression measurement\nand the \"daunting array of available scales. Whether one\nwishes to measure depression, pain, or patient satisfaction, it\nseems every article published in the field has used a different\napproach to the measurement problem. This proliferation\nimpedes research\" (Streiner & Norman, 2008, p. 5).\nConsequently, 20th-century social science research became\nincreasingly fragmented and incoherent represented by \"nar-\nrowly defined sub-disciplines, each pursuing objectives in\nA consequence of decades of undisciplined ordinal score\napplications is an enormous range of intractable practical\nproblems and limitations. A sample of those issues is repre-\nsented below:\n\u00b7\n\u00b7 Construct fragmentation\n\u00b7\n\u00b7 Scale indeterminacy\n\u00b7\n\u00b7 Nonfunctional scale magnitudes\n\u00b7\n\u00b7 Restriction to population parameters\n\u00b7\n\u00b7 Gain reliability\n\u00b7\n\u00b7 Conflation of person measures and population\nparameters\n\u00b7\n\u00b7 Measuring objective, meaningful change\nWhile each of these issues deserves discussion, the following\nsections review strategies to mitigate concerns about mea-\nsuring change with ordinal scores.\nOrdinal Strategies for Measuring Change\nA central theme of this report is ordinal and linear branches\nnow rationalize separate approaches to measuring gain.\nUnderstanding these alternative approaches may increase\neffective communication with laypersons, as well as pro-\nvide insight into special measurement issues that might\notherwise seem illogical. For example, many contempo-\nrary social science researchers show a remarkable insis-\ntence on preserving simple ordinal score methods (Thomas\n& Zumbo, 2012). Some researchers emphasize that ordinal\nscores have advantages for identifying persons with unusu-\nally high or low item response patterns (Cicchetti, 2014;\nWeiss, 1986), while others suggest improving scores by\nshifting to ranks (Lloyd & Zumbo, 2007; Lloyd, Zumbo, &\nResidualized scores, which embellish ordinal gain with\nregression-mediated values, have also been proposed to\nimprove ordinal score measurement (Cronbach & Furby,\nness of simple ordinal gain and residualized scores but rejected\nimpracticalmultiwavedesigns(seealsoZumbo&Zimmerman,\n1993). A reoccurring strategy recommended data transforma-\nVelleman and Wilkinson (1993) concurred that ordinal scores\nshould be transformed to linear values to support statistical\ninference. While methods for transformation are available,\nsocial science researchers rarely implement them.\nSocial science researchers have struggled with incorrigible\nmeasurement of change problems for virtually the entire 20th\ncentury. A relatively recent reaction to these problems is to\nassert that gain scores and measurement of change are obso-\nWilliams & Zimmerman, 1996), and a multitude of statistical\nmodels now purport to analyze change rather than measure it.\nTheir central strategy is to construct multivariate correlation\nmodels and more complex multilevel models to interpret \"rate\nof change\" instead of explicit unit magnitudes (Kissane,\n1982). Hierarchical linear modeling (Bryk & Raudenbush,\nlevel models (Singer, 1998) are examples of this strategy.\nOther correlation-based procedures are covariance analysis\nWillett & Sayer, 1994), structural equation models (Mun, von\nEye, & White, 2009), and latent variable modeling (Raykov,\nwith speculation about variance components that contribute to\nBezruczko et al. 7\ngain reliability (Rogosa, Brandt, & Zimowski, 1982; Rogosa\nIn general, strategies to analyze change described above\nshould be viewed cautiously, as they tend to distract from\nfundamental measurement issues. For example, their authors\nexpress disdain for pre- and post-assessments and, instead,\nemphasize collection of multiple observation waves (Rogosa\negies typically require extensive longitudinal designs.\nConsequently, their usefulness for routine school evaluations\nis limited and largely impractical for clinical reporting cycles\nor psychological and health care outcome evaluations. In\naddition, their dependence on correlations requires distribu-\ntional assumptions, sample size issues abound, and compli-\ncations arise from collinearity and normality violations.\nCategorically, these methods lack an explicit measuring unit,\nhence they are fundamentally ordinal. Even under optimal\nconditions, concerns arise about their qualitative meaning-\nfulness compared with objective, linear measures of change\nA less well-recognized limitation of multilevel, multivari-\nate correlation models is conflation of persons with groups,\nwhich seriously affects accuracy and validity of patient and\nstudent evaluations. In addition, published comparisons of\nthese methods are rare, which inhibit critical discussions about\nthem. Investigations by health care researchers have found\nordinal-based methods consistently associated with distor-\ntions, which discourage their clinical use (Hobart, Cano,\nZajicek, & Thompson, 2007; Norquist, Fitzpatrick, Dawson,\n& Jenkinson, 2004; Stucki, Daltroy, Katz, Johannesson, &\nLiang, 1996). These reservations aside, correlation-based sta-\ntistical methods to analyze change represent an important\nmovement to reform social science methods to better accom-\nmodate developmental and performance measurement.\nDespite their limitations, statistical models now provide an\nalternative to simple gain scores and may be useful for identi-\nfying significant long-term change if not precise magnitude.\nImportant exceptions to above criticisms are hierarchical\nlinear models, which explicitly implement a linear unit (Bryk\n& Raudenbush, 1987). \"In our example we used item\nresponse theory (IRT) to construct a common metric for each\ntest, in logits, specifically to facilitate measurement of\nchange\" (p. 150). Readers should note that their reference to\nIRT refers to one-parameter logistic because two-, three-,\nand four-parameter IRT models are ordinal (De Ayala, 2008).\nOther statistical strategies with an explicit measuring unit are\nGLM (Nelder & Wedderburn, 1972) and general additive\nmodels, which implement mathematical link functions\n(Poisson, gamma, Bernoulli, and binomial) to transform\nordinal scores to linearized units.\nLinear Measurement Models\nMeasuring change in the 20th and 21st century without a scien-\ntific unit of measure has been an ongoing catastrophe for social\nscience researchers filled with denial and resistance, as well as\ncreative adaptation and innovation. In fact, data-type and scale\nproperties are no longer formally recognized in published pro-\nfessional measurement guides (American Educational\nResearch Association, American Psychological Association, &\nNational Council on Measurement in Education, 1999), which\nis not surprising because the central priority of mental testing\nhas historically been rank order selection not measuring\nchange. Only urgency from contemporary practical needs to\nmeasure learning and growth has forced recognition of limits\nand constraints of ordinal scores. For some, this legacy has led\nto frustration and resignation as well as visions of future devel-\nopments, which is captured by the following quote:\nInstead of relying on classical test theory ideas about reliability,\nit is time to develop . . . practical instrument(s) . . . designed for\nmeasurement of change . . . Development of these approaches\nwill be a challenge. . . . They must find a way to distinguish real\nchange over time from random fluctuations . . . masquerading as\nBy 1950s, a linear path to measuring change was finally\nStone, 1979). Around this time, mathematical philosophers\nfinally clarified logical foundations for nonphysical scientific\nmeasurement with probabilistic simultaneous conjoint addi-\ntivity theory (Luce & Tukey, 1964), which brings subjective\nmeasurement in line with classical criteria such as additive,\nconcatenated units. Social measurement models under this\ntheoretical framework now demonstrate additive units in a\nprobabilistic framework and explicit empirical operations for\ncorroborating qualitative sensory experiences with them. In\nother words, through an articulation between logic and empir-\nical operations, both logical structure and meaningful qualita-\ntive relations can now be established between numerical scale\nmagnitudes and perceived experience. Instead of frustration,\neducational and psychological measures are now commensu-\nrable with traditional scientific methods when empirical\nobservations fit conjoint additive models.\nMajor linear model elaborations of Rasch models to mea-\nsure change are now also provided by multidimensional ran-\ndom coefficients multinomial logit model (Adams, Wilson,\n& Wang, 1997), general multicomponent latent trait model\nand linear logistic model for measuring change (Fischer,\nclass of probabilistic measurement models distinguished by\nlinear units that support axiomatic scale operations with\nmodest sample requirements and few assumptions.\nPresent Research\nStudies of Change\nStudies comparing ordinal and linear units have found dis-\ntortions in upper and lower tails of ordinal scores with\n8 SAGE Open\nimplications for measuring change (Bezruczko, 2004;\non population parameters raises concerns about objectivity.\nIn contrast, objective linear units transcend populations\nbecause their link functions are mathematical abstractions,\nwhich assert universal applications. Consequently, mathe-\nmatical measurement models, in principle, offer measures\nthat are not limited by available sample observations.\nDespite mathematical differences between ordinal and\nlinear units, prior studies seemed to support at least limited\ninterchangeability for describing change (Bezruczko &\nFatani, 2010. Several general principles from that research\nfollow below:\n1. Pre\u00adpost differences. When pre- and post-test ordinal\ndifferences were near test mid-range, simple gains\ntypically corresponded well to linear gains.\n2. Items on target. Test targeting strongly influenced\nordinal and linear correspondence. Ordinal scores\nand linear measures were very consistent when item\ndifficulties were aligned with sample ability, which\ntends to diminish scores in distribution tails.\n3. Variability of gain. Uniform ordinal gains between\npre- and post-assessments corresponded well to lin-\near gains, while highly variable ordinal gains, in gen-\neral, did not.\n4. Predictable distortions. Ordinal gains in upper and\nlower score distribution tails were predictably incon-\nsistent with linear gains. Ordinal scores in lower tail\ngenerally overestimated gain, while scores in upper\ntail underestimated gain. These results are consistent\nIn general, key limiting conditions on correspondence\nbetween ordinal and linear were initial (pre-test) scores,\namount of gain, and test difficulty. Very low and very high\npre- and post-test scores presented major inconsistencies with\nlinear gain. Otherwise, results cautiously supported inter-\nchangeable use of linear and ordinal scores to describe gains.\nPurpose\nPresent research reexamined summary presented above by\ncomparing three widely implemented methods of measuring\ngain, namely ordinal scores, residualized raw scores, and\nRasch model linearization of ordinal scores. An intention\nwas to demonstrate through graphical comparisons compa-\nrable patterns of ordinal and linear gains, which could sup-\nport their complementary use for reporting change or gain.\nFirst, results from these gain methods (ordinal scores,\nresidualized gains, and linear Rasch logits) were compared\nfor a locally developed, standardized preschool skills survey\nthat had been parameterized, then supported by reliability\nand validity studies. Raw score summation then provided a\ntotal score for computing simple gain and residualized gain,\nwhich is also a sufficient statistic for estimating Rasch model\nparameters. These methods then were compared for a nation-\nally normed, commercially produced preschool assessment.\nPresent research addresses the following questions:\n1. Ordinal scores versus linear gain: Are preschool\ngains between fall and spring based on ordinal scores,\nresidualized scores, and linear measures generally\ncomparable? Do their differences affect objective,\nnumerical descriptions of student gain? In the broader\ncontext of health care evaluation, would these differ-\nences have implications for measuring patient status?\n2. Situational determinants. Under what conditions\nmight ordinal and linear gain measures be interchange-\nable? What practical rules might guide their comple-\nmentary use for reporting purposes? Conversely, what\nviolations or limitations restrict their validity?\nMethod\nSample\nAssessments were collected from more than 1,500 randomly\nselected children attending urban neighborhood preschools\nin Chicago, United States (4-year-olds, n = 1,548; N ~\n25,000). Preschools were first randomly sampled across pre-\nschool programs (Head Start, Preschool for All, Community\nPartnerships, etc.) and stratified by race and income, then\n4-year-old children were randomly selected from each pre-\nschool. In general, preschools were located in socioeconomi-\ncally disadvantaged, multicultural neighborhoods. Ethnic\ncomposition was distributed almost equally across Whites,\nAfrican Americans, Hispanics, and Asians. Approximately\nhalf of children were from non-English backgrounds. Only\nchildren enrolled both fall and spring were included in this\nresearch.\nData\nDistrict-based preschool learning assessment.Preschool Mini-\nAssessment (PMA; Caradine & Borger, 2010) was developed\nby the Office of Early Childhood Education, CPS, to improve\nalignment between preschool learning and assessment goals.\nPMA consists of 18 standard interview items organized\naround several preschool activities conducted with props and\nmanipulates. This emphasis on an engaged interview format\ndiminishes random responses, which is especially problem-\natic with preschool children. PMA items are distributed\nequally across the following content: (a) sound and letter\nidentification, (b) rhyming, (b) sorting, (c) comprehension,\n(d) print awareness, and (e) story retelling, which were sys-\ntematically sampled from State of Illinois, Department of\nEducation learning domains. In addition, children were\nrequested to write their name; hence, name writing was also\nscored. Expert judges confirmed that skill sequences were\nBezruczko et al. 9\nrepresentative of learning in CPS preschools.\nMost PMAitems were dichotomously scored (0/1) though\ntwo items required polytomous scoring (0, 1, 2, 3, and 4).\nMaximum PMA score was 24 points. Factor analysis during\nscale development yielded a single dominant dimension\n(eigenvalue = 4.7), and PMA score reliability (alpha) with\nthis population is typically moderate (~.80). Concurrent and\ncriterion validation correlations with Woodcock\u00adJohnson\nand Peabody Picture Vocabulary Test (PPVT) are consis-\ntently positive, between .30 and .40, which supports unique\nstructure.\nNorm-referenced standardized achievement test.Woodcock\u00ad\nJohnson III (WJ III) is a commercially prepared assessment\nof preschool learning that was based on Rasch measurement\nprinciples of explicit, linear units (McGrew, Schrank, &\nWoodcock, 2007; Woodcock, Schrank, Mather, & McGrew,\n2007). Children are individually presented table-top flip\ncharts and directed \"to point\" at specific images. Word Letter\nIdentification subtest (76 items) was selected from the\nachievement battery for this research. Item responses were\nscored dichotomously (0/1), and number correct was summed\nfor total score. According to published documentation, WJ\nIII validity was based on developmental predictions that\nwere empirically examined in a national sample during test\ndevelopment. Its property of equal interval units defined by a\nhierarchy of items was the central purpose for including it in\nthis research.\nProcedure\nIdentical PMA observations were collected at fall and spring\nassessments. Trained PMA evaluators met with individual\nchildren and marked responses on a standard interview form.\nWJ III was also collected in fall and spring according to stan-\ndard manual procedures.\nAnalysis\nMeasurement of change was calculated by three methods: (a)\nordinal (spring\u00adfall) gains, (b) residualized raw score gains,\nand (c) linear Rasch model measures. Both raw score deltas\nand residualized gain methods continue to be recommended\nfor measuring change (Llabre, Spitzer, Saab, Ironson, &\nSchneiderman, 1991), and social science research literature\nis replete with contemporary applications (Salkind, 2010).\nMissing values. High transiency in socioeconomically disad-\nvantaged neighborhoods presented issues concerning miss-\ning values. After reducing data to children only present at\nboth fall and spring assessments, remaining cases of PMA\nscores missing at random were less than 10%. They were\naddressed with mean substitution. PMA mean comparisons\nbefore and after imputation did not show significant\ndifferences.\nOrdinal gain. In this research, simple difference scores were\ncalculated, where X = pre-test score, Y = post-test score, and\nD = difference score (Zimmerman & Williams, 1982a):\nD =Y-X,\nand they show reliability of gain can be high for effective\ntreatments that demonstrate incremental increase in true\nscore variance of Y. Zimmerman and Williams (1982a) also\nprovide the standard error of simple differences:\n    \nxx yy\n( ) ( )\n\n\n\n ,\n' '\nwhere  = \nx\n/ \ny,\nreliability of X = \nxx\n, and reliability of Y =\n\nyy\n.\nResidualized gain. Residualized gain was described by Cron-\nto remove pre-test variation from post-test scores, so \"true\"\ngain remains:\nResidualizing removes from the posttest score, and hence from\nthe gain, the portion that could have been predicted linearly\nfrom pretest status. One cannot argue that the residualized score\nis a \"corrected\" measure of gain, since in most studies the\nportion discarded includes some genuine and important change\nin the person. The residualized score is primarily a way of\nsingling out individuals who changed more (or less) than\nResidualized gains are more complicated than simple\ngain because an artificial linear construct, Y, is defined by\nthe regression of X on Y. Then, the residuals, Y \u00ad Y, which\nare uncorrelated with X and presumably related to true gain,\nare commonly substituted for observed gains, Y - X. The\nresidual gain model in this study was discussed by\nZimmerman and Williams (1982b), where Y is the regres-\nsion of X on Y:\n'\nY = E Y +\nCov X, Y X -E X\nVar X\n,\n( )\n( ) ( )\n\n \n\n\n\n\n\nwhere E denotes the expectation of post-test scores Y, Var is\nthe variance of pre-test X, and Cov is their covariance\nimplemented this model to estimate Y, and a residualized\ngain score was obtained from Y - Y.\nRasch model foundations.Rasch models were originally\ndeveloped for transforming ordinal scores to objective, linear\nman data structure, which is already familiar to early child-\nhood researchers (Puranik & Lonigan, 2011). Guttman\nscales, however, assume an unrealistic deterministic relation\nbetween scores and human behavior that is addressed by\n1989). Specifically, Rasch models implement an empirical\nconcatenation procedure called simultaneous probabilistic\nconjoint additivity during Rasch model parameter estima-\ntion, which constructs a measurement dimension conforming\nto classical scientific traditions purportedly with \"fundamen-\ntant property of Rasch models is statistical separation of\nitems and observations (responses) during estimation, which\nestablishes a framework useful for monitoring item invari-\nance, while allowing a separate child parameter sensitive to\ngrowth and learning to vary.\nFirst, WJ III ordinal scores were transformed to linear\nprobabilistic item and person parameters with a Rasch model\nfor dichotomous items (Wright & Stone, 1979, see also\nWright, 1997). Following expression presents probability of\npassing an item when WJ III responses are dichotomously\n\n \n \n-\n-\nn i\nn i\n=\nexp\n( )\n( ),\nwhere probability of person n responding 1 to item i is repre-\nsented by the parameterized differences between person abil-\nity \nn\nand item difficulty \ni\ndivided by its inverse [1 +\nexp(\nn\n\u00ad \ni\n)]. It can be shown from this formulation that stu-\ndents' total test scores can be transformed to locations on a\ncommon, linear scale where log-odds (logits) represent uni-\nform unit increments of child ability and item difficulty. Fall\nand spring assessments were not statistically equated but\ncalibrated simultaneously in a stacked data file with Winsteps\nPMAscores were transformed with a Rasch model for rat-\ning scales, which, like the dichotomous model, is mathemati-\ncally based on differences between  and  (Wright &\nMasters, 1982). The rating scale model, however, also esti-\nmates \nj\nwhich is a rating scale step parameter. Tau is the\nability needed to pass from one rating category to the next,\nand PMA rating steps j ranged from 0 to 5. PMA scores were\nlinearized with the following expression:\n\n \n \n\n\nnix\nn i j\nx\nn i j\nk\nm\n=\nexp +\nexp +\n[ ( )]\n[ ( )]\n-\n-\n\n\n\n,\nwhere  = child ability,  = item difficulties, and  = rating\nscale thresholds for j categories. \nnix\nis the probability child n\nwill be rated in category x on item i, where x takes a value\nfrom a fixed range (j = 0, 1, 2, 3, 4, 5, . . . m), m = number of\nresponse thresholds or steps for an item, and k = ith step.\nModel prediction (P) for each item and observed ratings (O)\nare statistically analyzed for significant departures from\nexpectation (O-P) and their differences are standardized by\nthe estimated binomial standard deviation and examined with\nan aproximated chi-square (see Wright and Stone, 1979).\nRegression on linear gain. Following model was implemented\nto investigate statistical relations of ordinal gains and residu-\nalized gains with linear gains:\n\nY = a + b X + b X + b X + b X\n,\nwhere Y = linear gain computed after transformation of\nordinal gain to logits, X\n= ordinal gains, X\n= residualized\ngains, X\n= fall ordinal scores, X\n= spring ordinal scores.\nResults\nRasch Model Calibration\nOrdinal scores in this research were linearized with a Rasch\nmodel, which mathematically transforms ordinal scores to\nlog-odds or logits with a one-parameter logistic function.\nConsequently, a high priority was confirming statistical fit\nof both items and children to the measurement model.\nResults here found virtually no evidence of misfitting chil-\ndren, which is not surprising given the highly controlled\nprocedures for collecting child responses--both PMA and\nWJ III responses were collected by technicians with scripted\ninstructions. However, item fit evaluation found four PMA\nitems with significantly larger than expected standardized\nfit t values, and they were deleted from this study. Therefore,\nFigure 1 presents transformation of PMA, as well as WJ III,\nordinal scores to linear measures after deleting PMA misfit-\nting items. Otherwise, PMA here demonstrated typical\nogive relations between scores and linearized values, while\nWJ III shows somewhat higher than expected correspon-\nAnother Rasch model concern was construct comparabil-\nity between fall and spring assessments (see Engelhard,\n2013). Figure 2 presents empirical corroboration of the mea-\nsurement construct, which shows 12 of 15 PMA items statis-\ntically invariant with 95% confidence, and similar results\nwere obtained for WJ III. These results demonstrate replica-\ntion of the PMA item hierarchy between fall and spring,\nwhich is essential for valid measurement of learning and\ngrowth. Finally, another concern was clarification of dimen-\nsionality threats presented by item dependencies, which were\ninvestigated with principal components analysis of Rasch\nmodel residuals after item calibration. Those results revealed\nthat only 7.8% of total item residual variance was associated\nwith nonrandom structures, which support item indepen-\ndence. Residual analysis summary, as well as other results\ndescribing Rasch model parameterization, is elaborated in a\nsupplement to this report.\nSummary of Scores and Measures\nTable 1 presents fall and spring PMA and WJ III assessment\nresults. In general, scores were normally distributed, but\nWJ III results were very low though without floor effects.\nAs expected, average spring scores were significantly\nhigher, and respective fall and spring total score correla-\ntions were positive. In general, PMA and WJ III standard-\nized effect sizes were comparable between ordinal and\nlinear methods. Alpha reliability was higher for WJ III\nCorrelations Between Fall and Spring\nTable 2 elaborates correlations, and, as expected, PMA ordi-\nnal scores and logit measures between fall and spring were\npositively correlated at .62 and .62, respectively. In addition,\nPMA fall ordinal scores were correlated with fall logits, .98,\nand spring ordinal scores were highly correlated with spring\nFall WJ III ordinal score correlation with fall logits was\nvery high, .99, while spring ordinal scores and spring logits\nwere correlated at .97. As expected, residualized scores,\nwhich are deviations from predicted values when X (fall)\nwas regressed on Y (spring), were uncorrelated with either\nfall ordinal scores or logits. Moreover, PMA and WJ III fall\nordinal scores presented very high correlations with pre-\nFigure 1. \"Scores to measures\" plot showing PMA and WJ III ordinal scores transformed to logits.\nNote. Arrows point to mean of test difficulty and sample ability. These data show that WJ III was significantly off-target. PMA = Preschool Mini-\nAssessment; WJ III = Woodcock\u00adJohnson III.\nFigure 2. Bi-calibration (logits) plot of fall and spring PMA items.\nNote. PMA = Preschool Mini-Assessment.\nCorrelations With Gain\nIn general, fall scores and logits showed negative correla-\ntions with gains though WJ III correlations might arguably\nbe considered random. Negative gain correlation with ini-\ntial status appears in the literature (Rogosa & Willett,\n1985), but systematic differences in magnitude between\nordinal and linear values have not been reported. PMA and\nWJ III ordinal gains correlated with logit gains at .93 and\nResidualized score correlation with PMA ordinal gain\nwas positive at .85. As expected, correlation of residualized\nscores with fall scores and logits was .0. Correlation of resid-\nualized scores with WJ III ordinal gain was also positive but\nunexpectedly high, r = 1.0. However, both PMA and WJ III\nordinal gains presented negative correlations with Y, though\nPMA was significantly stronger, r = -.53. In other words,\nwhile Y defined a positive linear relation between X and Y,\ndifferences between Y and X (Y - X) tended to decrease. In\nTable 1. Summary of Scores and Measures.\nFall Spring\nGain Stand. ES p < Ceiling Floor Off-target Missinga  reliability\nNote. Sample sizes for PMA and WJ III were 1,548 and 1,522, respectively. Stand. ES = standardized effect size and was computed using (Y \u00ad X) / [(SD\nY\n+\nSD\nX\n) / 2; PMA = Preschool Mini-Assessment; WJ III = Woodcock\u00adJohnson III, where X = fall and Y = spring scores. Gain = (Y \u00ad X) Maximum PMA total\nscore is 15 points, and WJ III Word Letter Identification subtest total score is 76 points.\naMissing values: Mean substitution was imputed for ordinal scores missing at random. Expectation Maximization (EM) algorithm was implemented in\nWinsteps software during Rasch model parameter estimation to impute missing values (Linacre, 2015).\nbFour misfitting items were deleted during Rasch calibration of PMA items. Therefore, PMA is represented by 15 items.\nTable 2. Correlations Among Simple Ordinal Scores, Residualized Scores, and Rasch Logit Measures.\nOrdinal scores Logit measures\n Fall Spring Gain Fall Spring Gain\nPMA\n Ordinal scores\nGain -- -- -- -- -- .93\nLogits\n Ordinal scores\nGain -- -- -- -- -- .94\nLogits\nNote. PMA (n = 1,548) and WJ III (n = 1,522), logits were estimated with a Rasch model. All gains represent Y \u00ad X, X = fall and Y = spring. Residualized\ngain was defined by (Y - Y), where Y is predicted by regression of X on Y. PMA = Preschool Mini-Assessment; WJ III = Woodcock\u00adJohnson III.\nconcrete terms, as the convergence of X and Y on the regres-\nsion line defined a linear predictive construct, Y, fall item\ndifficulties increased; hence, magnitude of gain for this sam-\nple declined.\nRegression on Linear Gain\nStepwise regression of ordinal gains and residualized scores\non linear gain (logits) accounted for 86% of logit gain vari-\nance (R2 = .86). However, residualized scores and ordinal\ngains also demonstrated unique contributions (sr2 = .08 and\n.42, respectively), which were statistically significant (p <\n001). Their unique contributions suggest gain fragmentation\nduring PMA assessment. In contrast, unique variance could\nnot be associated with either ordinal or residualized scores\nwhen regressed on WJ III linear gain though R2 was higher,\n88%. These results, especially fragmentation of PMA linear\ngains, raise a question whether underlying unit structure\ninfluences agreement among methods of measuring gain.\nTable 3 presents regression results.\nTable 3. Regression of Ordinal and Residualized Gains on Linear Measures (Logits).\nB SE  t p < R2 change p < R2 total sr2a\nPMA\n Residualized gains -- -- -- -- -- -- -- -- --\nNote. PMA = Preschool Mini-Assessment; WJ III = Woodcock\u00adJohnson III.\naSquared semi-partial is portion of variance uniquely associated with a predictor.\nFigure 3. PMA and WJ III intra-child change.\nNote. PMA = Preschool Mini-Assessment; WJ III = Woodcock\u00adJohnson III.\nFigure 4. PMA ordinal gains plotted with logit gains.\nNote. Each ordinal value corresponds to a range of linear values. PMA = Preschool Mini-Assessment.\nIntra-Child Change\nFigure 3 presents ordinal scores and logits between fall and\nspring assessments. In general, PMA logits were denser\naround the mean than ordinal scores, and higher and lower\nscoring children were systematically located further out from\nthe mean, which would be consistent with intended score\ncorrection in upper and lower tails during ordinal score trans-\nformation. In general, WJ III results showed substantial neg-\native skew of scores and measures, and relatively few\nchildren shifted from low to high between fall and spring\nassessments. Shape and form of WJ ordinal scores appear\nsomewhat more consistent with linear measures than PMA.\nOrdinal Versus Linear Gain\nFigure 4 offers additional insights into ordinal and linear gain\nmeasurement by plotting respective PMAgains for each child\nbetween fall and spring. In general, logit gains show a range\nof corresponding ordinal gain values. For example, children\nwith exactly 0 logit gain show a range between 0 and 1 ordi-\nnal score gain, but surprisingly, several children with 0 ordi-\nnal gain demonstrated positive logit gain. Recall that ordinal\nscores uniformly assign 1 unit (point) to every item, while\nlogit estimation is governed by a mathematical function,\nwhich assigns values based on item difficulty--more difficult\nitems have higher item logit values. In other words, simple\ngain is \"disembodied\" from the context of total scores and\nmarginal gain, which creates an entity independent of under-\nlying scale magnitude. Consequently, ordinal gain gathered\nfrom across the ability range can be represented together for\nany measure of linear gain. Nonetheless, these results show\nsurprising consistency between ordinal score gain and cumu-\nlative logit gain. Approximately, three items were needed to\ndemonstrate continuous positive PMA gain across the param-\neterized construct.\nFigure 5 presents WJ III results, which show that 0 ordinal\nscore gain here corresponded exactly to 0 logit gain and, in\ngeneral, positive ordinal score gain corresponded to positive\nlogit gain. Moreover, 10 points of ordinal score gain corre-\nsponded to a range of linear gains between 3 and 7 logits. Exact\nrelationship for any particular child again depended on initial\nfall status and specific item difficulties. Children with lower\ninitial ordinal scores corresponded to lower logit gains, while\nthose with higher initial scores corresponded to higher gains\nbecause those children probably passed items of much higher\ndifficulty. These results indicate reasonable gain correspon-\ndence between WJ III ordinal and linear values of almost 15\nordinal score points. This relation between ordinal scores and\nlinear units, however, \"breaks up\" around ordinal gains of 25\npoints, which is probably an artifact of this less able sample.\nResidualized Scores\nFigure 6 presents residualized scores with ordinal and logit\ngains, respectively. As expected, residualized scores pre-\nsented generally positive relations with ordinal and logit\ngains. There is some question here whether the obtained high\npositive correlation between residualized and ordinal gains\nmay be mediated by the higher positive correlation between\nfall ordinal and Y. Predicted Y here was identical to fall\nordinal scores (r = 1.00); hence, residualized gains (Y - Y)\nwould be identical to ordinal gains (Y - X).\nDiscussion\nMeasuring change is central to scientific investigations, and\nprominent in growth and learning studies, psychological\nappraisal, and PROs. In the 1950s, Cronbach brought attention\nto individual change and seriously questioned whether it\nshould be measured. That period of doubt has passed, and\nWestern societies have shifted to a post-industrial economy\nwhere measuring change is fundamental to efficiency across\neducation, psychology, and health care. Measurement of\nhuman capital, its formation, infrastructure, and return are\ndependent on accurate measures of change. Tension between\ntechnologies that address those needs and reasonable layper-\nson expectations to understand methods of measuring gain in\ncommon sense terms has motivated this research. A central\ngoal here is to clarify correspondence between ordinal and lin-\near gains during early childhood and preschool assessments.\nSocial science research methodology for most of the past\ncentury has tended to dismiss underlying irregularities of\nordinal scores, specifically distortions in upper and lower\ndistribution tails, by emphasizing robustness of group param-\neters in statistical analysis. For empirical applications that\nemphasize static performance relative to norm groups, this\nstrategy may be surprisingly adequate. In general, results\nhere suggest that ordinal score status associated with dichot-\nomously scored items can be meaningfully described to lay-\npersons if appropriate verification has been conducted with a\nmathematically defined linear model. Present research, in\nfact, endorses ordinal score reports when they demonstrate\nconvergence with linear measures. In this research, ordinal\nscores were found to be virtually indistinguishable from lin-\near measures, which offers convenient opportunities for sim-\nple measurement of growth and learning.\nStudy Questions\nLearning in fact did not occur across the entire PMA ordinal\ngain score range. For example, children showing less than\nthree items of PMA ordinal gain between fall and spring\nprobably did not demonstrate learning though ordinal score\ndifferences were positive. However, once three PMA items\nwere reached, relations between PMA ordinal and linear\ngains were surprisingly uniform. Likewise, WJ III ordinal\nscore and linear gains showed even broader agreement. WJ\nIII gain between 0 and 25 ordinal items demonstrated a\nFigure 5. WJ III ordinal score gains plotted with logit gains.\nNote. Although WJ III ordinal and linear gain correlation was high, .94, linear values were associated with a range of ordinal gains. These results show\nordinal scale integrity and specifically correspondence to logits degenerate dramatically as gains increase. PMA = Preschool Mini-Assessment.\nmonotonic correspondence to linear gain between 0 and 18\nlogits. This correspondence did not degrade until gains\nreached upper WJ III levels, which probably reflects ability\ninfluence of this sample on statistical range. In general, this\nresearch found useful correspondence between ordinal and\nlinear gains across major portions of the PMA and WJ III\nachievement dimensions.\nNegative correlation with gain.A negative correlation between\nfall PMA and gain, both scores and measures, indicated that\nsome students with low fall scores demonstrated proportion-\nately more gain than higher ability students. PMAnegative cor-\nrelation here was exacerbated by many children scoring low at\nboth fall and spring assessments where ordinal and linear val-\nues present their greatest discrepancy. While counterintuitive,\nthis relation between ability and gain frequently occurs in edu-\ncation because difficult items require more ability than easier\nitems, but socioeconomically disadvantaged samples have\nfewer higher ability children to pass them. As the response dis-\ntribution shifts between fall and spring, challenge of more dif-\nficult items for lower ability children becomes apparent and\ngain declines. In addition, less able children frequently show\n\"surges\" on easier items at initial assessment, which give an\nillusion of large, immediate ordinal score gains because ordinal\nscore units are smaller. A similar problem occurs with PROs in\nhealth care when lower functioning patients are targeted for\nintervention. Distortion in lower ordinal score tail would give\nan illusion of effective patient treatment.\nIn contrast, WJ III alleviated negative gain correlation by\nproviding children ample opportunities to pass items within\nFigure 6. Residualized gains plotted with ordinal and logit gains, respectively.\nNote. PMA = Preschool Mini-Assessment; WJ = Woodcock\u00adJohnson.\ntheir ability zone. Although this sample was very low, WJ III\nconsists of many items directly targeting lower ability, which\nvirtually eliminated the negative correlation of initial status\nwith gain.\nResidualized gains.A rationale for residualized scores is to\nshift the measurement framework from differences between\npost (spring) and initial (fall) status assessment to post-\nassessment and the regression line, which conveniently elim-\ninates negative correlation between initial status and gain\ndescribed above. The regression line imposes uniformity on\ninitial status without destroying the underlying order neces-\nsary to represent valid change. In this research, WJ III ordi-\nnal and linear units were so highly correlated that fall ordinal\nand Y were identical, which eliminated initial status, so\nordinal gain was equal to residual gain. In other words,\nresults here suggest that under conditions of high initial cor-\nrelation and uniform units, ordinal and residual scores con-\nverge on linear units. These results point to dramatic\ncoherence among these three perspectives on measuring\nchange, opening the possibility of theoretical integration of\nmathematical measurement with GLM, which currently\nlacks a measurement theory.\nOrdinal and linear disagreement. Results here suggest the ques-\ntion of comparability between ordinal and linear measures\nshould be considered cautiously. Disagreement between ordi-\nnal gain, residualized scores, and linear gain was roughly 10%\nto 15% when items are scored dichotomously, and practical\nimplications of these differences have not yet been explored.A\nconsequence is general principles to guide an articulation\nbetween scores and measures are still weak. Although corre-\nspondence between ordinal and linear gains was remarkably\nhigh, that correspondence depended on several conditions that\nmay vary unpredictably sample to sample and are profoundly\ninfluenced by instrument properties. This research proposes\nseveral rules to improve correspondence between ordinal\nscores and linear measures for describing gain:\n1. Minimize off-target assessments. Off-target assess-\nments especially with low-performing samples tend to\ninflate raw score gains between fall and winter, while\nunderestimating gain for higher performing cases.\nUnless units are highly regular, off-target assessment\ndoes not support complementary ordinal and linear\ngain interpretations. Much of what currently is consid-\nered ordinal score gain in educational studies and pro-\ngram evaluation may be simply off-target samples\nconfounded by unequated assessments. This issue is\nespecially pernicious for lower scoring populations\nwhere this combination leads to an illusion of growth\nand learning when there may be none and makes virtu-\nally any intervention look marginally effective.\n2. Evaluate correspondence. Assessments that rely pre-\ndominantly on ordinal score descriptions should\nconduct empirical studies that establish minimum\nnumber of score points corresponding to linear gain\nin context of item standard errors. These results then\ncan provide logical foundations for communicating\ngains and learning in ordinal score units.\n3. Standards for ordinal score assessments. The sim-\nplicity and intuitive appeal of ordinal gains under-\nstandably support their use throughout the social\nsciences. Consequently, some development of stan-\ndards could improve consistency if not transparency\nof measuring change with ordinal scores, as well as\nestablish greater sensitivity to sample characteristics\nand instrument properties.\nIn conclusion, ordinal scores and linear units fundamen-\ntally differ, yet results reported here were surprisingly coher-\nent about the possibilities and opportunities to articulate gains\nbetween them. The separate sciences that rationalize these\nmethods, in principle, need not create irresolvable conflicts.\nThese results, in fact, suggest the inherent order of scores and\nratings and rigorous precision of exact linear units can articu-\nlate simple gains in a meaningful and transparent manner.\nFurther empirical investigations are encouraged to better\nunderstand the conditions that optimize their correspondence,\nand clarify their agreement, which should alleviate the cloud\nof confusion that has shrouded social science research for\nmany decades. This shift to understanding should make their\ncorrespondence explicit hence clarify their respective contri-\nbutions to the growth of scientific knowledge.\nFuture Research\nFuture studies are needed to clarify contextual characteris-\ntics that mediate ordinal gain scores. Replication of present\nresearch with child samples of higher ability and broader\nsocioeconomic background would clarify generality of cor-\nrespondence between ordinal scores and linear measures\nfound here. Likewise, agreement of gain methods and\nimplications should be investigated further with rating\nscales and Likert-type responses, which are widely assumed\nto have linear properties but without objective empirical\nfoundations.\nWhile general importance of these results for health care\nwas emphasized in this report, future studies are needed with\ndedicated patient populations. Chronically ill samples and\nhighly skewed symptom populations present substantial chal-\nlenges to simple gain measurement. In general, additional\nstudies of gain measurement are needed for insights they may\nprovide into the current reproducibility crisis reported in\nbehavioral research (Open Science Collaboration, 2015).\nAuthors' Note\nPortions of this article were presented at the 75th annual meeting of\nNational Council on Measurement in Education in San Francisco.\n"
}