{
    "abstract": "Abstract\nQualitative research plays a vital role in political development and in the design of statutes and directives. Consequently,\nensuring the quality of this research is important. However, the current literature on evaluation of quantitative and qualitative\nresearch does not reflect this importance, and we have identified a need to establish guidelines for evaluating qualitative\nresearch projects for quality. Therefore, and based on existing research, we have developed a framework for assessing\nthe research quality of large complex projects using qualitative methods. In our study, as presented in this article, we\noperationalize and apply the framework to evaluate six specific methods in the large European research project, Secured\nUrban Transportation--A European Demonstration (SECUR-ED); each method is assessed according to the quality criteria\nof \"transferability,\" \"systematic design/reliability,\" and \"transactional validity.\" Overall, we find that half of the SECUR-ED\nproject methods demonstrate thorough documentation and transferability, and that half of the methods lack consistent\nusage and therefore score low on both reliability and validity. We also find that one method, the capacity mapping matrix,\nscores high on all quality parameters. Accordingly, we suggest that future European Union (EU) projects replicate the\ndocumentation efforts demonstrated in relation to several of the SECUR-ED methods, and consider the capacity mapping\nmatrix as \"best practice\" standard. We conclude that the framework represents a novel approach to quality assessments of\nqualitative project methods across research topics and contexts.\n",
    "reduced_content": "journals.sagepub.com/home/sgo\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License\n(http://www.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nIntroduction\nQualitative research plays a vital role in political develop-\nment and the design of statutes and directives, including\nresearch projects funded by the European Commission.\nConsequently, ensuring the quality of this research is impor-\ntant. Moreover, a large and still growing body of scientific\nliterature addresses the issue of quality and quality criteria in\nresearch. Within this body, greater agreement exists regard-\ning quality criteria in quantitative research compared with\nqualitative, particularly in terms of validity and reliability\n(Bryman, Becker, & Sempik, 2008). An explanation for this\nstate of affairs lies in the diagnostic exploratory nature of\nqualitative research (de Ruyter & Scholl, 1998; Kapoulas &\nMitic, 2012), which rests on the belief that the social world is\ncomprised of subjective experiences and understandings that\ncan change over time and social contexts (Dew, 2007). This\nmultifaceted, contextual nature of qualitative/exploratory\nresearch makes it difficult to establish common quality crite-\nria, as reflected in the lack of consensus among researchers\nregarding how to approach quality criteria in qualitative\nHowever, clear guidelines for how to judge the quality of the\nparticular research account or project remain desirable\nThe lack of clear quality guidelines in qualitative research\ninspired us to design a framework for assessing the research\nquality of qualitative project methods employed in the\nEuropean Commission financed project Secured Urban\n1University of Stavanger, Norway\n2The Norwegian Water Resources and Energy Directorate, Oslo,\nNorway\n3Fraunhofer Institute for Industrial Engineering, Stuttgart, Germany\nCorresponding Author:\nSindre A. H\u00f8yland, Faculty of Social Sciences, Centre for Risk\nManagement and Societal Safety (SEROS), University of Stavanger, 4036\nStavanger, Norway.\nEmail: sindre.hoyland@uis.no\nDeveloping and Applying a Framework\nfor Assessing the Research Quality of\nQualitative Project Methods in the EU\nProject SECUR-ED\nSindre A. H\u00f8yland1, Janne M. Hagen1,2, and Wolf Engelbach3\n Keywords\nresearch quality, project methods, quality assessment framework, qualitative assessment criteria, social sciences\n2 SAGE Open\nTransportation--A European Demonstration (SECUR-ED).\nThe SECUR-ED project, which ran from April 2011 to\nSeptember 2014, was a complex security demonstration\nproject involving 40 European partners across industry,\nresearch, and transport operations. The project aimed to pro-\nvide public transport operators with a set of tools to improve\nurban transport security, identified through development and\ntesting of a number of security capacities (which can be\nsecurity processes, technological tools, or training\napproaches) in demonstration scenarios in several European\ncities. The project came up with security solutions and rec-\nommendations that can bring value to public transport secu-\nrity stakeholders, as outlined in the \"White Paper for Public\nTransport Stakeholders\" (SECUR-ED, 2014b).\nThe SECUR-ED project applied a wide range of project\nmethods, including common research methods such as inter-\nviews, questionnaires, and literature studies; project manage-\nment methods such as mailing lists, cooperation portals,\nphysical meetings, and teleconferences; methods that sup-\nport project coordination, cooperation, and collaboration;\nand methods developed for the transport operator partners\n(capacities/solutions). However, the assessment framework\npresented in this article had a different focus on methods; it\nwas designed by the authors to study the qualitative methods\nthat the SECUR-ED project developed to handle its size and\ncomplexity. The six selected methods are qualitative by\nnature: common glossary, interoperability notation, scenario\ndescription with business process model and notation\n(BPMN), capacity mapping matrix, demo city dashboards,\nand capacity evaluation approach. (The following sections\nwill discuss these evaluations and methods.)\nThe agenda for this article is to describe our study and\napproach to the development of the assessment framework,\nthe framework itself, and application of the framework to a\nselection of SECUR-ED project methods. In addition, we\ndiscuss the outcome of our assessment process, including the\nnovelty and possible broader application of the framework,\nto assess similar research projects. It is important to note that\nour analysis did not address whether the methods would have\nbeen sufficient or necessary to achieve the SECUR-ED proj-\nect's overall goals regardless of how well they were imple-\nmented. In other words, our assessments do not say anything\nabout the degree of success of the SECUR-ED project as a\nwhole.\nMethod\nFor an overview of current research into quality and quality\ncriteria, we searched electronic online databases, such as\nAcademic Search Elite, ScienceDirect, Web of Science, and\nSwetsWise, for combinations of the key words: research\nquality, qualitative research, validity, and reliability. Based\non the resulting publications and reference lists (snowball-\ning) (Greenhalgh & Peacock, 2005; Webster & Watson,\n2002), we identified the quality criteria of transferability,\nsystematic design/reliability, and transactional validity,\nwhich we integrated into a framework to assess the quality of\nthe SECUR-ED project methods. The criteria identification\nand framework integration are described and illustrated in\nthe next section.\nAs partners of the SECUR-ED consortium, we had access\nto working papers in progress and peer-reviewed research\nreports from the project that described and discussed\nSECUR-ED methods, which were evaluated in our study.\nThese reports constituted important empirical data for our\nevaluation of the SECUR-ED methods and for developing\nand testing the framework. In addition to these documents,\nwe had access to project partners, covering researchers,\nindustry partners, and transport operators. Specifically, indi-\nviduals involved in the development and application of the\nmethods provided oral and written input based on their expe-\nriences. Also, our assessments of each method as well as dif-\nferent versions of our assessment framework were presented\nto project members (including dedicated reviewers) on sev-\neral occasions, such as in meetings, teleconferences, and\nemails. Thus, written and oral contributions were succes-\nsively collected and triangulated over time. In sum, we\nobtained specific rich information to enable comparison and\nreview of a selection of SECUR-ED methods.\nDeveloping the Assessment Framework\nAs described, qualitative research provides important\ninsights and input for development of policies and statutes,\nwhich implies the need for guidelines to judge the quality of\na particular research account or project. Accordingly, in this\nsection, we argue for the identified assessment criteria and\nthe integration of the criteria into a framework to assess proj-\nect methods.\nAssessment Criterion: Transferability\nPolit and Beck (2010) suggested that \"generalization is an\nact of reasoning that involves drawing broad conclusions\nfrom particular instances--that is, making an inference\nabout the unobserved based on the observed\" (p. 1451).\nSome qualitative researchers have questioned generaliz-\nability of any type of findings. In their view, findings are\nalways embedded within a context, making it problematic\nto extrapolate \"the particular\" (Erlandson, Harris, Skipper,\nnate approach of transferability should be considered.\nTransferability, or the case-to-case translation model in\nFirestone (1993), assumes that the researcher's job is to\ndescribe the time and context in which the particular find-\nings were true, whereas the reader's job is to determine the\nextent to which the findings may apply in another context\n(Polit & Beck, 2010). Therefore, the degree of transferabil-\nity between two given contexts depends on the thorough-\nness of the researcher's contextual descriptions that allow\nH\u00f8yland et al. 3\nother researchers to determine the applicability of the find-\nApplying the same understanding to the SECUR-ED\nproject methods, the logic becomes the following: The better\nthe context or background described in documents, the better\nthe reader will be able to assess the applicability and trans-\nferability of the method to another setting both within the\ntransportation sector and in other sectors. In this study, we\nare the readers who will assess transferability, based on the\navailable descriptions of context and background. On other\noccasions, readers may be decision makers or advisers within\nthe private sector or public authorities.\nAssessment Criterion: Systematic Design/\nReliability\nFor quantitative research, the concept of reliability implies\n\"dependability, stability, consistency, predictability, accu-\ntion for validity. More specifically, Kirk and Miller (1986)\nidentified three types of reliability: The degree to which a\nmeasurement, given repeatedly, remains the same; the stabil-\nity of a measurement over time; and the similarity of mea-\nsurements within a given time period. Careful considerations\nof measurements are imperative in quantitative research.\nHowever, the quantitative researcher's focus on measure-\nments and stability does not align well with the qualitative\nresearcher's view of the social world as complex, multifac-\nTherefore, an alternative perspective suggests that reliability\nin qualitative research can be reached through systematic\noperation at the design level (methods and techniques, inter-\nview protocols, and so forth; de Ruyter & Scholl, 1998, p.\n13), which implies keeping \"detailed account of the research\nTo assess the degree of systematic research design\nemployed in the SECUR-ED project methods, we reviewed\nproject documents and gathered oral and written inputs from\nindividuals involved in the project.\nAssessment Criterion: Transactional Validity\nWithin qualitative research, validity has traditionally been\ndefined as the researcher's efforts to determine the degree of\ncorrespondence between claims about knowledge and the\nreality being investigated (Eisner & Peshkin, 1990). This\ndefinition is comparable with how quantitative research\napplies internal validity and is understood as a means to\ndetermine whether observations and measurements truly\ncapture what they are intended to capture (LeCompte &\nGoetz, 1982). More recently, the concept of validity in quali-\ntative research has evolved to an approach Cho and Trent\n(2006) labeled transactional validity, which \"assumes that\nqualitative research can be more credible as long as certain\ntechniques, methods, and/or strategies are employed during\nthe conduct of the inquiry\" (p. 322). Based on this develop-\nment of transactional validity, we considered the following\ntechniques or strategies to assess the SECUR-ED project\n\u00b7\n\u00b7 Prolonged engagement: Entails investing enough time\nto understand the culture, establish trust with study\nparticipants, and check for distortions such as a priori\nvalues and constructions (Lincoln & Guba, 1985). In\nthe context of the SECUR-ED project, prolonged\nengagement translates to the participants' investment\nto understand and use project methods. As suggested\nthis investment cannot be measured in numbers, but\nmust be long enough to establish a familiarity with\nproject methods.\n\u00b7\n\u00b7 Persistent observations: Aims to identify the charac-\nteristics and elements in a situation that are most rel-\nevant to the phenomena under investigation and to\nfocus on them extensively to achieve depth (Lincoln\n& Guba, 1985). In the context of the SECUR-ED\nproject, persistent observations refer to the partici-\npants' investment to identify and observe the most\nrelevant characteristics of project methods over time,\nwhich implies that prolonged engagement to under-\nstand and use the methods is required. This invest-\nment cannot be measured but must be long enough to\nenable identification and observation of the most rel-\nevant characteristics of project methods.\n\u00b7\n\u00b7 Triangulation: Involves combining multiple and dif-\nferent sources of information to deepen and widen\nunderstanding of the multifaceted, complex nature of\nthe social world and phenomena (Moran-Ellis et al.,\nbility of chance associations and prevalent, systematic\nbiases, and thereby increases the confidence in the\nresearch data and interpretations (Denzin, 1970;\n\u00b7\n\u00b7 Member-checking/informant feedback: Member-\nchecking, also known as informant feedback, is a con-\ntinuous process in which the researcher seeks feedback\non collected data, analytic categories, interpretations,\nand conclusions from the study group (Cho & Trent,\nSECUR-ED project, member-checking entails partici-\npants' active and continuous discussions of project\nmethods with other project members.\n\u00b7\n\u00b7 Peer debriefing/review: Constitutes an external and\nlogical evaluation of the research process, such as\nprocedures, methods, interpretations, and conclusions\n4 SAGE Open\n\u00b7\n\u00b7 Rich and thick description: The researcher provides\nrich and thick description of the setting, participants,\nmethods, data collection (such as observations of spe-\ncific events and behaviors), data analysis, interpreta-\ntions, the researcher's role, and so forth (Becker, 1970).\nThis detailed information strengthens the credibility of\nfindings and enables the reader to assert whether the\nfindings can be transferred to another setting because of\nshared similarities (Erlandson et al., 1993).\nTo summarize, we can determine the transactional valid-\nity of the selected project method and the overall validity of\nthe project methods by assessing whether and to what degree\nthese techniques/strategies apply to each SECUR-ED project\nmethod.\nThe Quality Assessment Framework\nThe three identified quality criteria--transferability, system-\natic design/reliability, and transactional validity--combined\nwith our operationalization approaches and sources of data\nform a framework for assessing the quality of SECUR-ED\nproject methods (Figure 1). The figure shows how quality\nwas assessed through a five-step process: First, the assess-\nment criteria were identified, and then operationalized. Next,\nthe assessment process was operationalized, and by exten-\nsion, the method's intended use was evaluated against its\nactual application, and quality was assessed using the three\nidentified criteria. Thereafter, the identified and operational-\nized criteria were applied to selected SECUR-ED methods.\nThese methods were first identified and their use within the\nproject was described. Against that baseline, the quality\nassessment of the project methods was conducted. Data for\nthe assessment included written project documents and oral\nand written input from project partners, which was docu-\nmented and systematically reviewed. Finally, the quality\naspect of the project methods was assessed and synthesized.\nApplying the Quality Assessment Framework on\nSECUR-ED Developed Methods\nTo assess each project method, we reviewed and compared\nthe data (documents and input from project members) and\nresolved discrepancies and disagreements in our assessments\nand descriptions to reach a consensus (Cho & Trent, 2006;\nprocess started with the simple formula of identifying the\nmethod intent (the intention behind the particular method)\nand method use (the method's actual use during the project),\nwhich subsequently informed our assessment of transferabil-\nity, reliability, and validity in the quality assessment step\nillustrated in Figure 1.\nWe subsequently assessed each SECUR-ED method qual-\nitatively, according to the three criteria of transferability,\nreliability, and validity. The results of this evaluation were\nsummarized and merged into a few variables and presented\ntogether in a matrix (Table 1): research quality measured by\ntransferability, reliability, and validity, using a 3-point scale\nof satisfactory, less satisfactory, and unsatisfactory. This\nmatrix provides a holistic picture of the quality dimensions\nof the applied methods.\nEvaluation Results\nThis section presents the results of our assessment for each\nof the SECUR-ED project methods. It follows the schema of\nmethod intent, method use, and quality assessment.\nFirst Method Assessed: Common\nGlossary\nMethod Intent\nThe common glossary document provided a set of security\nterminology, definitions, and acronyms used throughout the\nSECUR-ED project, with the main intention to create a com-\nmon communication platform. The document also was\nintended to be a living document, and therefore to be con-\ntinuously updated according to the needs and circumstances\nthroughout the project's life.\nFigure 1. The quality assessment framework.\nNote. SECUR-ED = Secured Urban Transportation--A European\nDemonstration.\nH\u00f8yland et al. 5\nMethod Use\nWithin the SECUR-ED project, a discussion about some\nwords and their underlying concepts clarified different\napproaches, making the establishment of a glossary a valu-\nable process. This discussion was important because the\nproject involved partners from different backgrounds, such\nas security, public transport, and information technology.\nHowever, active usage of the glossary among the various\nproject working groups was diverse. Not everybody was\naware of the glossary, and some conceptual structures were\nreinvented first and aligned later. Some teams also used\nterms and structures provided in other project documents.\nAlthough the glossary's initial version was finalized in\nFurthermore, city and organizational representatives par-\nticipating in the project mainly had to follow their local ter-\nminologies, often in their national language, to interact with\nlocal stakeholders. This additional challenge did not encour-\nage a strict adherence to the terminology in the glossary.\nQuality Assessment\nTransferability.A number of the security terminologies and\ndefinitions included in the common glossary were relevant to\na wide range of sectors, from transportation and industries to\noffshore operations and health care. Examples of transfer-\nable terms include accident, crisis, security, cybersecurity,\ninformation security, risk, risk management, risk assessment,\nsafety, access control, incident management system, user\nrequirements, and interoperability. In addition, most defini-\ntions were quite thorough and detailed, implying that the\nglossary had transferability potential both within and outside\nthe transport sector.\nReliability.This method appeared to lack reliability because\nthe glossary was not applied consistently and systematically\nduring the SECUR-ED project period, and some teams were\nunaware of the glossary or used only local terminologies.\nValidity. In addition to the lack of peer review for this method,\nactive usage and revisiting/refinement of the glossary\nthroughout the project period were not done, but could have\nimproved understanding of project tasks (utilizing glossary\nterminologies). In other words, the SECUR-ED project\nlacked prolonged engagement of the method. The existing\nunderstanding of terminologies also should have been vali-\ndated through member-checking as data were gathered\nthroughout the project. In sum, the method did not add to\noverall validity of the SECUR-ED project.\nSecond Method Assessed:\nInteroperability Notation\nMethod Intent\nThe SECUR-ED project developed a specific notation for\ninteroperability (the extent to which systems and organiza-\ntions are able to work together) in the domain of public\ntransport security (SECUR-ED, 2014a). The intention was\nto enable later subprojects to describe their components\nand systems in a consistent format and to use a common\ninterface specification language that would reduce the risk\nof misunderstandings between SECUR-ED partners and\nactivities.\nMethod Use\nThe method was applied to several security demonstration\nscenarios in the SECUR-ED project in which the method\nhelped to identify and visualize relevant roles and systems,\nincluding their relationships. However, these scenarios were\npart of only two project tasks in SECUR-ED. The method\nalso was adapted and applied in a software tool, RED\n(\"Requirements Editor and Designer\"), but the process for\nusing this tool was neither conceptualized nor implemented\nduring the SECUR-ED project period.\nQuality Assessment\nTransferability.As described in SECUR-ED project docu-\nments, the interoperability notation is rich in background and\ncontext, which facilitates its use in similar contexts within\nthe transportation sector and across sectors.\nTable 1. An Overview of Our Quality Evaluations of the Applied SECUR-ED Project Methods.\nSECUR-ED method\nQuality criteria\nTransferability Reliability Validity\nCommon glossary Satisfactory Unsatisfactory Unsatisfactory\nInteroperability notation Satisfactory Less satisfactory Less satisfactory\nScenario description with BPMN Less satisfactory Unsatisfactory Unsatisfactory\nCapacity mapping matrix Satisfactory Satisfactory Satisfactory\nDemo city dashboards Less satisfactory Unsatisfactory Unsatisfactory\nCapacity evaluation approach Less satisfactory Less satisfactory Less satisfactory\nNote. BPMN = business process model and notation; SECUR-ED = Secured Urban Transportation--A European Demonstration.\n6 SAGE Open\nReliability.Although useful during two project tasks, the\nmethod had limited usage in the SECUR-ED project as a\nwhole. Although the method was applied in the development\nof a software tool, the process for using the tool was never\nproperly established. Subsequently, we found limited sys-\ntematic design related to this method and a limited contribu-\ntion to reliability in the SECUR-ED project.\nValidity. The method was used to identify and visualize rel-\nevant roles, systems, and their relationships during two of\nthe SECUR-ED project tasks, and thus facilitated triangula-\ntion of data and understandings in the project, which\nstrengthened validity. However, the method was not imple-\nmented and documented more broadly in the SECUR-ED\nproject and constituted a limited contribution to overall\nproject validity.\nThird Method Assessed: Scenario\nDescription With BPMN\nMethod Intent\nThe BPMN method provided a graphical representation for\nspecifying business processes, which through consistent use\nis intended to improve project communication and setup\nbetween the tested security capacities and demonstration\nscenarios in the SECUR-ED project. Through a credible\nstory, these demonstration scenarios illustrated how a secu-\nrity risk may materialize and how SECUR-ED results could\nimprove preparedness of involved parties.\nMethod Use\nBPMN process diagrams were used for different approaches\nand deliverables in the SECUR-ED project, but with dif-\nferent intentions. The quality of process documentation\nalso varied; for example, some deliverables described the\nprocess diagrams in plain text only. In sum, the BPMN\ndiagrams had diverse use and documentation in the\nSECUR-ED project.\nQuality Assessment\nTransferability.BPMN is an established and broadly used\nnotation, which makes it highly transferable to other contexts\nand sectors. However, as applied in the SECUR-ED project,\nBPMN involved fragmented descriptions with varying levels\nof detail, which resulted in limited transferability to contexts\nwithin the transportation sectors and across sectors.\nReliability.Variations in the project's usage of the BPMN\nmethod, accompanied by inconsistent levels of descriptions,\ndemonstrate a lack of systematic design related to this\nmethod. Therefore, the method as used in the SECUR-ED\nproject did not strengthen overall reliability of the project.\nValidity. Variations in BPMN's usage and inconsistent levels\nof descriptions implied a lack of a consistent focus, persis-\ntent observation, and \"rich and thick descriptions,\" prerequi-\nsites for establishing validity. Consequently, BPMN as used\nin the SECUR-ED project did not contribute to the project's\ncredibility.\nFourth Method Assessed: Capacity\nMapping Matrix\nMethod Intent\nThe capacities mapping matrix (CMM) is an Excel spread-\nsheet listing project capacities, demonstration cities, and\nproject deliverables, and was developed to provide an over-\nview of the status of all the deliverables and capacities. The\nintention was to identify and list proposed SECUR-ED\ncapacities and to link them to the deliverables to ensure that\ninformation about the capacity is captured in any deliverable.\nAnother reason for developing the CMM was to link the\ncapacities to the demonstration cities to get an overview of\neach demonstration site. Moreover, the CMM identified the\nresponsible capacity providers and points of contact to facili-\ntate the handover between the capacities and the demonstra-\ntion teams.\nMethod Use\nIn addition to its intended use, the CMM was used in project\nmanagement to check and confirm capacity assignment to a\ndemonstration. A project partner regularly updated the\nCMM, allowing project management to keep track of the sta-\ntus of the capacities. Various partners in the project utilized\nthe CMM. For example, public transport operator used the\nCMM to identify capacities of interest, which deliverables\nmight provide more technical information about the capaci-\nties, and who to contact for more information. Furthermore,\nthis contact information was used to benchmark project per-\nformance, as the evaluators updated and adjusted the CMM\nto identify which capacities were planned and demonstrated\nin which cities. This modification helped project managers to\nidentify any capacities that were planned but not demon-\nstrated, and the reasons why.\nQuality Assessment\nTransferability. The CMM is a simple tool that can easily be\nused by other large-scale demonstration projects to map\nsolutions and other technologies and actions to demonstra-\ntion sites or scenarios. The CMM also was regularly updated\nand well documented, which suggests potential transferabil-\nity of the method within the transportation sector.\nReliability. The CMM gave a clear overview of all the capaci-\nties in the project and which were or were not ready. As the\nH\u00f8yland et al. 7\nproject progressed, the CMM increasingly served as a tool to\nmap the capacities and to link them to the demonstration cit-\nies. Overall, the CMM method saw active/systematic use\nduring the SECUR-ED project, and thus contributed toward\nstrengthening reliability of the project.\nValidity.The CMM benefited both project management and\nother partners in the project. Overall, the method's extended\nuse project-wide improved the overview and understanding of\nthe status of each capacity, and thus served to triangulate data.\nThe method also became well documented (\"rich and thick\ndescriptions\") through regular updates, indicating that the\nmethod contributed to the SECUR-ED project's credibility.\nFifth Method Assessed: Demo City\nDashboards\nMethod Intent\nThe dashboard, which is an Excel spreadsheet file summariz-\ning the process of each of the demonstrations, initially was\nintended as a tool for reporting progress to project manage-\nment. The intention was to regularly update these dashboards\nduring the preparations of the demonstrations.\nMethod Use\nThe dashboard was used as a reporting tool in a project coor-\ndination report, with summarized information about project\nmilestones completed, selected capacities for each demon-\nstration, and potential planning and implementation risks and\nmitigation actions taken. In addition, information from the\ndashboards was used to present progress of demonstration\ncities during project management board meetings. However,\nthe SECUR-ED partners did not integrate the dashboards\nfrom the beginning of planning the demonstrations and\nupdated them only sporadically.\nQuality Assessment\nTransferability. The initial intention was for the dashboards to\nbe used as a tool to report the progress of the demonstrations.\nSimilar spreadsheet files can be used easily in other compa-\nrable projects for that aim. However, the method was not\nused systematically and actively, and was not well docu-\nmented, which suggests limited transferability potential\nwithin the transportation sector.\nReliability.As a reporting tool, the dashboards' design with\ndifferent color schemes was simple and comprehendible and\ncould serve a coordination purpose. However, the dashboards\nwere neither integrated into preparations for the demonstra-\ntions nor regularly updated. We concluded that although the\ndashboards were used in the coordination report, city dem-\nonstrations, and presentations at project management board\nmeetings, the tool did not achieve systematic use during the\nSECUR-ED project and therefore did not contribute toward\nstrengthening reliability of the project.\nValidity.\nSystematic usage of the dashboards was not\nachieved, indicating a lack of triangulation and documenta-\ntion efforts; therefore, the method did not contribute toward\nimproving understanding and overview of project status,\nplanning, and coordination. Consequently, the method did\nnot strengthen the SECUR-ED project's credibility.\nSixth Method Assessed: Capacity\nEvaluation Approach\nMethod Intent\nThe objective of the capacity evaluation approach was to pro-\nvide a comprehensive assessment of all capacities considered\nin SECUR-ED. Specifically, the assessment process was\nintended to identify the most promising capacities and meth-\nods based on their performance in the demonstrations, using a\ncost-effectiveness measure.Arelated objective was to general-\nize the findings from the individual demonstrations, to evalu-\nate how the demonstrated capabilities might work in other\nsettings and in other cities. To achieve these objectives, a com-\nmon framework (method for assessing capacities) was estab-\nlished to align all parts of the work involved in the assessment.\nThis framework specified how to use all available data and\nexpertise to evaluate individual capacities, combine results\nfrom separate assessments, and generalize the conclusions.\nMethod Use\nProject documents demonstrated active usage of the capacity\nevaluation approach, including preliminary assessments of\nnumerous capacities in the SECUR-ED project derived from\ninterviews and material produced in the project. As the over-\narching framework for the assessment approach, project\ndocuments revealed active usage of the method for assessing\ncapacities. However, the assessment was very incomplete for\nmany capacities and was done very differently for each\ncapacity, with almost no or a much differentiated reflection.\nTo really assess capacities using the suggested criteria was\ndifficult because the capacities are intertwined with other\ncapacities and existing solutions.\nQuality Assessment\nTransferability.The thoroughness of the assessment frame-\nwork/method described in various project documents suggests\npotential transferability of the capacity evaluation approach.\nHowever, the assessments--based on the framework--were\nincomplete and had little and/or differentiated reflections,\nindicating a lack of documentation and transferability. Over-\nall, we found limited transferability of this method.\n8 SAGE Open\nReliability. The capacity evaluation approach, as described in\nproject documents, demonstrates a high degree of systematic\ndesign. In this sense, the method contributed positively to the\nreliability of the SECUR-ED project as a whole. However,\ninconsistent usage of the method as well as incomplete\nreflections demonstrated a lack of systematic design, which\nindicated an overall limited contribution to the reliability of\nthe overall project.\nValidity.\"Rich and thick descriptions\" of the assessment\nmethod and the capacities evaluated suggested this particular\nmethod's contribution to the credibility of the SECUR-ED\nproject. Various project documents, reports, and evaluations\nfrom operators and observers at various demonstration\nevents--supportive of both triangulation and member-check-\ning--further improved the credibility contribution of the\nmethod. However, the framework was not applied systemati-\ncally across capacities and the reflections were incomplete\nand/or differentiated, which suggested a lack of persistent\nobservation and prolonged engagement. Consequently, the\ncredibility contribution of this method was limited.\nEvaluation Summary\nOur evaluations of the quality of the different methods applied\nin the SECUR-ED project are summarized in Table 1, using a\nsimple 3-point scale: If the method did not comply with good\npractice as described by a criterion, we enter a score of \"unsat-\nisfactory\" along this criterion; if there was limited compli-\nance with a criterion, the method was scored as performing\n\"less satisfactory\" along this criterion; and if the criterion was\nroughly met, we enter a score of \"satisfactory.\"\nAs shown in Table 1, we have assessed six SECUR-ED\nproject methods. Only one of these (capacity mapping matrix)\ndemonstrated satisfactorily in the quality criteria of transfer-\nability, reliability, and validity. One method (common glos-\nsary) demonstrated satisfactorily in transferability but\nunsatisfactorily in both reliability and validity. One method\n(interoperability notation) also demonstrated satisfactorily in\nterms of transferability, but less satisfactorily in both reliabil-\nity and validity. Two methods (scenario description with\nBPMN and demo city dashboards) scored similarly: They\nboth demonstrated less satisfactorily in transferability and\nunsatisfactorily in both reliability and validity. Finally, one\nmethod (the capacity evaluation approach) scored uniformly\nless satisfactorily in all quality criteria of transferability, reli-\nability, and validity.\nOverall Assessment of Project Methods\nThree of the six SECUR-ED project methods assessed in our\nstudy were accompanied by detailed descriptions and/or rep-\nresent established methods or notations, which demonstrate\npotential transferability to other contexts both within and\noutside the transportation sector (see Table 1). On the flip-\nside, the scoring shown in Table 1 demonstrates that three of\nthe six methods had unsatisfactory reliability; therefore,\nclearer requirements and better internal communication and\ninformation-sharing could be obtained if such project-wide\nmethods were used consistently and in a way that ensured\ncomparable results. Stability and consistency are prerequi-\nsites for rich and thick descriptions, triangulation, persistent\nobservation, and prolonged engagement, and therefore, three\nof the six SECUR-ED project methods had unsatisfactory\ncontributions to the validity to the project because of the\nidentified variations and inconsistent usage. It should be\nnoted that four of the six methods scored somewhere in\nbetween (less satisfactorily) on one or more of the quality\nparameters, implying that the overall score picture has many\nnuances. Nevertheless, a ranking can be made as follows.\nThe capacity mapping matrix comes out best, interoperabil-\nity notation second, capacity evaluation approach third, and\ncommon glossary fourth, and scenario description with\nBPMN and demo city dashboards both came fifth. By scor-\ning high on all quality parameters, the capacity mapping\nmatrix should be of particular importance as \"best practice\"\nstandard for future European Union (EU) projects. Moreover,\nwe suggest improved coordination and routines for project\ninternal information-sharing be applied to the design and\nimplementation of methods in future projects, and that these\nprojects replicate the documentation efforts demonstrated in\nrelation to several of the SECUR-ED methods. It must be\nnoted the scoring could turn out totally differently if applied\nin another project.\nLimitations and Strengths of the Study\nViswanathan (2005) pointed out the general difference\nbetween conceptual and operational in measurements and\nscientific research. Time and length can be defined conceptu-\nally as the shortest distance between two points, and mea-\nsurement follows directly. Weight and temperature involve\nmore abstract conceptual definitions as well as larger dis-\ntances between the conceptual and the operational. In the\nfield of social science, for example, when measuring of atti-\ntudes toward objects, the distance between conceptual and\noperational can be large. As the distance increases, so do the\ndifferent ways to measure something and measurement error,\nand accurate measurement is central to scientific research.\nThe difference between conceptual and operational--which\ncan be huge--is very relevant for this evaluation and could\nbe considered a weakness of our study.\nHowever, one can also argue that qualitative data and\nfindings are embedded in a given context and subjective\nexperiences, and therefore can change from time to time and\nplace to place (Dew, 2007). This fact makes any extrapola-\ntion of \"the particular\" in qualitative research highly chal-\nincluding attempts at repeated and stable measurements, and\ntherefore necessarily less accurate (see our discussion in\ndetermining the reliability criterion). Consequently, while\nwe acknowledge that the way we assessed and measured\nH\u00f8yland et al. 9\nquality criteria in our study--using a 3-point scale of satis-\nfactory, less satisfactory, and unsatisfactory--was largely\nexperimental and not an accurate science, our approach nev-\nertheless complies with the nature of qualitative research.\nWe compensated for lack of measurement precision with\ntriangulation of data across several researchers and the use of\nseveral methods (meetings, teleconferences, and emails) and\nsources of data (documents and inputs from project mem-\nbers), which increases our study's credibility. We also docu-\nmented and applied our assessment framework in a systematic\nand consistent manner, which strengthened both the transfer-\nability and reliability of our study.\nConclusion\nSECUR-ED was one of the largest demonstration projects in\nthe EU, with 40 partners operating in different cultural and\nlegal environments. Managing such a comprehensive con-\nsortium with partners from different countries, with different\nmother tongues, cultures, and understandings of how to do\nthe job, is a daunting endeavor. A huge amount of tasks had\nto be performed within a short timeline, and one failure to\ndeliver would delay somebody's work somewhere in the\nconsortium. This fact is important to take into consideration\nas it is much easier to get a favorable score if you have full\ncontrol of the activity and people involved.\nAs a contribution to existing research, we have demon-\nstrated how to conceptualize, operationalize, and apply qual-\nity criteria in the assessment of project methods that are\nqualitative in nature, thereby answering the identified need\nfor clear quality guidelines within qualitative research.\nTherefore, our assessment approach and framework is a\nnovel approach that we believe can hold value in future proj-\nect evaluations across research areas and sectors. More spe-\ncifically, we believe that the three quality criteria at the core\nof the framework and assessment process constitute higher\nlevel concepts; that is, the concepts are not embedded in the\nparticular findings and are therefore contextual independent\nand applicable to other settings and participants (Erlandson\nbased on the framework can potentially be compared across\nresearch areas and sectors. Consequently, we recommend\nthat research projects conducted across contexts and topics\nshould employ this framework to facilitate further develop-\nment and validation of the existing assessment framework\nand process. To take this initiative further along into stan-\ndards development would provide additional value to policy\nand statute development.\n"
}