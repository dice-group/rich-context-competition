{
    "abstract": "Abstract\nWhen large-scale assessments (LSA) do not hold personal stakes for students, students may not put forth their best effort.\nLow-effort examinee behaviors (e.g., guessing, omitting items) result in an underestimate of examinee abilities, which is a\nconcern when using results of LSA to inform educational policy and planning.The purpose of this study was to explore the\nrelationship between examinee motivation as defined by expectancy-value theory, student effort, and examinee mathematics\nabilities.A principal components analysis was used to examine the data from Grade 9 students (n = 43,562) who responded to\na self-report questionnaire on their attitudes and practices related to mathematics.The results suggested a two-component\nmodel where the components were interpreted as task-values in mathematics and student effort. Next, a hierarchical linear\nmodel was implemented to examine the relationship between examinee component scores and their estimated ability on\na LSA.The results of this study provide evidence that motivation, as defined by the expectancy-value theory and student\neffort, partially explains student ability estimates and may have implications in the information that get transferred to testing\norganizations, school boards, and teachers while assessing students' Grade 9 mathematics learning.\n",
    "reduced_content": "http://sgo.sagepub.com\nWhen test scores from large-scale assessments (LSA) are\nreported and interpreted for individual students, classes,\nschools, states, or nations, there is often an implicit assump-\ntion that the scores represent the best effort of the student\n(Wolf & Smith, 1995). Researchers in the field of educa-\ntional measurement have questioned this assumption by stat-\ning that if the test score is not consequential or important to\nthe student, then one cannot be sure how much the observed\nscore is influenced by the lack of effort (DeMars, 2000; Wolf\n& Smith, 1995). This leads to the argument that test conse-\nquences influence motivation, and motivation influences stu-\ndents' effort, test performance, and estimates of academic\nHarlen and Crick (2003) have suggested that motivation\nis related to effort and the assessment learning context. Other\nresearchers have proposed that motivation is closely aligned\nwith the will to learn. Motivation encompasses self-esteem,\nself-efficacy, values, and students' perception of their abili-\nties to accomplish a particular task. All these components of\nmotivation affect effort and ultimately achievement\nIn Ontario, Canada, LSA of mathematics do not explain\nthe effect of students' motivation and effort on their esti-\nmates of academic achievement. This is a concern because\nthe results from these LSA are used to provide accountability\nin the educational system for the allocation of funds, justify\nchanges to the mathematics curriculum, inform parents about\ntheir children's progress, and help children adapt to changes\nin today's world (Education Quality Accountability Office\n[EQAO], 2011).This is one of the reasons why the current\nstudy focused on examining student motivation and student\neffort in relation to their estimates of academic achievement\nby using Grade 9 LSA of mathematics based on self-report\ndata and test scores. The mathematics self-report question-\nnaires used in the current study to measure student motiva-\ntion were administered by the EQAO in 2007. Because\nEQAO self-report questionnaires were not designed as a\nmeasure of student motivation, we decided to use the research\n1University of Lakehead,Thunder Bay, Ontario, Canada\n2University of Ottawa, Ontario, Canada\nCorresponding Author:\nCarlos Zerpa, University of Lakehead, Faculty of Education, 955 Oliver\nRoad,Thunder Bay, Ontario P7B 5E1, Canada\nEmail: czerpa@lakeheadu.ca\nModeling Student Motivation and Students'\nAbility Estimates From a Large-Scale\nAssessment of Mathematics\nCarlos Zerpa1, Krystal Hachey2, Christina van Barneveld1,\nand Marielle Simon2\n Keywords\nachievement, educational testing, large-scale assessments, motivation, self-report data\n2 SAGE Open\nwork done by Wigfield and Cambria (2010) to guide us in\nidentifying motivation items from EQAO self-report ques-\ntionnaires related to students'values and effort. Wigfield and\nCambria's research provided us with an extensive review of\nstudents' motivation constructs related to achievement val-\nues, goal orientations, and interest that could be used in edu-\ncational research to measure student motivation. For the\ncurrent study, we used these motivation constructs (achieve-\nment values, goal orientations, and interest) and related them\nto EQAO self-report items to address the research question.\nFor example, EQAO self-report items such as \"I like math\"\nwas classified as intrinsic or interest value because it related\nto students' enjoyment from doing mathematics; \"Math is\nboring\" was classified as attainment value because it is\nrelated to the importance that the student placed on the math-\nematics tasks; \"The math I learn now is very useful for\neveryday life\" and \"I need to keep taking math for the kind\nof job I want after school\" were classified as utility values\nbecause they reflected the importance of mathematics for\nstudent future plans; \"I am good in math\" and \"Mathematics\nis an easy subject\" were classified as achievement values\nbecause they related to student goal orientations (Wigfield &\nyou complete your math homework\" and \"How much time\ndo you usually spend in math homework\" were classified as\nstudent effort because these items related to students' well-\ndeveloped interest to engage in mathematics tasks frequently\ncurrent study sheds light and provides an avenue for research-\ners, teachers, and educational agencies to better explain\nthe impact of motivation on the estimates of students' aca-\ndemic achievement, while using EQAO's LSA of Grade 9\nmathematics.\nContext\nIn the context of education, when researchers address stu-\ndents' motivation, they focus on the theory of motivation\nrelated to students' beliefs, values, and goals to better assess\nstudents' academic performance and achievement (Bishop,\nGivven, Salmon, & MacGyvers, 1998; Sundre & Moore,\nand Wigfield (2002) stated, these constructs (beliefs, val-\nues, and goals) are the most immediate and direct predictors\nof academic achievement, performance, and choice, and are\nthemselves influenced by a variety of psychological, social,\nand cultural determinants. One theory that encapsulates\nthese constructs is the expectancy-value theory of motiva-\nvalue theory links achievement performance, persistence,\nand choice directly to individuals' expectancy-related\nand task-value beliefs. Expectancy-related beliefs refer to\nindividuals' beliefs about how well they will do on an\nupcoming task, either in the immediate or upcoming future\n(Eccles & Wigfield, 2002). Task-value beliefs are defined\nby four components: (a) attainment value--the personal\nimportance of doing well on a task, (b) intrinsic value--the\nenjoyment the individual gets from performing the task, (c)\nutility value--how well the task relates to current and future\ngoals, such as career goals, and (d) cost--negative aspects\nof engaging in the task, such as fear of failure (Eccles &\nAs defined by expectancy-value theory, students' motiva-\ntion in relation to academic achievement depends on stu-\ndents' general-ability beliefs and task-value beliefs (Eccles\nSingh, 1994). Applied to LSA, general-ability beliefs relate\nto a student expectancy-related belief about his or her ability\nto be successful on LSA. Task-value beliefs relate to the\nimportance the student places on a successful performance\non LSA. In effect, if the student values the outcome of the\nLSA, then there are more chances that the student will be\nmotivated, make an effort on tasks, and engage with the tasks\nto the best of his or her ability (Dweck & Elliot, 1983; Eccles\nConditions ofTesting and Low-Effort\nStudent Behaviors on LSA\nDuring the administration of LSA, students' low motivation\nand the conditions of testing may influence their effort in\nresponding to mathematical test items (DeMars, 2000;\nlow motivation as a result of not valuing the outcome of the\ntest may trigger certain low-effort test-taking behaviors such\nas guessing, omitting items, or quitting entirely on the large-\nscale examination, and these behaviors may cause an under-\nestimation of students' abilities (De Ayala, Plake, & Impara,\nconditions of the test such as item difficulty, mental taxation,\nand item position may also affect students' motivation and\neffort, and may affect their ability estimates, especially in\nsituations when the students do not value the outcome of the\nWolf et al. (1995) studied students' motivation and effort\nduring a large-scale examination by examining item-by-item\ndifferences in performance between two groups of students\ntaking the same test under different conditions. Participants\nsame high school. The researchers found that the conditions\nof testing influenced test performance, and this influence\nvaried for different kinds of items. For instance, if there were\nno consequences linked to assessment results, items related\nto nonconsequential conditions appeared unnaturally diffi-\ncult because they did not motivate and capture the complete\neffort of students. The researchers concluded that this lack of\nstudent motivation and effort due to the conditions of testing\nZerpa et al. 3\nposes a threat to the validity of interpretation of test results\nwhen assessing students' test performance.\nImpact of Low-Motivation Behaviors\non theValidity of Interpretation of\nTest Scores\nLow motivation and the conditions of testing may affect\nstudents' effort and the estimates of students' abilities during\nal., 1995) and may also affect the validity of the interpreta-\ntion of test scores when assessing student academic perfor-\ninstance, Schmitt, Chan, Sacco, McFarland, and Jennings\n(1999) found that low-motivation behaviors (e.g., guessing,\nomitting items, or quitting entirely on a test) affect the valid-\nity of interpretation of test results during LSA and can either\nartificially inflate or deflate estimates of students' abilities.\nThe researchers stated that inaccurate estimates of students'\nabilities negatively affect individuals and test organizations\nwhen assessing student academic performance and achieve-\nment. For example, an inflated estimate of ability, as a result\nof guessing the correct answer (an examinee of low ability\nguesses the correct answer on medium difficulty items and\non more difficult items), may cause educational authorities\nto think that students are able to perform at the expected\nlevel. On the contrary, a deflated estimate of ability, as a\nresult of omitting items on a test, may deprive students of\nopportunities where they can be exposed to higher levels of\nknowledge. These are examples of concerns to highlight the\neffect of low-motivation behaviors on the validity of LSA\n(1989) stated, to validate a proposed interpretation or use of\ntest scores, is to evaluate the rationale used for the interpre-\ntations of the test scores. Valid interpretations of test results\nsuch as those obtained from LSA may lead to a more rational\nargument of why certain changes need to be implemented in\nthe educational system (Kane, 2006). These changes may\ninclude curriculum modifications and allocation of funding\nby administrators and policy makers.\nTechniques Used to Measure\nMotivation\nThere are models and indices used to identify low motiva-\ntion that affects students' performance on tests. These mod-\nels and indices provide an avenue to better understand\nstudents' academic performance and make valid interpreta-\ntions of LSA results (Fraire, Tideman, & Watts, 1997;\nmodeling index that researchers have used quite extensively\nto identify low motivation is the lz index statistic, which was\ndeveloped by Drasgow, Levine, and Williams (1985). The lz\nis a standardized statistical estimate used to detect the per-\ncentage of low motivation test-taking behaviors manifested\nin the data for low- and high-stake examinations (Drasgow\ntion of the lz statistical technique, however, is that it is only\nbased on test scores and does not measure students' effort\n(DeMars, 2000). Based on this concern, researchers have\ndeveloped statistical indices to measure low motivation by\nusing response-time effort (S. Wise & Kong, 2005).\nResponse-time effort is based on the hypothesis that when\nan item is administered, unmotivated students have the ten-\ndency to answer the item too quickly (S. Wise & Kong,\n2005). This means that students lack performance effort in\ntheir responses on the test due to low motivation. Although\nthe response-time effort technique seems to be more promis-\ning than the lz statistical index in detecting low motivation\nbecause it takes into consideration student effort, the chal-\nlenges with the response-time effort technique is that it\nrequires the use of computer-based technology. LSA and\ninstruction in the classroom, however, are usually conducted\nusing pencil-and-paper methods (Camara, 2009; S. Wise &\nDeMars, 2006). This is one of the reasons why some\nresearchers choose to use student self-reported questionnaire\nmeasures as another method to examine the relationship of\nmotivation and effort with student estimates of academic\nachievement during LSA (Eklof, 2006; Pintrich, Smith,\nA number of self-report questionnaires have been devel-\noped, validated, and used to measure motivation. For instance,\nthe Motivated Strategies Learning Questionnaire (MSLQ;\nPintrich et al., 1993) and the Student Opinion Survey (Sundre\nresearchers to measure students' motivational beliefs and val-\nues ranging in age from late elementary to university. Marsh,\nKoller, Trautwein, Ludtke, and Baumert (2005) developed a\nlearning survey to measure how much students look forward\nto learning mathematics, how important mathematics is to\nthem, the importance of being a good mathematician and the\nenjoyment of learning mathematics by drawing on the expec-\ntancy-value theory of motivation. O'Neil, Abedi, Miyoshi,\nand Mastergeorge (2005) used an adaptation of the State\nThinking Questionnaire (O'Neil, Sugrue, Abedi, Baker, &\nGolan, 1997) to measure motivation using a monetary incen-\ntive as a way to increase student effort and performance.\nRoderick and Engel (2001) used the Reynolds Adolescent\nDepression Scale (Reynolds, 1984) to cross check interview\ndata of students' descriptions of their motivation.\nOne of the strengths of using self-reported questionnaires\nto measure motivation is that they can be easily implemented\nusing a pencil-and-paper method as opposed to other mea-\nsurement techniques that may require the use of computer-\nbased technology (Camara, 2000). In addition, self-reported\n4 SAGE Open\nquestionnaires' variables and constructs can be grounded on\nthe expectancy-value theory of motivation or other motiva-\ntion theories (i.e., attribution theory, achievement goal the-\nory, and self-efficacy) to assess students' motivation and\neffort in relation to their academic achievement (Eccles &\nchallenges, however, is to develop and use self-reported\nquestionnaires that have a clear structure, high internal con-\nsistency, and a strong evidence of validity measures. Another\nchallenge is to create a motivation self-report questionnaire\nthat clearly addresses factors and variables as they relate to\ntest effort and performance using a motivation theory (Cole\nIn Ontario, Canada, Grade 9 EQAO self-reported ques-\ntionnaires are used to obtain students' background informa-\ntion, which can be linked to their achievement, interest,\nvalues, effort, and goals in different mathematic strands such\nas numeracy, algebra, and geometry. For instance, how often\ndo you complete all your mathematics homework? I like\nmath and I am good in math are questions that can be related\nto student effort and task-value beliefs as defined by the\nexpectancy-value theory of motivation (Brookhart, Walsh, &\nThe intention of the self-reported questionnaires and tests\nare to monitor how well students are meeting the expecta-\ntions of the mathematics curriculum. The information\nobtained from these assessment tools is used to better inform\nschools, teachers, and parents about students' mathematics\nachievement in relation to a provincial standard (Volante,\n2006). These provincial self-reported questionnaires and\nmathematics tests are administered each year by teachers in\nthe schools and then returned to EQAO for marking and\nreporting. The first administration takes place in the winter\nsemester and the second one in the spring.\nA portion of these EQAO Grade 9 mathematics tests (0%-\n30%) may or may not count toward students' final grades. If\nstudents know that assessment results do not count, their\ntask-values and effort may change as they may not place too\nmuch importance on a successful performance for the large-\nThese variations in test stakes as well as the results from\nEQAO tests not explaining if students' motivation affects\ntheir academic achievement led us to the development of the\ncurrent study. We addressed a portion of the motivation\nproblem in the current study by using students' self-reported\nquestionnaires data from the EQAO Grade 9 LSA of mathe-\nmatics. This approach allowed us to identify motivation\ncomponents (task-values and effort) as defined by the expec-\ntancy-value theory and examine if these motivation compo-\nnents were significant predictors of students' academic\nachievement. The intention of this research was to provide\nan avenue for researchers, teachers, and the EQAO officials\nto better explain the effect of student motivation on his or her\nacademic performance during LSA of mathematics.\nIn summary, the research work presented in this article is\nmostly concerned with theoretical constructs, predictions,\nand relationships among motivational variables such as\nexpectancy value, achievement goals, effort, and interest\nwith students'academic achievement during LSA. The study\nalso builds on work done by DeMars (2000), Maehr and\nguided this study was as follows:\n1. To what extent does the expectancy-value theory\nof motivation and student effort relate to students'\nacademic achievement on a large-scale assessment\nof mathematics?\nMethod\nInstrument and Participants\nEQAO Grade 9 assessment of mathematics, 2007, was used\nas the source of data. The EQAO data included students'\nself-reported questionnaires and test scores. The EQAO tests\nwere administered twice during the year. The first adminis-\ntration took place in the winter semester and the second\nadministration in the spring semester. For the current study,\nthe results from students in the academic program (students\nwho will be attending university) who wrote the test in the\nspring were used. We chose these results because they rep-\nresented the largest sample of data from the EQAO student\nself-reported questionnaires and test scores. This provided\nProcedure and Analysis\nFirst, 11 items related to expectancy-value theory of motiva-\ntion and student effort were selected from Grade 9 LSA of\nmathematics using EQAO student self-report question-\nnaires. This selection was based on Wigfield and Cambria's\n(2010) study as previously stated. See Formula 1 and 2 for a\nlist of items. Once the items were selected from the student\nself-reported questionnaire data, a principal components\nanalysis (PCA) for categorical data was conducted. From the\nPCA, the selected items from the student self-reported ques-\ntionnaire data were reduced into two components, one com-\nponent representing task-values and the other representing\nstudent effort (Wigfield & Cambria, 2010). After the motiva-\ntion components were identified based on the literature and\nthe PCA, component scores were computed for each case\nusing the modeling Equations 1 and 2.\nTask Value = \na\nX\n+ a\nX\n+ a\nX\n+ a\nX\n+ a\nX\n+ a\nX\n+ a\nX\n+ a\nX\n+ a\nX\n+ a\nX\n,\nZerpa et al. 5\nEffort = \nb\nX\n+ b\nX\n+ b\nX\n+ b\nX\n+ b\nX\n+ b\nX\n+ b\nX\n+ b\nX\n+ b\nX\n+ a\nX\nwhere X\n= I like Math, X\n= I am good in math, X\n=\nI understand most of the mathematics I am taught, X\n= The\nmathematics I learn now is very useful for everyday life,\nX\n= I need to keep taking mathematics for the kind of job\nI want after I leave school, X\n= Mathematics is boring,\nX\n= Mathematics is an easy subject, X\n= How much time\ndo you usually spend on mathematics homework (in or out\nof school) on any given day? X\n= How often do you com-\nplete all of your mathematics homework? X\n= How often\nhave you been absent from your Grade 9 mathematics class\nthis year? X\n= How often have you been late for your Grade\n9 mathematics class this year; \n, \n, \n, \n, \n, \n, \n, \n, \n,\n\n, \nare the coefficients for the variables in the task-value\ncomponent; \n, \n, \n, \n, \n, \n, \n, \n, \n, \n, \nare the\ncoefficients for the variables in the expectancy-performance\ncomponent.\nNext, a two-level hierarchical linear model (HLM)--stu-\ndents nested in schools--was used to determine the signifi-\ncance of the relationship between students' motivation and\ntheir mathematics achievement, as measured by the EQAO\ntest. The first level of the HLM contained a fixed-model\neffect, which was used to determine how significant the\nextracted components (task-values and effort) were in rela-\ntion to students' academic achievement. The second level\ncontained a random-model effect that was used to determine\nthe impact of different schools on students' academic\nachievement at random. The modeling Equations 3, 4, and 5\nwere used for the two-level HLMs.\nLevel 1 or fixed-model effect:\ny = a\n+ a\nx\n+ a\nx\nLevel 2 or random-model effect:\na\n= b\n+ b\nz\nCombined:\ny = b\n+ b\nz\n+ a\nx\n+ a\nx\nwhere y = students' academic achievement, x\n= task-values,\nx\n= expectancy performance, r = fixed-model effect residual\nvariance, \n= mean student academic achievement for a giv-\nen school, \n= intercept with represents the grand mean due\nto schools, \n= variance of the intercept due to schools, \n,\n\n= coefficient for the fixed factors of Level 1.\nResults\nThe results of the PCA suggested that the students' self-\nreported EQAO questionnaire variables clustered into two\ncomponents. These two components were interpreted by the\nresearch team as task-values and effort in mathematics based\non Wigfield and Cambria's (2010) study. See Table 1 for\nresults of the PCA.\nThe results of the HLM first-level analysis (fixed analysis\neffect model) depicted in Table 2 suggested that the task-\nvalues and effort components were significant predictors of\nstudents' academic achievement. Although significant in the\nTable 1. Results of the PCA\nComponents\nVariables Task-values Effort\nI understand most of the mathematics I am taught .77 -.24\nThe mathematics I learn now is very useful for everyday life .56 .18\nI need to keep taking mathematics for the kind of job I want after I leave\nschool\nHow much time do you usually spend on mathematics homework (in or out\nof school) on any given day?\nHow often do you complete all of your mathematics homework? -.45 -.54\nHow often have you been absent from your Grade 9 mathematics class this\nyear?\nHow often have you been late for your Grade 9 mathematics class this year? .27 .50\nNote: PCA = principal components analysis.Variables with a component loading equal or higher than .40 were considered to have high loadings.\n6 SAGE Open\nfirst-level analysis of the HLM, these predictors only\nthe variance unexplained in Level 1. The second level of the\nHLM analysis, which was the intraclass correlation coeffi-\ncient computations showed that from the total variance\naccounted for at this level, there was 18.18% of between-\nschool variance and 81.81% of within-school variance that\naffected students' academic achievement as shown in Table\n3. In addition, the total coefficient of determination (R2 =\n34.69%) between the observed and predicted students'\nachievement data computed for the entire HLM model sug-\ngested that the motivation components (task-values, student\neffort, and score variability within and between schools)\naccounted for 34.69% of the variance in relation to students'\nacademic achievement. This means that 65.31% of the vari-\nance was unaccounted by the HLM model and might be\nrelated to other factors besides motivation and student score\nvariability within and between schools that affected students'\nacademic achievement.\nDiscussion\nThe results of the PCA using students' self-reported EQAO\nquestionnaire data suggest a two-component model, which\nwas interpreted as task-values and effort using the research\nframework from Wigfield and Cambria (2010). The results\nof the PCA support the research literature in that it is possi-\nble to identify motivation components related to student\ntask-values as defined by the expectancy-value theory and\nstudent effort, using students' self-reported data (Cole et al.,\nThe results of the first level (fixed-model effect) of the\nHLM statistical analysis conducted on the Grade 9 students'\nself-reported questionnaire EQAO data suggest that students'\ntask-values and effort are significant predictors of their aca-\ndemic achievement on the EQAO test. These findings may\nprovide relevant information for teachers and educational\nagencies to help them make more valid interpretations of\nEQAO data when assessing student academic performance\nThe second level of the HLM analysis suggests that the\nvariance within and between schools is also a significant pre-\ndictor of student academic achievement for EQAO Grade 9\nmathematics assessments. There is 81.81% of score variabil-\nity within schools and 18.18% between schools based on\n16.79% of the total variance accounted at this level. The\nresults from the HLM statistical analysis indicated that stu-\ndent task-values, effort, and students nested in schools are\nsignificant predictors of achievement for EQAO data. These\nfindings support the literature as student task-values and\neffort are considered among the most immediate predictors\nof academic achievement and performance (Eccles &\nOne of the limitations of the current study is that it only\ntakes into consideration students' overall score, and it does\nnot explain the motivation effect per item. This is because\nitem position can have an effect on student motivation in\nrelation to the effort that the student puts forth in answering\nthe item correctly on the LSA (Wolf et al., 1995). Another\nlimitation is that the current study does not explore the\nimpact of low motivation on student academic achievement\nper item or question but rather the impact of motivation as a\nwhole. There is, however, a need to use other statistical tech-\nniques to explore the relationship of low motivation with\nitems that count and do not count on EQAO LSA toward the\nstudent grade. There is also a need to develop item response\ntheory models using pencil-and-paper tests that include\nmotivation as a parameter estimate in the model. This will\npermit a better design of EQAO tests to more accurately esti-\nmate student abilities. As a result of addressing these needs,\nmore valid interpretation of test results can be made when\nassessing student academic achievement in relation to the\nGrade 9 mathematics curriculum.\nTable 2. HLM Fixed Analysis Effect Model\nSource Numerator df Denominator df F p R2 %\nNote: HLM = hierarchical linear model. Dependent variable-academic achievement.\nTable 3. HLM Random Analysis Effect Model\nParameter Estimate Std. Error % Wald Z p\nNote: HLM = hierarchical linear model. Dependent variable-academic achievement.\nZerpa et al. 7\nConclusion\nThe research question that guided the current study was \"to\nwhat extent does the expectancy-value theory of motivation\nand student effort relate to students' academic achievement\non a LSA of mathematics?\" The outcome of the study sug-\ngests that student effort and task-values as defined by the\nexpectancy-value theory are related to students' academic\nachievement on EQAO Grade 9 mathematics assessments.\nIn the context of educational assessment, the current study\nprovides important evidence against the common assumption\nthat the impact of student motivation on LSA results may be\nnegligible, as the outcome of this study reveals that these\nmotivation components (task-values and effort) are significant\npredictors of students' Grade 9 EQAO academic achievement\nin mathematics. The results of this study, however, can have\nimplications for teachers and test organizations to help them\nbetter understand and assess students' academic performance\nin relation to motivation while using LSA. It also may help\nthem make better decisions about educational policies and\ncurriculum changes, which are sometimes implemented based\non EQAO test scores. The results of this study can also have\nimplication for research because it builds on the work by\nSmith (1995) by providing another avenue to examine student\nmotivation in relation to LSA.\nDeclaration of Conflicting Interests\nThe opinions presented in this paper are solely those of the authors\nand do necessarily reflect the opinions of SSHRC and EQAO\nFunding\nThe authors received the following financial support for the\nresearch and/or authorship of this article.\nThis research was funded by the Social Science and Humanities\nResearch Council (SSHRC) and the data were provided by the\nEducation Quality Accountability Office (EQAO). The authors\nwould also like to acknowledge the contribution of Karieann\nBrinson for her comments and edits related to this manuscript.\nReferences\nAtkinson, J. (1964). An introduction to motivation. Princeton, NJ:\nVan Nostrand.\nBishop, A., Clark, B., Corrigan, D., & Gunstone, D. (2006). Values\nin mathematics and science education: Researchers' and teach-\ners' views on the similarities and differences. International\nBrookhart, S. M., & Durkin, D. T. (2003). Classroom assessment,\nstudent motivation, and achievement in high school social stud-\nBrookhart, S. M., Walsh, J., & Zientarski, W. (2006). The dynamics\nof motivation and effort for classroom assessments in middle\nschool science and social studies. Applied Measurement in Edu-\nCamara, W. (2009). Moving large scale assessments to computer\nplatform. Large-Scale Assessment SIG, 2(1), 2-4.\nCole, J., Bergin, D., & Whittaker, T. (2008). Predicting student\nachievement for low stakes tests with effort and task value.\nDe Ayala, R., Plake, B., & Impara, J. (2001). The impact of\nomitted responses on the accuracy of ability estimation in\nitem response theory. Journal of Educational Measurement,\nDeMars, C. (2000). Test stakes and item format interactions.\nDrasgow, F., Levine, M., & Williams, E. (1985). Appropriateness\nmeasurement with polychotomous item response models and\nstandardized indices. British Journal of Mathematical and Sta-\nDweck, C. S., & Elliot, E. S. (1983). Achievement motivation. In\nE. M. Hetherington (Ed.) & P. H. Mussen (Series Ed.), Hand-\nbook of child psychology: Vol. 4. Social and personality devel-\nEccles, J. S., & Wigfield, A. (2002). Motivational beliefs, values\nEducation Quality Accountability Office. (2011, May 12). EQAO.\nAvailable from http://www.eqao.com\nEklof, H. (2006). Development and validation of scores from an\ninstrument measuring student test-taking motivation. Educa-\nFraire, R., Tideman, T., & Watts, T. (1997). Indices of cheating\non multiple-choice tests. Journal of Educational Statistics, 6,\nHarlen, W., & Crick, R. D. (2003). Testing and motivation for learn-\nKane, M. (2006). Current concerns in validity theory. In R. L.\nBrennan (Ed.), Educational measurement (4th ed., pp. 17-64).\nNational Council on Measurement in Education and American\nCouncil on Education. Westport, CT: Praeger.\nKarabatsos, G. (2003). Comparing the aberrant response detection\nperformance of thirty six person-fit statistics. Applied Measure-\nKloosterman, P. (1996). Students' beliefs about knowing and learn-\ning mathematics: Implications for motivation. In M. Carr (Ed.),\nMotivation in mathematics (pp. 131-156). Cresskill, NJ: Hamp-\nton Press.\nLinn, R., & Baker, E. (1996). Assessing the validity of the National\nAssessment of Educational Progress: NAEP technical review\npanel white paper. Washington, DC: National Center for\nResearch on Evaluation, Standards, and Student Testing/\nNational Center for Education Statistics.\nMaehr, M. L., & Meyer, H. (1997). Understanding motivation and\nschooling. Where we've been, where we are, and where we\nMarsh, W., Koller, O., Trautwein, U., Ludtke, O., & Baumert, J.\n(2005). Academic self-concept, interest, grades, and standard-\nized test scores: Reciprocal effect models of casual ordering.\n8 SAGE Open\nMcMillan, J., Simonetta, L., & Singh, J. (1994). Student opinion\nsurvey: Development of measures of student motivation. Edu-\nMeijer, R. (1996). Person-fit research: An introduction. Applied\nMeasurement in Education, 9, 3-8.\nMeijer, R., & Sijtsma, K. (1995). Detection of aberrant item score\npatterns: A review of recent developments. Applied Measure-\nMeijer, R., & Sijtsma, K. (2001). Methodology review: Evaluating\nMessick, S. (1989). Validity. In R. L. Linn (Ed.), Educational\nCouncil of Education.\nNering, M., & Meijer, R. (1998). A comparison of the person\nresponse function and the lz person fit statistics. Applied Psy-\nO'Neil, H. F., Abedi, J., Miyoshi, J., & Mastergeorge, A. (2005).\nMonetary incentives for low-stakes tests. Educational Assess-\nO'Neil, H. F., Jr., Sugrue, B., Abedi, J., Baker, E. L., & Golan, S.\n(1997). Final report of experimental studies on motivation and\nNAEP test performance (CSE Tech. Report No. 427). LosAnge-\nles: University of California, National Center for Research on\nEvaluation, Standards, and Student Testing.\nPintrich, P. R. (2004). A conceptual framework for assessing moti-\nvation and self-regulated learning in college students. Educa-\nPintrich, P. R., & Schunk, D. H. (1996). Motivation in education:\nTheory, research, and applications. Englewood Cliffs, NJ:\nMerrill\u00adPrentice Hall.\nPintrich, P. R., Smith, D. A. F., Garcia, T., & McKeachie, W. J.\n(1993). Reliability and predictive validity of the motivated\nstrategies for learning questionnaire (MSLQ). Educational and\nPutwain, D. (2007). Test anxiety in UK school children: Prevalence\nand demographic patterns. British Psychological Society, 77,\nPutwain, D. (2008). Do examinations stakes moderate the test anx-\niety-examination performance relationship? Educational Psy-\nReynolds, W. M. (1984). Depression in children and adolescents:\nPhenomenology, evaluation and treatment. School Psychology\nRoderick, M., & Engel, M. (2001). The grasshopper and the ant:\nMotivational responses of low-achieving students to high-\nstakes testing. Educational Evaluation and Policy Analysis, 23,\nRyan, K., Ryan, A., Arbuthnot, K., & Samuels, M. (2007). Stu-\ndents' motivation for standardized math exams. Educational\nSalomon, G. (1983). The differential investment of mental effort\nin learning from different sources. Educational Psychologist,\nSalomon, G. (1984). Television is \"easy\" and print is \"tough\":\nThe differential investment of mental effort as a function of\nperceptions and attributions. Journal of Educational Psychol-\nScheifele, U. (1991). Interest, learning, and motivation. Educa-\nSchmitt, N., Chan, D., Sacco, J., McFarland, L., & Jennings,\nD. (1999). Correlates of person fit and effect of person fit\non test validity. Applied Psychological Measurement, 23,\nSotaridona, L., Linden, W., & Meijer, R. (2006). Detecting answer\ncopying using kappa statistics. Applied Psychological Mea-\nSotaridona, L., & Meijer, R. (2003). Two new statistics to detect\nanswer copying. Journal of Educational Measurement, 40,\nStipek, D., Givven, K. B., Salmon, J. M., & MacGyvers, V. L.\n(1998). The value (and convergence) of practices suggested by\nmotivation research and prompted by mathematics education\nreformers. Journal for Research in Mathematics Education, 29,\nSundre, D., & Kitsantas, A. (2004). An exploration of the psychol-\nogy of the examinee: Can examinee self-regulation and test-\ntaking motivation predict consequential and non-consequential\ntest performance? Contemporary Educational Psychology, 29,\nSundre, D., & Moore, D. (2002). The student opinion scale: A\nmeasure of examinee motivation. Retrieved from http://www.\njmu.edu/assessment/resources/resource_files/sos_assessment_\nupdate_article.pdf\nvan Barneveld, C. (2007). The effect of examinee motivation on\ntest construction within an IRT framework. Applied Psycho-\nWigfield, A., & Cambria, J. (2010). Students' achievement values,\ngoal orientations, and interest: Definitions, development, and\nrelations to achievement outcomes. Developmental Review, 30,\nWigfield, A., & Eccles, J. (2000). Expectancy-value theory of\nachievement motivation. Contemporary Educational Psychol-\nWise, L. (1996). A persistent model of motivation and test perfor-\nmance. Paper presented at the annual meeting of the American\nEducational Research Association, New York, NY.\nWise, S., & DeMars, C. (2006). An application of item response\ntime: The effort-moderated IRT model. Journal of Educational\nWise, S., & Kong, X. (2005). Response time effort: A new measure\nof examinee motivation in computer-based tests. Applied Mea-\nWolf, L., & Smith, J. (1995). The consequence of consequence:\nMotivation, anxiety and test performance. Applied Measure-\nWolf, L., Smith, J., & Birnbaum, M. (1995). Consequences of per-\nformance, test motivation, and mentally taxing items. Applied\nVolante, L. (2006). An alternative vision for large-scale assessment\nin Canada. Journal of Teaching and Learning, 4, 1-14.\nZerpa et al. 9\nBios\nCarlos Zerpa is a PhD candidate in the Faculty of Education at\nLakehead University. His research interest is based on measure-\nment and evaluation of human physical and cognitive performance\nby implementing mathematical modeling techniques.\nKrystal K. Hachey is a full-time PhD student at the University of\nOttawa in Education. Her research interests are focused on mea-\nsurement and evaluation including Item Response Theory,\nStructuralEquationModeling,Validation,andSurveyDevelopment.\nDr. Christina van Barneveld is an Associate Professor in the\nFaculty of Education at Lakehead University. Her research interests\ninclude educational measurement and program evaluation.\nDr. Marielle Simon is a full professor at the Faculty of Education,\nUniversity of Ottawa. She teaches and conducts research in the\nareas of classroom assessment, large-scale assessments and\nresearch methods."
}