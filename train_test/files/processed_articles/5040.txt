{
    "abstract": "Abstract. Although personal liking varies considerably, there is a general trend of liking shared\nby many people (public favour). Visual liking in particular may be largely shared by people, as it\nis strongly influenced by relatively low-level perceptual factors. If so, it is likely that people have\ncorrect knowledge of public favour. We examined the human ability to predict public favour. In three\nexperiments, participants rated the subjective likability of various visual objects (e.g. car, chair),\nand predicted the mean liking rating by other participants. Irrespective of the object's category, the\ncorrelation between individual prediction and actual mean liking of others (prediction validity) was not\nhigher than the correlation between the predictor's own liking and the mean liking of others. Further,\nindividual prediction correlated more with the predictor's own liking than it was with others' liking.\nNamely, predictions were biased towards the predictor's subjective liking (a variation of the false\nconsensus effect). The results suggest that humans do not have (or cannot access) correct knowledge\nof public favour. It was suggested that increasing the number of predictors is the appropriate strategy\nfor making a good prediction of public favour.\n",
    "reduced_content": "a Pion publication\nRyosuke Niimi\nDepartment of Psychology, Graduate School of Humanities and Sociology, The University of Tokyo, Tokyo, Japan;\ne-mail: niimi@L.u-tokyo.ac.jp\nKatsumi Watanabe\nResearch Centre for Advanced Science and Technology, The University of Tokyo, Tokyo, Japan;\ne-mail: kw@fennel.rcast.u-tokyo.ac.jp\n Keywords: object perception, preference, aesthetics, false consensus effect, gender difference.\n1 Introduction\nLiking--that is, affective attitude towards objects, people, and events--is difficult to understand. Hu-\nmans like some and dislike others, but underlying rules and mechanisms of liking are obscure in most\ncases--there is no accounting for tastes. Nevertheless, scientists, including psychologists, have tried to\nexplain liking, as it frequently influences human behaviour, ranging from daily shopping to elections.\nLiking is a complex phenomenon influenced by multiple perceptual, psychological and social fac-\ntors, including perceptual regularity, personality, culture and so forth. For example, the popularity of\nphotographs--as measured by the number of views in an online community (Flickr)--is related to a\nnumber of variables, ranging from image features (colour, object) to social features of the individual\nwho uploaded the photograph (Khosla, Sarma, & Hamid, 2014). Recent studies have demonstrated,\nhowever, that visual liking is determined very fast and even unconsciously. For instance, subjective\nlikability of faces and common objects is reliably determined within 100 ms of stimulus presenta-\ntion (Niimi & Watanabe, 2012; Willis & Todorov, 2006). Results of neurophysiological experiments\nhave also suggested that the human brain automatically encodes subjective values of visual objects.\nActivities of reward-related brain regions such as the orbitofrontal cortex (OFC), cingulate cortex\nand striatum correlate with aesthetic/economic values (Ishizu & Zeki, 2011; Kawabata & Zeki, 2004;\nPadoa-Schioppa & Assad, 2006; Plassmann, O'Doherty, & Rangel, 2007; Yue, Vessel, & Biederman,\n2007), and some of those regions encode subjective values even when the observer is not engaged in a\nvalue-related task (Kim, Adolphs, O'Doherty, & Shimojo, 2007; Lebreton, Jorge, Michel, Thirion, &\nPessiglione, 2009; Tusche, Bode, & Haynes, 2010). Further, relatively low-level visual features, such\nas image contrast, contour roundedness, symmetry and complexity influence visual likability (Bar &\ntual fluency also affects likability (Reber, Winkielman, & Schwarz, 1998). For face attractiveness,\naverageness and symmetry are suggested to be the determinants (Thornhill & Gangestad, 1999; but\nsee DeBruine, Jones, Unger, Little, & Feinberg, 2007; Halberstadt & Winkielman, 2014). Thus, it is\napparent that visual liking is influenced by not only high-level social factors but also lower, perceptual\nfactors. Models of liking judgment should contain multiple processing stages (Leder, Belke, Oeberst,\nDo we know others' visual liking?\nIf a considerable part of liking judgment is determined by perceptual factors, it is not surprising\nthat there is a general trend of liking shared among many people; some are liked by many people\nand become popular, while others are not. It has been reported that there is a considerable consensus\namong people for picture preference (Vessel & Rubin, 2010) and face attractiveness ratings (Wood\n& Brumbaugh, 2009). Therefore, liking can be described as having two components, namely, public\nfavour and idiosyncratic liking (e.g. H\u00f6nekopp, 2006). The former may be more likely to be determined\nby perceptual factors. It seems harder to identify the origins of idiosyncratic liking, but familiarity may\nbe proposed to play a role (Reis, Maniaci, Caprariello, Eastwick, & Finkel, 2011; Zajonc, 1968).\nKnowing the common, average liking of others (public favour) is critical for human social behav-\niour. For instance, if we do not know a person beforehand, we will choose a popular product as a\npresent. In this study, we questioned whether thinking and predicting public favour of others is actu-\nally helpful.\nSocial psychology tells us that it would be difficult because people often misperceive social con-\nsensus. The false consensus effect (FCE) is the phenomenon that people overestimate the consensus\nbetween one's own and others' opinions (Krueger, 1998; Marks & Miller, 1987; Ross, Greene, &\nHouse, 1977). For instance, a person is asked to express his/her opinion about a given issue (e.g. vote\nfor/against a new nuclear plant) and to estimate percentage consensus, that is, the percentage of peers\nwho share the opinion. On average, the estimated consensus is higher than the actual consensus. The\neffect is known to occur with liking and disliking judgments (e.g. Gershoff, Mukherjee, & Mukho-\nFCE is not observed and even presents as the opposite phenomenon (false uniqueness effect; Bosveld,\nMost studies of FCE used materials that were presented as written word stimuli rather than visual\nstimuli from various categories (e.g. film, music genre, actor, sundae; Gershoff et al., 2008 used movie\nposters as stimuli, though the experiment did not focus on visual liking). If people explicitly know\nthe public favour of visual stimuli and discriminate it from their own liking, predicting others' liking\nwould correlate better than simply stating one's own liking. To address the issue, we conducted three\nexperiments in which participants reported their own liking (rating task) and predicted average others'\nliking (prediction task) for visually presented common objects.\nMethod\nThe stimuli were computer-generated colour images of 38 common objects (32 for experimental tri-\nals, 6 for practice trials; see Additional Material at http://i-perception.perceptionweb.com/journal/I/\nvolume/5/article/i0661). They were created by 3D computer graphic software (Shade 9, e-frontier Inc.,\nTokyo), and had been used as stimuli in another study (Niimi & Watanabe, 2012). We included various\nobject categories (e.g. furniture, vehicles and kitchenware). The objects were placed on a square stage\nand oriented to the camera (frontal view) or 30\u00ba rotated (3/4 view), resulting in 76 stimulus images (64\nfor experimental and 12 for practice trials; see Figure 1 for examples).\nProcedure\nThe experiment consisted of a rating task and a predicting task. The apparatus, stimuli and procedure--\nexcept the instructions--were identical between the two tasks. In the rating task, participants were\ninstructed to rate the visual likability of the object, that is, \"how good is the object's appearance?\"\nParticipants were also told that the task was to rate the visual likability of the object, not necessity/\nwanting. In the prediction task, the experimenter first explained the procedure of the rating task. As\na fictional cover story, the participants were told that 20 individuals (11 males, 9 females) living in\nTokyo had performed the rating task and they were 21.4 years of age on average, ranging from 18 to\n36 years. They were then asked to predict the result, namely, the average of the likability ratings made\nby the 20 fictional participants. The age and gender data in the cover story were obtained from a group\nof participants in another experiment not reported in this paper, as it was a good estimation of age/\ngender of participants recruited by the authors' laboratory.\nThe rating/prediction was reported through a response box on which seven buttons were horizon-\ntally aligned. The buttons were marked by number (1\u00ad7), simulating a 7-point Likert scale ranging\nfrom 1 (\"very unlikable\") to 7 (\"very likable\"). Button 4 indicated a neutral response (\"neither\").\nDo we know others' visual liking? 574\nIn each trial, a stimulus image was shown at the centre of a computer screen (379  302 mm,\nLCD) and subtended 303  227 mm. The background was uniformly grey. The viewing distance was\napproximately 80 cm, although the participant's head position was not fixed by any apparatus. The\nimage was presented until a response was made. Participants were told that there was no need to hurry\nto respond (i.e. self-paced task).\nEach participant performed 64 experimental trials (32 objects in two views) for each task. In\nadvance, a practice session (6 trials) was conducted in which the 6 practice objects were shown either\nin frontal view (3 trials) or in 3/4 view (3 trials). At the end of the experiment, any participant who\nperformed the prediction task was debriefed that the cover story was fictional.\nThis experiment, as well as the subsequent experiments, was approved by the institutional review\nboard and conducted in accordance with the Code of Ethics and Conduct (2009) of the Japanese Psy-\nchological Association and the Declaration of Helsinki. Written informed consent was obtained from\nall participants in advance.\nParticipants\nForty-eight individuals were recruited and paid for their participation. Written informed consent was\nobtained in advance. They were graduate/undergraduate students of universities in/nearby Tokyo. As\n8 had participated in other experiments using an identical set of stimuli, we omitted their data and ana-\nTwenty participants (randomly chosen) performed both the likability rating task and the predic-\ntion task. In order to counterbalance any carryover effects between the tasks, we randomly divided\nparticipants into a rating-first group (n 5 10), who performed the rating task first, and the prediction-\nfirst group (n 5 10), who performed the prediction task first. The remaining 20 participants performed\nonly either the rating task (n 5 10, rating-only group) or the prediction task (n 5 10, prediction-only\ngroup). Mean age was not significantly different among the four groups. Gender was as equalized as\npossible among the groups.\nWithin- and between-group designs\nWe planned two designs of analysis, within- and between-group designs. In the within-group design,\nwe examined the correlation between likability rating and prediction made by the identical set of\nparticipants, namely, the 20 who performed both tasks. In the between-group design, we examined a\ncorrelation between likability rating and prediction made by the different sets of participants. For this\ndesign, we adopted ratings made by the rating-only group and the rating-first group (n 5 20 raters\nin total), and adopted predictions made by the prediction-only group and the prediction-first group\nThe 20 participants (11 females, 9 males) who performed both tasks had a mean age of 21.4\nFigure 1. Examples of the stimuli. Experiments 1 and 3 adopted identical sets of stimuli, which included various\nobject categories. The objects were shown in either frontal view or 3/4 view. Experiment 2 examined two sets\nof single-category objects (cars and chairs; 3/4 view only). See Additional Material for the entire list of objects.\nResults\nGroup analysis\nFirst, we examined how well a group of 20 participants could predict the average likability rating\nof 20 participants. For each view of each object, the rated/predicted likability scores were averaged\nacross participants. We examined the object-wise correlation between mean prediction and mean rat-\ning, which reflected prediction validity as a group. In the within-group analysis, the mean prediction\nfor 3/4 view. As a group of 20 individuals, they successfully predicted the average liking of others.\nIndividual analysis\nThe central interest of the present paper was the validity of predictions made by individuals. To ad-\ndress this issue, for each participant, we computed three indices (Figure 2): prediction validity (r\nval\n),\nrating consistency (r\ncon\n) and prediction bias (r\nbias\n).\nPrediction validity r\nval\nwas defined as the correlation between a single participant's prediction (P\ni\n)\nand the mean of other participants' ratings (L\n\u00af). In the within-group design (Figure 2A), participant\n(i) was not included in L\n\u00af, that is, L\n\u00afwas the mean of the other 19 participants' ratings. This procedure\nprevented overestimation of r\nval\n. In the between-group design (Figure 2B), L\n\u00afwas simply defined as the\nmean of ratings made by the 20 raters. Rating consistency r\ncon\nwas the correlation between a single par-\nticipant's rating (L\ni\nor L\nj\n) and the mean of other participants' ratings (L). In both within- and between-\ngroup designs, the participant in question (i or j) was not included in L\n\u00af(i.e. L\n\u00afwas the mean of the other\n19 participants). Prediction bias r\nbias\nwas a correlation between a single participant's rating (L\ni\n) and\nthe prediction by that same participant (P\ni\n). This index was available only in the within-group design.\nThe results of the three indices, averaged across participants, are listed in Table 1. Prediction\nvalidity r\nval\nwas approximately .3 on average for both analysis designs and both object views, which\nwas lower than the prediction validity observed in the group analysis (.68\u00ad.87). This was not surpris-\ning, as individual differences in the data were overlooked by averaging in the group analysis.\nThere were two critical findings here. First, r\nval\nwas not higher than r\ncon\n; the mean likability rating\nL\n\u00af, which was predicted by each individual participant, was equally correlated with individual predic-\ntion P\ni\nand individual rating L\ni\n. In other words, participants could predict mean others' liking only to\nthe degree to which the individual's own liking was similar to the mean others' liking. A repeated-\nmeasures analysis of variance (ANOVA) on Fischer's Z-transformed correlation coefficients (r\nval\nand\nFigure 2. Schematic diagram of the three correlation coefficient indices (rval\n, rcon\n, rbias\n) used in individual analysis.\nEach square represents a set of 32 values (rating/prediction for 32 stimulus objects). A Within-group design\n(each participant performed both the rating task and the prediction task), B between-group design (the two tasks\nwere performed by separate groups). Li\nand Pi\nrepresent likability ratings and predictions, respectively, made by\nparticipant i. L\n\u00afrepresents mean rating of others. It was either an average of n \u00ad 1 ratings or n ratings (depending\non design and analysis). Prediction validity (rval\n) of participant i was defined as a correlation between Pi\nand L\n\u00af.\nRating consistency rcon\nwas a correlation between Li\nand L\n\u00af. Prediction bias (rbias\n) was a correlation between Pi\nand L\ni\n, which was available only in the within-group analysis.\nDo we know others' visual liking? 576\nr\ncon\n) confirmed this observation. For the within-group design, an ANOVA with two within-participant\nfactors (2 indices  2 views) showed no significant effect (main effect of index, F  1; main effect\nANOVA with two factors (index as between-participant factor and view as within-participant factor)\ninteraction, F  1).\nSecond, r\nbias\nwas higher than r\nval\n; individual prediction P\ni\nwas more correlated with one's own\nrating L\ni\nthan the mean others' rating L\n\u00af, even though the participants were required to predict L\n\u00af. In\norder to confirm this observation, a repeated-measures ANOVA with two factors (2 indices  2 views)\nwas conducted on Fischer's Z-transformed correlation coefficients (r\nval\nand r\nbias\n). This analysis was\navailable only for the within-group design. The main effect of index was significant, F(1,19) 5 19.35,\nbias\nwas higher than r\nval\n. Neither the main effect of view (F  1) nor the\ninteraction (F  1) was significant.\nAnalysis of consensus\nWe also conducted an analysis in the manner usually adopted in FCE studies--testing whether pre-\ndicted consensus is higher than real consensus or not. This analysis was available only in the within-\ngroup design. First, we transformed the rating/prediction responses (1\u00ad7) to binary data by consider-\ning responses 1\u00ad3 as \"bad\" and responses 5\u00ad7 as \"good\". The neutral response (4) was omitted. For\neach participant and each view of each object, we computed predicted consensus and real consensus.\nPredicted consensus is an agreement between prediction and rating made by the same participant.\nFor instance, if participant i rated a chair as \"good\" and predicted others' ratings for the same chair\nas \"good (bad)\", the predicted consensus for the object is 1 (0). We averaged this across objects. Real\nconsensus is the proportion of others whose rating agreed with the participant's rating. For instance, if\na chair was rated by participant i as \"good\", and 12 of 19 other participants rated the chair as \"good\",\nthe real consensus was .63 (12/19). We averaged this across objects.\nThe mean predicted consensus and mean real consensus are shown in Table 2. For both views,\npredicted consensus was higher than real consensus. The two measurements were arcsine transformed\nand tested by a repeated-measures ANOVA with two factors (2 measurements  2 views). The main\n(F  1) and the interaction (F  1) were not significant. These results confirmed the occurrence of\nFCE and were consistent with the finding that prediction bias r\nbias\nwas higher than prediction validity r\nval\n.\nDiscussion\nThe results clearly demonstrated that participants were not good at predicting others' average\nliking ratings. As r\nval\nwas not higher than r\ncon\n, we did not find evidence that individual participants\nsuccessfully utilized any knowledge on the difference between one's own liking and others' average\nTable 1. Results of individual analysis in Experi-\n, prediction validity; rcon\n, rating\nconsistency; rbias\n, prediction bias. These indices\nwere determined for each participant. This table\nshows their averages.\nStimulus rval\nrcon\nrbias\nExperiment 1(within-group)\nExperiment 1 (between-group)\nExperiment 2 (within-group)\nliking. The result implies that, if one wants to predict others' liking of visual objects, there is no need\nto \"predict\". Simply trusting one's own liking would be sufficient.\nDid our participants give up on the prediction task and simply respond in the same manner as in\nthe rating task? This did not seem to be the case. An additional analysis on response time revealed that\nparticipants spent a significantly longer time on the prediction task than they did on the rating task. We\nanalysed log response times after excluding outlier response times (defined as shorter than 200 ms or\nlonger than 10 s). As shown in the Additional Material, the mean log response time was longer for the\nprediction task in both views and both designs. An ANOVA demonstrated that the main effect of task\nwas significant, p  .01. In addition, the grand averages of rating/prediction differed between the tasks\n(see Additional Material). The mean predicted likability score was significantly (p  .001) higher than\nthe mean rated likability score in both views and both designs. In short, participants performed the\ntwo tasks differently. They tried to predict mean others' liking, but the results were not better than the\ncorrelation between one's own liking and others' liking.\nThe higher grand average for scores in the prediction task than in the rating task implies that\nparticipants predicted that others would like the objects more than they did. It may be more precise\nto say that the participants could not have correct knowledge of what others dislike, rather than what\nothers like.\nThe second critical finding was that participants predicted others'liking to be more similar to their\nown liking than it really was. The analysis of consensus further supported this conclusion, namely,\nthat the FCE occurred. These results imply that people do not consciously know the public favour\nof visual stimuli and discriminate it from one's own liking. Consequently, it seems plausible that a\nperson with ordinary, similar-to-average liking would perform the prediction task well. In fact, for the\n20 individuals who performed both tasks, their rating consistency r\ncon\nwas positively correlated with\nprediction validity r\nval\nfor 3/4 view). A similar finding was reported for predictions of a familiar individual's liking (Lerouge\nThe stimulus objects used in Experiment 1 included various categories. However, in our daily lives, it\nseems more critical to predict others'like/dislike in a single object category, for instance, \"this car will\nbe liked by many others, but that car will not be liked\". Stich, Kn\u00e4uper, Eisermann, and Leder (2007)\nshowed that observers utilize different sets of aesthetic criteria for different object classes. Thus, if\nthe stimulus objects are derived from a single category (e.g. car), observers might adopt a fixed set of\nevaluation criteria. Such a situation would increase prediction validity. Therefore, in Experiment 2, we\nfocused on two sets of single-category object, namely, cars and chairs. In order to verify the robustness\nof result, we examined these two categories.\nMethod\nParticipants\nTwenty-one individuals were recruited and paid for their participation. Written informed consent\nwas obtained in advance. Participants were graduate/undergraduate students of universities in/nearby\nTokyo. For one participant, the experiment was terminated halfway through due to technical difficul-\nties with the apparatus. We analysed the data of the remaining 20 participants (11 males, 9 females),\nwho had a mean age of 21.9 years (range 19\u00ad35). None had participated in Experiment 1.\nTable 2. Results of consensus analysis in Experiments 1 and 2.\nStimulus Real consensus Predicted consensus\nExperiment 1 (within-group)\nExperiment 2 (within-group)\nDo we know others' visual liking? 578\nStimuli\nWe adopted 32 cars and 32 chairs as stimulus objects (see Additional Material). In addition, four cars\nand four chairs were used for practice trials. The objects were selected from available commercial\npackages of 3D object model data. The objects were rendered into coloured stimulus images in the\nsame way as in Experiment 1.As FCE occurred equally for frontal views and 3/4 views in Experiment 1,\nwe used 3/4 view only in Experiment 2 (Figure 1).\nDesign and procedure\nThe apparatus was identical to that used in Experiment 1. The instructions to participants were also\nidentical to those in Experiment 1, except that the stimulus images would show various examples of a\nsingle category (car/chair). Each participant performed both rating and prediction tasks, and the order\nof the tasks was counterbalanced. In each task, two sessions were conducted, namely, a car session and\na chair session. Each session had 4 practice trials and 32 experimental trials. The order of object cat-\negory was also counterbalanced; half of the participants performed the car session first, while the other\nhalf performed the chair session first. Thus, each participant performed 4 sessions (2 tasks  2 object\ncategories). The mean age of participants was not significantly different among the four groups (rating/\nprediction first  car/chair first). Gender was as equalized as much as possible among the groups. A\nself-paced break was given in between the sessions. At the end of the experiment, participants were\ndebriefed that the cover story for the prediction task was fictional.\nResults\nGroup analysis\nThe mean prediction of the 20 participants was highly correlated with the mean rating, r 5 .88,\nAs in Experiment 1, the group of 20 predicted the mean liking well.\nIndividual analysis\nIn the same manner as in Experiment 1, we examined individual prediction validity (r\nval\n), rating consist-\nency (r\ncon\n) and prediction bias (r\nbias\n) for each object category. Only the within-group design (Figure 2A)\nwas adopted in Experiment 2. A summary of the indices is shown in Table 1. Prediction validity r\nval\nwas\nnot higher than rating consistency r\ncon\n. A repeated-measures ANOVA (2 indices  2 object categories)\non the Fischer's Z-transformed r\nval\nand r\ncon\nfound no significant effect (F  1 for main effects and in-\nteraction). Prediction bias r\nbias\nwas higher than prediction validity r\nval\n, and the ANOVA on the Fischer's\nZ-transformed r\nbias\nand r\nval\nthe main effect of object category and the interaction were not significant (F  1 and F(1,19) 5 1.96,\np 5 .178, respectively). In summary, the pattern of results was qualitatively similar to that of Experi-\nment 1. There were no reliable differences among the object categories.\nAnalysis of consensus\nWe compared predicted consensus and real consensus in the same way as in Experiment 1 (Table 2). For\nboth object categories, predicted consensus was higher than real consensus; according to an ANOVA\n(2 measurements  2 object categories) on arcsine-transformed predicted/real consensus measure-\neffect of object category (F  1) nor the interaction (F  1) was significant. As in Experiment 1, FCE\nwas observed.\nDiscussion\nWe replicated the pattern of results from Experiment 1. The failure in predicting others' liking was\nobserved again for the likability ratings of single-category objects (car/chair). We also confirmed that\nmean log RT was longer for the prediction task than the rating task (see Additional Material), suggest-\ning that the participants did not give up on the prediction task, but the predictions were biased towards\none's own liking.\nIn the preceding experiments, the fictional age/gender profile in the cover story for the prediction task\nwas comparable to the actual profile of the participants themselves. Such a situation might encourage\nthe participants to utilize their own liking as reference data. If so, it is reasonable that they drew their\npredictions to their own liking, resulting in the biased prediction. Indeed, some studies showed that\nFCE is dependent on attributes of the group for which the liking is predicted (Bosveld et al., 1995;\nHoch, 1987). In Experiment 3, therefore, we asked participants to predict the mean liking of a group\nthat was different from the participants, namely, the other-gender group.\nMethod\nThe stimuli were identical to those used in Experiment 1. Forty participants (20 males, 20 females)\nand female participants, respectively. They were graduate/undergraduate students of universities in/\nnearby Tokyo. All the participants performed both rating and prediction tasks. The order of the tasks\nwas roughly counterbalanced; for each gender group, 11 participants performed the rating task first,\nand the remaining 9 performed the prediction task first. Age and gender were as equalized as much as\npossible among the sub-groups of task order.\nThe instructions for the rating task were identical to those in Experiment 1. In the prediction\ntask, the participants were given a fictional cover story, which stated that 40 individuals (20 men,\n20 women) living in Tokyo had performed the rating task. Then, the male (female) participants were\nasked to \"predict the result of the 20 women (men)\", namely, the average of the likability ratings made\nby the 20 individuals. As a part of the cover story, participants were also told that the 20 women (men)\nwere 21.0 years of age on average, ranging from 18 to 32. In addition, the procedure of the rating task\nwas explained to the participants.\nResults\nGroup analysis\nOverall, the mean prediction was positively correlated with the mean rating. The mean prediction by\nthe 20 male participants was correlated with the mean rating by the 20 female participants, r 5 .61,\nprediction by the 20 female participants was also correlated with the mean rating by the 20 male par-\nview. Again, as a group, both gender groups well predicted the mean liking of the other-gender group.\nThe correlation between the mean rating by males and the mean rating by females was r 5 .55 for\nIndividual analysis\nIn the same way as in the preceding experiments, r\nval\n, r\ncon\nand r\nbias\nwere calculated for each participant.\nNote that r\nval\n/r\ncon\nhere indicates a correlation between individual prediction/rating and mean rating of\nthe other-gender group, not the same-gender group (Table 3).\nFirst, we compared prediction validity r\nval\nwith rating consistency r\ncon\n. On the Fischer's Z trans-\nforms of r\nval\nand r\ncon\n, we conducted a mixed-design ANOVA with three factors--gender as a between-\ngroup factor and index (r\nval\n/r\ncon\n) and view (frontal/three-quarter) as within-group factors. No main\nindex and view, respectively. Interactions were also non-significant (p  .1). In both gender groups,\nr\nval\nwas not higher than r\ncon\n. Next, we compared prediction validity r\nval\nwith prediction bias r\nbias\n. On\nthe Fischer's Z transforms of the indices, a 3-way factorial ANOVA (gender  index  view) was\np 5 .030, respectively. No interaction was significant (p  .1). Consistent with Experiments 1 and 2,\nin both gender groups, r\nbias\nwas higher than r\nval\n.\nAnalysis of consensus\nPredicted consensus and real consensus were examined (Table 4). A mixed-design ANOVA with gen-\nder as a between-group factor and measurement (predicted/real consensus) and view (frontal/three-\nquarter) as within-group factors was conducted on arcsine-transformed predicted/real consensus data.\nDo we know others' visual liking? 580\nIn other words, male participants' predicted consensus was higher than female participants' predicted\nconsensus, although there was no gender difference in real consensus. The simple main effect of meas-\nurement was significant in both gender groups (p  .001), confirming that FCE occurred irrespective\neffects were non-significant.\nDiscussion\nExperiment 3 demonstrated that the prediction of mean liking of the other gender is similarly unreli-\nable. Consequently, it was implied that the biased predictions in Experiments 1 and 2 were not due\nto the experimental design in which the participants were asked to predict liking of people similar to\nthemselves. Analysis on consensus revealed the reliable FCE as well.\nOne curious finding was the gender difference in FCE; predicted consensus was higher for the\nmale participants than it was for the female participants. Therefore, we further examined FCE size,\ndefined as predicted consensus minus real consensus. An ANOVA (gender  view) revealed that FCE\nsize was significantly larger for the male participants than for the female participants (F(1,19) 5 5.69,\np 5 .028). Although FCE occurred in both gender groups, the magnitudes of the effect were not equal.\nSimilarly, mean r\nbias\ntended to be higher for male participants than for female participants (Table 3),\nthough this difference was not statistically significant.\n5 General discussion\nIf people consciously know the public favour of visual stimuli and discriminate it from their own lik-\ning, they could predict others' liking better than they could by simply relying on their own liking. The\nresults of the present study showed this is not the case; the validity of prediction was not as good as the\nconsistency of one's own liking with the public favour. Further, the predictions were biased towards\none's own liking. FCE was replicated as well, irrespective of multiple-category versus single-category\nobjects.\nTable 4. Results of consensus analysis in Experiment 3.\nStimulus Real consensus Predicted consensus\nMale participants\nFemale participants\nTable 3. Results of individual analysis in\nExperiment 3. rval\n, prediction validity;\nrcon\n, rating consistency; rbias\n, prediction\nbias. These indices were determined for\neach participant. This table shows their\naverages.\nStimulus rval\nrcon\nrbias\nMale participants\nFemale participants\nThese findings suggest that people do not have correct knowledge of the difference between their\nown liking and public favour. Because unconscious, automatic processes/factors play a considerable\nrole in preference judgments, it would be more difficult to understand one's own liking than we\nbelieve. In addition, it is also likely that prediction of an individual other's liking (e.g. friend, neigh-\nbour) is more critical for determining our social behaviour, and our mind is tuned more to individual\nliking than to public favour. The current finding may provide new insights into the adaptive function\nof favour/preference, which is still far from fully understood.\nIs group prediction biased as well?\nAs shown by the group analysis, the predictions as a group (i.e. mean prediction) were highly cor-\nrelated with actual mean liking. On the other hand, the prediction validity analyses revealed that indi-\nvidual prediction was biased towards one's own liking. Is this also the case for prediction as a group?\nWe addressed this issue with the data from Experiments 1 and 3. As predictors, we adopted 20 partici-\npants who performed both rating and prediction tasks in Experiment 1. As a target to be predicted, we\nadopted the mean rating by 22 participants (11 males, 11 females) who performed the rating task first\nin Experiment 3. The group r\nval\n, a correlation between mean prediction by the predictors and the target\nmean rating, was .642 and .681 for frontal and 3/4 view, respectively. The group r\nbias\n, a correlation\nbetween mean prediction by the predictors and the mean rating by the same predictors, was .849 and\n.875 for frontal and 3/4 view, respectively. Therefore, it seems that group r\nbias\nwas higher than group\nr\nval\n, confirming that prediction as a group is also biased towards the group's mean liking.\nThe power of number of predictors\nIt is noteworthy that increasing the number of predictors would yield greater prediction validity. As\nnoted earlier, the group r\nval\nwas .642 and .681, which seems much higher than mean individual r\nval\nobserved in Experiments 1 and 3. This was likely because individual variations in prediction were\ncompromised by averaging across predictors. We conducted a simulation in which the effect of predic-\ntor number was examined. Given the 20 predictors in Experiment 1 and the target mean rating by 22\nfrom Experiment 3 (as noted earlier), we computed mean r\nval\n, r\ncon\nand r\nbias\nfor each number of predictors\n(n 5 1\u00ad20). For instance, when n 5 5, we randomly selected 5 individuals among the 20 predictors,\nand r\nval\nwas computed as a correlation between mean prediction by the 5 and the target mean rating\nby the 22. We tried a maximum of 1,000 combinations of 5 predictors and averaged the results. The\ncase of n 5 1 corresponds to individual r\nval\n, r\ncon\nand r\nbias\n, and the case of n 5 20 corresponds to the\ngroup r\nval\n, r\ncon\nand r\nbias\ndescribed earlier. Figure 3 shows the result. As discussed earlier, the predictions\nwere biased towards own liking values, irrespective of n, but the absolute value of prediction valid-\nity increases drastically as a function of n. That is, the prediction validity is simply a function of the\nnumber of predictors.\nFigure 3. Prediction bias (rbias\n), prediction validity (rval\n) and rating consistency (rcon\n) as a function of number of\npeople who predicted the mean likability rating. This analysis is based on the data of Experiments 1 and 3. rbias\nis\na correlation between the mean prediction by predictors and the mean likability rating of identical predictors. rval\nis a correlation between the mean prediction and the actual mean rating by the target group (22 individuals from\nExperiment 3). rcon\nis a correlation between the mean rating of the predictors and the mean rating by the target\nDo we know others' visual liking? 582\nRemaining issues\nInterestingly, we found a gender difference in FCE. Experiment 3 showed that the magnitude of FCE\nwas greater for male participants than it was for female participants. There was no significant gender\ndifference in FCE size in any other experiment (p  .1). Hence, the more biased prediction by male\nparticipants was found only when the task was to predict the liking of the other-gender (i.e. female)\ngroup. In other words, male participants tended to judge that the female group would similarly like\nparticular visual objects. An analogous phenomenon was reported by Marcus and Miller (2003), who\nshowed that women know face attractiveness as rated by other-gender individuals better than men do.\nHowever, the cross-gender effect warrants further investigation.\nAnother remaining issue is the effect of object class. We tested everyday objects in this study,\nand it is still unclear whether the current finding holds for other object classes (e.g. faces, artwork), as\naesthetic criteria can differ across different object classes (Stich et al., 2007) and/or there may be less\nconsensus in aesthetic criteria for everyday objects than those for faces and artwork.\nConclusions\nIn conclusion, our participants showed little indication that prediction was better than making deci-\nsions based on their own rating when judging others' public favour of visual objects. That is, people\ndo not seem to have access to or utilize knowledge of public favour. Thus, the lesson from the current\nstudy is simple, namely, do not make predictions independently (because it is not better than choosing\nwhat you like), but rather, ask other people, as two heads are better than one.\nReferences\nBosveld, W., Koomen, W., van der Pligt, J., & Plaisier, J. W. (1995). Differential construal as an explanation for\nfalse consensus and false uniqueness effects. Journal of Experimental Social Psychology, 31, 518\u00ad532.\nDeBruine, L. M., Jones, B. C., Unger, L., Little, A. C., & Feinberg, D. R. (2007). Dissociating averageness and\nattractiveness: Attractive faces are not always average. Journal of Experimental Psychology: Human\nGershoff, A. D., Mukherjee, A., & Mukhopadhyay, A. (2008). What's not to like? Preference asymmetry in the\nGilovich, T. (1990). Differential construal and the false consensus effect. Journal of Personality and Social\nHalberstadt, J., & Winkielman, P. (2014). Easy on the eyes, or hard to recognize: Classification difficulty\ndecreases the appeal of facial blends. Journal of Experimental Social Psychology, 50, 175\u00ad183.\nHoch, S. J. (1987). Perceived consensus and predictive accuracy: The pros and cons of projection. Journal of\nH\u00f6nekopp, J. (2006). Once more; is beauty in the eye of the beholder? Relative contributions of private\nand shared taste to judgments of facial attractiveness. Journal of Experimental Psychology: Human\nJacobsen, T., Schubotz, R. I., H\u00f6fel, L., & von Cramon, Y. (2006). Brain correlates of aesthetic judgment of\nKhosla, A., Sarma, A. D., & Hamid, R. (2014). What makes an image popular? Proceedings of the 23rd\nKim, H., Adolphs, R., O'Doherty, J. P., & Shimojo, S. (2007). Temporal isolation of neural processes underlying\nface preference decisions. Proceedings of the National Academy of Sciences of the USA, 104, 18253\u00ad\nKrueger, J. (1998). On the perception of social consensus. Advances in Experimental Social Psychology, 30,\nLebreton, M., Jorge, S., Michel, V., Thirion, B., & Pessiglione, M. (2009). An automatic valuation system\nLeder, H., Belke, B., Oeberst, A., & Augustin, D. (2004). A model of aesthetic appreciation and aesthetic\nLerouge, D., & Warlop, L. (2006). Why it is so hard to predict our partner's product preferences: The effect\nLindel, A. K., & Mueller, J. (2011). Can science account for taste? Psychological insights into art appreciation.\nMarcus, D. K., & Miller, R. S. (2003). Sex differences in judgments of physical attractiveness: A social relations\nMarks, G., & Miller, N. (1987). Ten years of research on false-consensus effect: An empirical and theoretical\nNiimi, R., & Watanabe, K. (2012). Consistency of likeability of objects across views and time. Perception, 41,\nNisbett, R. E., & Kunda, Z. (1985). Perception of social distributions. Journal of Personality and Social\nPadoa-Schioppa, C., & Assad, J. A. (2006). Neurons in the orbitofrontal cortex encode economic value. Nature,\nPlassmann, H., O'Doherty, J., & Rangel, A. (2007). Orbitofrontal cortex encodes willingness to pay\nReber, R., Winkielman, P., & Schwarz, N. (1998). Effects of perceptual fluency on affective judgments.\nReis, H. T., Maniaci, M. R., Caprariello, P. A., Eastwick, P. W., & Finkel. E. J. (2011). Familiarity does indeed\npromote attraction in live interaction. Journal of Personality and Social Psychology, 101, 557\u00ad570.\nRoss, L., Greene, D., & House, P. (1977). The `false consensus effect': an egocentric bias in social perception\nStich, C., Kn\u00e4uper, B., Eisermann, J., & Leder, H. (2007). Aesthetic properties of everyday objects. Perceptual\nSuls, J., & Wan, C. K. (1987). In search of the false-uniqueness phenomenon: fear and estimates of social\nTinio, P. P. L., Leder, H., & Strasser, M. (2011). Image quality and the aesthetic judgment of photographs:\nContrast, sharpness, and grain teased apart and put together. Psychology of Aesthetics, Creativity, and the\nTusche, A., Bode, S., & Haynes, J.-D. (2010). Neural responses to unattended products predict later consumer\nVessel, E. A., & Rubin, N. (2010). Beauty and the beholder: Highly individual taste for abstract, but not real-\nWood, D., & Braumbaugh, C. C. (2009). Using revealed mate preferences to evaluate market force and\ndifferential preference explanations for mate selection. Journal of Personality and Social Psychology, 96,\nYue, X., Vessel, E. A., & Biederman, I. (2007). The neural basis of scene preferences. Neuroreport, 18,\nZajonc, R. B. (1968). Attitudinal effects of mere exposure. Journal of Personality and Social Psychology,\nPublished under a Creative Commons Licence a Pion publication\nKatsumi Watanabe is Associate Professor of Cognitive Science at the\nUniversity of Tokyo. He received B.A. in experimental psychology (1995)\nand M.A. in life sciences (1997; highest honors) from the University of\nTokyo and his PhD in Computation and Neural Systems from California\nInstitute of Technology, in 2001, for his work in cross-modal interaction\nin humans. He was a research fellow at the National Institute of Health\n(USA) and a researcher at the National Institute of Advanced Science\nand Technology (Japan). His research interests include: scientific\ninvestigations of explicit and implicit processes, interdisciplinary\napproaches to cognitive science, and real-life applications of cognitive\nscience.\npsychology from the University of Tokyo. He was a post doc researcher\nat RIKEN Brain Science Institute and Research Centre for Advanced\nScience and Technology (RCAST) of the University of Tokyo. He is now\nAssistant Professor (Jokyo) of the Department of Psychology at the\nUniversity of Tokyo. His research focuses on visual cognition, including\nobject recognition, visual preference, attention, optical illusion, and\nshape perception. For more information visit rnpsychology.org."
}