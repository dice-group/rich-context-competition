{
    "abstract": "Abstract\nOne of the challenges for instructors in online education is to create learning opportunities through online text-based\ndiscourse. The goal of this study was to examine the use of content analysis to better understand graduate students'\nlearning in online discussions. The discussion transcripts of a hybrid graduate course were analyzed to determine levels\nand frequencies of learning or cognitive presence. The analysis of the online discussions was guided by social constructivist\nrationale, and includes descriptive statistics and inter-rater reliability measurements. Findings show that cognitive presence\nlevels were concentrated in categories marked for exploration and integration. Non-cognitive messages resulted in the\nhighest frequencies of discussion messages, demonstrating indications of social presence and teaching presence. Training\nwas found to be a key factor in resolving coding inconsistencies to improve the reliability of the content analysis. The\nprocesses of content analysis applied in this study to evaluate learning in online discussions provided useful information for\nthe development and study of online discussions for learning.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nArticle\nOnline education continues to grow. Over the past decade,\nincreasing numbers of students in the United States have\nenrolled in online college courses. In 2011, approximately\nUnited States had enrolled in one or more online courses\n(Allen & Seaman, 2013). The literature suggests that online\neducation will continue to expand throughout the upcoming\ndecade, although not without concerns. Even though higher\neducation administrators and faculty seemingly have\nbecome more accepting of online courses as an extension to\nthe physical campus (Bowen, 2013), chief academic offi-\ncers of universities perceive that the majority of university\nfaculty members remain skeptical of teaching online (Allen\n& Seaman, 2013). Many faculty have concerns about the\nintegrity and quality of online courses (Jaggars & Bailey,\n2010), although the literature suggests that learning out-\ncomes in hybrid courses are the same and in some respects\nsuperior to learning outcomes in face-to-face courses\n(Means, Toyama, Murphy, Bakia, & Jones, 2010). More\nresearch of online discussions across the various disciplines\nwould be helpful those interested in applying asynchronous\ndiscussions in their courses. In this research, an instructor\nof a hybrid course applied an empirical method to assess\nevidence of learning in the course's online discussions.\nFaculty who want to substantiate the learning in their\ncourses' asynchronous discussions may want to consider\napplying similar methods described in this study, and con-\nsider publishing the results.\nResearch on the use of asynchronous discussions used in\ncourses for learning is relatively new, considering that the\nInternet has been available in schools for only a few decades.\nin the United States had Internet access, which is in stark\ncontrast with less than 5% of classrooms having Internet\npast decade, there has been an increase in the use of asyn-\nchronous discussions as an instructional strategy in face-\nto-face courses as well as in hybrid and online courses. More\nrecently, strategies such as \"flipping a classroom\" have\nincreasingly involved the use of online discussions for learn-\ning in face-to-face courses to extend instruction outside the\nphysical classroom in preparation for learning inside the\nIn this study, content analysis was applied to examine the\ndiscussion transcripts for learning with respect to a concep-\ntual framework developed from a 4-year study of computer\nconferencing (Garrison, Anderson, & Archer, 2001). The\ngoal was to gain a broader, deeper understanding of the lev-\nels of learning in the discussions. An assumption of this\nstudy is that online discussions are essential elements of\n1California State University, Sacramento, USA\nCorresponding Author:\nMark A. Rodriguez, California State University, Sacramento, 6000 J Street,\nEmail: rodrigue@csus.edu\nContent Analysis as a Method to Assess\nOnline Discussions for Learning\nMark A. Rodriguez1\n Keywords\ncognitive presence, asynchronous discussions, content analysis\n2 SAGE Open\nonline learning. Throughout this article, the terms online dis-\ncussions or asynchronous discussion are used interchange-\nably and are intended to refer to time-delayed discussions,\nnot synchronous real-time discussions.\nLiterature Review\nLearning theories provide a basis for understanding how\nlearning occurs in online discussions. Principles of learning\ntheories are used to explain or support teaching in meeting\nthe learning needs of students (Borich & Tombari, 1997;\nonline learning are the principles of constructivist learning\ntheories, which include collaboration, interaction, and the\nVygotsky, 1980). In turn, these constructivist principles fun-\ndamentally characterize how learning occurs in asynchro-\nnous discussions (Lebow, 1993; McLoughlin & Oliver,\nunderlying the approach to this study in examining the dis-\ncussion transcripts of graduate students for evidence of\nlearning.\nChoosing discussion strategies is an important step in the\nplanning of online learning. A review of literature identifies\nfactors to consider in planning for online discussions, which\nmay help planning for the discussions.\nOnline Discussion Strategies\nAsynchronous discussions are typically an important part of\nlearning in online or hybrid courses. Discussions offer a way\nfor students to learn and articulate their understanding of\nlearning through interactions with each other and the instruc-\ntor (Parker & Hess, 2001). Darabi, Liang, Suryavanshi, and\nYurekli (2013), in their meta-analysis of 80 studies of online\ndiscussions, found that \"learners responded better to strate-\ngic and productive discussion than when they were asked to\nelaborate on a topic\" (p. 239). Although the purpose of this\nstudy does not involve an in-depth analysis of the various\nonline discussion strategies, it does give an overview of\nstrategies and factors that are relevant in planning. Further\nstudies that describe the processes of choosing and applying\ntext-based discussion strategies would be helpful.\nThe Socratic method of discussion has traditionally been\nrecognized as an effective way to help students learn\nbeyond memorizing and regurgitating facts (Hansen, 1988).\nSocratic discussion methods employ thought-provoking\nquestions which are evident in online discussion strategies\nsuch as in online debates and case studies (Yang, Newby, &\nBill, 2005). Literature circles are another Socratic method\nof discussion that has been used effectively to support\nlearning in both online and face-to-face discussions.\nLiterature circles begin with a discussion of a text, and are\nguided by open-ended questions that are designed to pro-\nmote deeper thinking and discussion of the text. Having\nliterature circles online provides for added reflection oppor-\ntunities for students in reading the transcripts of the discus-\nsions, which supports broader and deeper perspectives and\nunderstanding of the discussion topic (Larson, 2009). The\nthink-pair-share discussion method has been structured\neffectively for use in online discussions (Johnson &Aragon,\n2003). In this strategy, students first think independently\nabout a problem or a question, are then paired or grouped to\ndiscuss their thoughts, ending with the whole class sharing\nand discussing the topic. The think-pair-share discussion\nstrategy builds a strong foundation for communicating\nideas with a range of fellow students and the instructor\nIn a study by R. S. Anderson, Goode, Mitchell, and\nThompson (2013), they examined a group of doctoral stu-\ndents' perceptions of four different online constructivist dis-\ncussion strategies, including problem-based learning (PBL),\ndiscussion web strategy, 3-2-1 strategy, and case study. Each\nof the strategies was said to reflect social constructivist learn-\ning because of the interaction involved in the discussions and\nthe development of personal meanings associated with the\ndiscussions. In the PBL discussion strategy, the students dis-\ncussed self-directed problem-solving objectives. In the dis-\ncussion web strategy, the students identified and described\nthe pros and cons of a particular topic, justifying their posi-\ntion. In the 3-2-1 online discussion strategy, the students read\na text, selected three key points from the text, provided two\nsupportive explanations, and then posed one question related\nto the reading. In the case study strategy, the students ana-\nlyzed real-life situations, read case information, made reflec-\ntive connections, stated opinions, and asked and responded\nto questions. Students'perceptions of these discussion strate-\ngies were collected through surveys and interviews, and\nwere analyzed, coded, and categorized by four coders based\non a process of inductive reasoning. The results showed that\nthe students believed all of the discussion strategies to be\neffective for particular types of discussions.\nFactors in Planning Online Discussions\nConsideration of the practical aspects or factors of online\ndiscussions can be helpful in planning for the use of them in\nonline learning. Among the factors to consider are time, stu-\ndent attitudes, and technology.\nAsynchronous discussions require reading, writing, and\ncomprehension tasks that arise from students needing to\ndevelop, post, and read text-based communication. This can\nresult in more time required of the students to complete\nonline discussions compared with the amount of time typi-\ncally spent in face-to-face class discussions (Meyer, 2003).\nInstructors would be wise to consider the coordination needs\nin scheduling the time for students to read and post to the\ndiscussions. Reading, writing, and the increased amount of\ntime that students spend in text-based discussions can pro-\nvide for more focused and deeper learning through the\nRodriguez 3\nincreased opportunities to read, reflect, and respond\n(Benbunan-Fich & Hiltz, 1999; Bliuc, Ellis, Goodyear, &\nAsynchronous text-based discussions may have a positive\nimpact on student attitudes toward discussions by lessening\nany difficulties arising from students who are reluctant to\nparticipate, feel confronted in the discussions, or are not\ngiven enough time to voice their opinions. The atmosphere in\nasynchronous discussions is seen by some students to be\neasier to handle, less stressful, equitable, and less likely to be\ndominated by a few individuals (Wang & Woo, 2007).\nHowever, there are disadvantages of text-based discussions\nthat students may perceive negatively. Text by itself does not\ncommunicate the nuances of tone of voice nor does it replace\nvisual cues of face-to-face discussions. The use of emoticons\nin online discussions can suggest tone of voice that may clar-\nify the meaning behind the text (Tiene, 2000). More studies\nare needed to understand the effect and practice of using\nemoticons in online academic discussions.\nStudents need to possess functional technical skills to par-\nticipate in asynchronous discussions. The necessary level of\ntechnical difficulty is not extremely high for most students,\nbut a lack of familiarity with the online discussion features\nmay cause frustration among some students. Frustration with\ntechnical difficulties may affect the quality of participation\nin the discussions (Davidson-Shivers, Muilenburg, & Tanner,\n2001). The knowledge and skills necessary for students to\nparticipate in asynchronous discussions can be effectively\naddressed in instruction.\nReviewing various discussion strategies and considering\nfactors that can affect asynchronous discussions will aid in\nplanning. Analyzing the discussions is an additional task that\ncan provide information for reflecting and revising the over-\nall discussion strategy in efforts to understand and improve\nthe online discussion's effectiveness for learning.\nMethods to Analyze Online Discussions\nThere are many analysis methods reported in the literature\nthat have been used to assess learning in online discussions,\nfew have been applied extensively. Among the analysis\nmethods, two methods appeared to have been applied in sev-\neral other studies, and were considered for this study, the\nInteraction Analysis Model (Lucas, Gunawardena, &\nMoreira, 2014) and the Community of Inquiry (COI) frame-\nA study that used the Interaction Analysis Model from\nGunawardena, Lowe, and Anderson (1997) described an\nanalysis of an online debate. Gunawardena et al. character-\nized online debate as a \" . . . co-creation of knowledge and\nnegotiation of meaning\" (p. 406). The Interaction Analysis\nModel consisted of five categories or phases, including (a)\nsharing or comparing information, (b) discovery and explo-\nration, (c) negotiating of meaning or co-construction of\nknowledge, (d) testing and modification or synthesis, and (e)\nagreeing or application. From the coding and analysis, based\non these five categories, much of the participants' discus-\nsions were determined to be characteristic of two categories,\nexploration or discovery, and negotiation of meaning. The\nInteraction Analysis Model categories have some similarities\nof categories with Garrison's COI framework's practical\ninquiry model. However, the COI framework, which was\nchosen for this study, appeared more broad-based, including\ncategories of teaching presence, social presence, and cogni-\ntive presence.\nTheoretical Rationale\nThe theoretical framework of this study is based on the COI\nframework (Garrison et al., 2001), which is grounded in con-\nstructivist learning theory that considers collaboration,\nreflection, and critical analysis as essential to learning. The\nCOI framework is comprised of three core elements: social\npresence, teaching presence, and cognitive presence. Social\npresence, teaching presence, and cognitive presence are all\nintertwined. \"Cognitive presence is defined as the extent to\nwhich learners are able to construct and confirm meaning\nthrough sustained reflection and discourse\" (p. 11), and can\nbe used as a \" . . . means to assess the nature and quality of\ncritical, reflective discourse that occurs within the text based\neducational environment\" (p. 7).\nThe element of cognitive presence was selected as a pri-\nmary focus for this study based on reviews of research stud-\nies that employed content analysis to assess online\ndiscussions. Many of the content analysis studies that were\nreviewed were identified in articles from De Wever,\nSchellens, Valcke, and Van Keer (2006) and Rourke,\nAnderson, Garrison, and Archer (2001). Content analysis is a\nqualitative research method that has been used fairly exten-\nsively, and its selection followed a \"directed approach,\"\nwhere the analysis method is based on methods identified in\nthe literature (Hsieh & Shannon, 2005). In research pertain-\ning to the content analysis of asynchronous discussions,\nresearch literature highlights the importance of reporting the\nbasis of its theoretical foundation including information\nregarding the unit of analysis and the reliability of the study\n(De Wever et al., 2006). The theoretical rationale, unit of\nanalysis, and reliability of analysis are all reported in this\nstudy.\nIn developing the COI framework, a practical inquiry\nmodel was developed. The practical inquiry model was\ndesigned to be applied to analyze transcripts of online dis-\ncussions for cognitive presence. The practical inquiry model\nconsists of four phases grounded in perception, deliberation,\nconception, and action. Each phase reflects a process leading\nto problem resolution beginning with identifying or under-\nstanding the problem, termed the triggering event. The sec-\nond phase involves exploration of the topic. In the third\nphase, integration, possible resolutions or solutions or con-\nclusions are identified, and the fourth phase, resolution, is\n4 SAGE Open\nwhere the solutions or conclusions are selected and applied.\nThese four phases or categories (triggering event, explora-\ntion, integration, and resolution) represent a process of\nevolving learning in asynchronous discussions that can be\nsupported and analyzed. The process to identify evidence of\neach of the four phases is an interpretive process. To guide\nthe process of identifying cognitive presence in online dis-\ncussions, categories of descriptors, indicators, sociocogni-\ntive processes, and examples act as guidelines to facilitate\ncontent analysis coding of the discussion transcripts\nis important to note that the practical inquiry model indica-\ntors should \"not be seen as immutable\" (p. 9), meaning that\nother studies using the practical inquiry model may find a\nneed to refine or revise the criteria to meet specific analysis\nneeds, as was the case for this study.\nMethod\nIn this study, an empirical method, content analysis, was\nchosen to assess cognitive presence of three asynchronous\ndiscussions of graduate students in one of their courses.\n\"Content analysis is a research technique for making repli-\ncable and valid inferences from texts (or other meaningful\nmatter) to the contexts of their use\" (Krippendorff, 2012, p.\n24). Content analysis enables a process to systematically\nexamine the quality of learning in online discussions\n(Gunawardena et al., 1997). Although the use of content\napplied to study learning in asynchronous discussions.\nHenri (1992) described computer conferencing as a \"gold\nmine of information\" (p. 118) that would provide research-\ners a rich resource to analyze and advance online learning.\nUse of content analysis to assess online discussions has\nincreased over the past 20 years, just as Henri had pre-\ndicted, but concerns about lack of uniformity and disclo-\nsure of the analysis methods have arisen (De Wever et al.,\nIssues in comparing content analysis studies of online dis-\ncussions have arisen due to a lack of consistency in the dif-\nferent analysis instruments used (Rourke &Anderson, 2004).\n\"This lack of replication (i.e., of successful applications of\nother researchers' coding schemes) should be regarded as a\nserious problem\" (Rourke et al., 2001, p. 6). Consequently,\nresearch literature has stressed the need for more studies to\nemploy similar instruments (T. Anderson, 2005; De Wever\net al., 2006), which in turn should increase the reliability and\nvalidity of these types of studies (Stacey & Gerbic, 2003).\nThe importance of building on previous research influenced\nchoosing the method and the coding instrument of this study.\nRepeating research designs helps establish the reliability of\nthe results, which can be obtained from repeated use of the\nsame instrument. Further information regarding this practi-\ncal inquiry model, which has been applied in other content\nanalysis studies of online discussions, can be found in stud-\nies from Akyol and Garrison (2011), de Leng, Dolmans,\nJ\u00f6bsis, Muijtjens, and van der Vleuten (2009), De Wever\nAnalysis of the results of this study is aimed to (a) aid the\nresearcher in the planning of future asynchronous discus-\nsions in the graduate program, (b) provide information for\ninstructors who are interested in studying the learning effec-\ntiveness of their asynchronous discussions, and (c) apply les-\nsons learned from this study to further studies involving the\nuse of content analysis as a method to evaluate online discus-\nsions. One goal is to continue using content analysis to better\nunderstand the use of this method to assess online learning\nstrategies. Analysis of other discussions from the same grad-\nuate cohort of this study is planned.\nResearch Questions\nResearch Question 1: What were the levels of cognitive\nand non-cognitive presence in a cohort of graduate stu-\ndents' online discussion transcripts from one of their col-\nlege courses?\nResearch Question 2: Considering that the use of roles in\nonline discussion was the main topic in the first online\ndiscussion in the course, what were the students' percep-\ntions at the end of the semester regarding the use of roles\nin online discussions?\nIn the \"Discussion\" section of this study, the researcher\nreflects on the use of content analysis and the practical\ninquiry model as a means to assess the learning effectiveness\nof online discussions.\nParticipants and Context\nIn this study, three online discussions of a hybrid graduate\ncourse, the \"Fundamentals of Online Pedagogy,\" were exam-\nined for cognitive presence. The researcher of this study was\nthe instructor of the course, which was primarily online, but\nincluded three face-to-face class sessions at the beginning,\nthe middle, and the end of the semester. The course is part of\na master's program designed to be completed entirely\nthrough a hybrid delivery system over four consecutive\nsemesters. All of the courses in the students' graduate pro-\ngram typically met face-to-face only 2 to 3 times each semes-\nter. The course examined for this study was situated in the\nfirst semester of the students' graduate program. There were\na total of 19 students (10 females, 9 males) participating in\nthe course, although only 15 students finished the course. A\nrandom sample size of students (N = 15) was chosen for the\nanalysis. Three separate discussions were analyzed out of\neight total online discussions in the course. Each of these\ndiscussions included three discussion groups, except the first\ndiscussion which had five groups, and each of the discus-\nsions occurred over a week time period. The reasoning\nRodriguez 5\nbehind selecting and analyzing the first three online discus-\nsions of the course was to allow for the opportunity to make\nobservations of the students' learning progression in the\ncourse from the beginning of the semester. Studies of the\nother discussions in the same course and same students will\nbe based on the results of this study.\nOnline Discussion Strategy\nThe online discussion strategy used in each of the three dis-\ncussions of the study was identical, consisting of a format\nsimilar to the 3-2-1 discussion strategy, which included read-\ning a text, finding key points in the text, providing supportive\nexplanations, and posing questions. The instructions for the\ndiscussion included several items:\n\u00b7\n\u00b7 Students were divided into groups for the online\ndiscussions.\n\u00b7\n\u00b7 Each discussion required reading of an initial journal\narticle.\n\u00b7\n\u00b7 Groups were to identify and discuss three main points\nfrom the article with a goal of discussing and identify-\ning related topics to broaden and deepen students'\nunderstanding of the topic.\n\u00b7\n\u00b7 Students were then to search for another article based\non the three main points that had been identified and\ndiscussed; read the new article; and then describe, dis-\ncuss, and synthesize the concepts from the two\narticles.\n\u00b7\n\u00b7 To conclude the discussions, summaries were posed\nsynthesizing each group's discussions.\n\u00b7\n\u00b7 Role assignments were assigned to each member of\nthe discussion group, which included monitor, encour-\nager, facilitator, quality assurance checker, and\nsummarizer.\nData Collection\nThe analysis of the online discussions was unobtrusive,\noccurring after the students had already completed the course\nand been given their final grades. Technology was used to\ncollect, process, and organize the data for analysis.\nBlackboard, the University's learning management system,\nwas used to archive the discussion transcripts. Learning\nmanagement systems, such as Blackboard, support the use of\ncontent analysis to evaluate online discussions with their\ncapacities to store online discussion transcripts that are eas-\nily accessible for later analysis. The discussion transcripts\nfor this study were exported from Blackboard, and then\nimported into HyperRESEARCH, a qualitative software pro-\ngram that was used to organize and code the discussion\ntranscripts.\nA secondary data source included student survey data\nwhich were collected at the end of the semester. The survey\ninquired of the students about their perceptions regarding the\nuse of roles in online asynchronous discussions, and was the\nsubject of Discussion 1, which was based on the reading of\nan article on the use of roles in online discussions from De\nWever, Van Keer, Schellens, and Valcke (2010).\nCoding\nTwo coders, the researcher, who was the instructor of the\ncourse, and another university instructor coded the three sets\nof discussion transcripts. Training was provided to the coders\nto aid them in coding the discussion transcripts. The training\ninvolved the researcher/instructor of the course describing\nand demonstrating to the second coder how to apply the cod-\ning instrument (Table 1) to the discussion transcripts.\nIncluded in the training were discussions between the two\ncoders regarding the instructions for Discussion 1 that had\nbeen given to the students who participated in the discussion.\nThe training occurred during 1 week, totaling approximately\n3 to 4 hr. Discussion transcripts from Discussion 1 were used\nin the training to demonstrate the coding. Following the\ntraining, the coders worked independently in coding the units\nof analysis.\nThe units of analysis consisted of each message thread\nfrom each of the discussion participants in the three discus-\nsions. Clear demarcation at the beginning of each message\nidentified for the coders where to begin and end each coding\neffort. Each of the messages was analyzed by the two coders\nand classified according to the indicator representing its level\nof cognitive presence (see Table 1), which was based on a\nmodified version of the practical inquiry model (Garrison\net al., 2001). During the coding, if the coders determined that\nmore than one indicator of cognitive presence was evident in\na message thread, the indicator selected was to be based on a\npreponderance of evidence in the message. Training for the\ncoders was considered a critically important factor that could\naffect the reliability of the content analysis applied in this\nstudy.\nVariables\nThe variables related to cognitive presence were defined\nbased on four categories of Garrison's practical inquiry\nmodel (Garrison et al., 2001), including triggering events,\nexploration, integration, and resolution. Each broad category\ncontained a sub-level of indicators, which were used as the\nbasis to code each discussion message. Although the sub-\nlevel indicators were drawn from Garrison's practical inquiry\nmodel, they were modified to facilitate the coders in identi-\nfying evidence in the online discussion transcripts of this\nstudy. Categorization of the sub-level indicators provided\ndeeper insight into the cognitive and non-cognitive presence\ndisplayed by the students in the discussions. Table 1 shows\nthe sub-level indicators for the broad categories of cognitive\nand non-cognitive presence, including the percentages of\noccurrence for the three discussions. The percentages of\n6 SAGE Open\nmessages identified for each indicator reflected in Table 1\nrepresent an agreed-on coding of each message based on dis-\ncussion and reconciliation of messages between the two cod-\ners after all of the discussion messages had been coded.\nThe discussion messages were separated for each cogni-\ntive and non-cognitive broad category by group for each of\nthe three discussions (see Tables 2, 3, and 4). These three\ntables show the number of messages coded for each broad\ncategory, including the triggering event, exploration, integra-\ntion, resolution, and non-cognitive. Inter-rater reliability of\nthe coding was calculated for Discussions 2 and 3 with the\nuse of Cohen's kappa statistical measurements, including\npercentages of agreement between coders (see Table 5).\nPotential for Coding Error and Bias\nThe researcher of this study was the instructor of the course\nwith 20 years of experience in the field of educational technol-\nogy. Steps were taken to reduce any ambiguity and bias in the\ncoding process: (a) Student names were removed from the dis-\ncussion transcripts in preparation for the coding. This elimi-\nnated coder bias toward any student names. (b) Training and\npractice coding of Discussion 1 was provided to emphasize\nprocedures for the coding based strictly on the criteria estab-\nlished for each indicator (see Table 1). Training was consid-\nered essential to reduce ambiguity in the judgment process for\ncoding each message. (c) Demarcation of the unit of analysis\nTable 1. Messages Coded for Cognitive Presence by Indicators by Discussion.\nCognitive presence Indicators D1 (%) D2 (%) D3 (%)\n2. Exploration a. Adds to established points but does not systematically defend/justify/develop. 11.5 4.5 4.5\nb. Presents relevant background information related to discussion topic. 2.4 13.0 2.1\n3. Integration a. Explores potential solutions, applications, or conclusions. 1.7 3.5 3.1\nc. \nReference to previous message followed by substantiated agreement, for\nexample, \"I agree because . . .\"\nd. Substantiated building on, adding to others' ideas. 1.3 1.0 4.8\ne. \nSynthesis: Connecting ideas. Integrating information from various sources--\nTextbook, articles, and personal experience.\n4. Resolution a. Applying, testing, defending, or critiquing solutions or conclusions. 0.0 0.0 0.7\nTable 2. Total Messages by Cognitive and Non-Cognitive Levels by Group.\nDiscussion 1 Group 1 Group 2 Group 3 Group 4 Group 5 Total %\nRodriguez 7\nof the coding was discussed between the coders to eliminate\nany errors from confusion in the unit of data to code. The unit\nof analysis for coding consisted of the message as opposed to\nsentences or paragraphs. However, there was the possibility of\nmessages containing evidence of more than one indicator,\nwhich was addressed in this study by coding the messages\nbased on the preponderance of evidence in the message.\nResults\nA total of 724 messages were posted in the three discussions.\nDiscussion 1 included a total of 233 messages, Discussion 2\nFor all of the three discussions, the average percentage of\nmessages coded for each cognitive presence category resulted\ntion, and 2.9% resolution. In addition, messages coded as\nnon-cognitive accounted for 37.1% of the total messages.\nThree categories, non-cognitive, exploration, and integra-\ntion, accounted for roughly 90% of the total messages in the\nthree discussions. The lowest percentage of responses was in\nthe categories of resolution (2.9%) and the triggering event\ncategory (6.8%), which were both considerably lower than\nany of the other three categories. The messages categorized\nas triggering events primarily consisted of descriptions of the\narticles assigned by the instructor for the discussions. The\ndiscussion messages categorized as resolution primarily con-\nsisted of student summaries of the discussions with little\nback-and-forth discussion among the students about the\nsummaries. In online discussions, students appear to need\nmore direct guidance to increase discussion that would be\ncharacteristic of resolution.\nMessages coded as exploration (29.4%) and integration\n(23.8%) accounted for the highest frequencies of cognitive\npresence in this study. High frequencies of exploration and\nintegration messages have been similarly reported in the\nfindings of other studies that also relied on the primary\ninquiry model and content analysis to evaluate learning in\nThe non-cognitive category accounted for the overall highest\nfrequency (37.1%) of messages, which was 7.7% higher than\nthe next highest category of exploration (29.4%). Students\nwho did not follow the instructions in Discussion 1 appeared\nto directly influence the higher frequency of messages coded\nas non-cognitive (47.1%), which was substantially higher\nthan non-cognitive messages in Discussions 2 (27.5%) and 3\n(35.5%). Discussions 2 and 3 did not have as many messages\nasking questions about the instructions and procedures of the\nonline discussion. A solution to reduce potential non-cogni-\ntive messages is to teach the students about the procedures\nand processes prior to the first discussion. In a fully online\ncourse, this could be accomplished through a separate dis-\ncussion thread that focused exclusively on protocols of the\ndiscussions. In a hybrid course, this could be addressed in the\nfirst face-to-face class.\nTable 3. Total Messages by Cognitive and Non-Cognitive Levels by Group.\nDiscussion 2 Group 1 Group 2 Group 3 Total %\nTable 4. Total Messages by Cognitive and Non-Cognitive Levels by Group.\nDiscussion 3 Group 1 Group 2 Group 3 Total %\nTable 5. Inter-Rater Reliability and Percent Agreement.\n% agreement Cohen's kappa\n8 SAGE Open\nDisplayed in Tables 2, 3, and 4 are the total number of\nmessages for each of the three discussions which are listed\nby group showing the total percentages of cognitive presence\n(triggering event, exploration, integration, and resolution)\nand non-cognitive presence. There were five groups partici-\npating in Discussion 1, three groups in Discussion 2, and\nthree groups in Discussion 3.Apattern appeared to be emerg-\ning from the overall frequency of messages coded for the\ndifferent categories of cognitive and non-cognitive presence.\nMessages coded as non-cognitive or exploration were usu-\nally the most frequent type of messages identified in each of\nthe three discussions. Messages coded as resolution or as\ntriggering events were identified as the least frequent mes-\nsages in all three discussions. Messages coded as integration\nwere usually identified as the third most frequently occurring\nmessages in the three discussions, except in Discussion 3\nwhere they were second highest, only 3.2% lower than total\nnon-cognitive messages.\nDiscussion 1 Cognitive and Non-Cognitive\nPresence\nDiscussion 1 had the highest percentage of messages coded\nas non-cognitive, 47.6% (see Table 2) of the messages, com-\npared with an average of 37.1% of non-cognitive messages\nfor all three discussions. Table 1 shows that most of the mes-\nsages classified as non-cognitive were coded for clarifying\ndiscussion procedures. Messages coded as exploration were\nslightly higher at 30.5%, when compared with an average of\n29.4% for all three discussions. Integration messages in\nDiscussion 1 were lower at 18%, similar to Discussion 2's,\nbut considerably lower than Discussions 3's percentage of\nmessages categorized for integration at 32.3%. Few mes-\nsages (0.9%) were coded as triggering events. However,\nsome of the messages that were coded as exploration and\nintegration contained evidence characteristic of triggering\nevent criteria, but were coded as exploration or integration\ndue to the preponderance of evidence in the message.Another\nfactor affecting the low frequency of triggering events in\nDiscussion 1 was partially a result of the students being\nintroduced to the topic of Discussion 1 during the first face-\nto-face class meeting. Students having the opportunity to talk\nto each other about Discussion 1's reading assignment in a\nface-to-face environment eliminated some of the online dis-\ncussion that likely would have occurred.\nDiscussion 2 Cognitive and Non-Cognitive\nPresence\nIn Discussion 2 (see Table 3), the data show an increase of\ncompared with Discussion 1 at 0.9%, while the average for\ntriggering events for all three discussions was at 6.8%. The\ncontent of the triggering event messages in Discussion 2\nclearly showed that the students were able to adequately\nidentify and describe the topic of the discussion, which was\nbased on an article focused on social presence in asynchro-\nnous discussions.\nAlso in Discussion 2, students were much more active\nexploring and discussing additional perspectives related to\nthe topic of the assigned reading. The increased discussion\nactivity resulted in the highest percentage of messages coded\nfor exploration with 41%, compared with the 29.4% explora-\ntion average of all three discussions. Furthermore, data for\nDiscussion 2, as seen in Table 1, show that 14% of the stu-\ndents' messages focused on asking questions seeking spe-\ncialized information. Discussion 2 also showed a 20.1%\ndecrease of messages coded as non-cognitive compared with\nDiscussion 1, due to fewer messages coded for clarifying\ndiscussion procedures. It appeared that the students had\nbecome more comfortable with the procedures for the online\ndiscussion. Also, in Discussion 2, messages identified as\nresolution remained low at 2% of the total messages, which\nconsisted of students reporting a summary that synthesized\nthe overall discussion, although there continued to be limited\ndiscussion among the students regarding the summary.\nDiscussion 3 Cognitive and Non-Cognitive\nPresence\nDiscussion 3 had the highest frequency of messages coded\nfor all three discussions. Integration reflects a higher level of\nthinking skills reflecting students' ability to synthesize the\ndiscussion concepts and form summary conclusions. In\nDiscussion 3, there was a higher percent of non-cognitive\nThe increase of non-cognitive messages in Discussion 3\nfrom Discussion 2 was due to a moderate increase in mes-\nsages that were seeking clarification (see Table 1). Discussion\n3 was focused on \"concept mapping,\" which was for some\nstudents a difficult concept to read about and discuss and\nthen apply. In the course, there was an assignment that\nrequired the creation of concept maps. Some confusion arose\namong the students between the assignments requiring the\nactual building of a concept map with the online discussion\nabout concept mapping. Conversely, the increase in integra-\ntion messages appeared to be a result of more discussion\nrequiring the forming of solutions of how to develop concept\nmaps that would be applied.\nReliability\nInter-rater reliability identifies the extent of agreement\namong the coders and takes into account any agreement\noccurring by chance. Information of the inter-rater reliability\nprovides an indication of the reproducibility and stability of\na study, and is considered an essential element of content\nanalysis (De Wever et al., 2006; Lombard, Snyder-Duch, &\nRodriguez 9\nThere are differences in interpretations regarding accept-\nable levels of inter-rater reliability coefficients. According to\nexistence of differing interpretations regarding coefficient\nlevels, including interpretations saying that coefficients\nbelow .8 give rise to concern (Lombard et al., 2002). In a\nsimilar study, Garrison et al. (2001) assessed cognitive pres-\nence in two online discussions and calculated kappa coeffi-\nincreasingly higher coefficients, from .35 to .75, and from\n.45 to 84, were thought to be a result of increased training for\nthe coders. Training appeared to be an influencing factor.\nIn content analysis, latent content is often considered a\nchallenging area of concern. The latent nature of discussions\ninvolves potential reliability issues related to the coders'\" . . .\ninterpretations of the meaning of the content\" (Potter &\nreliability associated with latent content, training proves to\nbe a key factor. Higher coefficients are thought to be more\nachievable as a result of coders having more training with the\nprocesses related to coding (Riffe, Lacy, & Fico, 1998).\nIn Table 5, the kappa coefficients and percentages are dis-\nplayed for Discussions 2 and 3. Discussion 1 is not displayed\nas it was used in the training process for the two coders. There\nwere three separate group discussions in Discussion 2 and\nthree group discussions for Discussion 3. Inter-rater reliabil-\nity coefficients were calculated for each group discussion.\nMost of the coefficients were in the moderate range from .41\nto .60. However, for Discussion 2, Group 2, an inter-rater reli-\nability of .85 was calculated, which is a substantially higher\nkappa coefficient than all of the other discussions. The coders\nperiodically compared and discussed their coding of\nDiscussion 1. For Discussions 2 and 3, the coders compared\nand discussed their coding only after the entire coding was\ncompleted, and did not periodically compare coding results\nduring the coding of the discussion transcripts.\nThe training that was provided to the coders of this study\nincluded (a) discussions of the coding instrument and the\nindicators, (b) discussions on how to apply and record the\ncoding, (c) a review of the instructions and procedures for\nthe discussions, and (d) practice coding of Discussion 1,\nincluding discussing the differences in the coding results and\narriving at an agreement to resolve differences.\nThe low reliability coefficients were a result of differing\ninterpretations between the two coders regarding the content\nof the reading assignments. These differences were exam-\nined, discussed, and resolved after the coding of Discussions\n2 and 3. The sub-level indicators appeared to be a source of\nthe problem in applying and coding the discussion messages.\nIn discussing the differences of the coding, each message\nwas considered in relation to the sub-level indicator and in\nrelation to the broader category (triggering event, explora-\ntion, integration, and resolution).\nDescriptions of the type of content in a message would\nmeet specific categorization criteria based on the sub-level\nindicators of each broad category. Messages coded as trig-\ngering event were to basically describe the initial article.\nDiffering scores arose from the coder not knowing the differ-\nence between new information from a new article from infor-\nmation that was derived from the assigned article of the\ndiscussion. Thorough review of the assigned article was\nneeded.\nSub-level indictors for integration and exploration were\nthe areas where many other scores differed. Examination of\ndifferences of integration and exploration made apparent a\nneed to further refine the sub-level indicators along with an\nexamination of the prompts or instructions for the discus-\nsion. Each of the messages coded differently in these two\nareas were discussed in relation to the articles associated\nwith the discussions until an agreement was made.\nAs a result, specific type of training was also identified\nthat would need to focus on the subject matter of the topics\nthat were the focus of each online discussion. Overall, the\nprocess of evaluating the coding provided insight into refine-\nments or changes to the discussion strategy that may have a\npositive impact on the use of online discussions for\nlearning.\nSurvey Results\nAmong the learning goals for the students in the course\ninvolved in this study was to develop knowledge of online\ndiscussion strategies. A survey was given to the students at\nthe end of the semester to examine their perceptions of a dis-\ncussion strategy that was the main topic of Discussion 1,\nwhich was focused on the use of roles in online discussion.\nThe survey results collected at the end of the semester\nshowed that all of the students, except one, believed that the\nuse of roles in online discussions would result in generating\ngreater participation from all the members of the discussions,\nwhich is a contributing factor for quality and productivity in\nonline discussions. The student who did not wholeheartedly\nsupport the use of roles as essential to online discussions\nbelieved that roles required too much structure in an online\ndiscussion, which could inhibit free-flowing discourse.\nSecond, the role of summarizer was identified by a majority\nof students as the most important and difficult role in an\nonline discussion. Students also indicated in the survey that\nthey had difficulty with summarizing skills, although they did\nbelieve that summarizing discussions would help foster\ndeeper understanding of the topics being discussed. In addi-\ntion, students were evenly divided in their beliefs about rotat-\ning roles in discussions. Half of the students believed that\nmaintaining the same role was more beneficial to the discus-\nsions than changing roles from discussion to discussion.\nMaintaining the same role was believed to help the students\nmaster the role, which would lead to higher quality discus-\nsions. The other half believed that changing roles from\ndiscussion to discussion would help learning new skills from\nroles that may not have been chosen. Most students believed\nthat their choice of roles would be influenced by their comfort\nlevel, although the students also indicated that there was\nvalue in being forced out of their comfort zone. Overall, there\nwas a strong indication that the students favored the use of\nroles as an important online discussion strategy for learning.\nDiscussion\nExamining the three discussions through the research frame-\nwork of this study provided deeper insights into the impact\nof online discussion strategies in learning. Reviewing the lit-\nerature for online discussion strategies revealed a need for\nmore perspectives of research-based reviews of online dis-\ncussion strategies. For example, Darabi et al. (2013), in their\nmeta-analysis of online discussion strategies, could only\nlocate eight articles that met their rigorous research criteria.\nIn determining a theoretical rationale to frame this study of\nonline discussions, also revealed was a dearth of frameworks\nthat had been applied extensively in research. However, the\npractical inquiry of Garrison et al. (2001) that was applied in\nthis study appeared to be flexible and adaptable enough to\nserve as a basis to analyze many types of online discussions.\nApplying the practical inquiry framework to analyze the\nthree online discussions of this study made evident the need\nfor careful instructional planning of the online discussion\nprompts with consideration of triggering events, exploration,\nintegration, and resolution.\nOverall, Tables 2, 3, and 4 show strong evidence of cogni-\ntive presence in the three online discussions. The distribution\nof discussion frequencies was acceptable with a substantial\nvolume of the discussion posts evident of exploration and\nintegration. The small percentage of triggering event mes-\nsages was acceptable with 8% to 10% of the messages ade-\nquately covering the necessary information essential to the\ndiscussions. Students appeared to progress fairly well\nthrough the discussions, first identifying the topic of discus-\nsion (triggering event), expanding with ideas for further dis-\ncussion (exploration), and then discussing and integrating\nnew ideas. Strengthening the discussions strategy could be\naccomplished with more direction in applying solutions or\nconclusions to real or virtual situations. In the original\ninstructions for the online discussions given to the students,\nthere were not any directions instructing the students to apply\ntheir conclusions to real-life or virtual settings, although\nthere were a few students who were K-12 teachers that did\ndiscuss applications of the conclusion to their real-life class-\nroom situations.\nNon-Cognitive Presence\nThe percentages of non-cognitive messages appeared rather\nhigh in the first discussion, went down in the second discus-\nsion, and then increased slightly in the third discussion.\nOnline discussions are sometimes filled with non-cognitive\noff-task messages, which was not a problem in this study.\nNon-cognitive messages that seek clarifications of the dis-\ncussion instructions can help facilitate the discussion, and\ncan be supported in various ways depending on whether the\ncourse is a hybrid or fully online course. Clarification of\ninstructions can be taught in face-to-face class sessions, or\nseparate threads dedicated to clarifying instructions for the\ndiscussion would help. \"Teaching presence,\" which is a\nmajor concept of the COI (Garrison et al., 2001), can be ele-\nvated in online discussions through separate discussion\nthreads led by the instructor of the course. Students appear to\nexpect the instructor to have strong teaching presence in\nonline discussions.\nThe messages coded as encouraging are characteristic of\nsocial presence, also a major concept of the COI model.\nMessages coded as encouraging were moderately evident\nthroughout the three discussions. Encouraging messages can\nincrease social presence in online discussions, which pro-\nmotes positive outcomes. Students in this study were influ-\nenced by the use of encourager roles, which was used in all\nthree discussions. The use of roles as a discussion strategy\nhas been shown to have a positive impact on student interac-\ntion in asynchronous discussions (De Wever et al., 2010;\nYeh, 2010), which was confirmed by this study.\nReflections on Content Analysis and the Practical\nInquiry Model\nT. Anderson (2005) suggested that there must be an easier\nway to support and confirm acceptance of online learning\nother than through content analysis, and that few of the con-\ntent analysis methods for online discussions have been used\nrepeatedly enough, creating a problem comparing studies.\nAlthough difficulties do arise in applying content analysis to\nstudy online discussions, for those desiring to improve their\nonline discussions, the benefits of applying content analysis\noutweigh the difficulties in its use.\nMajor difficulties in using content analysis include (a)\nmodifying the indicators' criteria, (b) developing training\nfor the coders, (c) training coders, and (d) the time it takes\nto complete these tasks. Content analysis is a tedious pro-\ncess to apply in the analysis of online discussion tran-\nscripts. Thorough training is required for the coders to\nestablish acceptable reliability measurements. There has\nbeen some difficulty in training others to discern the dif-\nferences between phases of the practical inquiry model.\nThe integration \" . . . phase is the most difficult to detect\nfrom a teaching or research perspective\" (Garrison et al.,\naffected by issues that typically affect content analysis\nstudies, relating to the coders being knowledgeable of the\nsubject matter in the study, and crafting measurability\ninstruments that address this issue (Potter & Levine-\nLimitations\nLimitations of this study include a small sample size involv-\ning 15 graduate students from an educational technology\nmaster's program in their first semester of the program. The\nresults may be generalizable to analogous populations using\ncomparable online discussion strategies. However, the\ndesign of coder training and the development of criteria for\ncoding indicators may possibly affect the results. The pri-\nmary inquiry model, which includes the categories of trig-\ngering events, exploration, integration, and resolution,\nappears to provide a strong framework to serve as a base for\nthe development of the coding criteria. Other factors that\nmay affect generalizability include variables related to stu-\ndent backgrounds (e.g., student context factors including\nacademic, language, social, and socioeconomic factors).\nThese factors should be considered in developing and exam-\nOther limitations include the following: The study only\nexamined one element, cognitive presence, from the COI\nmodel (Garrison et al., 2001), excluding elements of teach-\ning presence and social presence. Although there were no\nplans at the onset of the study to examine teaching and social\npresence, there is some indication of their presence in the\nnon-cognitive messages. Social presence appears to be evi-\ndent in the messages containing \"encouraging\" comments,\nand teaching presence is potentially present in the messages\ncontaining comments related to instructional procedures. It is\nworthwhile to examine these two areas more closely in future\nstudies.\nConclusion\nThe research literature on content analysis of online discus-\nsions calls repeatedly in its conclusions for the need of stud-\nies that contain all of the following: (a) a systematic\ncoherence between theory and analysis categories, (b) clear\nchoice of the unit of analysis, and (c) information about the\ninter-rater reliability procedures (De Wever et al., 2006).\nThese are the essential requirements needed for content anal-\nysis studies that provide the means for professional compari-\nsons that may contribute to improved conditions in online\nteaching and learning. In addition to meeting the above three\nrequirements necessary to compare studies applying content\nanalysis to online discussions, there are other concerns to\naddress in the process of applying content analysis to study\nonline discussions.\nThere are several important steps in applying methods of\ncontent analysis to analyze learning in online discussions.\nThese steps are not meant to be comprehensive but are les-\nsons learned from this study:\n1. Use a software program such as HyperResearch or\nAtlas.ti, which is very helpful in sorting, analyzing,\nand reporting data.\n2. Use an existing theoretical rationale or research\ndesign such as the COI (Garrison et al., 2001).\nAlthough there is pressure to develop an individual-\nized instrument, researchers and practitioners can\nbenefit from repetitive studies that thoroughly vet\nresearch designs.\n3. Examine and understand the online discussion strate-\ngies in meeting learning objectives associated with\nthe online discussions.\n4. Modify the criteria (indicators) of the coding instru-\nment to align with the identification of the content of\nthe online discussion messages in relation to the\nlearning objectives.\n5. Select coders who are knowledgeable with the sub-\nject matter of the online discussions.\n6. Plan thorough training for the coders.\n7. Assess training of coders for inter-rater reliability\nmeasurements that are at least .8.\n8. In analyzing the online discussions, analyze for cog-\nnitive presence, teaching presence, and social\npresence.\nThis study used an existing model for content analysis to\nanalyze online discussions of a course previously taught. The\npurpose was to improve the use of online discussions to sup-\nport learning in hybrid courses in education. The study con-\nfirms the value by applying the framework practical inquiry\nmodel (Garrison et al., 2001) and further identifies important\nsteps or lessons learned in applying content analysis to assess\nlearning of online discussions. Finally, it was the experience\nof the researcher of this study that the process of applying\ncontent analysis to examine a course's online discussions\nwill provide additional perspectives on development and use\nof online discussions for learning.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nAkyol, Z., & Garrison, D. R. (2011). Understanding cognitive pres-\nence in an online and blended Community of Inquiry: Assessing\noutcomes and processes for deep approaches to learning. British\nAllen, I. E., & Seaman, J. (2013). Changing course: Ten years of\ntracking online education in the United States. Newburyport,\nMA: Sloan Consortium.\nAnderson, R. S., Goode, G. S., Mitchell, J. S., & Thompson, R. F.\n(2013). Four online discussion strategies: Perceptions of seven\ndoctoral students. Journal of Literacy and Technology, 14(2),\nAnderson, T. (2005). Online education innovation: Going boldly\nwhere others fear to thread. In G. Kearsley (Ed.), Online learn-\ning: Personal reflections on the transformation of education\n(pp. 1-11). Englewood Cliffs, NJ: Educational Technology\nPublications.\nBenbunan-Fich, R., & Hiltz, S. R. (1999). Impacts of asynchro-\nnous learning networks on individual and group problem solv-\ning: A field experiment. Group Decision and Negotiation, 8,\nBergmann, J., & Sams, A. (2012). Flip your classroom: Reach every\nstudent in every class every day. Eugene, OR: International\nSociety for Technology in Education.\nBliuc, A. M., Ellis, R. A., Goodyear, P., & Piggott, L. (2011). A\nblended learning Approach to teaching foreign policy: Student\nexperiences of learning through face-to-face and online dis-\ncussion and their relationship to academic performance.\nBorich, G. D., & Tombari, M. L. (1997). Educational psychology:\nA contemporary approach. New York, NY: Longman.\nBowen, W. G. (2013, March 25). Walk deliberately, don't run,\ntoward online education. The Chronicle of Higher Education.\nRetrieved from http://chronicle.com/article/Walk-Deliberately-\nBruner, J. (1986). Actual minds, possible worlds. Cambridge, MA:\nHarvard University Press.\nDarabi, A., Liang, X., Suryavanshi, R., & Yurekli, H. (2013).\nEffectiveness of online discussion strategies: A meta-Analysis.\nDavidson-Shivers, G. V., Muilenburg, L. Y., & Tanner, E. J. (2001).\nHow do students participate in synchronous and asynchro-\nnous online discussions? Journal of Educational Computing\nde Leng, B. A., Dolmans, D. H., J\u00f6bsis, R., Muijtjens, A. M., & van\nder Vleuten, C. P. (2009). Exploration of an e-learning model\nto foster critical thinking on basic science concepts during\nDe Wever, B., Schellens, T., Valcke, M., & Van Keer, H. (2006).\nContent analysis schemes to analyze transcripts of online\nasynchronous discussion groups: A review. Computers &\nDe Wever, B., Van Keer, H. V., Schellens, T., & Valcke, M. (2010).\nRoles as a structuring tool in online discussion groups: The dif-\nferential impact of different roles on social knowledge con-\nDillenbourg, P. (1999). What do you mean by collaborative learn-\ning? In P. Dillenbourg (Ed.), Advances in Learning and\nInstruction: Collaborative-learning--Cognitive and compu-\ntational approaches (pp. 1-19). Bingley, UK: Emerald Group\nPublishing.\nFahy, P. (2005). Two methods for assessing critical thinking in\ncomputer-mediated communications (CMC) transcripts.\nInternational Journal of Instructional Technology and\nGarrison, D. R., Anderson, T., & Archer, W. (2001). Critical\nthinking, cognitive presence, and computer conferencing in\ndistance education. American Journal of Distance Education,\nGarrison, D. R., & Arbaugh, J. B. (2007). Researching the\nCommunity of Inquiry framework: Review, issues, and future\nGunawardena, C. N., Lowe, C. A., & Anderson, T. (1997). Analysis\nof a global online debate and the development of an interaction\nanalysis model for examining social construction of knowledge\nin computer conferencing. Journal of Educational Computing\nHansen, D. T. (1988). Was Socrates a \"Socratic teacher.\"\nHenri, F. (1992). Computer conferencing and content analysis. In\nA. R. Kaye (Ed.), Collaborative learning through computer\nGermany: Springer.\nHofer, B. K. (2001). Personal epistemology research: Implications\nfor learning and teaching. Educational Psychology Review, 13,\nHsieh, H. F., & Shannon, S. E. (2005). Three approaches to quali-\ntative content analysis. Qualitative Health Research, 15,\nJaggars, S. S., & Bailey, T. (2010). Effectiveness of fully online\ncourses for college students: Response to a Department\nof Education meta-analysis. Available from http://ccrc.\ntc.columbia.edu/\nJohnson, S. D., & Aragon, S. R. (2003). An instructional strategy\nframework for online learning environments. New Directions\nJohn-Steiner, V., & Mahn, H. (1996). Sociocultural approaches\nto learning and development: A Vygotskian framework.\nKing, A. (1993). From sage on the stage to guide on the side.\nKrippendorff, K. (2012). Content analysis: An introduction to its\nmethodology. Beverly Hills, CA: SAGE.\nLandis, J. R., & Koch, G. G. (1977). The measurement of observer\nLarson, L. (2009). Reader response meets the new literacies:\nEmpowering readers in online learning communities. The\nLebow, D. (1993). Constructivist values for systems design: Five\nprinciples toward a new mindset. Educational Technology\nLiu, C. J., & Yang, S. C. (2012). Applying the practical inquiry\nmodel to investigate the quality of students' online discourse in\nan information ethics course based on Bloom's teaching goal\nLombard, M., Snyder-Duch, J., & Bracken, C. C. (2002). Content\nanalysis in mass communication: Assessment and reporting of\nintercoder reliability. Human Communication Research, 28,\nLucas, M., Gunawardena, C., & Moreira, A. (2014). Assessing\nsocial construction of knowledge online: A critique of the\ninteraction analysis model. Computers in Human Behavior, 30,\nMaor, D. (2003). The teacher's role in developing interaction and\nreflection in an online learning community. Educational Media\nMcLoughlin, C., & Oliver, R. (1998). Maximising the language\nand learning link in computer learning environments. British\nMeans, B., Toyama, Y., Murphy, R., Bakia, M., & Jones, K.\n(2010). Evaluation of evidence-based practices in online\nlearning: A meta-analysis and review of online learning\nstudies. Washington, DC: U.S. Department of Education,\nOffice of Planning, Evaluation, and Policy Development.\nMeyer, K. A. (2003). Face-to-face versus threaded discussions: The\nrole of time and higher-order thinking. Journal of Asynchronous\nParker, W. C., & Hess, D. (2001). Teaching with and for discussion.\nParsad, B., Jones, J., & Greene, B. (2005). Internet access in\nEducation.\nPotter, W., & Levine-Donnerstein, D. (1999). Rethinking valid-\nity and reliability in content analysis. Journal of Applied\nRiffe, D., Lacy, S., & Fico, F. G. (1998). Analyzing media messages:\nUsing quantitative content analysis in research. Mahwah, NJ:\nLawrence Erlbaum.\nRourke, L., & Anderson, T. (2004). Validity in quantitative content\nanalysis. Educational Technology Research & Development,\nRourke, L., Anderson, T., Garrison, D. R., & Archer, W. (2001).\nMethodological issues in the content analysis of computer\nconference transcripts. International Journal of Artificial\nStacey, E., & Gerbic, P. (2003). Investigating the impact of com-\nputer conferencing: Content analysis as a manageable research\ntool. In G. Crisp, D. Thiele, I. Scholten, S. Barker & J. Baron\n(Eds.), Interact, Integrate, Impact. Proceedings of the 20th\nAnnual Conference of the Australasian Society for Computers\nAdelaide: Australasian Society for Computers in Learning in\nTertiary Education.\nSwan, K. (2001). Virtual interaction: Design factors affecting stu-\ndent satisfaction and perceived learning in asynchronous online\nSwan, K. (2005). A constructivist model for thinking about learn-\ning online. In J. Bourne & J. C. Moore (Eds.), Elements of\nquality online education: Engaging communities (pp. 13-30).\nNeedham, MA: Sloan-C.\nTiene, D. (2000). Online discussions: A survey of advantages and\ndisadvantages compared to face-to-face discussions. Journal of\nVygotsky, L. S. (1980). Mind in society: The development of higher\npsychological processes. Boston, MA: Harvard University Press.\nWang, Q., & Woo, H. L. (2007). Comparing asynchronous online\ndiscussions and face-to-face discussions in a classroom setting.\nYang, Y. T. C., Newby, T. J., & Bill, R. L. (2005). Using Socratic\nquestioning to promote critical thinking skills through asyn-\nchronous discussion forums in distance learning environments.\nYeh, Y. C. (2010). Analyzing online behaviors, roles, and learning\ncommunities via online discussions. Journal of Educational\nYuan, J., & Kim, C. (2014). Guidelines for facilitating the devel-\nopment of learning communities in online courses. Journal\nAuthor Biography\nMark Rodriguez is a professor of Education. He is currently the\nAdministrator in Charge of Academic Technology & Creative\nServices at California State University, Sacramento."
}