{
    "abstract": "The aim of this paper is to present a methodology where the extent of information sharing among team members is used as an indicator of shared mental mod- els (SMM) and situation awareness (SA). Data collec- tion procedures and probe materials are described for two field experiments performed among emergency management teams in the hydrocarbon industry. Meth- ods are suggested for calculating a \"similarity index\" by comparing a team member's responses with the aver- age response in the team or with the responses of the team member assumed to be best informed.It is argued that similarity to team average could be a measure of",
    "reduced_content": "The aim of this paper is to present a methodology\nwhere the extent of information sharing among team\nmembers is used as an indicator of shared mental mod-\nels (SMM) and situation awareness (SA). Data collec-\ntion procedures and probe materials are described for\ntwo field experiments performed among emergency\nmanagement teams in the hydrocarbon industry. Meth-\nods are suggested for calculating a \"similarity index\" by\ncomparing a team member's responses with the aver-\nage response in the team or with the responses of the\nteam member assumed to be best informed.It is argued\nthat similarity to team average could be a measure of\nSMM, whereas similarity to the best-informed team\nmember could be argued to be an indicator of SA.The\ndegree of compliance in responding to the probes is\nreported,as is the degree to which the extent of shared\ninformation differed between the probe questions or\naccording to team positions. Lessons learned from the\ndata collection are summarized, and the applicability of\nthe similarity index as a measure of SA is discussed.\nSome advantages of the current approach are pre-\nsented, as are challenges and inherent assumptions in\nfuture applications of this approach.\nKeywords: situation awareness, shared situation\nawareness, shared mental models, teamwork, field\nstudy, crisis response\nIntroduction\nEndsley defined situation awareness (SA) as\n\"the perception of the elements in the environ-\nment within a volume of time and space, the\ncomprehension of their meaning and the projec-\ntion of their status in the near future\" (Endsley,\nthe basis for decision making in dynamic and\ncomplex environments, such as a pilot's cockpit,\na ship's bridge, or an emergency preparedness\nteam in the hydrocarbon industry. Endsley saw\nSA as both the cognitive process of gathering\nand organizing information about the environ-\nment and the resulting knowledge states. How-\never, Patrick and Morgan (2010) argued that it is\ndifficult to clearly separate the process from the\nproduct, in terms of distinguishing the cognitive\noperations involved in perception, pattern rec-\nognition, and sense making from the knowledge\nstates produced. Bolstad, Cuevas, Gonzalez,\nand Schneider (2005) argued that the three main\ncomponents affecting SA formation are the\noperators' abilities, their interaction with other\noperators, and their interaction with the environ-\nment. Although different conceptualizations of\nSA have been proposed, they are all based on\nthe assumption that SA is susceptible to change\nand is vital to decision making in safety critical\norganizations.\nMeasurement Approaches for Situation\nAwareness\nAs the definitions vary, it is not surprising\nthat various approaches to measuring SA have\nbeen suggested. A common and convenient\napproach is to use subjective self-report items,\nwhich is done by asking operators to report\nthe extent to which they feel they are suffi-\nciently aware of their environment. Given that\nthe cognitive process and products are private\nexperiences, it may be argued that letting the\noperators express their confidence in their cog-\nnition is a valid approach. A frequently used\nself-report approach is the situational awareness\nrating technique (SART; Taylor, 1989), which\nasks the operator to rate his or her SA on a scale\nwith separate dimensions indicating demand for\nattention, availability of attentional resources,\nand situational understanding.\nAddress correspondence to Bj\u00f8rn S\u00e6trevik, Faculty of\nPsychology, University of Bergen, Christies Gate 12, NO-\n5015 Bergen, Norway, satrevik@gmail.com.\nThe \"Similarity Index\" as an Indicator of Shared\nMental Models and Situation Awareness in Field\nStudies\nBj\u00f8rn S\u00e6trevik and Jarle Eid, University of Bergen, Bergen, Norway\nJournal of Cognitive Engineering and Decision Making\nCopyright \u00a9 2013, Human Factors and Ergonomics Society.\nA different approach is to compare the opera-\ntor's knowledge about the environment against\nwhat is considered to be objectively true. To use\nthis approach, the researcher has to establish a\nknown \"ground truth,\" for example by arranging\na closely controlled exercise or by meticulous\nexamination of audiovisual material or event\nlogs in retrospect of an incident. One such objec-\ntive SA measure is the situation awareness\nglobal assessment technique (SAGAT; Endsley,\n1995a), where factual probe questions are devel-\noped for the specific setting and the operator's\nanswers express the extent to which the operator\nhas an accurate understanding of the situation.\nObjective SA measures thus tap the knowledge\nstates produced by the perceptual processes\ninvolved in gathering and organizing informa-\ntion. The quantitative assessment of situation\nawareness technique (QUASA; Edgar, Smith,\nStone, Beetham, & Pritchard, 2000; McGuin-\nness, 2004) combines the subjective and objec-\ntive approaches by asking operators to judge\nfactual knowledge about the situation and to\nstate their confidence in the judgment.\nAthird approach to measuring SAis to directly\nmeasure the cognitive process involved when the\noperator gathers and organizes information.\nOperators may for instance be asked to describe\ntheir actions and thoughts while performing a\ntask, examine what information is accessed in a\ncontrol system, analyze the operators' eye move-\nments as an indicator of attention, or record psy-\nchophysiology as an indicator of emotional or\ncognitive activation.An approach frequently used\nin field studies is to have subject matter experts\nobserve the task work and rate the operators'\nassumed SA based on their behavior (for more\nextensivereviewsofSAmeasurementapproaches,\nsee Salmon, Stanton, Walker, & Green, 2006;\nSaner, Bolstad, Gonzalez, & Cuevas, 2009).\nChallenges for Existing Situation\nAwareness Measurements\nThere are various challenges and caveats\ninvolved in each of the approaches to mea-\nsuring SA listed above. Several measurement\napproaches require an individual technique to\nbe tailored to the context in question (e.g., by\ndeveloping specific probe questions suited to\nthe scenario), which may be a time-consuming\nprocess drawing on resources from experts\nwithin the work setting being measured. This\napplies in particular to objective SA measure-\nments, where relevant probes have to be adapted\nto the operator's task, to the work setting, and\nto a specific time point in the exercise scenario,\nand the correct answer for each probe must\nbe defined. Developing and arranging training\nexercises for the sole purpose of SA measure-\nment may extensively draw on the organiza-\ntion's limited resources of personnel, time, and\nequipment.\nAlthough approaches that require operators\nto answer standardized subjective SA questions\nduring exercises and simulations may require\nfewer resources to develop the approach, they\nconstitute interruptions in the task work, and the\nresearcher has to consider whether the SA mea-\nsured is representative of what the SA would be\nduring natural task work. Subjective SA mea-\nsures where operators quantify to what extent\nthey have an overview of the situation rely on\noperators' access to metacognitive information,\ntheir interpretation of it, and their ability and\nwillingness to report it. Several authors have\nargued that subjective SA actually measures\noperators' confidence rather than reflecting the\nprocess or product of the cognitive information\ngathering processes (Endsley, 1994; Rousseau,\nTremblay, Banbury, Breton, & Guitouni, 2010).\nFor instance, Matthews, Eid, Johnsen, and Boe\n(2011) found that self-ratings of SAwere inflated\nand did not correlate with expert ratings of SAin\na military training exercise. This raises episte-\nmological questions about what we consider to\nbe the best source for knowledge of private cog-\nnitive states, and what we define SA as being.\nProbe questions asking about factual relation-\nships may direct the operator's attention toward\nor away from specific aspects of the context,\nthus influencing the cognitive process. More-\nover, probe questions that ask what the operator\nknew at a previous time point in the scenario are\nsubject to the operator's recall quality.\nObjective SAmeasures need a ground truth to\nbe able to compare the operator's knowledge of\nthe situation to what can be said to be objec-\ntively true. Although this may be achievable in\ntightly controlled scenarios, it is often impossi-\nble in real-life incidents or in more realistic\nSimilarity Index in Field Studies 121\ntraining to determine what the external situation\nis at a given scenario time point, also when mak-\ning a detailed examination of an incident in ret-\nrospect. Even in training exercises, it may often\nbe difficult to predict the exact details of how a\nscripted scenario will play out. A true-to-life\ntraining scenario will be influenced by a number\nof unstable factors such as the actions of other\nteam members, variations in information per-\nspective, idiosyncrasies among actors acting out\nthe scenario, equipment malfunctions, and\nweather conditions. These threats to experimen-\ntal control also pose challenges to studies seek-\ning to compare SA between different teams or\ndifferent team members in the same scenario.\nObserver ratings of SA are limited by their\nneed to make assumptions based on what is\nempirically available as observable actions and\ncommunication, typically having to assess SA\nquality based on what information the operator\ncan be shown to have accessed or on the deci-\nsions the operator has made. Without compari-\nsons to other SA measures, it is difficult for the\nresearcher to know how accurate the observer\nratings are. The observers may for instance be\nbiased to look for what they think is the correct\nbehavior, although this does not necessarily cor-\nrespond to the operator's SA (Salmon et al.,\n2006). Process measures for SA may be mis-\nleading, in that information-gathering actions do\nnot necessarily reflect the knowledge states\n(Hone, Martin, & Ayres, 2006). Think-aloud\nprotocols and eye-tracking and control system\nlogs may indicate that information has been\ngathered, yet we do not know whether the infor-\nmation has been perceived and remembered,\nhow its significance is understood, or to what\nextent the knowledge influences decision mak-\ning (the \"looked but failed to see\" phenomenon;\nBrown, 2002). Some level of intrusion into the\ntask work appears to be inevitable in all SAmea-\nsures, yet the amount of intrusion varies accord-\ning to the approach used. Some measures require\ncontrolled settings where the objective facts of\nthe task situation can be known with some cer-\ntainty, whereas others are better suited to eco-\nlogical field studies and can be applied during or\nafter naturalistic task performance. Thus, some\napproaches require resource-intensive bespoke\nexercises to be carried out, whereas others can\nbe combined with exercises that a team would\ncarry out regardless of SA measurement, or can\neven be collected during actual incidents.\nThe above overview has outlined a number of\nmethodological challenges in SA measurement.\nSalmon and colleagues (2008) recommended\nthat studies should combine several of the differ-\nent SA approaches to optimize the measurement\nstrategy.\nA sense of normativism seems to be funda-\nmental in most approaches to SA measurements,\nin the sense that there is an ideal state of knowl-\nedge that the operator should have, and that the\nquality of SA is measured in terms of deviation\nfrom this ideal. However, as Dekker, Hum-\nmerdal, and Smith (2010) pointed out, it is ques-\ntionable whether we can confidently define what\na \"correct\" view of a situation is. As mentioned\nabove, the actual relationships may be unknown\nas a situation develops, or unknowable even in\nretrospect of the incident. It thereby seems more\nreasonable to evaluate the operator's SAin terms\nof the information he or she has had access to or\nhas been able to access, rather than to a platonic\nideal state of knowledge. It should also be noted\nthat some emergency training strategies focus\non training operators to assume and prepare for\nthe \"worst-case\" scenario based on current data\n(Nudell & Antokol, 1988). Such an approach\ncould result in a state where desired knowledge\nrepresentations deviate from an SA based on a\nmore sober and realistic assessment of informa-\ntion. Thus SA, in the sense of similarity to objec-\ntive reality, may in some cases be difficult to\nestablish, will typically be unavailable in real-\nlife situations, and may not be compatible with\nthe operator's strategy. Hence, it may be of inter-\nest to examine other approaches to measuring\nSAthat use validity criteria that may be assumed\nto correspond to accurate (or at least suitable)\nbeliefs about the situation, but without measur-\ning their correspondence to an objective reality.\nSituation Awareness in Teams\nComplex work tasks are often performed by\nteams, in which two or more individuals with\nspecialized expertise and responsibility coop-\nerate to achieve a shared goal (Salas, Dickin-\nson, Converse, & Tannenbaum, 1992). Endsley\n(1995a) used the term team SA to describe the\nextent to which each member of a team has SA\nof the aspects of the situation that he or she is\nresponsible for. Thus, team SA represents an\naggregation of individual SAs, and team per-\nformance often relies on all team members hav-\ning sufficient information about their own task\nrequirements and sharing the information that is\nrelevant to other members. Jones and Endsley\n(1996) used the term shared SA to describe\nthe extent to which different team members\nhave the same information about an aspect of\nthe task that is relevant to them. Shared SA\nthus describes an overlap in SA requirements.\nThis distinction emphasizes that knowing what\ninformation to share and what not to share is\ncrucial for well-functioning teams. Shared SA is\nachieved through team processes, such as coor-\ndinating and prioritizing tasks, sharing informa-\ntion, and checking each other's beliefs and basic\nassumptions. Cooke, Salas, Kiekel, and Bell\n(2004) used the term team cognition to refer to\nthe kind of processes that allow team members\nto coordinate their efforts and achieve a shared\nunderstanding of the external situation, the task,\nand the resources of other team members. An\nimportant element in establishing shared SA is\nfor the team members to develop shared men-\ntal models (SMM; Cannon-Bowers, Salas, &\nConverse, 1993), which enable team members\nto understand and interpret the situation in the\nsame way, know what the other team members\nalready know, what they need to know, and\nwhat they are doing. According to Salas, Rosen,\nallows for accurate causal explanations and\nprojection of future states, better adaptation to\nthe environment, more reliable communication,\nmutual assessment of team member workload,\nand supportive behavior within the team.\nEndsley and Jones (2001; Jones & Endsley,\n2002) noted that a team member's beliefs about\nthe situation could be rated both in terms of\naccuracy and in terms of similarity to other team\nmembers' beliefs, and proceeded to discuss a\nnumber of factors that may be involved in devel-\noping team SA. Saner and colleagues (2009)\nsuggested an approach for comparing two-and-\ntwo team members'knowledge states in order to\nmeasure shared SA. Team members answer fac-\ntual questions and the researcher scores each\nresponse both according to whether it is objec-\ntively correct and according to whether the team\nmembers give the same or different answers,\nthus yielding measures for both accuracy and\nsimilarity. Woods and Sarter (2010) argued that\nthe focus of SA research should shift from indi-\nvidual perception to interdependent groups. It\nwould be beneficial to have tools that can mea-\nsure the SA both of individuals in a team and of\nthe entire team, rather than separate tools for\nindividual SA and for the SA of team member\ndyads.\nResearch Aims\nOur motivation for the current study was\nto develop and field test a new approach for\nassessing and comparing beliefs within teams\nas a measure of SMM and as an indicator of SA\nin an applied setting. We were particularly inter-\nested in approaches that could be used in situ for\nteams working in safety critical organizations,\nsuch as emergency management teams in the\nhydrocarbon industry. It should also be possible\nfor organizations to apply the approach in their\nregular training exercises without outside assis-\ntance (without arranging a resource-intensive\nscripted scenario).\nAs mentioned above, in all but the most arti-\nficial situations, it is difficult to establish a\n\"ground truth\" for SA. In other words, it is dif-\nficult to confidently state what the operator can\nbe expected to know about the situation at hand,\ngiven that the researcher has limited access to\nevaluate the actual situation at a given time, and\nwhat information the operator and the team has\nhad access to. To get around this, our approach\nwas to ask multiple-choice questions about fac-\ntual aspects of the situation and the team's work\nthat all team members were expected to be\nfamiliar with, and in evaluating the answers,\neach team member's answer was compared to\nthe rest of the team's answers or to the answers\noftheteammemberassumedtobebestinformed.\nA \"similarity index\" is introduced for calculat-\ning to what extent a team member's answers to\nmultiple-choice questions match the answers of\nothers. The degree of similarity between team\nmembers is considered a measure of SMM, as it\nrepresents the degree of overlap in the team\nmembers'beliefs about the situation and the task\nSimilarity Index in Field Studies 123\nwork. Moreover, if one member of the team, for\nexample the team leader, can be assumed to be\nbetter informed on average than the rest of the\nteam, we argue that the similarity a team mem-\nber has to the best-informed team member can\nbe linked to SA, although this does not assess\nthe accuracy of the cognitions per se. A team's\naverage similarity index may be seen as a mea-\nsure of the team's SMM and an indicator of the\nteam's average degree of shared SA, where high\nsimilarity index scores indicate a well-function-\ning team with good information flow that is\nfacilitated by an efficient team leader. To assess\nthe methodology, the level of questionnaire\ncompliance is examined, as team members may\nbe reluctant to answer questions they regard as\nirrelevant. The similarity score is compared\nbetween different team positions and between\ndifferent fields of expertise to verify that the\ninformation requested in the probe questions is\nrelevant to all team positions.\nResearch Setting\nThe research was conducted within a sec-\nond-line emergency preparedness center in a\nlarge hydrocarbon energy company. The center\nmusters the emergency team whenever there is\nan alarm on an offshore hydrocarbon produc-\ntion installation. Typical incidents are fire or\ngas detection, ships on a collision course, and\npersonnel injuries. The center is tasked with\ngathering and structuring information from other\nsources, creating a coherent dynamic picture\nof the ongoing situation, assisting in transfer-\nring information and orders, and advising the\nfirst-line (tactical) emergency management on\nthe installation and the third-line (strategic)\nemergency management at the corporate level.\nThe team consists of an emergency commander\n(the chief of staff) and eight team members\nwith separate areas of responsibility, with the\nteam positions of personnel coordinator, medical\nadvisor, air transport officer, maritime resource\nofficer, maritime communications officer, gov-\nernment liaison, communications officer, and\nstrategic line leader. The chief of staff has a key\nrole in managing the team's work, charged with\nmaintaining an overview of the team's work\ndomains, knowing the competencies and work-\nload of all team members, prioritizing among\ndifferent goals, and planning the ongoing work.\nMost team members have regular office jobs\nin the company and are on call to muster in a\ncontrol room at an hour's notice. The company\nhas six emergency teams with a similar structure\nand responsibilities. They work a rota system\nof 1 week on and 5 weeks off. Team members\nare given individual training appropriate to their\nteam position and monthly scenario team train-\ning. The team musters in a large room equipped\nwith individual workstations (PC and com-\nmunication systems) positioned in a V shape\nopening toward multiple large-screen displays.\nTeam members gather information individually\nfrom direct contact with external units such as\nthe offshore installation manager, ships' bridge\ncrew, or hospital staff or consult documentation,\nprocedures, and maps. The majority of the team\nmembers' communication is with third parties\noutside the room or in conversation with the\nchief of staff, and there is less communication\nbetween the team members. The chief of staff\narranges brief (2- to 3-min) status update meet-\nings at his or her own discretion, typically every\n20 to 30 min. In these meetings, the chief of staff\nstands in front of the large-screen displays and\nsummarizes to the team how he or she views the\ncurrent situation, the ongoing emergency man-\nagement work, and current main team goals. The\nchief of staff functions as an information hub, in\nterms of collecting information from team mem-\nbers and distributing information and commands\nto individual team members or to the entire team\naccording to perceived needs. It could thus be\nargued that the chief of staff is expected to be on\naverage the best-informed team member.\nSubject matter experts in the organization\nwere interviewed to understand the setting's\nfunction, aims, and challenges. Methodologies,\nscenarios, and probe questions to be used in the\ndata collection were developed in collaboration\nwith senior researchers and practitioners within\nthe field of operational team processes and team\nleadership. Particular attention was given to\nidentifying task information that all team mem-\nbers were expected to know throughout their\ntask work regardless of their team position.\nBefore the Experiment 1 data collection, three\npilot runs were conducted, where the team per-\nformed scheduled training scenarios while\nresponding to pilot versions of the measurement\ntools in order to develop and adapt materials.\nSince the intention of this paper is to describe\nthe methodological approach, results are pre-\nsented to evaluate whether the data collection\nwas effective and meaningful, whereas analyses\nfor hypothesis testing are reserved for future\npublications.\nExperiment 1\nMethod\nThe intention of Experiment 1 was to measure\nnaturally occurring SA in an ecologically valid\nsetting. Only minimal intrusions were therefore\nmade into the organization's scheduled training\nexercises, and the only request was that each\nteam should be tasked with a scenario lasting\nbetween 2 and 3 hr and that they have compa-\nrable workloads. All the scenarios involved inci-\ndents of fire or gas leaks on a production facil-\nity, some with additional personnel injury prob-\nlems. In handling the scenarios, the emergency\ncenter cooperated with the actual personnel who\nwould be involved in a real incident using the\nreal equipment and communication channels.\nInvolved parties (external units) included the\nemergency organization on offshore hydrocar-\nbon installations, the bridge crews of nearby\nships, the onshore corporate-level emergency\norganization, hospital staff, helicopter crew, and\ngovernment agencies. The on-call emergency\npreparedness team members received the mus-\nter call at around 7:00 a.m. on the day of the data\ncollection and began working when they arrived\nin the emergency center shortly before 8:00\na.m. Whenever the chief of staff announced\nthat it was time for a status update meeting, the\nemergency center work was \"frozen\" for 2 min\nwhile external units involved in the exercise\ncontinued to act out the scenario without inter-\nruption. At this time point, a researcher handed\nout a sheet of paper with probe questions that all\nteam members were asked to answer. Each sheet\ncontained seven probes, some with multiple\nsubquestions. In responding to the probes, the\nteam members indicated their beliefs regarding\nwhere the incident was, what type of incident it\nwas, the current personnel status, the likelihood\nof different scenario outcomes, what the team's\npriorities should be, and how long the scenario\nwas expected to continue. The probes used\nin Experiment 1 are listed in the left column\nof Table 1. As pilot data collections showed\nthat everybody quickly reached consensus in\nProbe 1, this probe was used only in the first\ntwo freeze points. When all team members had\ncompleted the probes, the sheets of paper were\ncollected by a researcher and the chief of staff\ninitiated the meeting as planned.\nBackground variables including personality\nmeasures and meta-cognition (S\u00e6trevik, 2013)\nwere collected in advance of the scenario exer-\ncise but are not included in the current article.\nTwo observers individually monitored and\nrecorded the frequency and duration of the chief\nof staff's communication with the team mem-\nbers. A subject matter expert observed all the\nscenario exercises and scored team performance\nand the chief of staff's performance on an\nobserver scale for decision making under stress\n(inspired by the TADMUS program; see Can-\nnon-Bowers & Salas, 1998). After scenario\ncompletion, team members were asked which\nother members of the team they had communi-\ncated most with and whom they relied on most\nin the scenario. Each of the six teams in the orga-\nnization was tested once on separate days within\nthe course of a year.\nResults\nCompliance.When examining the answers\nto the probes, it became apparent that all of the\nprobe questions had not been answered (see the\nResults and Discussion sections), which had\nconsequences for the analyses. If a team mem-\nber did not respond to any of the probe questions\nat a freeze point, that team member was not\nscored for that freeze point, as we cannot know\nwhether the probes were left blank due to lack of\nknowledge or not being willing or able to answer\nthe probes. However, if a team member had\nanswered some but not all of the probe items at\na freeze point, the unanswered questions were\nscored as incorrect, based on the assumption that\nthe unanswered probes expressed lack of knowl-\nedge. If the chief of staff failed to answer a\nprobe, none of the team members could be com-\npared to the best-informed team member for that\nfreeze point. This scoring was used for all the\nfollowing analyses.\nSimilarity Index in Field Studies 125\nTable 1: Probe Item Text Used in Experiments 1 and 2\nProbe Experiment 1 Experiment 2\n1 \"Which installation is involved in the incident?\"\n(only at the two first time points)\nRemoved\n2 \"Which part of the installation is currently\ninvolved in the incident? Tick one or more\nboxes: Drilling, production, living quarters,\nauxiliary areas, other.\"\nAdditional answer categories added: \"production\npipe or subsea, the incident has been\nnormalized, don't know.\"\n3 \"What is the current status of the incident?\nTick one or more boxes: Ongoing gas\nleak, ongoing fire, ship on collision course,\nchemical leak, man overboard, well incident,\nongoing search for personnel, ongoing\nevacuation, helicopter accident, medical\ntreatment on board, hydrocarbon leak to\nenvironment, heavy weather.\"\nResponse categories were changed to list all\nthe company's \"defined situations of hazard\nand accident\": \"oil or gas leak, acute oil\nrelease, active fire or explosion, loss of well\ncontrol, falling cargo, medical emergency,\nman over board, diving incident, loss of\ninstallation stability, loss of installation position,\nuncontrolled radioactivity, ship on collision\ncourse, helicopter incident, terrorism, heavy\nweather.\" In addition, some alternatives were\nincluded to describe the team members' view\nof what emergency management actions were\ncurrently in effect on the installation: \"ongoing\nevacuation, ongoing search for personnel, the\nincident has been normalized, don't know.\"\n4 \"What is the current status of the crew? Fill\nin numbers: Number of injured personnel,\nnumber of deceased personnel, number\nof missing personnel, number of personnel\ncurrently on board, number to evacuate,\nnumber that have been evacuated.\"\nReplaced by \"What is the current status of the\ncrew? How many are missing, injured, and\ndeceased. For each item, tick `none,' `don't\nknow,' or type a number.\"\n5 \"How likely do you think each of these\noutcomes are? Rate on a 7-point scale from\n`certain to occur or has occurred' to `will\ncertainly not occur': Medical evacuation,\nevacuation of nonessential personnel, full\nevacuation, short production stop (less\nthan 1 day), long production stop (1 day or\nmore), sending back-up personnel offshore,\nestablish next-of-kin call center.\"\nUnchanged\n6 \"What should the team's three main priorities\nbe until the next status update meeting?\nFill in the numbers 1, 2, and 3 to show\nranking of priorities. Alerting, evacuating,\nacquiring technical status of the incident,\nacquiring medical status of the incident,\nacquiring personnel status, establishing\nand communicating with next-of-kin call\ncenter, mobilizing resources, demobilizing\nresources, coordinating with authorities,\ncoordinating with higher organizational\nlevels, coordinating sea and air resources,\ncoordinating land resources.\"\nRephrased to \"What should the team focus\non from now on?\" Rather than filling in\nnumbers, the answer was selected from three\nindependent pull-down menus for the team's\nfirst, second, and third priority.\n7 \"For how much longer do you expect the state\nof emergency on the installation to last? Fill\nin hours and minutes.\"\nRemoved\nData were collected when the chief of staff\nannounced a status meeting, and the number of\nfreeze points was thus determined by the fre-\nquency of meetings deemed necessary by the\nchief of staff during the scenario. Two of the\nteams had four meetings, three teams had five\nmeetings, and one team had six meetings. Over-\nall compliance in answering the probes through-\nout the scenario was considered to be acceptable\n(percentage of questions answered M = 87.08%,\nSD = 33.55%). Compliance was lowest in Probe\nAssessment of data collection.A closer\nexamination of the probe responses indicated\nthat some of the questions appear to have\nworked better than others. In Probe 4, where the\nstatus of crew members was to be expressed in\nnumbers, it appears to have been unclear to\nsome team members how they were to respond\nwhen they did not know the exact number, as\nthey left the item blank rather than estimate a\nnumber as the instructions had requested. Sev-\neral team members misunderstood Probe 6,\nwhich asked team members to write the num-\noptions for what they thought the team's current\nfirst, second, and third priorities were. The\nproblem persisted despite piloting various\nphrasings of the probe, and offering guidance\nduring the data collection. In Probe 7, the phrase\n\"how long will the state of emergency last\" was\ninterpreted differently by respondents, yielding\ngreat variation in responses. Some team mem-\nbers assumed the probe referred to how long the\ntraining exercise would last, whereas others\nassumed it referred to how long it would take\nfor the offshore installation to return to normal\nproduction. Discussing the probe with the orga-\nnization's subject matter experts revealed that\nthis probe may not have been meaningful to all\nteam members.\nAnalytical approach. In line with our expec-\ntations from previous research, it proved difficult\nto establish a ground truth in naturalistic exer-\ncises such as the ones included in Experiment 1.\nA true-to-life exercise was also arranged among\nthe other involved parties, most crucially by the\nemergency organization on the offshore produc-\ntion installation, and the studied team's external\ninput could thus not be strictly controlled or\nmonitored. Even for structured bits of informa-\ntion such as knowing how many personnel were\ncurrently injured, the experts were reluctant to\nsay at what point individual team members or the\nwhole team could be expected to have that infor-\nmation. For example, the chief of staff and the\nmedical advisor are expected to know the injured\npersonnel status at all times and as soon as such\ninformation is received, whereas other team\nmembers are expected to know about it only after\nthe chief of staff arranges the next status update\nmeeting. Moreover, as not all input into and\nwithin the team was recorded and analyzed\n(which would be a monumental task for a\nresearch project, not to mention for a regular\ntraining exercise), it was not possible to deter-\nmine what information had been given to what\nteam members about injured personnel at what\ntime, how clearly it had been expressed and\nemphasized, how it had been communicated\nwithin the team, and what other supporting or\ncontradicting sources of information could exist.\nOur conclusion was that it would be difficult to\narrange an ecologically valid exercise while at\nthe same time controlling the incoming informa-\ntion to a sufficient extent to be able to conclude\nwhich team members had had access to which\ncritical information at what time.\nTo develop a scoring system for SMM and\nSA that was able to adapt to a dynamic and often\nambiguous information situation, which it would\nbe possible to employ in a variety of contexts at\nlow cost, the team members' answers to probes\nwere compared to each other. Given also that the\nchief of staff in this setting has a supervisory\nrole, functioned as a communication and com-\nmand hub, and was tasked with informing the\nrest of the team of critical information, it was\nassumed that the chief of staff would on average\nbe the team member who was best informed\nabout general scenario information.\nComparing team members to team aver-\nage. One way to analyze the data set could be\nto calculate the average answer for the team\n(including the chief of staff) at each time point\nSimilarity Index in Field Studies 127\nand calculate each team member's deviation\nfrom the team average. To achieve this, all\nresponse options to probes with multiple choice\nanswers (Probes 1-3, see Table 1) were scored\nas 1 or 0 according to whether each response\noption had been ticked. For probe items where a\nvalue or a graded answer was entered (Probes\n4-7), the analysis used the number of personnel\nstated as injured, missing, deceased, and cur-\nrently on board (Probe 4), numbers 0 to 6 for\neach graded answer category for the likelihood\nestimates (Probe 5), the number 3 for the high-\nest team priority, the number 2 for the second\npriority, the number 1 for the third priority, the\nnumber 0 for priorities not selected (Probe 6),\nand the stated number of minutes estimated to\nbe left in the scenario (Probe 7). At each time\npoint, the average value for all response options\nwas calculated, yielding an \"average team\nanswer\" for each probe. Then the difference\nbetween each team member's score and the\naverage was calculated, and divided by the\nhighest number given by any team member in\nany team on this probe on this time point. The\nalgorithm is stated mathematically in Algorithm\n1 below. For example, if two team members tick\nthat there is an incident in the living quarters on\nProbe 2 whereas eight team members do not\ntick this response option, the similarity index\nfor the members answering yes on this response\neight other team members' similarity index is\nteam consisting of six members answers that\nthe number of people missing (Probe 4) is 5, 5,\n6, 6, 6, and 8, the three team members answer-\ning 6 would get a similarity index of 1 [1 \u00ad | (6\n\u00ad 6)/8 | ], the two team members answering 5\n6)/8 | ], and the team member answering 8\n6)/8 | ]. The similarity index is calculated for\neach response option and averaged for each\nprobe. If a response option is answered as 0 for\nall team members, the similarity index for this\noption is set to 1.\nPlease note that in the above algorithm, probe\nitems that have been responded to by only one\nteam member are scored as having optimal lev-\nels of shared beliefs (as the team member's\nresponse corresponds to the team average).\nDepending on the researcher's theoretical\napproach, some may prefer to refrain from scor-\ning probes in which only one team member has\nanswered. The researcher should consider\nremoving from the data set extreme outlier val-\nues reported by team members, as an increased\nrange reduces the sensitivity of the similarity\nindex for remaining team members.\nComparing team members to team leader.A\ndifferent way to approach the data set would be\nto compare each team member's answers with\nthe answers of a team member who was assumed\nto be the best-informed member of the team. A\nscoring algorithm was developed to index the\ndegree of similarity between an individual team\nmember's answers and the chief of staff's\nanswers to all questions. As the chief of staff\nforms the basis for comparison, he or she can-\nnot be scored. The index should increase when\na team member gives the same response as the\nchief of staff, whereas it should decrease when\na team member gives a response that the chief\nof staff did not give, or the team member does\nnot give a response whereas the chief of staff\ndoes. For probes in which response options are\nticked (Probes 1-3; see Table 1), each response\noption was scored according to whether it\nmatched (1) or did not match (0) the chief of\nstaff's response. For probes where a value or a\ngraded response is given (Probes 4-7), responses\nwere converted into numbers in the same way\nas when calculating team average. For each\nprobe option, the numerical difference between\nthe team member and the team leader was\ndivided by the highest answer given by any of\nthe team members or team leaders across team.\nThis is stated as Algorithm 2 below. For exam-\nple, when expressing where he or she thinks the\nincident is located (Probe 2), a team member\nmay select two of the five response options to\nstate that the incident is in the drilling and pro-\nduction areas, whereas the chief of staff knows\nthat the incident has been normalized in the\ndrilling areas and selects only the production\nTeamaveragesimilarityindex\nteammember sanswer averageof t\n=\n-\n-\n' e\neam sanswers\nhighest answer\n'\nareas. In this case, the team member's response\nthat matches is scored as 1 [1 \u00ad | (1 \u00ad 1)/1 | ], the\nnonmatching response is scored as 0 [1 \u00ad | (0 \u00ad\n1)/1 | ], whereas the three response options not\nselected by neither team member or chief of\nstaff are set at 1, yielding an average similarity\nindex of 0.8 [4/5] for this team member on this\nprobe. If another team member reported the\ndrilling area only, this member would receive a\nsimilarity index of 0.6 [3/5] for this probe. If\ntwo team members rate the likelihood of a full\nevacuation as 5 (quite unlikely) and 6 (very\nunlikely) on Probe 5, whereas their chief of staff\nrates this as 1 (quite likely), the team member\nwhereas the team member that reported 6 is\nscores from Probes 1 to 7 was calculated for an\noverall similarity index. A similarity index\nscore of 1 would indicate that all questions were\nanswered the same way as the chief of staff,\nwhereas a score approaching 0 would indicate\nthat all questions were answered differently\nfrom the chief of staff.\nAlthough developed independently, the math-\nematics of calculating a response's similarity as a\nfraction of full similarity is comparable to the\napproach used by Saner and colleagues (2009) to\ncalculate similarity between dyads of team mem-\nbers, but note that their calculation and theoretical\napproach emphasized that responses should be\nevaluated for accuracy as well as for similarity.\nThe current approach compares team members'\nresponses to a single team member who is assumed\nto be best informed overall in all the queried\nprobes. An alternative approach would be to iden-\ntify different team positions as best informed for\ndifferent knowledge domains (different probes).\nThe current data collections are not suitable for\nsuch an approach as the probes were designed to\nquery general information that is relevant to all\nteam members (although they may be more or less\nsuccessful in obtaining that information).\nDistribution of shared beliefs.As the chief\nof staff may be assumed to be the on average\nbest informed team member in the current set-\nting (see the Research Setting section above,\nand discussed later), we use the calculation\nfrom Algorithm 2 to examine the applicability\nof the current method in Experiments 1 and 2.\nExamining the scores for similarity to the chief\nof staff's responses across scenario (shown in\nFigure 1) showed that the highest degree of\nagreement was on the installation name (Probe\n1, measured for the first two time points only, M =\nlower for estimated duration (Probe 7, M = 0.86,\nSD = 0.20), for incident location (Probe 2, M =\nincident outcomes (Probe 5, M = 0.69, SD =\n0.14). Correlations were calculated between the\nteam members' scores for similarity to the chief\nof staff for the various probe questions at all\ntime points. Probe 2 was correlated at p < .05 to\ntively), and probe 3 was correlated to probe 4\n(r = .54). The positive correlations indicate that\nanswering the same as the chief of staff in some\nprobes increased the likelihood of also answer-\ning the same in other probes. One-wayANOVAs\nfor similarity to chief of staff with team position\nas an independent factor showed no significant\ndifferences (p = .54), indicating that no team\nposition gave advantages over the others in\nattaining the same beliefs as the chief of staff.\nA possible interjection against the use of the\nsimilarity index as a measure of SMM or an\nindicator of SA could be that it does not neces-\nsarily represent the information a team member\nhas gathered and organized but merely repre-\nsents how much information the team member\nrecalls from the previous status meeting. Decid-\ning when a meeting was needed was left to the\ndiscretion of the chief of staff, and two of the\nteams had four meetings during the scenario,\nthree teams had five meetings, and one team had\nsix meetings. Note that the probes were\nresponded to immediately prior to each meeting.\nIn order for probes to merely measure recall\nfrom the previous meeting one must therefore\nassume that there is little development in\nthe chief's and the team members'beliefs about the\nTeamleader similarityindex\nteammember sanswer teamleader\n=\n-\n-\n' 's\nsanswer\nhighest answer\nSimilarity Index in Field Studies 129\nsituation between each meeting, which seems\nunlikely in a dynamic environment. Analyses\nshowed that the number of meetings did not cor-\nrelate with team average similarity metrics, and\na linear regression showed no predictive value\nof the number of meetings on the team average\nsimilarity index scores across scenario and\nprobes (p = .94). The question of whether the\nsimilarity index represents recall from status\nmeetings can also be addressed by examining\nthe observer recordings of the communication to\ncalculate the time that had passed since the pre-\nthe time of the data collections, the number of\nminutes since the last meeting did not correlate\nwith the similarity index measures at that time\npoint, and a linear regression showed no predic-\ntive value of the time since the last meeting on\nthe average similarity index scores across\nDiscussion\nThe timing of probes in Experiment 1 can be\nsaid to have been at the \"worst possible time,\"\nas they were distributed when the chief of staff\nfelt it was time to update the team's beliefs.\nThe chief of staff's decision to arrange a status\nupdate meeting may have been motivated by\nnew developments in the scenario situation that\nnot all team members were aware of, or due to\na long period of individual work having passed\nwithout team calibration. This may thus indicate\nthat the measurement underestimates the team's\nlevel of similarity compared to what would have\nbeen the average during the scenario.\nThe aim of maintaining ecological validity in\nExperiment 1 allowed for factors other than the\nteamwork and the study design to influence\nteam members'performance. For example, since\nthe exercises were performed in cooperation\nwith the actual units and under the conditions\nthat would be involved in a real scenario of this\ntype, factors such as the quality and timeliness\nof information input, availability of scenario\nresources, and weather influenced the level of\ntask complexity and challenge for the team\nmembers. Thus, some of the differences in\nshared beliefs between teams may have been\ncaused by differences in task factors rather than\nindividual or team factors.\nExperiment 2\nMethod\nThe intention of Experiment 2 was to improve\nthe probes from Experiment 1 as well as to col-\nlect data in a more controlled setting, where\nFigure 1. Average similarity index for all probes in both experiments, calculated by comparing team member\nanswers to team leader answers.\nmore of the scenario variables were stable\nacross the different teams tested. In contrast\nto Experiment 1, team members were notified\nabout the times of data collections in advance.\nTeam members were asked to report to the\nemergency center at 8:30 a.m. on the morning\nof data collection, and the scenario started at\nroom acted out the roles of the external units\nthat the emergency center interacted with, such\nas the offshore installation manager, captains\nof nearby ships, hospital emergency staff, and\nthe police. The scenario was acted out accord-\ning to a predetermined script to ensure that the\nfive teams' performances were comparable. In\nthe scenario, an offshore hydrocarbon produc-\ntion installation was threatened by a subsea\ngas leak from an unknown source, which was\nfurther complicated by difficult weather condi-\ntions and injured and missing personnel. The\nscenario was scripted to last 160 min. Probes\nwere collected by sending an email to each\nteam member's workstation containing a link\nto an online questionnaire. Individual team\nmember responses were identifiable in the data\nfile through the workstation the emails had\nbeen sent to. In addition to the email notifica-\ntion, oral reminders were given at the time of\ndistribution to ensure that all team members\ncompleted the probes. The probe questionnaires\nwere distributed at predetermined time points,\nindependent of the chief of staff's status update\nmeetings. The freeze points were scheduled at\nscenario start, and the scenario was frozen for\n3 to 5 min (in the sense that the actors stopped\nconversations and did not provide information\nto the team during freezes) to allow all team\nmembers to have time to complete the question-\nnaires without falling behind on their workload.\nThe probes were largely the same as those in\nExperiment 1, but Probes 1 and 7 were removed\nand others were changed to allow more rapid\nresponses and to ensure more easily quantifiable\nforced-choice answer categories (listed in the\nright column of Table 1). The probe items were\ndisplayed on three sequential screen displays,\nPage 2, and confirmation of receipt and instruc-\ntions to return to task work on Page 3.\nThe same background variables were col-\nlected and observations made as in Experiment\n1. In addition, the head of the acting staff\nanswered a similar online form at each freeze\ntime point, containing questions about which\nteam members could be expected to know the\naspects of the scenario queried in Probes 2 to 6.\nThe chief of staff was fitted with audio-record-\ning equipment to allow offline analysis of con-\nversations to score communication type and\ncontent. All team members were fitted with\nheart monitoring equipment for offline analysis\nof heart rate variability. Five teams (with some\nrepeating team members) were tested within the\ncourse of 6 weeks.\nResults\nCompliance. Arranging the freeze points for\ndata collection required coordination between\nthe team members and the actors running the\nscripted scenario. In cases where the freeze\npoints would have coincided with the chief of\nstaff's intention to conduct a status update meet-\ning, the freeze points were brought forward or\ndelayed by a few minutes in relation to the\nplanned schedule. In the first team's data collec-\ntion, delays forced us to cancel two of the nine\nensure the exercise remained meaningful to the\nteam members.\nThe online questionnaires gave team members\nfeedback if they missed a response, and team\nmembers were not allowed to continue to the next\npage until all the questions had been answered.\nThis increased overall compliance in Experiment\n2 compared to Experiment 1. Nevertheless, some\nteam members missed all probes at some time\npoints (scenario compliance M = 93.35%, SD =\n21.09%), due to technical difficulties (not receiv-\ning or reading the email), being unavailable or too\nbusy at the time, or declining to participate. Some\nanswered the first page of the questionnaire\n(Probes 2-4) but failed to complete the second\npage of the questionnaire (Probes 5-6).\nAssessment of data collection. Probe 1 was\nnot used in the data collection for Experiment 2,\nas it had shown little variation in Experiment 1.\nProbe 7, which had proved problematic in\nSimilarity Index in Field Studies 131\nExperiment 1, was removed. It seems the com-\nplexity of Probes 4 and 6 was hard to express in\npen-and-paper data collection, and these probes\nwere better suited to the more structured\nresponse modality used in Experiment 2.\nAnalytical approach.Post hoc discussion\nwith subject matter experts in the organization\nrevealed that establishing a ground truth for\nvalidating the accuracy of probe answers was\nalso difficult in Experiment 2. Despite running\na scripted scenario, the actors had to adapt the\nscenario to the team's requests and other idio-\nsyncrasies of the scenario performance. Some\nof the information that was queried in the probes\n(such as the number of personnel reported miss-\ning by the production installation) was described\nin the scenario script, whereas other probes que-\nried information that was generated within the\nteam or in the interaction between the team and\nthe actors (such as how to evaluate possible out-\ncomes and how the team prioritized different\nobjectives).\nAnalysis of the head of the acting staff's\nreports of who was expected to know what at\neach freeze point showed that the chief of staff\nwas the team position expected to be optimally\ninformed about the task and teamwork most of\nthe time (97% of the freeze probes), whereas the\nrest of the team was expected to be optimally\ninformed slightly later in the scenario (across all\nteam positions, the estimates for the first four\nand then 100% for the final five freeze probes).\nNote that this does not necessarily correspond to\nthe head of the acting staff expecting the various\nteam members to have accurate information at\nthese times.\nComparing team members to team average\nand to the best-informed team member.To\nobtain an index for individual similarity to team\naverage and similarity to the best-informed\nteam member, the same approach as in Experi-\nment 1 was used to calculate responses to freeze\nProbes 2 to 6 according to Algorithms 1 and 2.\nThe following analyses are based on Algorithm\n2, comparing responses to the chief of staff.\nDistribution of shared beliefs.The similar-\nity to chief of staff was highest in the probe\nconcerning incident type (Probe 3, M = 0.96,\nSD = 0.05), followed by personnel status (Probe\nagreement on incident outcomes was lower\ndard deviations for all probes in Experiment 1\nand 2 are shown in Figure 1. Correlations across\nall measurement points showed Probe 2 to be\nway ANOVAs for scenario average similarity to\nchief of staff with team position as an indepen-\ndent factor showed no significant differences\n(p = .63), indicating that no team position gave\nadvantages over the others in attaining the same\nbeliefs as the chief of staff.\nIn Experiment 2, the probe questions were\ndistributed at predetermined time points not\nknown to the teams in advance. The difficulty in\nanswering the questions may therefore have var-\nied randomly between teams and time points\naccording to how long it had been since the last\nstatus update meeting. Analyses were conducted\nto test whether the degree of similarity was\ndetermined by the total number of meetings or\nthe time that had passed since the last meeting.\nThe five teams had four, five, six, seven, and\neight meetings. The number of meetings did not\ncorrelate with team average similarity metrics,\nand a linear regression showed no predictive\nvalue for the number of meetings on the similar-\ntime of the freeze probes, an average of 12.45\nmin (SD = 9.26) had passed since the last meet-\ning. There was no correlation between the time\npassed since the last meeting and the similarity\nto chief of staff for each time point, and a linear\nregression showed no predictive value in the\ntime passed since the last meeting on the simi-\nDiscussion\nOne of the main challenges in both experi-\nments was to make the team members prioritize\nanswering the probes over performing their task\nwork. This was a particular problem in Experi-\nment 1, where the external parties continued\nto act out the scenario while the team froze,\nbut even in Experiment 2, where the exercise\nwas arranged for the sole purpose of data col-\nlection, some team members were so involved\nin their task work that probe items were left\nunanswered. As in other studies with intermit-\ntent data collection, the multiple-choice probe\nquestions may have steered the team members'\nattention and their approach to the task work.\nIn fact, one chief of staff informed us that he\nhad used the probe items as a \"checklist\" for\nwhat he needed to include in the status update\nmeetings. Some team members appeared to be\nfrustrated when they were given factual ques-\ntions early in the scenario that they could not\nanswer confidently, and when they felt their task\nwork was too frequently interrupted by probe\nquestions. Several team members expressed\ndismay over having to answer probes only 5\nmin into the scenario, which was before the\nfirst status update meeting for some teams. Our\nintention in including this freeze point was to\nmeasure the team's baseline beliefs or assump-\ntions before they were informed by the chief of\nstaff or through their task work. However, given\nthe team's frustration and their reluctance to\nestimate uncertain information, using such early\nfreeze points may not be advisable.\nA scripted scenario and a staff of actors were\nused in an attempt to control some of the factors\nthat varied arbitrarily in Experiment 1, but there\nmay still have been differences between the dif-\nferent teams, for example, due to changes in the\nresearcher or acting staff's experience between\neach scenario run, and adapting the script to sce-\nnario feedback.\nGeneral Discussion\nThe current study developed and field tested\na novel approach for measuring shared beliefs\nin teams working in operative settings such as\nan emergency preparedness organization in the\nhydrocarbon industry. The motivation for this\napproach was to develop a reasonable measure-\nment of SMM and an indicator of SA that can\nbe applied for teams working in complex and\ndynamic situations in which it is difficult to\nestablish a ground truth. Based on discussions\nwith professionals and pilot data collections, we\ndeveloped probes for measuring beliefs about\nthe team's work and the external situation.\nTwo different experiments were conducted with\nsomewhat different research goals and thus\nsome differences in methodological approach.\nExperiment 1 aimed to have minimum intru-\nsion into a standard training exercise, whereas\nExperiment 2 aimed to have more experimental\ncontrol, more data points, and higher compli-\nance. Algorithm 1 was developed to index the\nextent to which individual beliefs coincided\nwith the team's average belief. Algorithm 2 was\ndeveloped to obtain a similarity index between\nthe responses of individual team members and\nthe best-informed team member. High similarity\nindex values indicated that an individual team\nmember or the team in general chose response\nalternatives that had also been chosen by others,\nand low values indicated that team members\nchose response alternatives that had not been\nchosen by others, or had missed response alter-\nnatives that others had chosen. The similarity\nindex scores between probes showed the same\npattern for Experiment 1 and 2 (see Figure 1).\nThe probes appear to have been meaningful and\nrelevant to the team members in both experi-\nments. There was no indication that any team\npositions had advantages in answering any of\nthe probes, indicating that the probes described\ngeneral team aspects. The fact that the similarity\nindex did not vary according to the time passed\nsince last meeting indicated that the similarity\nindex did not simply reflect the team member's\nrecall from the previous status update meeting.\nThe study provides a new approach to objec-\ntively measuring SMM and indicating SA in\nsituations where a ground truth is unavailable.\nWe argue that the similarity to team average or\nbetween team members can be used as a mea-\nsure of SMM, as answering the same as the rest\nof the team can be said to represent the team\nconverging on the same understanding of the\nsituation, task aspects, and task work. If the\nmeasurement is made in a setting where it can be\nassumed that the person in a given role should\non average be better informed than other team\nmembers, one may assume that this person's\nresponses on average are closer to the objective\nreality, or at least that the person has on average\nmore reliable responses than the other team\nmembers. It may thus be argued that the similar-\nity between a team member and the best-\ninformed team member can be used as an indica-\ntor of SA. The similarity index for individual\nSimilarity Index in Field Studies 133\nteam members provides an indication of the\nextent to which that member has the same beliefs\nabout the situation as the rest of the team and a\nwell-informed member of the team. Moreover,\nthe average similarity index for a team indicates\nthe extent to which the team is well coordinated\nand to what extent important information is dis-\ntributed among team members.\nThe measurement approach does not require\na carefully controlled exercise or simulation to\nbe arranged, or to carefully examine the events\nin retrospect, as the researcher makes no assump-\ntions about what the actual external situation is,\nbut merely compares beliefs within the team. In\ntheory, the approach can also be applied to real-\nlife incidents if the workload allows team mem-\nbers to simultaneously answer probes.\nFurther studies may compare a similarity\nindex to other measures of SMM and SA, such\nas subjective self-rating scales (e.g., SART; Tay-\nlor, 1990) or to measures of objectively accurate\nsituation knowledge (e.g., SAGAT; Endsley,\n1995a). The measurement approach can be used\nto identify consistencies within teams or within\npositions, to see the extent to which SMM or SA\nis determined by team-level characteristics or by\nhaving a given position in the team. The current\nexperiments measured team members' beliefs\nover the course of the scenario, with four to six\nmeasurements in Experiment 1 and nine mea-\nsurements in Experiment 2. By calculating probe\nresponses at different time points as repeated\nmeasures, one can examine how SMM or SA\ndevelops over time. For example, it is possible\nto envisage team members' beliefs catching up\nwith the chief of staff's beliefs over time, or that\nhigher levels of similarity are achieved as a sce-\nnario stabilizes. The input of new information\ninto the scenario can also be followed to see if\nand when changes in the situation are made\nknown to the whole team. In applied settings,\ndifferences in similarity index between the vari-\nous task aspects can be used to inform the orga-\nnization about what aspects should be empha-\nsized in team training or in structural changes to\nthe team's work environment.\nThe comparison of beliefs between team\nmembers is crucial to the current approach.\nSaner and colleagues (2009) listed five different\nrelationships that could exist between two team\nmembersAand B and the objective reality R: (a)\nboth A and B could have the same accurate\nbeliefs (A = B = R), (b) A could have accurate\nbeliefs, whereas B has inaccurate beliefs (A = R\n B), or (c) the other way around (A  R = B),\n(d) both A and B could have inaccurate but dif-\nferent beliefs (R  A  B  R) or (e) they could\nboth have inaccurate but similar beliefs (A = B \nR). The same relationships could exist in the\ncurrent study, although we are restricted to\nknowing whether Persons A and B (or a larger\nteam) have the same understanding of the situa-\ntion, as the current approach is na\u00efve with regard\nto what is objectively true (R), instead compar-\ning only the information overlap between A and\nB. Conditions a and e would thus be rated as\nhigh similarity, whereas b, c, and d would be\nrated as low similarity. Instead of comparing\nresponses to \"reality,\" the current approach\ncompares responses to the team average, as\ndescribed in Algorithm 1 in the Results section,\nwhich presents given assumptions for and\nrestrictions on the conclusions. Alternatively, if\nit can reasonably be assumed that one team\nmember on average has better access to informa-\ntion or a better overview than the rest of the\nteam, the other team members' similarity to the\nbest-informed team member can be calculated,\nas described in Algorithm 2 in the Results sec-\ntion. This would serve the same function as\ncomparisons to reality for Saner and colleagues\n(2009), but with different inherent assumptions\n(see below).\nAlgorithm 2 in the current approach calcu-\nlated similarity between the team leader and the\nindividual team members in a manner similar to\nthe calculation used by Saner and colleagues\n(2009) to compare dyads of team members.\nHowever, Saner and colleagues emphasized the\ndistinction between the concepts of similarity\nand accuracy in a team's SA, and argued the\nneed to measure both. Although we agree that\nsuch an approach may be preferable in situations\nthat allow for it, in contexts where it is not pos-\nsible to assess the accuracy of beliefs, we may\nneed to rely on similarity both as a measure in\nitself and as a proxy for the accuracy of the team\nmembers' beliefs. Our attempts to measure\naccuracy objectively in the current study proved\nfutile, even with a scripted scenario and exami-\nnation of audio recordings and event logs. A\nsimilarity index may easily be measured and\ncalculated in ordinary training scenarios,\nwhereas measuring accuracy requires consider-\nably more resources to develop and analyze the\nscenario, which may be difficult to achieve for\npractitioners in most settings.\nThe underlying assumption of the current\nresearch approach is that shared cognitive states\nare beneficial to team processes. Higher degrees\nof shared information between team members\nwere thus considered to reflect higher degrees of\nSMM and SA and lead to improved team perfor-\nmance. However, situations can also be envis-\naged where a uniform view of the situation\nwithin the team is unfortunate. In situations\nwhere there is high group cohesion among a\nhomogenous group, high stakes, an external\nthreat, and recent failures, team processes known\nas groupthink may emerge (Janis, 1972). Under\nsuchconditions,thedecisionprocessisadversely\naffected, leading to effects such as rationalizing\nwarning signals, overestimating the group's\nabilities, and pressure for conformity. One\nshould therefore be wary not only of teams with\nlow similarity indices, but also of teams show-\ning uniformly high degrees of similarity in spite\nof uncertain and dynamic situations with distrib-\nuted access to information. In operative settings,\nthis can have the effect of a team being overly\nconfident and focused on a certain understand-\ning of an incident and acting accordingly, while\nignoring indications that it may be heading in\nthe wrong direction.\nMoreover, when evaluating a team or individ-\nual's beliefs, a high level of correspondence\nbetween team member beliefs and the objectively\ntrue situation may intuitively be considered to be\noptimal. In emergency management work, how-\never, organizations and individuals often undergo\n\"proactive leadership\" training (Nudell & Anto-\nkol, 1988), which fosters a strategy based not on\nestablishing an objectively true view of the situa-\ntion but rather on envisioning and preparing for a\nworst-case scenario on the basis of the available\ninformation. Thus, the focus may be not on know-\ning facts such as the current extent of a fire and\nwhen it can be expected to be put out, but rather\non how extensive the fire may become if initial\nfirefighting efforts fail and what additional\nresources or actions will then be needed. Given\nsuch training, comparing an emergency prepared-\nness team's representation of an incident to the\nobjective reality of the incident (as would be done\nin objective SA measures) may indicate that the\nteam overestimates the severity of the incident,\nwhereas this is in fact a function of the team oper-\nating in accordance with its training. Thus, if a\nteam follows such a strategy, it may result in\nlower scores for objective measures of SA. On\nthe other hand, the approaches suggested in the\ncurrent study compare the beliefs within the team,\nso a team working on the basis of proactive lead-\nership will be measured for the extent to which it\nenvisages the same worst-case scenario, and the\nsimilarity index approach will thus be well suited\nto settings where such strategies are used.\nParts of the analysis in the current approach\nassume that one team member (in our case the\nchief of staff) can be considered to be the best-\ninformed member of the team. This assumption\nshould perhaps be rephrased to read that the chief\nof staff will be the best-informed member in a\nwell-functioning team with an effective leader.\nHowever, a misinformed leader or a well-\ninformed leader who has failed to communicate\nhis or her understanding with the team will nega-\ntively affect the team members' similarity index\nscores. There are likely to be cases where an indi-\nvidual team member has an accurate understand-\ning of the situation, whereas the team leader does\nnot. In such cases, the team member's beliefs\nwill have the effect of lowering the estimates of\nthe team members' SMM or SA. This may seem\nunfair to the individual team member whose\nbeliefs are discredited despite an objectively\naccurate answer, yet the adjustment does reflect\npoor team processes. In the scenarios run in our\ntwo experiments, we suspect that in some cases\nan aspect of the incident had been normalized,\nbut the chief of staff only informed the most\ndirectly involved team members about this while\nthe rest of the team still believed the aspect to be\nin an alarm state. Nevertheless, if it can reason-\nably be assumed that a specific team member\nshould on average be better informed than the\nothers, the suggested method could be used for\nmeasuring the accuracy of the team's beliefs and\nthus predict SA, although there will be excep-\ntions contributing to measurement noise.\nIn the emergency preparedness center studied\nin the current experiments, the team consists of\nnine specialized experts responsible for han-\ndling different aspects of emergency. It can thus\nSimilarity Index in Field Studies 135\nbe assumed that some aspects of emergency\nmanagement would be better known to some\nteam members than to others (as argued by Jones\n& Endsley, 2002). In designing the probes, we\ntook steps to ensure that the questions described\nknowledge that was not considered to be limited\nto only some of the team positions, but addressed\naspects that all team members were expected to\nknow about at all times. Our feedback to the\norganization's management after data analysis\nconfirmed that in an optimally functioning team,\nall team members would be expected to agree on\nthe information measured by the probes at all\ntime points. As described in the Introduction, an\nalternative approach for future studies could be\nto design probes that cover different aspects of\ntechnical expertise more familiar to certain team\npositions than others, and the analysis could use\ndifferent team positions as the best-informed\nteam member for different probes.\nIn addition to comparing answers to the chief\nof staff's answers, the Results sections also pres-\nent an alternative analysis approach, namely, to\ncalculate the team's average response to each\nquestion and calculate the extent to which the indi-\nvidual team member's response deviated from the\naverage answer. This would avoid some of the\ncaveats discussed above (e.g., that an objectively\ncorrect answer may be scored as incorrect since\nthe chief of staff's answer is incorrect), but would\nintroduce different caveats (e.g., that a recent\nchange in the situation may be known only to a\nfew, well-informed team members who would\ndeviate from the team average). If a team member\ncan be assumed to be closer to the objective truth\nthan the others, it may make sense to follow our\nexample and use this member's answers as a basis\nfor comparison, while keeping the inherent\nassumptions mentioned above in mind.\nThe current paper describes the development\nof tools for measuring SMM and SA by compar-\ning individual team members' beliefs to the\nteam's average beliefs or to the best-informed\nteam member's beliefs. The current examples\nare from contexts involving hydrocarbon indus-\ntry emergency preparedness teams, but similar\napproaches may be useful in other situations\nwhere objective truth is unknown or unknow-\nable, although the assumptions inherent in the\napproach should be considered.\n"
}