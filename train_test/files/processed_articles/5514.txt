{
    "abstract": "Abstract\nIt is common in regression discontinuity analysis to control for third- or fifth-degree polynomials of the assignment\nvariable. Such models can overfit, leading to causal inferences that are substantively implausible and that arbitrarily\nattribute variation to the high-degree polynomial or the discontinuity. This paper examines two recent studies that\nmake use of regression discontinuity to discuss evident practical problems with these estimates and how they interact\nwith pathologies of the current system of scientific publication. First, we discuss a recent study that estimates the\neffect on air pollution and life expectancy of a coal-heating policy in China. The reported effects, based on a third-\ndegree polynomial, are statistically significant but substantively dubious, and are sensitive to model choice. This study is\nindicative of a category of policy analyses where strong claims are based on weak data and methodologies which permit\nthe researcher wide latitude in presenting estimated treatment effects. We then replicate a procedure from Green et al.,\nin which regression discontinuity is used to recover estimated treatment effects relative to an experimental benchmark,\nto illustrate one practical problem with the regression discontinuity estimates in the coal-heating paper: high-degree\npolynomials yield noisy estimates of treatment effects that do not accurately convey uncertainty. We recommend that\n(a) researchers consider the problems which may result from controlling for higher-order polynomials; and (b) that\njournals recognize that quantitative analyses of policy issues are often inconclusive and relax the implicit rule under which\nstatistical significance is a condition for publication.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nRegression discontinuity analysis\nRegression discontinuity (RD) methods are one of the\nstandard techniques used in statistics and econometrics to\nobtain causal inference from observational data. But imple-\nmentations of RD can have serious problems in practice,\nespecially with the common approach of controlling for\nhigh-degree polynomials of the underlying continuous pre-\ndictor. In a companion paper (Gelman and Imbens, 2014)\nwe present evidence that controlling for high-order polyno-\nmials in RD analysis results in noisy estimates with poor\nstatistical properties and confidence intervals that are too\nnarrow. In the present paper we discuss evident practical\nproblems with these estimates and how they interact with\npathologies of the current system of scientific publication.\nWe demonstrate with a recent well-publicized example\nin public health where a high-degree polynomial control in\nan RD analysis led to implausible conclusions. The magni-\ntude and significance of reported treatment effects were\nhighly sensitive to model specification. We then extend a\npaper by Green et al. (2009) to illustrate that high-degree\npolynomial estimates such as those reported in the public\nhealth paper are subject to uncertainty and noise not\ncaptured by reported p-values. In addition to implying that\nEvidence on the deleterious impact of\nsustained use of polynomial regression\non causal inference\nAndrew Gelman1 and Adam Zelizer2\n Keywords\nIdentification, policy analysis, polynomial regression, regression discontinuity, uncertainty\nDepartment of Statistics and Department of Political Science, Columbia\nUniversity, NY, USA\n2Department of Political Science, Columbia University, NY, USA\nCorresponding author:\nAndrew Gelman, Department of Statistics and Department of Political\nScience, Columbia University, New York, NY 10027, USA.\nEmail: gelman@stat.columbia.edu\nResearch Article\n2 Research and Politics \nresearchers should show much more caution with such\nmodels, this experience suggests a rethinking of conven-\ntional ideas of robustness to model specification.\nRD analysis, introduced by Thistlewaite and Campbell\n(1960), has recently enjoyed a renaissance, especially in\neconomics; Lee and Lemieux (2010) provide an influential\nreview. In their words, RD is \"a way of estimating treat-\nment effects in a nonexperimental setting where treatment\nis determined by whether an observed `assignment' varia-\nble (also referred to in the literature as the `forcing'variable\nor the `running' variable) exceeds a known cutoff point.\"\nTo the extent that the assignment depends (perhaps stochas-\ntically) only on this rule, and to the extent that there are no\nsystematic pre-treatment differences between the items\nbelow and above the cutoff, the RD design can be inter-\npreted as a quasi-experiment and the resulting inferences\ncan be interpreted causally.\nOne way to see the appeal of RD is to consider the\nthreats to validity that arise with five other methods used\nfor causal inference in observational studies: simple regres-\nsion, matching, selection modeling, difference in differ-\nences, and instrumental variables. These competitors to RD\nall have serious limitations: regression with many predic-\ntors becomes model dependent (using the least squares\napproaches that are traditional in econometrics, it is diffi-\ncult to control for large numbers of predictors, while non-\nparametric approaches such as Bart (Hill, 2011) have not\nyet gained wide acceptance); matching, like linear or non-\nlinear regression adjustment, leans on the assumption that\ntreatment assignment is ignorable conditional on the varia-\nbles used to match; selection modeling is sensitive to\nuntestable distributional assumptions; difference in differ-\nences requires an additive model that is not generally plau-\nsible; and instrumental variables, of course, only work\nwhen there happens to be a good instrument related to the\ncausal question of interest.\nFor all these reasons, in practice causal analyses often\nseem to flow from identification opportunities to inferences\nof interest (Gelman, 2009), a view that contrasts with the\nusual textbook presentation in which the research question\ncomes first and then the analyst finds an identification strat-\negy to attack the problem at hand.\nMany of the challenges of applying an identification\nstrategy arise in the data analysis. Sample sizes can be\nsmall (especially in areas such as political science or eco-\nnomics where one cannot simply augment a dataset by\ninstigating a few more wars, scandals, or recessions), and\ntheoretical results of unbiasedness do not always help\nmuch, first because low bias has no practical meaning in\nthe presence of high variance, and second because datasets\nare typically constructed by pooling over different subpop-\nulations or different time periods or different sorts of cases,\nso that any claims of unbiased estimates typically apply\nonly to aggregates that are not directly relevant to the ulti-\nmate questions of interest.\nFor these reasons, the Lee and Lemieux paper is wel-\ncome in that it continually returns to practical issues of esti-\nmation. Particularly relevant for the purposes of our\ndiscussion here are two of their recommendations for check-\ning the robustness of RD estimates of the treatment effect:\n1. \"From an applied perspective, a simple way of\nrelaxing the linearity assumption is to include poly-\nnomial functions of X in the regression model....it\nis advisable to try and report a number of specifica-\ntions to see to what extent the results are sensitive to\nthe order of the polynomial.\"\n2. \"Graphical presentation of an RD design is helpful\nand informative but the visual presentation should\nnot be tilted toward either finding an effect or find-\ning no effect.\"\nBoth these pieces of advice seem reasonable (although, in\nthe first case, we would prefer a spline or Gaussian process\nor some other such smooth model, as indeed has been sug-\ngested, for example, by Calonico et al., 2014). The chal-\nlenge is what to do after following this advice.\nExample: A claim that coal heating is\nreducing lifespan by five years for half\na billion people\nWe discuss in the context of a paper by Chen et al. (2013)\nthat received a great deal of attention with the following\nclaim:\nThis paper's findings suggest that an arbitrary Chinese policy\nthat greatly increases total suspended particulates (TSPs) air\npollution is causing the 500 million residents of Northern\nChina to lose more than 2.5 billion life years of life expectancy.\nThe quasi-experimental empirical approach is based on\nChina's Huai River policy, which provided free winter heating\nvia the provision of coal for boilers in cities north of the Huai\nRiver but denied heat to the south. Using a regression\ndiscontinuity design based on distance from the Huai River,\nwe find that ambient concentrations of TSPs are about 184 g/\nthe north. Further, the results indicate that life expectancies are\nincreased incidence of cardiorespiratory mortality.\nBefore going on, let us just say that these results are\ninteresting even if the 95% CIs happen to include zero.\nThere is an unfortunate convention that \"p less than 0.05\"\nresults are publishable while \"non-significant\" results are\nnot. The life expectancy of 500 million people is important,\nand it is inappropriate to wait on statistical significance to\nmake policy decisions in this area.\nWe have reproduced the key graph of Chen et al. as\nFigure 1 here. It is a beautiful graph, showing the model\nand the data together and following the advice of Lee and\nGelman and Zelizer 3\nLemieux reported above. However, we are far less than\n97.5% sure that the effects are in the direction that the\nauthors claim1.\nTable S.9 in the Supplementary Material online, repro-\nduced in part here as Figure 2, gives the authors' results\ntrying other models. The cubic adjustment gave an esti-\nmated effect of 5.5 years with standard error 2.4. A linear\nadjustment gave an estimate of 1.6 years with standard\nerror 1.7. The large, statistically significant, estimated\ntreatment effect at the discontinuity depends on the func-\ntional form employed. The higher-degree polynomials have\nthe advantage of being more general but the disadvantage\nof yielding noisy and often implausible estimates. The\nimplausibility is illustrated in Figure 1; the noise we shall\ndiscuss in a bit.\nOur point here is not to argue that the linear model is\ncorrect; the authors in fact supply data-based reasons for\npreferring the cubic model. Our point is rather that the\nFigure 1. Key graph from Chen et al. (2013) showing their regression discontinuity analysis. Each circle represents the average\nfrom a set of locations in China.\nCI: confidence interval; LE: life expectancy\nFigure 2. Excerpts of a table from the Supplementary Material online from Chen et al. (2013). Our problem with all these models\nis that they do not include other predictors and that the residual errors are large (see Figure 1). Thus the causal estimate based\non regression discontinuity is highly sensitive to the assumption that the other factors (represented by a combination of the\nnonlinearity and the error term in the regression model) are uncorrelated with the discontinuity.\n4 Research and Politics \nheadline claim, and its statistical significance, is highly\ndependent on a model choice that may have a data-analytic\npurpose, but which has no particular scientific basis. Figure\n1 indicates to us that neither the linear nor the cubic nor any\nother polynomial model is appropriate here. Instead, there\nare other variables not included in the model which distin-\nguish the circles in the graph.2\nWe suggest caution in the estimation and reporting of\nhigh-degree polynomial estimates. In the following section,\nwe demonstrate a general, undesirable feature of these esti-\nmates, one which may have contributed to the implausible\nestimates reported above: high-degree polynomials pro-\nduce noisy estimated treatment effects with standard errors\nthat do not accurately reflect the true degree of uncertainty\nin the estimate.\nReanalysis of RD models fitted to data\nfrom a voter mobilization experiment\nThe China pollution study illustrates an example where a\nfitted high-degree polynomial has the effect of adding noise\nto the estimated discontinuity treatment effect. The standard\nerror reported from a discontinuity regression does not\naccount for systematic error in the fitted model \u00ad in this\ncase, the high-degree polynomial \u00ad and thus represents a\nlower bound on uncertainty (Green et al., 2009). Similarly,\nGelman and Imbens (2014) demonstrate that RD inferences\ncan have much worse than nominal coverage (for example,\np-values of 0.05 occurring more than 10% of the time) in the\npresence of systematic error in the fitted curve.\nIt is not always so apparent how noisy the RD estimate\ncan be when a single model is being fitted to data. We dem-\nonstrate the problem here in an example of a randomized\nexperiment in which an RD structure is artificially created by\nremoving data. Our analysis elaborates on an example from\nGreen et al. (2009), who take data from a randomized experi-\nment that Gerber et al. (2008) conducted on potential voters.\nGreen et al. create an artificial discontinuity, removing all the\ntreated people in the sample who were below the age of 55\nand removing all the controls who were 55 and older. In their\n2009 study, Green et al. had the full data from the original\nrandomized experiment, which they used to estimate a\nbenchmark experimental treatment effect, and a partial data-\nset with a discontinuity, for which they used RD methods to\nestimate the effect at age 55. Figure 3 shows the observed (in\nred) and missing (in blue) data for this analysis.\nThe broken line in Figure 3 shows a fitted fourth-degree\npolynomial with a discontinuity at age 55; the fitted curve\nlooks reasonable (if perhaps a bit too sharply sloped just on the\nright-hand side of the breakpoint) and the estimated jump at\nthe discontinuity yields a treatment effect estimate that is con-\nsistent with that obtained from the experimental benchmark.\nBut the reasonability of this high-degree polynomial\nestimate depends on the breakpoint chosen, as we can see\nby repeating the discontinuity analysis at a range of poten-\nFigure 4 shows the RD estimates as a function of the\nage discontinuity, along with 95% error bounds from the\nestimated regressions. These graphs do not display fitted\ncurves; rather, each point on each graph shows the coef-\nficient estimate and uncertainty from a single RD model.\nThe RD estimates are very noisy and in several places\nThe estimate at age 55 happens to look good but this seems\nto be largely a matter of luck. For many age cutoffs, the\ncorresponding fitted models are noisy and implausible with\nFigure 3. Graph from Green et al. (2009) showing a missing-data construction used to study the performance of regression\ndiscontinuity estimates: \"The red circles depict average voting rates among observed voters, grouped by year of age, which has been\nrescaled so that zero (age 55) is the point of discontinuity. The blue circles depict average voting rates among counterfactual voters\n[who were artificially removed from the dataset]. The red circles to the left of the age cutoff (where age equals 0) represent the\ntreatment group, which received the experimental mailings. The red circles to the right of the cutoff represent the control group,\nwhich received no experimental mailings. The size of the circles is proportional to the number of observations in each age group.\"\nReproduced by permission of Oxford University Press.\nGelman and Zelizer 5\ndramatic up and down swings, chasing the data in a manner\nreminiscent of Figure 1.3\nThe point here is not that RD with high-order polynomi-\nals always gives bad answers. Rather, without some con-\nstraint on the smoothness of the fitted functions, we do not\nrecommend simply fitting such a model. Including high-\norder polynomials is not a universally conservative\napproach. These models can fail, generating noisy esti-\nmated discontinuity coefficients with p-values that do not\nreflect the uncertainty of the model specification.\nDiscussion\nPublication of speculative findings on particulate\npollution and life expectancy\nOur goal in reassessing the findings of Chen et al. is to call\ninto question the way scholars control for higher-order poly-\nnomials in RD analysis and the substantive implications that\nfollow from these data-analytic decisions. We are not saying\nthat particulate matter does not kill, that this topic should not\nbe studied, or that these findings should not be published in\na high-profile journal. The accompanying article by Pope\nand Dockery (2013) considers why the conclusions reached\nby Chen et al. might be scientifically plausible.\nRather, we see that example as indicative of a category\nof policy analyses where strong claims are based on weak\ndata, with high-order polynomial RD designs one example\nof how researchers can amplify the magnitude or signifi-\ncance of estimated treatment effects with an eye toward\npublication. Researchers are not alone in placing too much\nvalue on statistically significant findings or those with large\nsubstantive effects. What we suggest is a two-step: that\nauthors retreat from strongly model-based claims of\nstatistical significance and that journals accept that non-\nstatistically-significant findings on important topics are\nstill worth publishing.\nPlausibility of a regression discontinuity estimate\nin the context of the model\nAt a technical level, we understand the appeal of controlling\nfor high-order polynomials of the assignment variable, fol-\nlowing the general principle that it is safest and most con-\nservative to control for potential confounders to reduce bias.\nThis reasoning in terms of bias, however, does not\nalways work. And, more to the point, problems can be\napparent in particular cases. Again, return to Figure 1,\nwhich reveals how much of the estimated discontinuity\narises from the steep gradient estimated in life expectancy\nwith latitude near the discontinuity, or Figure 4, which\nshows the large differences in the magnitude of estimated\ntreatment effects across models.\nIn well-designed RD studies, the underlying predictive\neffect of the assignment variable is clear. For example, in\nthe Lee (2008) study of incumbent party and elections, it\nmakes perfect sense that there will be an approximately lin-\near relation between Democratic or Republican shares in\none election and the next; and in the Berger and Pope\n(2011) study of motivation in basketball, the probability of\nwinning the game is unsurprisingly strongly, smoothly, and\nmonotonically predicted by the score differential at half-\ntime. In both these cases, the fit from a cubic polynomial is\nnot far from a straight line on the original or logistic scale.\nThird Order Polynomial\nDiscontinuity (years)\nFifth Order Polynomial\nDiscontinuity (years)\nEstimated Treatment Effect\nEstimated Treatment Effect\nFigure 4. Regression discontinuity estimates of the effects of the voter mobilization treatment, based on replicating the procedure\nof Green et al. (2009) separately, for each age cutoff from 40 to 70. At each age, the graphs show the estimate and 95% confidence\ninterval from the fitted regression, controlling for a third or fifth-order polynomial. The red bar shows the result at the age cutoff\nof 55, which was used by Green et al. as illustrated in Figure 3, and the horizontal dotted line at 0.097 corresponds to the estimated\naverage treatment effect from the full data from the randomized experiment. The regression discontinuity estimates are noisy,\nespecially when controlling for the fifth-degree polynomial.\n6 Research and Politics \nThus, the higher-order polynomial has the effect of slightly\nmodifying and improving the fit of the natural linear model.\nIn criticizing the use of high-degree polynomials in RD\nadjustments, we are not recommending global linear adjust-\nments as an alternative. In some settings a linear relation-\nship can make sense (for example in data with a simple\nbefore\u00adafter structure), but in general our concerns about\nsystematic error will not disappear with the use of a simpler\nform. What we are warning against is the appealing but\nmisguided view that users can correct for arbitrary depend-\nence on the forcing variable by simply including several\npolynomial terms in a regression. We recommend that any\nRD analysis include a plot such as Figure 1 showing data\nand the fitted model, and that users be wary of any resulting\ninferences based on fits that don't make substantive sense.\nOur message is also consistent with that of Green et al.\n(2009), who recommend comparing observational studies\nwith controlled experiments where possible.\nSkepticism without nihilism\nThe current rules of publication seem to us to be simultane-\nously too loose (in the sense of accepting the highly ques-\ntionable analysis indicated in Figure 1) and too restrictive\n(in essentially demanding statistical significance, obtained\nsome way or another, as a condition for acceptance).\nOne might reply that the scientific literature is self-\ncorrecting and so we should not worry so much about\nimperfect or erroneous methods; shaky findings are unlikely\nto show up on replication. Unfortunately, things do not\nalways work out so well; once researchers know what to\nexpect, they can continue finding it, given all the degrees of\nfreedom available in data processing and analysis (Gelman\nten earlier (Gelman, 2013b) in the context of a different set\nof controversial claims, the systematic publication of statis-\ntically significant overestimates can lead to \"a boom-\nand-bust cycle of hype and disappointment or, worse, an\nexplaining-away of failed replications if too much trust is\nplaced in the original finding.\"\nAnd, in the meantime, speculations are presented as fact.\nFor example, the China air pollution study was featured in\na New York Times article (Wong, 2013) that referred\nunquestioningly to \"the 5.5-year drop in life expectancy in\nthe north,\" as well as in a New Yorker article by a Pulitzer\nprizewinning reporter (Johnson, 2013) who simply wrote\nthat a study \"noted that pollution from coal reduces average\nlife expectancy in northern China by five and a half years\"\nwith no indication that the \"five and a half years\" number\nwas just a point estimate, even setting aside questions about\nthe validity of that estimate.\nWe need a way of handling such claims \u00ad those that are\nprovocative and substantively important while falling short\nof conventional levels of statistical significance \u00ad that falls\nbetween acceptance and dismissal. We also are glad that\nChen et al. produced Figure 1, which made the problems\nwith their study so clear. We would not want criticisms such\nas ours to serve as a disincentive for authors to display the\nfit of their models to data. Better for problems to be out in\nthe open than swept under the rug. The authors were quite\ncorrectly transparent about their model choices and the\nimplications of these choices, and they created a plot that\nmade the data and model easy for the reader to digest.\nRegardless of this good-faith effort, there remains an inher-\nent problem with incentives in publication and publicity of\nresearch: the desire to achieve statistically significant\nresults can lead to the acceptance of modeling choices that\nare supported by neither theory nor data.\nWe have the impression that research journals have an\nimplicit rule that under normal circumstances they will\npublish this sort of quantitative empirical paper only if it\nhas statistically significant results. That is a discontinuity\nright there, and researchers in various fields (for example,\nButton et al., 2013) have found evidence that it introduces\nendogeneity in the forcing variable.\n"
}