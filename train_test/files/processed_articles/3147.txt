{
    "abstract": "Creative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).",
    "reduced_content": "Creative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nIn recent years, evidence of effectiveness from rigorous\nevaluations has begun to play an increasing role in educa-\ntional policy and practice. Evidence-based reform has\nadvanced in the policy arena from the ill-defined emphasis\non using practices \"based on scientifically based research\"\nenshrined in No Child Left Behind to much clearer defini-\ntion of what counts as strong, moderate, or promising evi-\ndence of effectiveness that appears in the Every Student\nSucceeds Act (ESSA), adopted in 2015. Legislation regard-\ning School Improvement Grants for struggling schools has\nestablished an eligible funding category in which low-per-\nforming schools may apply for funding to implement whole-\nschool reform programs supported by moderate to strong\nevidence of effectiveness, defined as at least one large, rigor-\nous experiment showing positive achievement impacts.\nThese policy developments are increasing the impor-\ntance of large-scale, especially third-party, evaluations of\neducational programs. Rigorous, replicated experiments\nare gradually becoming the gold standard for impact on\npolicy and practice.\nOne program central to the discussions of evidence-based\nreform is Success forAll (SFA), a whole-school reform model\nfocused on improving reading outcomes in high-poverty Title\nI elementary schools (Slavin, Madden, Chambers, & Haxby,\n2009). SFA provides reading materials, software, and exten-\nsive professional development to all teachers in Title I ele-\nmentary schools. The professional development focuses on\nbuilding teachers'skills in implementing cooperative learning\nand providing effective instruction in phonics, metacognitive\nstrategies in reading comprehension and writing, classroom\nmanagement, and other approaches. It also provides profes-\nsional development to tutors to work with struggling readers\nas well as family support and leadership approaches.\nSFAis composed of elements proven effective in research,\nand the program itself has been extensively evaluated.\nEvaluations include a randomized longitudinal study in 35\nschools by Borman et al. (2007) and another third-party ran-\nThese, and numerous first-party and third-party quasi-exper-\niments, have clearly established the effectiveness of SFA in\nincreasing reading achievement in the early grades (see\nSlavin, Lake, Chambers, Cheung, & Davis, 2009). However,\nthe largest study ever done to evaluate SFA has never been\nfully reported. This was a study carried out by Brian Rowan,\nRichard Correnti, and their colleagues at the University of\nInstructional Improvement (SII), compared SFA and two\nother whole-school reform models, America's Choice (AC)\nand Accelerated Schools Plus (ASP), to each other and to a\ncontrol group. The full study involved 115 high-poverty\nschools across the United States.\nRowan, Correnti, Miller, and Camburn (2009) and\nCorrenti (2009) reported the procedures and findings of the\nSII in terms of teachers'behaviors, derived from teacher logs\nthey never reported achievement outcomes in any detail, only\nEffects of Success for All on Reading Achievement: A Secondary\nAnalysis Using Data From the Study of Instructional Improvement (SII)\nAlan C. K. Cheung\nThe Chinese University of Hong Kong\nRobert E. Slavin\nJohns Hopkins University\nThis study examined the effects of the Success for All (SFA) whole-school reform approach on student reading achievement.\nThe data were collected for the Study of Instructional Improvement by the University of Michigan, which did not previously\nreport the achievement outcomes in detail but did make the data available online. Using propensity matching, we matched\n27 SFA with 27 comparable schools based on several key demographic variables. The evaluation used hierarchical linear\nmodeling with students nested within schools. Results showed that SFA students significantly outperformed their counterparts\nin the matched schools on reading achievement, with an effect size of +0.26 for students in a 3-year longitudinal comparison.\nEffect sizes were similar for 2-year cohorts (mean effect size= +0.31). Policy implications are discussed.\nKeywords: Success for All, Study of Instructional Improvement\nCheung and Slavin\nestimating that SFAstudents gained more than students in the\nother three programs on TerraNova reading tests, with an\neffect size of +0.43 at the end of 3 years, moving the average\nchild from the 30th to the 50th percentile. Even these find-\nings were presented only in a technical report and an\nAmerican Educational Research Association paper and were\nnever published. Insufficient detail was provided to enable\nthe achievement findings to be confirmed according to\ntoday's standards of rigor for experimental evaluations. The\nstudies examining teacher logs noted that teacher behavior in\nSFA and AC (but not ASP) changed in directions consistent\nwith these models' emphases (see below), but reading out-\ncome data have not been adequately reported.\nThe Present Paper\nThe analyses reported in the present paper involved a sec-\nondary analysis of data from the SII. They compare students\nin SFAschools to best-matched comparison schools through-\nout the sample, using propensity matching. The purpose is to\nuse the large and rich SII data set to investigate impacts of\nSFA on achievement outcomes.\nThe Importance of SFA for Evidence-Based Policy Today\nThe SII, though completed more than a decade ago, has\nparticular relevance to policy issues today. Anyone who fol-\nlows the What Works Clearinghouse or other summaries of\nresearch on educational interventions is aware that few pro-\ngrams are finding consistent positive impacts on achieve-\nment outcomes in rigorous, large-scale evaluations. Many of\nthe programs that have shown positive effects are one-to-one\nor one\u00adto\u00adsmall group tutoring models, not whole-school or\neven whole-class interventions with potential for broader\nimpact. If evidence of effectiveness is to become increas-\ningly important in federal, state, and local policy, it is essen-\ntial to have a broad array of proven programs meeting the\nhighest standards of rigor in their evaluations (see Slavin,\nThe SII happened to have evaluated two programs that\ndid make substantial differences in achievement and one that\ndid not. Further, it collected and analyzed detailed teacher\nlogs that made it possible to quantify what teachers were\ndoing differently in the different whole-school designs. It\nalso commissioned studies of the organizations that created\neach of the programs, making possible comparisons at that\nlevel (Cohen, Peurach, Glazer, Gates, & Goldin, 2014;\nThe only one of the programs evaluated by SII that is still\nin widespread use today is SFA. SFA has had positive read-\ning achievement outcomes in the great majority of its evalu-\nations, averaging an effect size of +0.31 (Slavin, Madden,\net al., 2009). For example, in Social Programs That Work\n(http://evidencebasedprograms.org), SFA in Grades K\u00ad2 is\none of just two whole-school or whole-class programs to\nmeet \"top-tier\" standards (the other is career academies for\nhigh schools). The What Works Clearinghouse (2009)\naccepted seven studies evaluating SFA, one \"without reser-\nvations\" and six \"with reservations.\" The weighted mean\neffect sizes across the seven studies were +0.25 for letter-\nprehension, and +0.27 for general reading.\nOne reason SFA may have had relatively consistent posi-\ntive effects for reading in the primary grades is that the pro-\ngram provides a clear structure for teachers and extensive\nprofessional development and on-site coaching for all school\nstaff, which lead to significant changes in daily instruction,\naligned with the program's theory of action (Peurach &\nGlazer, 2012). Summaries of teacher log data reported by the\nSII research make this point repeatedly for SFA with respect\nto early elementary reading (and with respect to writing for\nAC; Rowan & Miller, 2007). These are the areas in which\neach program produced significantly greater gain than the\nother programs and the control group. For example, Correnti\nand Rowan (2007) reported that SFA teachers taught reading\ncomprehension in 65% of lessons, in comparison to 50% in\ncomparison schools. SFA teachers reported spending signifi-\ncantly more time than controls on reading comprehension\nand word analysis, the very areas in which program impacts\nwere strongest. During reading comprehension instruction,\nSFA teachers were significantly more likely to have students\ndiscuss text with each other in cooperative groups, to focus\non literal comprehension, and to check students'comprehen-\nsion by eliciting brief answers from students. They were no\nless likely than control teachers to focus on advanced read-\ning strategies or to have students write extended text about\nwhat they read, but they provided much more direct instruc-\ntion on comprehension strategies. Further, teachers in SFA\nschools showed much less variability in instruction than did\nteachers in other schools.\nFocusing at the school level, Rowan and Miller (2007)\nreported that teachers in SFA schools (and in AC schools)\nfelt they had greater levels of instructional guidance than did\nteachers in ASP or control schools. They also reported that\ntheir improvement efforts were more closely monitored.\nTeachers in ASP reported higher autonomy, values-based\ndecision making, and strength of professional community.\nHowever, these same teachers'logs did not reveal any actual\nchange in daily teaching behaviors, in comparison to con-\ntrols. Based on their logs, teachers' behaviors in ASP were\nindistinguishable from those of teachers in control schools.\nAs a likely consequence, the ASP did not show any greater\ngains in student learning in comparison to controls.\nThe comparisons in emphases and outcomes among the\nthree whole-school reform models are crucially important for\nunderstanding the situation of evidence-based reform today.\nThe SII researchers concluded from their data that in order for\nwhole-school reform models to have significant impacts on\nstudent learning, especially in often-chaotic and stressed\nhigh-poverty schools, they must have a clear plan for reform\nand implement it with sufficient specificity, professional devel-\nopment, and classroom supports to ensure that teachers'behav-\niors change in directions likely to improve student outcomes.\nToday, SFA represents the main surviving example of a\nwhole-school reform with a strong emphasis on providing high-\npoverty schools with specific, well-structured strategies; sup-\nportive classroom materials and software; and extensive\nprofessional development and on-site coaching. The present\nstudy reaches back in time to explore data from the SII to better\nunderstand the impacts of SFA and implications for evidence-\nbased reform and educational policies of today. Our hope is that\nthe lessons of SFA derived both from the unreported achieve-\nment outcomes and from previously reported teacher logs and\nRowan & Miller, 2007) will help current and future designers\ncreate additional whole-school approaches as effective as SFA\nto serve the many disadvantaged children in need of higher-\nquality instruction and better learning outcomes.\nIn the Rowan et al. (2009) report was a URL for a website\ncontaining all of the data from the SII. We obtained these\ndata in order to carry out a summative evaluation of this\nmajor third-party evaluation in an attempt both to confirm\nthe reported findings and to add depth to them, examining\ndifferent program durations available from the data files.\nThe present analysis used a propensity-matching strategy\nin which SFA schools were matched based on multiple\ndemographic variables with schools not using SFA across\nthe entire SII sample, regardless of which non-SFA approach\nwas in use. Hierarchical linear modeling (HLM) was used to\ndeal with the clustered nature of the data.\nConfirmatory Hypothesis. The principal hypothesis for this\nsecondary analysis is that students in SFA schools over the\nentire 3-year period, from kindergarten to second grade,\nwould show greater gains on TerraNova Reading than stu-\ndents in propensity-matched schools who had also been in\ntheir schools for 3 years.\nExploratory Hypothesis.We hypothesized that students\nwho received fewer than 3 years of SFA would also show\ngreater gains on TerraNova Reading than would students in\nmatched schools but that the longer students were in SFA\nschools, the greater their gains relative to matched controls.\nTo test this hypothesis, we evaluated all possible 1-year and\n2-year cohorts, in comparison to their matched controls.\nMethod\nData\nAs noted earlier, the data used in the current study\nwere collected for the SII, conducted by the University\nof Michigan in collaboration with the Consortium for\nPolicy Research in Education. The SII was a large-scale\nquasiexperimental study that examined the effectiveness of\nthree widely disseminated comprehensive school reform\n(CSR) programs on instruction and student achievement in\nAs indicated in Table 1, the SII sample consisted of 115 ele-\naddition to following schools that adopted these three CSR\nprograms, the study also followed 26 control schools\n(Correnti, 2009). The data were made publicly available and\nwere downloaded through the Inter-University Consortium\nfor Policy and Social Research website (http://www.icpsr.\numich.edu/).\nDemographic Characteristics of Participating Schools\nBackground information on participating schools in the\nSII is summarized in Table 2. It is clear that the majority of\nthe participating schools served very disadvantaged minor-\nity students from high-poverty communities. Forty percent\nof students were from single-parent homes, and 70% of them\nqualified for free lunch. Approximately 70% of students\nwere ethnic minorities, mostly African Americans. In terms\nof academic achievement, only 30% of students met state\nproficiency standards in reading and mathematics.\nThere were some key differences among the three sets of\nCSR program schools and the comparison schools in terms\nof school characteristics. For instance, SFA and AC schools\nserved more disadvantaged students (75%) and had a higher\npercentage of minority students (75%) than ASP (63%) and\nthe comparison schools (65%). Students in SFA and ASP\nschools had lower reading scores at pretest (in kindergarten)\nthan those in AC and comparison schools on the Woodcock-\nJohnson assessments. AC (30%), SFA (30%), and ASP\n(31%) schools had lower percentages of students meeting\nstate proficiency standards in reading at pretest than com-\nparison schools (36%). As indicated in Table 3, before\nmatching, the SFA schools and the other schools showed\nconsiderable imbalance on various covariates.\nPropensity Score Matching Method\nGiven the fact that participating schools in the SII were not\nrandomly assigned and key differences existed between the\nParticipating Schools in the Study of Instructional Improvement\nIntervention program Total\nAccelerated Schools Plus 28\nSFA schools and other schools in the sample, we decided to\nuse propensity matching to locate better-matched schools\nfrom the other two CSR programs and the comparison group.\nThe problem with nonrandomized designs is that the treat-\nment group and the comparison group may systematically dif-\nfer from each other based on school characteristics or\nPropensity score matching was employed to control for demo-\ngraphic or pretest differences by excluding participating\nschools that could not be well matched, so that systematic\nerror could be reduced (Rosenbaum & Rubin, 1983).As Lane,\nTo, Henson, and Shelley (2012) argued, nonrandom sampling\nmay introduce bias when comparing treatment effects between\ngroups given an unequal and unknown probability of group\nassignment. Propensity score matching is an approach to\ntackle this problem by using regression techniques to generate\npredicted scores for each school regarding the likelihood of a\nschool to be assigned to the treatment group given theoreti-\ncally relevant covariates.Amatching method is applied subse-\nquently to the schools in treatment and comparison groups by\nthose predicted scores (i.e., propensity score) so that schools\nof both groups would have an equal likelihood of receiving\nWe calculated the propensity score as follows:\n\ni i i\n= =\n( )\n| ,\nwhere i\nis the propensity score for school i, which is the\nconditional probability (P) of assigning a school to treatment\ngroup (T= 1) give a set of covariates (X) of school i.\nThe four major steps for performing propensity score\nanalysis in this study were as follows:\n1. A list of eight relevant covariates was selected.\n2. The probabilities or propensity scores were calcu-\nlated for each school by using logistic regression\n3. A one-to-one nearest-neighbor matching method with\na caliper .25 standard deviations of the propensity\nscore (Stuart, 2010) was adopted in this study. The\naim was to pair each SFA school with a non-SFA\nschool in the sample with the nearest propensity score.\n4. An examination of the balance of covariates was\nconducted for the newly matched sample.\nWith the balance introduced by propensity score match-\ning, we expected that there would be no systematic differ-\nences between the SFA schools and matched schools, and\nthe treatment effect could be tested for the matched sample.\nSPSS custom dialog psmatching3.03, provided by\nThoemmes (2012), with the R plug-in Matchit package\n(Ho, Imai, King, & Stuart, 2007) was used to perform pro-\npensity score matching. The program generated 27 SFA\nschools matched with 27 non-SFA schools, which included\nschools using various CSR programs in the SII sample (see\nTable 4). According to Rubin (2001), the absolute stan-\ndardized difference in the mean of propensity score\nbetween two groups should be less than .20, and ratio of\npropensity score variances of both groups should be close\nto 1 for the matched sample. In our matched sample, the\nabsolute standardized difference in the mean propensity\nscore between two groups was reduced from a prematching\nof d= .89 to a postmatching of d= .05 (Table 3). The differ-\nence in propensity scores between the two postmatched\ngroups were not statistically significant, t(52)= \u00ad.20, p=\nto 0.99 in the postmatched sample. Thus, distribution of\npropensity scores of both groups was similar.\nA multivariate test generated from the SPSS R plug-in\nprogram was used to evaluate covariate balance. A balance\ntest developed by Hansen and Bowers (2008) that was anal-\nogous to Hotelling's T2 statistic tested overall covariate bal-\nance. In this study, the nonsignificant test result suggested a\nmean differences for each covariate between the two groups\nwere examined, and no covariate exhibited a large imbal-\nance (i.e., |d|\n> .25). Given the results of multivariate and\nunivariate tests, we assumed that covariate balance was\nDemographic Characteristics of Original Sample of SII Schools by CSR Program (in percentages)\nProportion individuals without a high school diploma in the community 35 34 32 34\nPercentage of students meeting state proficiency standards in reading 30 30 31 36\nNote. SII = Study of Instructional Improvement; CSR = comprehensive school reform; SFA = Success for All; AC = American's Choice;\nASP = Accelerated Schools Plus.\nestablished in the matched-schools sample. Thus, tests of the\ntreatment effect could proceed with the matched sample.\nFinal Sample\nThe final sample consisted of 54 matched schools: 27\nSFAschools and 27 matched control schools (16AC schools,\nfive comparison schools, and six ASP schools).1 Key demo-\ngraphic variables, such as a community disadvantage index,\nproportion of households with assistance income, proportion\nof households in poverty, free lunch, and so on, were used as\ncovariates in the propensity-matching method.\nThere were two phases in the data collection of SII. The\nfirst cohort of students began the study during the 2000\u00ad\n2001 academic year; the rest began the study during the\nkindergarten to second grade. There were also replacement\nstudents entering the study in the beginning of each aca-\ndemic year. For the purpose of our analysis, we treated the\nbeginning year of the longitudinal study as Year 1 regardless\nof the phase of study, the following year as Year 2, and so on.\nFor the longitudinal sample, that is, those who entered the\nprogram at Time Point 1, 842 kindergarten students (SFA=\n(SFA= 246) of the same cohort took tests in spring Year 2,\nCovariates Used in Propensity Score Matching\nBefore matching After matching\n\nOther schools\nSFA\n(n= 30) Standardized\nmean\ndifference\nMatched\nSFA\n(n= 27) Standardized\nmean\ndifference\nVariable M SD M SD M SD M SD\nCommunity disadvantage index\u00ad\nschool tracts\nProportion households with\nassistance income\nProportion individuals without high\nschool diploma\nResult of Propensity Score Matching\nIntervention program\nNo. of schools\nTotal\nNonmatched Matched\nFigure 1. Description of the sample. Students retained in\ngrade were included in the analysis.\nCheung and Slavin\n= 181) took tests in spring Year 3. Of those\nwho entered the study at Time Point 2 (mostly first graders),\n= 149) took the tests in the spring of same aca-\nof the following year. There were 191 students, mostly sec-\nond graders, who entered the program at Time Point 3 and\ntook the tests in the spring of the same academic year.\nMeasures\nThe primary outcome of interest was reading achieve-\nment. Participants were assessed on reading outcome mea-\nsures in the fall as pretests and in each spring thereafter as\nposttests. Two measures were used for the current study:\nThe Woodcock-Johnson III (WJIII) Tests of Achievement\nby Riverside Publishing and the TerraNova Tests and\nAssessments by McGraw-Hill. The Letter/Word\nIdentification scale of the WJIII subtest was used as a pre-\ntest for those students who started in kindergarten in the fall\nof Year 1. The internal reliability coefficient for the Letter/\nWord Identification subtest used was 0.92 (Woodcock,\nMcGrew, & Mather, 2001). The Reading/Language Arts\nsubtest of the TerraNova Assessment was used for all other\ngrades as pretests and posttests. The internal reliability\ncoefficient for the TerraNova Reading and Language Arts\ninterpretation, all scores were standardized.\nAnalyses\nDue to the nested nature of the data, we employed HLM\nwith the school as the unit of analysis (Raudenbush & Bryk,\n2002) for all comparisons. The pretests were used as the\ncovariate. The HLM approach was the optimal design for the\ncurrent study because it addressed the practical problems of\naccounting for the effects of attending a given school, using\ndegrees of freedom associated with the number of schools\nrather than students. HLM allows us to simultaneously\nmodel both student- and school-level sources of variability\nin the outcome (Raudenbush & Bryk, 2002). Specifically, a\ntwo-level hierarchical model that nested students within\nschools was developed to analyze the data collected. The\nfully specified Level 1, or within-school, model nested stu-\ndents within schools. The linear model for this level of the\nanalysis is written as\nY r\nij j j ij\n= + ( )+\n \nGrade .\nThis represents the post-test achievement for student i\nin school j regressed on the Level 1 residual variance, rij\n.\nWe also included a grade indicator as a predictor in the\nLevel 1 model. We treat the within-school grade-level\ngap--the difference between the posttest scores of differ-\nent grades in school j--as fixed at Level 2 because it is\nintended only as a covariate.\nAt Level 2 of the model, we estimate SFAtreatment effects\non the mean posttest achievement outcome in school j. We\nincluded a school-level covariate, the school mean pretest\nscore, to help reduce the unexplained variance in the outcome\nand to improve the power and precision of our treatment effect\nestimates. The fully specified Level 2 model is written as\n   \nj j j j\nu\n= + ( ) + ( ) +\nMean Pretest SFA ,\n \nj\n=\n,\nwhere the mean posttest intercept for school j, 0j\n, is\nregressed on the school-level mean pretest score, the SFA\ntreatment indicator, plus a residual, u0j\n. The within-school\nposttest difference between grades, 1j,\nis specified as fixed,\npredicted only by an intercept.\nIn the previous description of the sample, we concluded\nthat the analysis of the baseline data showed few important\ndifferences between the SFAand the matched control schools.\nResults\nPretest Differences\nAs indicated in Table 5, after matching, the SFA schools\nscored nonsignificantly higher than matched comparison\nschools at pretest for the 3-year longitudinal sample and\nrespectively. However, there was a significant difference at\npretest for the 2-year longitudinal sample, with an effect\nOutcomes for the 3-Year Longitudinal Sample\nThe confirmatory multilevel models, shown in Table 6,\nassessed student- and school-level effects on their posttest\nscores. In Year 1, the treatment students scored nonsignifi-\ncantly higher than the controls on the posttest, with an effect\nment condition significantly outperformed the controls, with\nrespectively.\nOutcomes for 2-Year Longitudinal Samples\nAs exploratory analyses, we also examined the effects of\nexperimental schools that had experienced 1 year and 2\nyears of treatment on posttest achievements. As indicated in\n1-Year Outcomes\nThe sample in the 1-year outcome included a total of\n1,325 kindergarteners and first- and second-grade students\nwho had been in the study for only 1 year. The 1-year impacts\nwere summarized in Table 8. At posttest, the SFA schools\nscored marginally higher than the controls, with an effect\nDiscussion\nThe purpose of this secondary analysis of data from the SII\nwas to provide well-supported achievement data for the study\nonly partially reported by Correnti (2009) and Rowan et al.\n(2009). Effect sizes after the confirmatory 3-year longitudinal\ncomparison in the present analysis were statistically signifi-\ncant in a rigorous HLM analysis. Correnti (2009) estimated an\neffect size of +0.43 for the cohort that received 3 years of\nSFA, whereas the current analysis found an effect size of\n+0.26, still an educationally meaningful effect.\nExploratory 2-year impacts of SFA were similar to 3-year\nimpacts, with an effect size of +0.36 for students in the 2nd\nyear of the 3-year longitudinal sample, +0.16 (p\nsample. The mean effect size for students who received 2\nyears of treatment was +0.31. For students who received 1\nyear of treatment, effect sizes averaged +0.16 (p< .08) in the\nHLM analysis. The substantial increase in effect sizes from\n1 year to 2 or 3 years of SFA experience matches findings\nthe 3-year Borman et al. (2007) study found that effect sizes\ncontinued to increase in the 3rd year of SFA.\nComparison of Standardized Pretest Reading Scores of Success for All (SFA) Schools and Matched Comparison Schools at the Student\nLevel\nGrade level\nOther schools SFA\nStandardized\nmean difference\n\nM SD n M SD n t value Sig.\nKindergarten (Woodcock-\nJohnson language arts)\nKindergarten, first grade,\nand second grade\n3-Year Longitudinal Results\nYear 1 (kindergarten) Year 2 (first grade) Year 3 (second grade)\nType of measure Effect SE t Effect SE t Effect SE t\nSchool mean achievement \nYear 1 (kindergarten) Year 2 (first grade) Year 3 (second grade)\nRandom effect Effect 2 df Effect 2 df Effect 2 df\nVariance explained (%) \nNote. SFA = Success for All.\nYear 1 and Year 2 Results, 2 Years of Treatment\nYear 1 (kindergarten and first grade) Year 2 (first and second grades)\nType of measure Effect SE t Effect SE t\nSchool mean achievement \nGrade \nYear 1 (kindergarten and first grade) Year 2 (first and second grades)\nRandom effect Estimate 2 df Estimate 2 df\nVariance explained (%) \nNote. SFA = Success for All.\n1-Year Results\nYear 1 (kindergarten to\nsecond grade)\nType of measure Effect SE t\nSchool mean achievement \nGrade \nYear 1 (kindergarten to\nsecond grade)\nRandom effect Estimate 2 df\n Within-school variation 0.55 \nVariance explained (%) \n Within-school variation 39.9 \n School mean achievement 28.4 \nNote. SFA = Success for All.\nThe observation that it takes 2 years or more to make\ncomprehensive school reforms show their full effects has\nalso been made by Fullan (2001); Borman, Hewes, Overman,\nThe findings provide further confirmation of the effec-\ntiveness of SFA in improving reading, though (as in previous\nstudies) the program had to be provided for at least 2 years\nto show its full effect (Borman et al., 2007). Because the data\ncome from a very large national study carried out by third-\nparty researchers, the SII adds confidence that SFA can be\neffective at substantial scale, an increasingly important issue\nin a policy climate of increasing focus on evidence of effec-\ntiveness in education.\nPolicy Implications\nThe findings of the SII, both as originally reported and as\nlargely confirmed in the secondary analysis, have broad\nimplications for educational policy. From reports on large-\nscale, mostly randomized experiments evaluating educa-\ntional interventions, it is becoming apparent that most\ninnovations do not consistently improve students' achieve-\nment outcomes. Most of the exceptions involve one-to-one\nor one\u00adto\u00adsmall group teaching. For whole schools and\nmajor subjects, there are few clear examples of programs\nthat are shown to be effective in comparison to control\ngroups on measures that fairly assess what was taught in\nexperimental and control groups.\nEffects of SFA on Reading\nSFA is one of few examples of whole-school reforms that\nhave had such positive impacts in two large, cluster random-\nand in the SII reported here as well as in many smaller\nexperiments (Slavin, Lake, et al., 2009).\nAt this point in time, it is important to ask why this par-\nticular program has been so consistent in its impacts. One\npotential answer is provided by the original SII, which\nobtained detailed teacher logs to characterize program\nimplementation. The teacher log data reported by Correnti\nclear impact of SFA on teachers' reported behaviors, which\nwere in line with the SFA theory of action and emphasis.\nSimilarly, the logs reported in the evaluation of AC also doc-\numented teaching in line with the program's theory of action\nand emphasis. In both cases, outcomes mirrored the pro-\ngrams' emphasis, with the main outcomes of SFA seen in\nearly elementary reading whereas those of AC were seen in\nupper elementary writing. ASP, whose teachers did not\nreport much change in behaviors, also did not find any\neffects on achievement. These findings suggest the possibil-\nity that many attractive-sounding interventions, such as ASP,\nmay be failing to show positive effects on achievement mea-\nsures because they are not achieving major changes in teach-\ners' behaviors.\nSFA places a substantial emphasis on extensive and\nexplicit coaching to help teachers change their daily teach-\ning behaviors (Slavin, Madden, et al., 2009). It provides all\n1st-year schools at least 26 person-days of on-site coaching,\nan in-school facilitator to work with all staff, a week of train-\ning for principals and facilitators, an annual conference, and\nconstant electronic communications and sharing of data,\nideas, and feedback. To bring about profound changes in\nteachers' daily behaviors, it might be argued that nothing\nless is likely to be effective.\nThe Success for All Foundation (SFAF), which devel-\noped and supports the program, has an institutional culture\nfocused on leaving as little as possible to chance. In fact,\nas part of the SII project, Peurach (2011) studied SFAF\nover more than a decade and documented this cultural\nfocus as well as ongoing attempts to learn from its own\nnetwork of schools to incorporate best practices in its\ntraining, coaching, and materials (see also Cohen et al.,\nIt is entirely possible that many whole-school reform\napproaches starting from very different theoretical or philo-\nsophical bases could be effective in improving student\nachievement. However, the experience of SFA, especially as\nrevealed in the SII, suggests that educational interventions\nare most likely to achieve their desired outcomes if they\nmake certain to provide the professional development and\nschoolwide supports necessary to bring about meaningful\nchanges in instructional practices throughout the school. If\nevidence-based reform is to transform America's schools,\nwe need many whole-school approaches with strong evi-\ndence of effectiveness from rigorous evaluations. The Study\nof Instructional Improvement and the broader experience of\nSFA suggests that whatever these approaches may be, ensur-\ning quality of implementation is essential.\nIf federal education policies are to make substantially\ngreater impacts on student outcomes, they must sooner or\nlater embrace policies promoting the use of federal resources\nto implement proven, replicable programs (Cohen & Moffitt,\n2009). However, this shift is unlikely to take place if there\nare too few proven programs for schools to choose. The les-\nsons of the SII, emphasizing the need to ensure that pro-\ngrams do whatever it takes to see that teachers embrace and\nregularly utilize innovative strategies, support the idea that\nsystematic, school-by-school reform can amount to impor-\ntant changes in outcomes.\nNote\n1. On average, the three unmatched Success for All (SFA)\nschools tended to serve more disadvantaged student populations\nthan their 27 matched SFA counterparts. For example, the propor-\ntion of students receiving free and reduced lunch was greater in\nthree unmatched schools (86% vs. 72%). In addition, these three\nschools had a higher percentage of minorities (94% vs. 81%) and\nhad lower Woodcock-Johnson pretest scores (\u00ad0.21 vs. \u00ad0.16).\nReferences\nBorman, G. D., Slavin, R. E., Cheung, A., Chamberlain, A, Madden,\nN. A., & Chambers, B. (2007). Final reading outcomes of the\nnational randomized field trial of Success for All. American\nBorman, G. D., Hewes, G. M., Overman, L. T., & Brown, S.\n(2003). Comprehensive school reform and achievement: A\nCTB/McGraw-Hill. (2001). TerraNova technical report. Monterey,\nCA: Author.\nCohen, D. K., & Moffitt, S. L. (2009). The ordeal of equality: Did\nfederal regulation fix the schools? Cambridge, MA: Harvard\nUniversity Press.\nCohen, D. K., Peurach, D. J., Glazer, J. L., Gates, K. G., & Goldin,\nS. (2014). Improvement by design: The promise of better\nschools. Chicago, IL: University of Chicago Press.\nCorrenti, R. (2009, March). Examining CSR program effects on\nstudent achievement: Causal explanation through examina-\ntion of implementation rates and student mobility. Paper pre-\nsented at the annual conference of the Society for Research on\nEducational Effectiveness, Crystal City, VA.\nCorrenti, R., & Rowan, B. (2007). Opening up the black box:\nLiteracy instruction in schools participating in three comprehen-\nsive school reform programs. American Educational Research\nFan, X., & Nowell, D. L. (2011). Using propensity score match-\ning in educational research. Gifted Child Quarterly, 55(1),\nFullan, M. (2001). The new meaning of educational change (3rd\ned.). New York, NY: Teachers College Press.\nCheung and Slavin\nGuo, S., & Fraser, M. W. (2015). Propensity score analysis:\nStatistical methods and applications (2nd ed.). Thousand Oaks,\nCA: Sage.\nHansen, B., & Bowers, J. (2008). Covariate balance in simple,\nstratified and clustered comparative studies. Statistical Science,\nHo, D., Imai, K., King, G., & Stuart, E. (2007). Matching as non-\nparametric preprocessing for reducing model dependence in\nLane, F. C., To, Y. M., Henson, R. K., & Shelley, K. (2012). An\nillustrative example of propensity score matching within educa-\ntion research. Career and Technical Education Research, 37(3),\nPeurach, D. J. (2011). Seeing complexity in public education:\nProblems, possibilities, and Success for All. New York, NY:\nOxford University Press.\nPeurach, D. J., & Glazer, J. L. (2012). Reconsidering replication:\nNew perspectives on large-scale school improvement. Journal\nQuint, J., Balu, R., DeLaurentis, M., Rappaport, S., Smith, T., &\nZhu, P. (2015). The Success for All model of school reform:\nFindings from the Investing in Innovation (i3) scale-up. New\nYork, NY: MDRC.\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear\nmodels: Applications and data analysis methods (2nd ed.).\nThousand Oaks: Sage.\nRosenbaum, P. R., & Rubin, D. B. (1983). The central role of the\npropensity score in observational studies for causal effects.\nRowan, B., Correnti, R., Miller, R.J., & Camburn, E.M. (2009).\nSchool improvement by design: Lessons from a study of compre-\nhensive school reform programs. Ann Arbor, MI: Consortium\nfor Policy Research in Education.\nRowan, B., & Miller, R. J. (2007). Organizational strategies for\npromoting instructional change: Implementation dynamics in\nschools working with comprehensive school reform providers.\nRubin, D. B. (2001). Using propensity scores to help design obser-\nvational studies: Application to the tobacco litigation. Health\nSlavin, R. E. (2013). Overcoming the four barriers to evidence-\nSlavin, R. E., Lake, C., Chambers, B., Cheung, A., & Davis, S.\n(2009). Effective reading programs for the elementary grades:\nA best-evidence synthesis. Review of Educational Research,\nSlavin, R. E., Madden, N. A., Chambers, B., & Haxby, B. (Eds.).\n(2009). Two million children: Success for All. Thousand Oaks,\nCA: Corwin.\nStuart, E. A. (2010). Matching methods for causal inference:\nA review and a look forward. Statistical Science, 25, 1\u00ad21.\nThoemmes, F., (2012). Propensity score matching in SPSS. Available\nWhat Works Clearinghouse. (2009). Success for All intervention\nreport. Retrieved from http://ies.ed.gov/ncee/wwc/pdf/inter-\nWoodcock, R. W., McGrew, K. S., & Mather, N. (2001). Woodcock\nJohnson III Tests of Achievement. Rolling Meadows, IL:\nRiverside.\nAuthors\nALAN C. K. CHEUNG, PhD, is currently a professor in the\nDepartment of Educational Administration and Policy and direc-\ntor of the Centre for University and School Partnership (CUSP)\nat The Chinese University of Hong Kong. His research areas\ninclude educational technology, school reform, and research\nreviews.\nROBERT E. SLAVIN, PhD, is the director of the Center for\nResearch and Reform in Education at Johns Hopkins University,\nand chairman of the board of the Success for All Foundation. His\nresearch interests include evidence-based reform in education,\nresearch reviews, and cooperative learning."
}