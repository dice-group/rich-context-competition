{
    "abstract": "Abstract\nThe online labor market Amazon Mechanical Turk (MTurk) is an increasingly popular source of respondents for social\nscience research. A growing body of research has examined the demographic composition of MTurk workers as compared\nwith that of other populations. While these comparisons have revealed the ways in which MTurk workers are and are\nnot representative of the general population, variations among samples drawn from MTurk have received less attention.\nThis article focuses on whether MTurk sample composition varies as a function of time. Specifically, we examine whether\ndemographic characteristics vary by (a) time of day, (b) day of week, and serial position (i.e., earlier or later in data collection),\nboth (c) across the entire data collection and (d) within specific batches. We find that day of week differences are minimal,\nbut that time of day and serial position are associated with small but important variations in demographic composition. This\ndemonstrates that MTurk samples cannot be presumed identical across different studies, potentially affecting reliability,\nvalidity, and efforts to reproduce findings.\n",
    "reduced_content": "journals.sagepub.com/home/sgo\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License\n(http://www.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nBackground\nAmazon Mechanical Turk (MTurk) is an online labor market\nin which people (\"requesters\") requiring the completion of\nsmall tasks (\"Human Intelligence Tasks\" [HITs]) are matched\nwith people willing to do them (\"workers\"). MTurk has\nbecome a popular data collection tool among social science\njournals (with impact factors greater than 2.5, according to\nThomson-Reuters InCites) published more than 500 articles\nthat relied on MTurk data in full or in part (Chandler &\nReflecting the popularity of MTurk, considerable effort\nhas been invested in evaluating data collected from it, with\nparticular emphasis on documenting the demographic and\npsychological characteristics of its population, the quality of\nrespondent data, and the methodological limitations of the\nplatform. As a result, MTurk workers have become one of\nthe most thoroughly studied convenience samples currently\navailable to researchers (for a review, see Chandler &\nShapiro, 2016), and researchers have learned a great deal\nabout the ways in which MTurk respondents are and are not\nsimilar to the general population. There are reasons to sus-\npect, however, that there are also important variations\nbetween different samples drawn from MTurk, and these\nvariations have received far less attention. This article\naddresses this question, using data from a study of approxi-\nmately 10,000 MTurk workers to examine whether sample\ncomposition varies as a function of the time that it is\ncollected.\nWe begin by reviewing what extant research reveals about\nthe demographic composition of the MTurk worker pool.\nThen, we describe the methods and measures that we use in\nour study, after which we present the results of our analyses,\n1Harvard T.H. Chan School of Public Health, Boston, MA, USA\n2University of Michigan, Ann Arbor, USA\n3Mathematica Policy Research, Ann Arbor, MI, USA\n4Cornell University, Ithaca, NY, USA\n5Princeton University, NJ, USA\nCorresponding Author:\nJesse Chandler, Research Center for Group Dynamics, Institute for Social\nResearch, University of Michigan, 426 Thompson St., Ann Arbor, MI\nEmail: jjchandl@umich.edu\nIntertemporal Differences Among MTurk\nWorkers: Time-Based Sample Variations\nand Implications for Online Data\nCollection\nLogan S. Casey1, Jesse Chandler2,3, Adam Seth Levine4,\nAndrew Proctor5, and Dara Z. Strolovitch5\n Keywords\npolitical methodology, political science, social sciences, politics and social sciences, research methods, data collection,\nresearch methodology and design, reliability and validity, political behavior/psychology, psychology\n2 SAGE Open\nwhich include a demographic description of the largest sam-\nple of MTurk workers we are aware of and an exploration of\nwhether the demographic characteristics of MTurk respon-\ndent samples vary across day and time and earlier versus\nlater in the data collection. We conclude with a discussion\nabout the implications of the temporal variations we uncover\nfor researchers using MTurk (and online data collection more\ngenerally).\nHow Representative of the General Population\nAre Samples of MTurk Workers?\nThe demographic characteristics of samples drawn from\nMTurk populations have been extensively studied. These\nstudies show that most MTurk workers live in the United\nStates and India (Paolacci, Chandler, & Ipeirotis, 2010), that\nU.S. MTurk workers are more diverse than many other con-\nvenience samples, and that they are not representative of the\npopulation as a whole (Paolacci & Chandler, 2014). However,\nwhile scholars caution that MTurk samples are typically less\nrepresentative than commercial web panels that make\nexplicit efforts to provide representative samples (Berinsky,\nHuber, & Lenz, 2012; Mullinix, Leeper, Druckman, &\nalso agree that MTurk samples are more diverse than student\nsamples or community samples recruited from college towns\nDifferences between the U.S. MTurk population and the\nU.S. general population parallel differences between sam-\nples recruited through other online methods and the U.S.\npopulation (Casler, Bickel, & Hackett, 2013; Hillygus,\nsignificantly, MTurk workers are typically younger than the\ngeneral population (Berinsky et al., 2012; Paolacci et al.,\n2010), have more years of formal education, and are more\nworkers are less likely to be married (Berinsky et al., 2012;\nShapiro, Chandler, & Mueller, 2013), and more likely to\nidentify as lesbian, gay, or bisexual (LGB; Corrigan, Bink,\nFokuo, & Schmidt, 2015; Reidy, Berke, Gentile, and\ntend to report lower personal incomes and are more likely to\nbe unemployed or underemployed than members of general\nWhites and Asian Americans are overrepresented within\nMTurk samples, while Latinos and African Americans are\nunderrepresented (Berinsky et al., 2012).\nAre Samples of MTurk Workers Representative\nof MTurk Workers?\nWhile the forgoing research makes clear that the U.S. MTurk\npopulation is not representative of the U.S. population as a\nwhole, there are also reasons to suspect that samples recruited\nfrom MTurk are themselves not representative of the MTurk\npopulation as a whole. Different studies occasionally observe\nsubstantially different demographic characteristics. For\nexample, the proportion of female respondents differed by\nabout 10% across two studies that each recruited several\nthousand participants (Chandler & Shapiro, 2016).\nThere are many potential causes for sampling variation\nacross studies. Anecdotal evidence suggests that MTurk\nsample composition might be influenced by the fact that\nworkers share information about available studies and that\nreputation effects might lead workers to gravitate toward\n(and to avoid) particular requesters (Chandler, Mueller, &\nPaolacci, 2014). Some of this variation is also surely the\nresult of MTurk workers self-selecting into the studies that\ninterest them (for a discussion, see Couper, 2000). Design\nchoices that are exogenous to a study design may also inad-\nvertently influence sample composition. The effects of such\nexogenous choices are of particular interest to researchers\nbecause they are both within their control and typically irrel-\nevant to the substance of the studies themselves.\nThe present study focuses on the impact of intertemporal\nvariation on sample composition across (a) time of day, (b)\nday of week and serial position (i.e., earlier or later in data\ncollection), both (c) across the entire data collection and (d)\nwithin specific batches. Extant evidence about sample differ-\nences across time and day are suggestive but limited by small\nsample sizes. Comparing samples of about 100 participants\nobtained within two different studies, Komarov, Reinecke,\nand Gajos (2013) observed that compared with workers\nrecruited later in the evening, workers recruited during the\ndaytime were older, more likely to be female, and less likely\nto use a computer mouse to complete the survey (suggesting\nthat they were using mobile devices). Lakkaraju (2015) com-\npared the gender, income, education and age of 700 workers\nacross different times and days, finding that only gender var-\nied as a function of the day a given HIT was posted.\nVariation among participants who complete a research\nstudy early or later in the data collection process (referred to\nhere as serial position effects) has been observed in other\nmodes of data collection, but has not been examined on\nMTurk. Changes in sample composition between \"early\" and\n\"late\" responders have been observed in mail and email sur-\nveys, in part because the easiest to contact participants tend\nto complete surveys earlier (for a review, see Sigman, Lewis,\nYount, & Lee, 2014). In general, people of color1 are under-\nrepresented among early respondents, as are men (Gannon,\nKoepsell, & Daling, 2003), younger people, and people with\nfewer years of formal education (Voigt et al., 2003; for a dis-\nExaminations of lab studies of college students have also\nshown that sample compositions can vary over time. For\nexample, women (Ebersole et al., 2016) and students with\nhigh GPAs (Aviv, Zelenski, Rallo, & Larsen, 2002; Cooper,\nBaumgardner, & Strathman, 1991) are more likely than men\nand students with lower GPAs to participate in lab studies at\nCasey et al. 3\nthe beginning of the semester. Personality variables also\ninfluence when students complete lab studies, with partici-\npants who report that they are less extraverted, less open to\nexperience, and more conscientious more likely to respond at\nthe beginning of the semester.\nInvestigating whether samples vary over the course of a\nsurvey fielding period is critical, because researchers tend to\nrecruit small samples for their research (Fraley & Vazire,\n2014). In fact, most of the existing studies of the characteris-\ntics of MTurk workers rely on relatively small samples (N <\n500) that capture only a small proportion of the approxi-\nIf researchers use only small samples, the samples they\nrecruit may differ systematically from the worker pool as a\nwhole. In addition, if researchers recruit unique workers to\nparticipate in a series of related experiments (as they should;\nsee Chandler et al., 2014; Chandler, Paolacci, Peer, Mueller,\n& Ratliff, 2015), sample composition may vary systemati-\ncally across the experiments, compromising both the reliabil-\nity and validity of their studies, and possibly complicating\nefforts to reproduce findings.\nA second potential serial position effect on MTurk is dif-\nferences between people who complete HITs shortly after\nthey are posted or later on. This factor is independent from\nearly versus late responding to the study because study data\ncan be collected through any number of batch postings. In\npractice, researchers often collect data from MTurk by post-\ning more than one batch of HITs, either to speed up data col-\nlection (data collection is faster immediately after an HIT is\nposted; Peer, Brandimarte, Samat, & Acquisti, 2017) or to\ncircumvent the fee Amazon charges for a batch that recruits\nmore than nine participants. When more (but smaller) batches\nare posted, the average batch will, by default, be closer to the\nfront of the queue, which could affect sample composition\nfor at least three reasons. First, a batch closer to the front of\nthe queue reduces the amount of work it takes to find it, espe-\ncially for workers who rely on the default sort order. Second,\nsmaller batches might limit the number of workers who dis-\ncover the survey through links on worker forums, because\nthe link will be valid for a shorter period of time. Third, some\nworkers use automated scripts or other tools to be alerted\nabout the availability of new work. In this study, we post\nmultiple batches that allow us to disentangle serial position\neffects within batches of posted HITs from serial position\neffects across the data collection as a whole.\nMethod\nTo explore whether MTurk worker demographics vary inter-\ntemporally, we crafted a brief HIT (average completion time\nwas approximately 5 min) that contained demographic ques-\ntions that are of interest to scholars across an array of\ndisciplines.\ntotal of 56 days (or 8 weeks). We began by posting the HIT\ntwice daily, at 3 p.m. and 10 p.m. Eastern Time (ET). After\nthe first week, we added a third posting at 10 a.m. ET.2\nOnly U.S.-based workers with a HIT acceptance ratio\n(HAR) greater than 95% and who had completed at least 100\nHITs were eligible to participate. We selected workers with a\n95% HAR because this subsample of workers has been\nshown to result in higher quality data (Peer, Vosgerau, &\nAcquisti, 2014) and, in our experience, to be favored by\nresearchers. We prevented workers from completing this sur-\nvey more than once across the entire fielding period.\nFor the first 3 weeks, workers were paid US$0.25 to com-\nplete the survey. After learning that the average time to com-\npletion was roughly 5 min, we increased the pay rate to\nUS$0.50 for the remainder of the fielding period to comply\nwith recommended pay norms of US$0.10 per minute (see\n\"Guidelines for Academic Requesters,\" 2014). By the end of\nthe study, we had posted the HIT 162 times and sampled\nMeasures\nAt the beginning of the study, we collected measures of age\nand the U.S. state in which respondents lived. Participants\nwere then asked to report demographic information includ-\ning their highest level of education, current employment sta-\ntus, and current occupation. We also asked a series of\nquestions about their current relationship status, sexual ori-\nentation, sex assigned at birth, and current gender identity. In\naddition, we asked questions about household size, race and\nethnicity, household income, religious denomination, how\noften they attend religious services, and self-perceived socio-\neconomic status (see Howe, Hargreaves, Ploubidis, De\nWe also included a 10-item measure of the \"Big Five\"\npersonality factors (Ten Item Personality Measure or TIPI;\nGosling, Rentfrow, & Swann, 2003). The \"Big Five\" is\namong the most widely accepted taxonomy of personality\ntraits within psychology (for a review, see John & Srivastava,\n1999) and conceptualizes personality as consisting of five\nbipolar dimensions: Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism. The ques-\ntionnaire and other materials are available online on the\nOpen Science Framework (osf.io/tg7h3).\nPrior to completing the survey, participants were asked\nwhether they learned about the survey on MTurk or some-\nwhere else. Those who indicated somewhere else were asked\nto specify where they learned about it.\nmitted over 3 years immediately prior to the present study\n(reported in Stewart et al., 2015), we were able to estimate\nindividual workers' relative experience completing MTurk\ntasks. Workers with no recorded experience during the\nStewart et al.'s study (N = 4,746) were assigned a value of\none and all other workers were assigned a value equal to their\n4 SAGE Open\ntotal number of previously completed HITs plus one.\nAlthough this measure does not capture total HITs a respon-\ndent has completed, it does allow us to analyze temporal\nvariations in workers' relative levels of experience (see\nResults\nData Cleaning and Survey Metadata\nData collection resulted in 10,121 survey attempts, of which\nduplicate responses. Duplicate responses were defined as\nany submission from a WorkerID in excess of one. For work-\ners with duplicate responses, the most complete response\nwas taken. When both responses were of equal length (typi-\ncally complete), the first response was taken. An additional\n182 responses that came from non-U.S. IP addresses and one\nrespondent without a WorkerID were also identified and\ndeleted, resulting in 9,770 valid survey attempts.\nOf the valid attempts, 780 (8%) were identified by\nQualtrics as incomplete. A visual inspection of these\nresponses found that 724 of these respondents answered the\nlast question in the survey and were functionally complete.\nOnly 56 respondents (0.6%) dropped out of the survey after\nproviding only partial data. These partial responses were\nincluded for analysis.\naddress shared by at least one other response. The majority\nof IP addresses (n = 196) contributed two responses, with\n10 contributing three responses, three contributing four\nresponses, two contributing 10 responses, one contributing\n26 responses, and another 39 responses. All responses from\nduplicate IP addresses were left in for this analysis, as\nshared IP addresses do not necessarily indicate the same\nworker repeating a task.\nFor example, the 433 responses from IP addresses that\ncontributed four or fewer responses were examined. Of\nthese, 233 were almost certainly unique respondents from\nthe same household: They came from people who listed the\nexact same household size, the same age of household mem-\nbers (\u00b12 years in aggregate) and reported an age that corre-\nsponded to an age that matched an age of a person that the\nother respondent reported that they lived with. An additional\n49 respondents were likely from the same household, report-\ning approximately the same total age of members (\u00b15 years\nin aggregate), or who appeared to have neglected to report a\nhousehold member (usually a child or much older adult).\nThree of the four IP addresses that generated the most\nresponses were servers registered to Amazon. It is likely that\nparticipants from these addresses are using either a proxy\nserver, or an ISP hosted on Amazon Web Services. These\nresponses varied in the time they were attempted, the spe-\ncific browser and operating system configuration used, and\nthe content of the survey responses.\nCharacteristics of the MTurk Sample\nTables 1 to 4 present summary data about the entire sample,\nabout participants in the first two batches only, and for\nnational estimates when available. The entire sample repre-\nsents the largest sample of MTurk workers we are aware of,\nand likely measures about two thirds of the available worker\npopulation (Stewart et al., 2015). The sample size of the first\ntwo batches (N = 438) approximates a sample slightly larger\nthan those typically used in behavioral science research\n(Fraley & Vazire, 2014) and is presented to enable compari-\nsons of this study to other, typically sized data collections.\nThe demographic data are reported in Table 1, including\ninformation about worker experience and where they learned\nabout the survey. Differences between this sample and the\nU.S. population as a whole are generally consistent with\nthose reported in previous analyses of smaller surveys\nin our sample are younger and more likely to be white than\nthe U.S. population as a whole. Workers residing in the\nEastern Time Zone are overrepresented compared with those\nin other parts of the United States. This variation is likely\nbecause the times that HITs were posted aligned most closely\nwith the times that workers in the time zone were likely to be\nactive.\nAlmost all (90.9%) workers reported finding the survey\non MTurk. Of the 868 workers who found the survey else-\nwhere, most (n = 671) named HitsWorthTurkingFor (a\nReddit forum), 29 listed Hit Scraper (an automatic alerting\nservice), and virtually all other respondents listed other\nMTurk discussion forums (e.g., TurkerNation).\nTable 2 summarizes the socioeconomic characteristics of\nour sample. Respondents to our survey generally reported\nmore years of formal education than the population as a\nwhole. Although Americans residing in the wealthiest house-\nholds are underrepresented in our data, household income\nwas much closer to the median U.S. income than would be\nexpected from previous measurements of individual worker\ntion of this difference is likely due to the fact that 16.5% of\nthe respondents in our sample are under 30 and living with\nsomeone at least 18 years older than they are, suggesting that\nour sample includes a substantial number of millennials with\nlow individual income but who are living with their higher\nincome parents.\nTable 3 summarizes the relationship status and characteris-\ntics of respondents, revealing that approximately a third of\nrespondents are married and another third are single. In addi-\ntion, we find that 1.5% of our sample reports are currently\nengaged in a consensually nonmonogamous relationship (see\nHaupert, Gesselman, Moors, Fisher, & Garcia, 2016). As has\nbeen observed in other studies of other MTurk workers\nthe proportion of lesbian, gay, and particularly bisexual\nCasey et al. 5\nrespondents is higher than it is in the U.S. population as a\nwhole. This is likely because online populations are dispropor-\ntionately young, and younger people are also more likely to\nFinally, summary statistics for the attitudinal and personal-\nity measures are summarized in Table 4. Consistent with ear-\nlier research, workers were more likely to identify as\nDemocrats than are members of the general population\nworkers identified as religious, a disproportionate number\nidentified as atheists, and reported rates of church attendance\nwere generally low. Relative to normed data obtained from a\nlarge convenience sample of Internet users (Gosling, Rentfrow,\n& Potter, 2014), MTurk workers reported being about two\nthirds of a standard deviation less extraverted, about a third of\na standard deviation less open to new experiences, and only\nslightly less agreeable, conscientious, or emotionally stable.\nThe vast majority (92.5%) of participants in our study\ncompleted the survey on a computer. Of the remaining par-\nticipants, 2% completed the survey using a tablet, 4.5% using\na phone, and the rest using other devices (e.g., game con-\nsoles) or devices that could not be identified. Rates of mobile\ndevice use are somewhat lower than have been noted in other\nSample Differences by Time of Completion\nThe focus of our investigation is how the composition of the\nMTurk worker pool varied across days of the week, across\ntime of day, and across the serial order in which they partici-\npated. Main findings of these analyses are summarized in\nTable 5. We looked for variations within the following vari-\nables: age, gender identity, education, employment, house-\nhold income, household size, race, Latino ethnicity,\nsocioeconomic status, sexual orientation, relationship status,\nparty identification, religion, and religiosity. Our survey\ndesign allowed respondents to identify as more than one\nrace, so we treated each racial category (White, Black or\nAfrican American, Asian American, American Indian or\nAlaskan Native, Native Hawaiian or Pacific Islander, or\nOther) as a single binary dependent variable. We also looked\nfor differences in the Big Five personality traits: extraver-\nsion, agreeableness, conscientiousness, emotional stability,\nand openness. Finally, we examined workers' prior experi-\nence and where they reported finding the survey.\nIn two instances, similar and highly correlated variables\nwere collected for purposes irrelevant to the present study. In\neach case, only one variable was selected for analysis. The\nfirst instance was marital status and relationship status. We\nTable 1. Demographic Characteristics of Workers.\nCharacteristic Total sample (N = 9,770) First respondents (N = 438) National estimates\nMean worker experience\n(Prior HITs completed)\nU.S. time zone\nRace and ethnicity\nNote. 95% CI indicated in parentheses. HITs = Human Intelligence Tasks; MTurk = Amazon Mechanical Turk; CI = confidence interval.\naU.S. Census Bureau (2016; mean age of adult population).\ncFlores, Herman, Gates, and Brown (2016).\neU.S. Census Bureau (2016) population estimates.\n6 SAGE Open\nTable 2. Socioeconomic Characteristics of Workers.\nCharacteristic Total sample (N = 9,770) First respondents (N = 438) National estimates\nHousehold income\nEmployment status\nEducation\nNote. 95% CI indicated in parentheses. CI = confidence interval.\nbBureau of Labor Statistics, U.S. Department of Labor (2016).\nTable 3. Relationship Characteristics of Workers.\nCharacteristic Total sample (N = 9,770) First respondents (N = 438) National estimates\nRelationship status\nMarital status\nSexual orientation\nNote. 95% CI indicated in parentheses. CI = confidence interval.\nbGeneral Social Survey (as reported and summarized in Gates, 2014).\nCasey et al. 7\nselected marital status for analysis because this variable is\nmore typically recorded in national surveys and therefore\nmore relevant for this demographic analysis. The second\ninstance was political ideology and party affiliation. We con-\nducted the analyses using political ideology, but results are\nidentical when party identification is used instead.\nTo limit the number of comparisons, some response\noptions were collapsed into broader categories (e.g., specific\ndenominations of Christianity were collapsed into a single\ncategory). In total, given the coding, our final analysis\nincluded 31 different demographic variables.\nFor all continuous, ordinal, and binomial variables, gener-\nalized linear modeling (GZLM) was used to regress (a) the\nday of the week (categorical), (b) the time of day the batch\nwas posted (categorical), (c) the serial position of the batch\nwithin the data collection run (continuous), (d) the serial\nposition of the individual response within the batch (continu-\nous), and (e) a dichotomous variable representing the amount\nof compensation (categorical) to control for possible effects\nof increasing payment part way through the study. Interval\ndependent measures were treated as linear effects, except for\nworker experience (i.e., the total number of MTurk HITs\nalready completed), which was modeled using a negative\nbinomial distribution. This approach was adapted to multino-\nmial regression to evaluate differences in religion, as SPSS'\nimplementation of GZLM cannot be used for multinomial\nvariables.\nIncluding so many independent and dependent variables\nbrings with it the risk of false positives. To mitigate this risk,\nwe limited the number of comparisons by not including\ninteractions in the model. We also limited the comparisons of\neach time or day to the grand mean for all times and days\n(rather than individual comparisons against all other times or\ndays). For example, we compared the mean percentage of\ncollege graduates in batches posted on Tuesdays with the\nmean percentage of college graduates in all batches (includ-\ning Tuesdays). This approach led to a total of 13 significance\ntests for each of the 29 demographic variables and two\nMTurk behavior variables (worker experience and where\nthey found the study), for a total of 403 comparisons.\nTo further reduce the potential for false positives, we set\nthe alpha criterion at .01, rather than the more typical .05,\nand used the Benjamini\u00adHochberg adjustment (Benjamini &\nHochberg, 1995) to hold the false discovery rate across all\nTable 4. Attitudinal and Personality Characteristics of Workers.\nCharacteristic Total sample (N = 9,770) First respondents (N = 438) National estimatesa\nPolitical affiliation\n Ideology (1 = extremely liberal,\n7 = extremely conservative)\nReligion\nReligiosity\nBig Five personality traits (1 = low, 7 = high)\nNote. 95% CI indicated in parentheses. CI = confidence interval.\naPopulation estimates derived from American National Election Studies 2012 time series unless otherwise noted.\n8 SAGE Open\nTable 5. Significant Results by Time of Day, Day of Week, Serial Position, and Pay Rate.\nOutcome Contrast Wald p d Interpretation\nTime of day effects\nDay of week effects\nSaturday\nThursday\nlack formal employment altogether (no change in part-\ntime status)\nSunday\nOverall serial position effects\ncollection\ndata collection\ndata collection\nWithin-batch serial position effects\nfaster\nto have found it outside MTurk\nPay effects\nNote. This table includes the 33 comparisons that revealed statistically significant differences. We only report effect sizes for statistically significant\nresults. The entries in the table are sorted by type of temporal variation, and then by ascending order of effect size. As noted in the text, we used\nthe Benjamini\u00adHochberg adjustment for multiple comparisons and consider all p-values less than .0007 to be statistically significant (this ensures\nthat the false discovery rate across all comparisons is held constant at .01). ET = Eastern Time; HITs = Human Intelligence Tasks; MTurk = Amazon\nMechanical Turk.\nCasey et al. 9\ncomparisons constant at .01 across all tests. Following these\nadjustments, no results with an unadjusted p value above\n.0007 are reported as statistically significant, and of the sig-\nnificant results that we report, only four are expected to be\nfalse positives observed by chance alone.3 Table 5 includes\nthe 33 statistically significant differences among the 403\ncomparisons.\nDay of week effects. Of our 217 day-of-week comparisons,\nwe found seven instances in which the attributes of partici-\npants recruited on a particular day of the week significantly\ndiffered from the sample as a whole.4 These findings are sum-\nmarized in Table 5.\nThe average age of respondents varied as a function of the\nday of the week. Participants on Wednesday (M = 32.4, SD =\nRespondents completing the survey on Saturday were some-\nPeople completing HITs on Sundays were more likely to\nbe employed full-time (52%) than the sample as a whole\ncorresponding decrease in the proportion of individuals\nwithout any formal employment (31.2% as compared with\n35.7%). The proportion of workers employed part-time was\nroughly the same across all days of the week.\nWorkers were less likely to find the survey outside of\nMTurk on Saturday (3.4%) or Sunday (6%) than the sample\ntively). Workers who completed the survey on a Thursday\nwere much more likely to have found it on a source outside of\nTime of day effects.Of our 93 time-of-day compari-\nsons, we found 12 instances in which attributes of par-\nticipants recruited at a particular time of day differed\nsignificantly from the grand mean.5 These differences\ngenerally reflected linear trends in the composition of the\nMTurk workforce throughout the day, and are summarized\nin Table 5.\nAs might be expected, one of the most pronounced conse-\nquences of posting at different times was variation in the pro-\nportion of workers from different time zones. People in\nearlier time zones were more likely than average to complete\nd = .17). Conversely, people in later time zones were more\nlikely to complete HITs posted at 10 p.m. ( = .13, Wald 2 =\nquences of this shift, 56.8% of respondents at 10 a.m. Eastern\nTime were from the U.S. Eastern time zone while only 10.9%\nof workers were from the Pacific Time zone. In contrast,\n48.6% of workers at 10 p.m. Eastern Time reside in the U.S.\nEastern time zone, while 18.9% of workers were from the\nU.S. Pacific time zone.\nThe proportion of Asian American respondents also\nincreased over the course of the day, growing from 5.9% at\nAsian Americans was significantly lower than average at 10\nsignificantly higher than average at 10 p.m. ( = .016, Wald\nsignificant, however, when controlling for time zone, sug-\ngesting that this difference reflects that moreAsianAmerican\nworkers live on the west coast.\nOther differences were observed that were not an artifact\nof time zone. The proportion of single workers increased lin-\nare single was significantly lower than average at 10 a.m. (\nMore workers who completed the survey at 10 p.m. used\nsmartphones (5.8%) than across the sample as a whole\nrecruited at 10 p.m. also reported being less conscientious (M\nWorkers who completed the HIT at 10 a.m. were less\nlikely to report having found the HIT outside of the MTurk\nthe HIT at 3 p.m. were more likely (9.7%) to have found the\nHIT outside of the MTurk interface ( = .013, Wald 2 =\nFinally, relative to the sample as a whole (M = 4.67, SD =\n10.04), more experienced workers tended to participate in\nOverall serial position effects. Of our 31 positional com-\nparisons, we found seven instances in which the attributes\nof participants differed over time.6 Workers who com-\npleted HITs earlier in the data collection process reported\nhigher levels of emotional stability, conscientiousness,\nand agreeableness. Participants who completed earlier\nbatches of HITs also tended to be older were more likely\nto have a full-time job and live in smaller households.\nWorkers who completed HITs earlier were also substan-\ntially more experienced than workers recruited later in the\nstudy (Table 6).\nWithin-batch serial position effects.Of our 31 positional\ncomparisons within batch, we found five instances in which\nthe attributes of participants recruited earlier in a given batch\ndiffered from the attributes recruited later in the same batch.\nWorkers who completed an available HIT earlier in a given\nbatch were on average older, more likely to be female, and\nless likely to be Asian American. Workers who completed\nHITs sooner were also less likely to have found the survey on\na source outside of MTurk but tended to be more experienced\nthan workers recruited later in the study (Table 7).\nPay effects. Pay effects were included primarily to con-\ntrol for a change in design part way through data collec-\ntion. Of the 31 payment comparisons, we found evidence\nof only two characteristics that changed once we offered to\npay more. Controlling for other variables, workers in the\nhigh-pay condition reported higher emotional stability (M\ners were also more experienced when pay was higher (M =\nresults and all other significant intertemporal differences\nare summarized in Table 5.\nDiscussion\nIn this article, we have described demographic characteris-\ntics of a large sample of MTurk workers and examined dif-\nferences across time, day, and serial position. Of our 403\ndemographic comparisons, we found 33 differences (8.2% of\ntested effects), and significant effects had an average effect\nsize of d = 0.11. These findings provide evidence that MTurk\nsamples vary intertemporally, but that in general these differ-\nences are small. An important caveat to these findings is that\nwe recruited workers without allowing for replacement--\nthat is, workers could only participate once. Differences\nbetween samples may be larger or smaller if workers are not\nrestricted from participating more than once.\nDemographic Differences by Day and Time\nDay of the week influenced few (2%, or 4/203) demographic\ncharacteristics, and these effects were small (M\nd\nthe extent that these effects were detectable, they suggest that\nsamples collected over the weekend are more likely to\ninclude older and more fully employed respondents. These\ndifferences seem plausible, but the lack of differences across\nother characteristics suggests that potential day of week\neffects can be safely ignored.\nTime of day resulted in similarly small effects (M\nd\nbut within a larger proportion (9%, or 8/87) of measured\nvariables. In almost all cases, these differences represented\nlinear trends in sample composition across the day, and thus\nwhen considering the potential impact of recruiting in the\nmorning or in the evening, the combined impact of both\neffect size estimates should be considered.\nOf particular note, contrary to previous research (Komarov\net al., 2013), we found that workers were more likely to use\nmobile devices late at night (5.8% of HITs posted at 10 p.m.\nwere submitted from mobile phones, compared with 3.7% of\nHITs submitted during the rest of the day). Mobile device use\ncan have adverse effects on data quality, including increased\nrates of attrition (Mavletova, 2013; Sommer, Diedenhofen,\nand fewer open-ended responses (Mavletova, 2013;\nStruminskaya, Weyandt, & Bosnjak, 2015). As a result,\nTable 6. Worker Characteristics as a Function of Serial Position Across Study.\nTable 7. Worker Characteristics as a Function of Serial Position Within Batches.\nFirst respondent in batch\n(+1 SD) Linear trend\nFound survey outside of\nMechanical Turk\nresearchers might consider adjusting the time of day at which\nthey post research studies or collect data if they hope to opti-\nmize mobile completion or collect open-ended responses.\nThe large proportion of observed differences suggest that\ntime of day effects might be a fruitful area of future research,\nboth through expanding the range of variables that are exam-\nined and with a particular effort to understand how regional\ndifferences, differences in the active user population across\ntime within regions, and changes in individual responses\nthroughout the day combine to produce these differences.\nDemographic Differences by Serial Position\nThe effects of serial position were more extensive than time-\nof-day and day-of-week effects; 21% (6/29) of across-sam-\nple serial position effects were significant, with an average\neffect size of M\nd\nserial position effects were significant, with M\nd\nMany of these across-sample findings are compatible with\nearlier studies of serial position effects. As observed in uni-\nversity subject pools, early respondents report higher levels\nof conscientiousness (Aviv et al., 2002; Ebersole et al.,\n2016). In general population samples, those who responded\nto surveys first tended to be older (Filion, 1975; Sigman\net al., 2014). We observe similar results both across our\nentire sample and within individual batches of HITs. While\nother studies find that women are more likely to respond to\nrequests to complete both mail surveys (Gannon et al., 1971)\nand web surveys (Sigman et al., 2014) quickly (Cooper et al.,\nmore quickly within batches, but not across the sample as a\nwhole. Contrary to studies of race and serial position effects\nVoigt et al., 2003), we found little evidence that racial diver-\nsity increased over time. Typically, later survey respondents\nbelong to groups that are possible but difficult to contact.\nOnly those who register with MTurk can take part in surveys\nposted on the platform.AfricanAmerican and Latino popula-\ntions are underrepresented on MTurk, and so it may be that\nthose individuals who may be possible but difficult to con-\ntact through other modes of survey data collection are simply\nimpossible to reach on MTurk.\nWhen sampling error is unsystematic, larger samples\nmore closely approximate the population. This is not so in\nthe presence of systematic bias. As our sample increased,\nsome biases (e.g., the democratic tendencies of respondents)\nremained the same. In other cases, biases actually increases\n(e.g., age, employment, conscientiousness, and emotional\nstability). Thus, it is not a given that making a sample more\nrepresentative of the U.S. MTurk worker population will also\nmake it more representative of the U.S. population as a\nwhole. Variations in demographic characteristics across the\nentire sample are also relevant to researchers who recruit\nworkers from the available pool without replacement (e.g., to\nprevent workers from completing the same study twice). Of\nparticular relevance, we found variations in the \"Big Five\"\npersonality factors as a function of serial position. Workers\nwho completed HITs earlier in the data collection process\nreported being slightly more emotionally stable, more con-\nscientious, and more agreeable. These traits are associated\nwith and may moderate other important variables including\nrespondent data quality, or political behaviors and attitudes\nthat might bias samples (for an excellent review, see Gerber,\nHuber, Doherty, & Dowling, 2011), or data quality.\nVariations in demographic characteristics associated with\nserial position within batches of HITs are important when\nconsidering whether to recruit respondents in large batch or\nsmall batches. It is particularly important to understand\npotential within-batch serial position effects because several\nthird-party solutions (e.g., TurkGate, Goldin & Darlow,\n2013; and TurkPrime, Litman, Robinson, & Abberbock,\n2017) make it easy to divide data collection efforts into a\nlarge number of very small batches. By and large, we find\nthat smaller batches will lead samples to be older and have\nmore women, but will attenuate the overrepresentation of\nAsian American workers.\nDifferences in Worker Experience and Forum Use\nTime of day and serial position were strongly related to how\nmuch MTurk experience respondents had and how workers\nfound the survey. More experienced workers completed the\nsurvey earlier in data collection (both within and across\nbatches). Variations in worker experience may be associated\nwith greater exposure to survey tactics, experimental manip-\nulations, which can have various effects on data quality. On\none hand, more experienced workers are more familiar with\ncommon research questions, leading to practice effects\n(Chandler et al., 2014), potentially smaller effect sizes on\ncommonly used experimental paradigms (Chandler et al.,\n2015) and potentially more extreme and less malleable atti-\ntudes toward topics that respondents are frequently asked\nabout (Sturgis, Allum, & Brunton-Smith, 2009). On the other\nhand, more experienced workers may be more attentive and\ntherefore may provide higher quality responses.\nWe also observed substantial intertemporal variation in\nworkers using forums, with more referrals from links shared\noutside of MTurk happening in the afternoon and on\nThursdays and less in evenings and weekends. These differ-\nences may be relevant if researchers are concerned about\nrespondents who have potentially seen information about a\nstudy prior to completing it. The longer a HIT is available,\nthe more opportunity workers have to find it on an outside\nforum.\nAlthough we did not vary pay rates experimentally, we\nnonetheless found that when we increased pay, there was a\nconcomitant increase in the experience of survey partici-\npants. Together, we thus observed two separate patterns: (a)\nEarly responders to the survey tended to be more experi-\nenced workers and (b) when we increased the pay,\nthe proportion of more experienced workers increased even\nfurther. If researchers are concerned that worker savviness\nmight affect their findings (Krupnikov & Levine, 2014), they\nshould be attentive to these possibilities when they post their\nstudies.\nConclusion\nThis study is the largest and most comprehensive description\nof MTurk demographics that we are aware of and the first\nlarge-scale effort to examine intertemporal differences in\nsample composition (however, for a similar project, see\nArechar, Kraft-Todd, & Rand, 2016). Data from our study of\napproximately 10,000 MTurk workers have allowed us to\nexamine three key possible sources of temporal variation in\nMTurk sample composition: (a) time of day, (b) day of week,\nand serial position both (c) across the entire data collection\nand (d) within specific batches.\nTaken as a whole, our results should serve as a source of\nboth comfort and caution to scholars who use MTurk to\nrecruit participants for their research. On one hand, we\nfound only minimal day-of-week differences. However,\nwe also showed that there are small but significant time-\nof-day variations in demographic composition--variations\nthat bear closer scrutiny. The effects of serial position also\nwarrant further study, as they emerged as persistent influ-\nences across multiple variables, including characteristics\nknown to affect political and psychological attitudes (e.g.,\nBig Five personality traits; Dietrich, Lasley, Mondak,\nin sample composition can compromise claims to general-\nizability and might lead to challenges with reproducing\nresearch findings as well (Peterson & Merunka, 2014). As\nis often the case, larger samples (and/or those recruited in\nsuch a way to be more representative) are especially criti-\ncal when researchers are concerned about heterogeneous\ntreatment effects may reduce the external validity of a\ngiven sample.\nResearchers should bear our findings in mind as they\nconsider how best to recruit samples from MTurk. The inter-\ntemporal dynamics we have detailed are likely to be most\nrelevant to researchers attempting to collect representative\nsamples of the MTurk worker population, such as studies of\nMTurk worker behavior and attitudes that attempt to under-\nstand the dynamics of contract labor and piece-work in the\n\"gig economy\" (Aguinis & Lawal, 2013; Brawley & Pury,\n2016). But researchers interested in other topics should pay\nattention to relationships such as those between serial posi-\ntion and psychological characteristics and consider includ-\ning information about when and how many times they\nposted their HIT when reporting results.7 Perhaps most\nimportantly, these findings demonstrate that the number of\nworkers recruited and the size of batches used to recruit\nthem can have a large effect on the average experience of\nsample respondents.\nAs MTurk and other similar online convenience samples\nbecome more widely used, it is increasingly important that\nwe better understand who participates in these subject pools\nand when certain kinds of respondents are more likely to opt-\nin relative to others. Such examinations will help researchers\nassess published results, especially (though not limited to)\ntheir generalizability across populations and over time.\nThis project suggests several directions for future\nresearch. Beyond extending the analysis of temporal effects\nto new variables, or examining intertemporal variation in\nother sources of data, future work could examine how other\ndesign choices affect sample composition, including whether\nresearchers with poor ratings or tasks with low pay get sub-\nstantively different samples than researchers with better rat-\nings or tasks with higher pay. This is an important area for\nfuture research to examine, particularly as researchers con-\ntinue or increase reliance on online data collection.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research, author-\nship, and/or publication of this article.\nNotes\n1. People of color is a commonly used umbrella term denoting\nracial and ethnic minorities in America, including African\nAmericans, Latinos, Asians, and others.\n2. We followed this general procedure when it was time to repost\nthe HIT: first, close the existing HIT; second, prevent the work-\ners who participated in the existing HIT from participating in\nfuture postings (using qualifications; see Chandler, Mueller, &\nPaolacci, 2014); third, post the new HIT.\n3. The Benjamini\u00adHochberg adjustment does not identify spe-\ncific false positives, but rather holds the number of false posi-\ntives constant across many tests to a specified level.\ngraphic variables produces 93 comparisons.\n6. Thirty-one demographic variables, treating time as a linear\neffect by batch number.\n7. The size of these effects will depend on both the magnitude\nof difference between the samples on a given variable and the\nmagnitude of the moderating effect this variable has on the\ntheoretical relationship of interest (Ho, Imai, King, & Stuart,\nReferences\nAguinis, H., & Lawal, S. O. (2013). eLancing: A review and\nresearch agenda for bridging the science\u00adpractice gap. Human\nSeries Study [dataset]. Stanford University and University of\nMichigan [producers]. Available from www.electionstudies.\norg\nArechar, A. A., Kraft-Todd, G. T., & Rand, D. G. (2016). Turking\novertime: How participant characteristics and behavior vary\nover time and day on Amazon Mechanical Turk. Retrieved\nAviv, A. L., Zelenski, J. M., Rallo, L., & Larsen, R. J. (2002). Who\ncomes when: Personality differences in early and later partici-\npation in a university subject pool. Personality and Individual\nBenjamini, Y., & Hochberg, Y. (1995). Controlling the false dis-\ncovery rate: A practical and powerful approach to multiple\ntesting. Journal of the Royal Statistical Society, Series B:\nBerinsky, A. J., Huber, G. A., & Lenz, G. S. (2012). Evaluating\nonline labor markets for experimental research: Amazon.com's\nBrawley, A. M., & Pury, C. L. (2016). Work experiences on MTurk:\nJob satisfaction, turnover, and information sharing. Computers\nBureau of Labor Statistics, U.S. Department of Labor. (2016).\nReasons people give for not being in the labor force, 2004 and\n2014 on the Internet. The Economics Daily. Retrieved from\nhttps://www.bls.gov/opub/ted/2016/reasons-people-give-for-\nCasler, K., Bickel, L., & Hackett, E. (2013). Separate but equal?\nA comparison of participants and data gathered via Amazon's\nMTurk, social media, and face-to-face behavioral testing.\nChandler, J., Mueller, P., & Paolacci, G. (2014). Nonnaive among\nAmazon Mechanical Turk workers: Consequences and solu-\ntions for behavioral researchers. Behavioral Research Methods\nChandler, J., Paolacci, G., Peer, E., Mueller, P., & Ratliff,\nK. A. (2015). Using nonnative participants can reduce\nChandler, J., & Shapiro, D. (2016). Conducting clinical research\nusing crowdsourced convenience samples. Annual Review\nCooper, H., Baumgardner, A. H., & Strathman, A. (1991). Do stu-\ndents with different characteristics take part in psychology exper-\niments at different times of the semester? Journal of Personality,\nCorrigan, P. W., Bink, A. B., Fokuo, J. K., & Schmidt, A. (2015).\nThe public stigma of mental illness means a difference between\nCouper, M. P. (2000). Review: Web surveys: A review of issues\nde Bruijne, M., & Wijnant, A. (2014a). Improving response rates\nand questionnaire design for mobile web surveys. Public\nde Bruijne, M., & Wijnant, A. (2014b). Mobile response in web\nDietrich, B. J., Lasley, S., Mondak, J. J., Remmel, M. L., & Turner,\nJ. (2012). Personality and legislative politics: The Big Five trait\ndimensions among U.S. state legislators. Political Psychology,\nEbersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H.\nM., Allen, J., Banks, J. B., . . . Nosek, B. A. (2016, June 28).\nMany labs 3: Evaluating participant pool quality across the\nacademic semester via replication. Retrieved from osf.io/ct89g\nFilion, F. L. (1975). Estimating bias due to nonresponse in\nFlores, A., Herman, J. L., Gates, G. J., & Brown, T. N. T. (2016).\nHow many adults identify as transgender in the United States?\nThe Williams Institute. Retrieved from https://williamsinsti-\ntute.law.ucla.edu/research/how-many-adults-identify-as-trans-\ngender-in-the-united-states/\nFraley, R. C., & Vazire, S. (2014). The N-pact factor: Evaluating\nthe quality of empirical journals with respect to sample size\nGannon, M. J., Nothern, J. C., & Carroll, S. J. (1971). Characteristics\nof nonrespondents among workers. Journal of Applied\nGates, G. (2014). LGBT demographics: Comparisons among pop-\nulation-based surveys. The Williams Institute. Retrieved from\nhttps://williamsinstitute.law.ucla.edu/wp-content/uploads/\nof U.S. adults identify as LGBT. Retrieved from http://opin-\nidentify-as-lgbt/\nGerber, A. S., Huber, G. A., Doherty, D., & Dowling, C. M. (2011).\nThe Big Five personality traits in the political arena. Annual\nRetrieved from http://gideongoldin.github.io/TurkGate/\nGosling, S. D., Rentfrow, P. J., & Potter, J. (2014). Norms for the\nTen Item Personality Inventory (Unpublished data).. Retrieved\nfrom http://gosling.psy.utexas.edu/scales-weve-developed/ten-\nitem-personality-measure-tipi/.\nGosling, S. D., Rentfrow, P. J., & Swann, W. B. (2003). A very\nbrief measure of the Big-Five personality domains. Journal of\nGuidelines for academic requesters. (2014). Version 1.1. Retrieved\nfrom https://irb.northwestern.edu/sites/irb/files/documents/\nguidelinesforacademicrequesters.pdf\nHaupert, M. L., Gesselman, A. N., Moors, A. C., Fisher, H. E.,\n& Garcia, J. R. (2016). Prevalence of experiences with con-\nsensual nonmonogamous relationships: Findings from two\nnational samples of single Americans. Journal of Sex & Marital\nHillygus, D. S., Jackson, N., & Young, M. (2014). Professional\nrespondents in nonprobability online panels. In M. Callegaro,\nR. Baker, J. Bethlehem, A. S. G\u00f6ritz, J. A. Krosnick, & P.\nJohn Wiley. Retrieved from http://onlinelibrary.wiley.com/\nHo, D. E., Imai, K., King, G., & Stuart, E. A. (2007). Matching as\nnonparametric preprocessing for reducing model dependence\nHowe, L. D., Hargreaves, J. R., Ploubidis, G. B., De Stavola, B. L.,\n& Huttly, S. R. A. (2011). Subjective measures of socio-eco-\nnomic position and the wealth index: A comparative analysis.\nJohn, O. P., & Srivastava, S. (1999). The Big Five trait taxon-\nomy: History, measurement, and theoretical perspectives. In\nHandbook of personality: Theory and research (Vol. 2, pp.\nKomarov, S., Reinecke, K., & Gajos, K. Z. (2013). Crowdsourcing\nperformance evaluations of user interfaces. In Proceedings\nof the SIGCHI Conference on Human Factors in Computing\nKrupnikov, Y., & Levine, A. S. (2014). Cross-sample compari-\nsons and external validity. Journal of Experimental Political\nLakkaraju, K. (2015). A study of daily sample composition on\nAmazon Mechanical Turk. In N. Agarwal, K. Xu, & N. Osgood\n(Eds.), Social computing, behavioral-cultural modeling, and\nprediction (pp. 333-338). Springer. Retrieved from http://link.\nLitman, L., Robinson, J., & Abberbock, T. (2017). TurkPrime.com:\nA versatile crowdsourcing data acquisition platform for the\nMavletova, A. (2013). Data quality in PC and mobile web sur-\nheterosexual. YouGov. Retrieved from https://today.yougov.\nheterosexual/\nMullinix, K. J., Leeper, T. J., Druckman, J. N., & Freese, J.\n(2015). The generalizability of survey experiments. Journal\nPaolacci, G., & Chandler, J. (2014). Inside the turk: Understanding\nMechanical Turk as a participant pool. Current Directions\nPaolacci, G., Chandler, J., & Ipeirotis, P. G. (2010). Running\nexperiments on Amazon Mechanical Turk (SSRN Scholarly\nResearch Network. Retrieved from http://papers.ssrn.com/\nPeer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond\nthe turk: Alternative platforms for crowdsourcing behavioral\nresearch. Journal of Experimental Social Psychology, 70,\nPeer, E., Vosgerau, J., & Acquisti, A. (2014). Reputation as a suf-\nficient condition for data quality on Amazon Mechanical Turk.\nPeterson, R. A., & Merunka, D. R. (2014). Convenience samples of\ncollegestudentsandresearchreproducibility.JournalofBusiness\nPew Research Center. (2016a). A new estimate of the U.S. Muslim\npopulation. Retrieved from http://www.pewresearch.org/fact-\ntion/\nPew Research Center. (2016b). 10 facts about atheists. Retrieved\nfacts-about-atheists/\nRavallion, M., & Lokshin, M. (1999). Subjective economic welfare.\nThe World Bank. Retrieved from http://elibrary.worldbank.\nReidy, D. E., Berke, D. S., Gentile, B., & Zeichner, A. (2014). Man\nenough? Masculine discrepancy stress and intimate partner\nShapiro, D. N., Chandler, J., & Mueller, P. A. (2013). Using\nMechanical Turk to study clinical populations. Clinical\nSigman, R., Lewis, T., Yount, N. D., & Lee, K. (2014). Does the\nlength of fielding period matter? Examining response scores of\nearly versus late responders. Journal of Official Statistics, 30,\nSommer, J., Diedenhofen, B., & Musch, J. (2016). Not to be con-\nsidered harmful: Mobile-device users do not spoil data quality\nStewart, N., Ungemach, C., Harris, A. J. L., Bartels, D. M., Newell,\nB. R., Paolacci, G., & Chandler, J. (2015). The average labora-\ntory samples a population of 7,300 Amazon Mechanical Turk\nStruminskaya, B., Weyandt, K., & Bosnjak, M. (2015). The effects\nof questionnaire completion using mobile devices on data\nquality: Evidence from a probability-based general population\npanel. Methods, Data, Analyses: A Journal for Quantitative\nSturgis, P., Allum, N., & Brunton-Smith, I. (2009). Attitudes\nover time: The psychology of panel conditioning. In P. Lynn\nJohn Wiley. Retrieved from http://onlinelibrary.wiley.com/\n5-year estimates. Retrieved from https://www.census.gov/data/\ndevelopers/data-sets/acs-5year.html\nU.S. Census Bureau. (2016). Current population survey, annual\nsocial and economic supplement. Retrieved from https://www.\ncensus.gov/cps/data/cpstablecreator.html\nVoigt, L. F., Koepsell, T. D., & Daling, J. R. (2003). Characteristics\nof telephone survey respondents according to willingness to\nWeinberg, J., Freese, J., & McElhattan, D. (2014). Comparing data\ncharacteristics and results of an online factorial survey between\na population-based and a crowdsource-recruited sample.\nWells, T., Bailey, J., & Link, M. (2013). Filling the void: Gaining a\nbetter understanding of tablet-based surveys. Survey Practice,\n6(1). Retrieved from http://www.surveypractice.org/index.\nphp/SurveyPractice/article/view/25\nAuthor Biographies\nLogan S. Casey received his PhD from the University of Michigan.\nHe is a research analyst in Public Opinion at the Harvard Opinion\nResearch Program in the Harvard T.H. Chan School of Public\nHealth. His research examines political psychology, emotion, and\npublic opinion, particularly in the context of LGBTQ politics.\nJesse Chandler received his PhD from the University of Michigan.\nHe is a researcher at Mathematica Policy Research and adjunct fac-\nulty at the Institute for Social Research. He is interested in survey\nmethodology, online research studies, decision-making, and human\ncomputation.\nAdam Seth Levine is an assistant professor of Government at\nCornell University.\nAndrew Proctor is a doctoral candidate in the Department of\nPolitics at Princeton University. His primary research interests are\nin LGBT politics, identity and political mobilization.\nDara Z. Strolovitch is an associate professor at Princeton\nUniversity, where she holds appointments in Gender and Sexuality\nStudies, African American Studies, and the Department of Politics.\nHer teaching and research focus on interest groups and social\nmovements, political representation, and the intersecting politics\nof race, class, gender, and sexuality. Her book, Affirmative\nAdvocacy, addressed these issues through an examination of the\nways in which advocates for women, people of colour, and low-\nincome people represent intersectionally marginalized subgroups\nof their constituencies."
}