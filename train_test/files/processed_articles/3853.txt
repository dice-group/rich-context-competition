{
    "abstract": "Abstract\nBackground/aims: The analyses of randomised controlled trials with missing data typically assume that, after conditioning\non the observed data, the probability of missing data does not depend on the patient's outcome, and so the data are `missing\nat random' . This assumption is usually implausible, for example, because patients in relatively poor health may be more likely\nto drop out. Methodological guidelines recommend that trials require sensitivity analysis, which is best informed by elicited\nexpert opinion, to assess whether conclusions are robust to alternative assumptions about the missing data. A major barrier\nto implementing these methods in practice is the lack of relevant practical tools for eliciting expert opinion. We develop a\nnew practical tool for eliciting expert opinion and demonstrate its use for randomised controlled trials with missing data.\nMethods: We develop and illustrate our approach for eliciting expert opinion with the IMPROVE trial (ISRCTN\n48334791), an ongoing multi-centre randomised controlled trial which compares an emergency endovascular strategy\nversus open repair for patients with ruptured abdominal aortic aneurysm. In the IMPROVE trial at 3 months post-rando-\nmisation, 21% of surviving patients did not complete health-related quality of life questionnaires (assessed by EQ-5D-3L).\nWe address this problem by developing a web-based tool that provides a practical approach for eliciting expert opinion\nabout quality of life differences between patients with missing versus complete data. We show how this expert opinion\ncan define informative priors within a fully Bayesian framework to perform sensitivity analyses that allow the missing data\nto depend upon unobserved patient characteristics.\nResults: A total of 26 experts, of 46 asked to participate, completed the elicitation exercise. The elicited quality of life\nscores were lower on average for the patients with missing versus complete data, but there was considerable uncer-\ntainty in these elicited values. The missing at random analysis found that patients randomised to the emergency endovas-\ncular strategy versus open repair had higher average (95% credible interval) quality of life scores of 0.062 (20.005 to\n0.130). Our sensitivity analysis that used the elicited expert information as pooled priors found that the gain in average\nConclusion: We provide and exemplify a practical tool for eliciting the expert opinion required by recommended\napproaches to the sensitivity analyses of randomised controlled trials. We show how this approach allows the trial analy-\nsis to fully recognise the uncertainty that arises from making alternative, plausible assumptions about the reasons for\nmissing data. This tool can be widely used in the design, analysis and interpretation of future trials, and to facilitate this,\nmaterials are available for download.\n",
    "reduced_content": "Article\nCLINICAL\nTRIALS\nClinical Trials\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\njournals.sagepub.com/home/ctj\nDevelopment of a practical approach\nto expert elicitation for randomised\ncontrolled trials with missing health\noutcomes: Application to the\nAlexina J Mason1, Manuel Gomes1, Richard Grieve1, Pinar Ulug2,\nJanet T Powell2 and James Carpenter3\n Keywords\nMissing data, sensitivity analysis, expert elicitation, Bayesian analysis, clinical trials, pattern-mixture models, quality of life\n1Department of Health Services Research and Policy, London School of Hygiene & Tropical Medicine, London, UK\n2Vascular Surgery Research Group, Imperial College London, London, UK\n3Department of Medical Statistics, London School of Hygiene & Tropical Medicine, London, UK\nCorresponding author:\nAlexina J Mason, Department of Health Services Research and Policy, London School of Hygiene & Tropical Medicine, 15-17 Tavistock Place, London\nEmail: alexina.mason@lshtm.ac.uk\nIntroduction\nIn randomised controlled trials (RCTs), outcome data\nare typically missing for some participants. Patient-\nreported outcomes such as health-related quality of life\n(QoL) are particularly prone to missing data because\npatients may fail to complete follow-up question-\nnaires.1,2 Missing data can reduce the power and effi-\nciency of an RCT and also lead to biased effectiveness\nestimates.3\u00ad6 In the primary trial analysis, studies are\nrecommended to take an approach that is valid under\nplausible assumptions about the missing data.7 Rather\nthan assuming that the data are `missing completely at\nrandom' (MCAR), the primary analysis should assume\nthey are `missing at random' (MAR), that is, the prob-\nability of missing data does not depend on the patient's\noutcome, after conditioning on the observed variables\n(e.g. the patients baseline characteristics). However, the\nMAR assumption may be implausible in many settings;\nfor example, patients in relatively poor health may be\nless likely to complete the requisite questionnaires, and\nso these outcome data may be `missing not at random'\n(MNAR). As the true missing data mechanism is\nunknown given the data at hand, it is important to\nexamine whether the study results are robust to alterna-\ntive assumptions about the missing data.\nThe US National Research Council (NRC) report\non missing data in clinical trials recommended sensitiv-\nity analyses that recognised the data could be MNAR,8\nin line with general methodological guidance for deal-\ning with missing data,9 and previous specific advice for\nintention-to-treat analysis in RCTs.6 However, sys-\ntematic reviews report that in practice RCTs do not\nA simple approach to sensitivity analysis is to\ninclude in the statistical model parameters representing\noutcome differences between individuals with complete\nversus missing data and explore how inference vary as\nthese `sensitivity parameters' take on specific values.12\nThe results and conclusions can then be compared over\na plausible range of values, possibly including a `tip-\nping-point' at which the results change. However, draw-\nbacks of this approach are (1) for each sensitivity\nanalysis the sensitivity parameters are treated as known,\nwithout uncertainty; (2) the challenge of determining\nwhat constitutes a plausible range, and relative diffi-\nculty of making statistical model parameters accessible\nto non-statistical experts; (3) the extent to which some\nvalues should be considered more plausible; and (4) the\ndifficulty that the plausibility of parameters/tipping\npoints are often assessed after the experts have seen the\npreliminary analyses. An alternative is to allow experts\nto quantify their views, rather than those of others. Not\nonly is this likely to be more intuitive and attractive for\nthem, but (as we show in this article) it allows us to take\na fully Bayesian approach and properly capture and\nreflect expert opinion (and associated uncertainty)\nabout the missing data in the posterior estimate of the\ntreatment effect and its credible interval.\nThis is particularly useful for those needing a quanti-\ntative summary of the trial, such as systematic\nreviewers, decision makers and health providers,\nbecause it provides a quantitative summary of how\nthose involved in the study (experts) would interpret its\nresults given the missing data. When reviewing the\nstudy, experts will automatically (implicitly) `fill in' the\ngaps created by the missing data to arrive at their con-\nclusions. The proposed elicitation approach coupled\nwith a Bayesian analysis allows the study to coherently\nquantify the impact of incorporating expert knowledge\nabout the missing data, through to the estimates of\ntreatment effectiveness.\nThe Bayesian approach allows uncertainty about the\nmissing data mechanism to be propagated through the\neventual estimates of relative effectiveness. Such sensi-\ntivity analyses require practical tools to facilitate expert\nelicitation, and recent research, for example, the\nSheffield Elicitation Framework and associated web-\nbased elicitation tool,13,14 has focused on elicitation\napproaches within group meetings. As Hampson\net al.15 illustrate, group-level elicitation has advantages\nfor training and clarification and facilitates behavioural\naggregation, such as Delphi processes, for achieving\nconsensus.16 However, because of the `feedback' loop,\nthese approaches are costly in both money and time,\nand in many RCTs, it may be infeasible to elicit opin-\nion from a sufficient number and range of experts. To\nimprove the uptake of recommended approaches to\nsensitivity analysis for missing data within RCTs\nrequires that more accessible, practical tools for elicit-\ning and synthesising expert opinion are developed and\nexemplified.8\nThis article directly addresses this gap in the litera-\nture, by developing a practical elicitation tool for elicit-\ning the expert opinion required for sensitivity analysis\nthat allows for data to be MNAR. The tool can quickly\nelicit views from tens of experts, who have limited time\nto devote to the elicitation exercise. We illustrate our\nelicitation tool with the motivating example of\nImmediate Management of the Patient with Rupture:\nOpen Versus Endovascular strategies (IMPROVE), an\nongoing multi-centre trial with a parallel design which\nevaluates the effectiveness of an emergency endovascu-\nlar strategy (eEVAR) compared with open repair\n(OPEN) for patients with ruptured abdominal aortic\nIn the IMPROVE trial, 21% of patients did not com-\nplete follow-up EQ-5D-3L questionnaires at 3 months\npost-randomisation.\nThe article proceeds as follows: section `Motivating\nstudy: the IMPROVE trial' outlines the IMPROVE\ntrial and the requirements for the elicitation exercise.\nSections `Development of the elicitation tool' and\n`Eliciting and synthesising expert opinion' explain how\nthe elicitation tool was developed and used. Section\n`Results' gives the results. Section `Discussion' discusses\nthe findings in the context of related research and out-\nlines areas for further research.\nMotivating study: the IMPROVE trial\nsites (29 in the United Kingdom, 1 in Canada). The\npublished analyses found that there was no difference\nin the primary endpoint of 30-day mortality between\nthe randomised arms,17 but that patients with ruptured\naneurysms who were randomised to the eEVAR strat-\negy had on average, a clinically significant improvement\nin their QoL score at both 3 and 12 months versus those\nrandomised to open repair.18 The QoL assessment used\nthe EQ-5D-3L19 questionnaire which requires patients\nto describe their own health according to five dimen-\nsions: mobility, self-care, usual activities, pain or dis-\ncomfort and anxiety or depression, with the option of\nthree levels of severity: `no problems', `some problems'\nand `extreme problems'. The responses to these ques-\ntions are then combined with preference values from\nthe published literature,20 to provide QoL index scores\non a scale anchored at 1 (perfect health) and 0 (death)\nwith health states judged worse than death assigned a\nnegative value. The published analyses used multiple\nimputation to handle the missing data assuming MAR,\nso it is unclear whether the reported gain in average\nQoL for the eEVAR strategy is robust to plausible\ndepartures from MAR.\nWe use expert elicitation to recognise that the\nfollow-up QoL data in IMPROVE may be MNAR. At\n3 months, eligible patients in both randomised arms\nfailed to return completed QoL questionnaires. As it is\nanticipated that the patients' response to treatment will\ndiffer, for example, hospital stay will be longer for the\nopen repair arm compared to the eEVAR arm, it is\nplausible that the reasons for the missingness and asso-\nciated missing values also differ by intervention.\nTherefore, we elicited expert beliefs about expected\nQoL differences between patients with missing versus\nfully observed QoL data for patients in each arm. Our\nelicitation was restricted to eligible survivors, that is,\nthose with confirmed ruptured aneurysms who had sur-\nvived up to 3 months.18 If differences in mortality rates\nbetween the two arms had been found, these would\nfeed through into quality-adjusted life years, but there\nwould be no implications for the elicitation or analysis\nmethod.\nEthical approval for this study\nEthical approval was given by the London School of\nHygiene and Tropical Medicine observational research\nethics committee. Also, this study was approved by the\nIMPROVE Trial Management Committee.\nPattern-mixture model\nMissing data that are MNAR may be modelled with\nselection models or pattern-mixture models.9 To\nencourage uptake of these sensitivity analyses in RCTs,\nwe adopt a pattern-mixture approach. This is because\nin our dealings with regulators and trial statisticians in\nacademia and industry, we have found the underlying\nassumptions are more accessible to those interpreting\nthe results of RCTs. Consistent with the published\nanalysis, we undertake an intention-to-treat analysis\nand estimate the effect of randomised arm on QoL at 3\nmonths for eligible patients.\nIn our example, the pattern-mixture model allows\nthe mean QoL to be calculated differently according to\nwhether the QoL is observed (pattern 1) or missing\n(pattern 2). For pattern 1, we can calculate the mean\nresponse for each arm from the observed data (m).\nHowever, for pattern 2, the outcome data are missing,\nand so we calculate the mean to be that for pattern 1\nplus an offset (d). As shown in Figure 1, the effective-\nness of treatment can then be estimated by weighting\nthe mean QoL scores in each pattern using the propor-\ntion of patients with missing data in each arm (p). The\noffset term, also known as a sensitivity parameter, may\nwell differ according to prognostic factors. So in the\nIMPROVE trial, dO\nand dE\nrepresent the difference in\nQoL between those who did and did not complete the\nquestionnaire for the eEVAR and OPEN arms, respec-\ntively. A key concern is that they cannot be estimated\nfrom the observed data.\nWhat information is required?\nTo estimate treatment effectiveness, recognising that\ndata may be MNAR, we require expert opinion about\nthe likely values of the difference in the mean QoL\nbetween patients who did and did not complete the\nQoL assessment. This comparison will be made for\npatients who are similar according to characteristics\nthat we have observed, such as age, gender and baseline\ndisease severity. We will also make this comparison to\nestimate this sensitivity parameter for each randomised\narm (dO\nand dE\n). To see how this might work, suppose\nthat an expert's views can be summarised by a mean\nwhich gives their most likely value and a standard\ndeviation, which represents their uncertainty about this\nvalue. Then, as we show in Figure 1, we could simply\nsubstitute the mean values into the formula to provide\nan estimate of treatment effectiveness that reflects this\nexpert's opinion about the outcome differences between\npatients with missing versus complete outcome data. In\nthe worked example, the expert expects patients in the\nopen repair arm who did not complete a questionnaire\nto have a lower QoL score than those who did. The net\neffect of including this elicited value for this sensitivity\nparameter is that the average effectiveness of eEVAR is\nsomewhat larger than in the MAR analysis. As the\nworked example shows, the expert's uncertainty about\nthe missing values can be propagated through into the\nestimates of effectiveness. The worked example in\nFigure 1 uses standard formulae, but we would like to\nperform a more sophisticated analysis incorporating\nelicited information from multiple experts, adjusting\nfor observed differences in baseline characteristics and\ncorrelations in the QoL scores between the trial arms.\nUsing the same principles, we show how Bayesian\nmethods can provide a practical way of implementing\nthese improvements.\nThis approach to missing data requires that beliefs\nare elicited from those experts with knowledge about\nthe likely outcomes of patients who did not complete\nQoL questionnaires. We identified 46 potential experts,\nwho were principal investigators (mainly consultant\nvascular surgeons) or trial coordinators (vascular nurse\nFigure 1. Illustration of the estimation of treatment effectiveness using a pattern-mixture model that allows for outcome data to\nbe MNAR.\nm represents the mean QoL for patients who returned their QoL questionnaires, d represents the difference in the mean QoL between patients who\ndid and did not return their QoL questionnaires and p represents the proportion of patients who did not return their QoL questionnaires. E and O\nindicate the eEVAR and open repair treatment groups respectively.\nSimple arithmetic example that uses hypothetical elicited values to re-calculate the effectiveness of eEVAR versus open repair on\nQoL score. The example uses a pattern-mixture model to allow for data that are MNAR.\nInformation from QoL data that are observed in the RCT:\nsample mean (SE) QoL score for patients who completed QoL questionnaire\n\u00f0 \u00de\nproportion of patients who did not return their QoL questionnaire\nInformation elicited from an expert:\nmean (SD) of difference in mean QoL between patients who did and did not return their QoL questionnaire\n\u00f0 \u00de\nThen a point estimate of the treatment difference can be calculated as\n(mE\n+ pE\ndE\n) \u00c0 (mO\n+ pO\ndO\nAssuming independence between variables, the variance (V) of the treatment difference is\nV(mE\nE\nV(dE\n) + V(mO\nO\nV(dO\nand a 95% confidence interval (CI) for the treatment difference can be estimated as\nHence using a pattern-mixture model with expert information reports an estimate of the effectiveness of eEVAR versus open repair of treatment\nconfidence interval from using the pattern-mixture model, as this approach takes account of the uncertainty from the missing data that may be\nspecialists or research nurses), had been in their post\nfor at least 2 years and had ongoing involvement in the\nIMPROVE trial. These experts were judged likely to\nhave knowledge about the prognosis and outcomes of\nthe trial patients, beyond that recorded in the data.\nDevelopment of the elicitation tool\nThe main purpose of the elicitation was to quantify dif-\nferences in the mean QoL score between patients who\ndid and did not complete QoL questionnaires.\nSpecifically, we asked the experts to provide their beliefs\nabout QoL for `typical' IMPROVE trial patients, stres-\nsing that these typical patients were similar according to\nobserved characteristics, and the only differences\nbetween them were the randomised arm and whether or\nnot they returned a completed QoL questionnaire.\nHogarth21 advised that `assessment techniques\nshould be designed both to be compatible with man's\nabilities and to counteract his deficiencies'. Following\nthis, our work builds on our and others' previous work\n(e.g. White et al.,22 Mason,23 and references therein)\nsuggesting the benefits of a graphical approach.\nWhereas previous work has elicited quantiles or other\nsummaries from experts and then provided graphical\nfeedback,24,25 we made our approach more intuitive\nand interactive, by allowing the expert to manipulate\nthe distribution directly from the start.\nWe developed an easy-to-use web-based elicitation\ntool using Shiny, a web application framework within a\nwidely used statistical software, R,26,27 which could be\nadministered by e-mail or in conference breaks. We\nminimised the administrative burden by collecting\ninformed consent electronically and offered a \u00a320\nAmazon gift card as a token of appreciation for com-\npleted surveys. `Good practice' recommendations for\neliciting expert opinion were followed, in particular by\nincluding a feedback question and allowing the experts\nto revise their answers.28\nThe scale for QoL scores for the elicitation exercise\nis the same as the original scale for the EQ-5D utility\nscore, multiplied by 100 for ease of completion. The\nexpert is provided with possible QoL scores for typical\npatients with six exemplar diagnoses on the scale\nto be familiar to our experts and spanned the QoL scale\n(see Figure 2). The QoL values for these diagnoses were\ntaken from published literature (see supplementary\nmaterial for details).\nThe questionnaire includes some free text questions\nasking the expert to explain the basis of their views, in\nterms of what they observed about the trial patients\nand any other reasons. These provided some useful\ncontext which we revisit in the discussion. The supple-\nmentary material provides further detail about the elici-\ntation questions.\nThe tool was pre-piloted to improve usability and\naccessibility to our target audience, using non-clinical\nLondon School of Hygiene and Tropical Medicine clin-\nical trials unit staff with no IMPROVE involvement.\nFollowing this, input was provided by the Trial\nManager and Chief Investigator. At their suggestion,\nfictitious cartoon patients (Alfred, Bill and Chris) were\nincorporated into the elicitation questionnaire, as an\naide-memoir of the typical IMPROVE patients.\nAt the pilot stage, we carried out face-to-face elicita-\ntions with four experts, representative of those selected\nfor the main elicitation. They took, on average, 20 min\nto complete the survey and provided feedback that led\nto wording changes for improved clarity. However, no\nmajor alterations to the structure of the tool were sug-\ngested, and the graphical approach with `sliders'\nreceived favourable comments.\nThe final version of the elicitation can be down-\nloaded from https://ajm-elicit.shinyapps.io/ElicitApp\nHighQ5, and screen shots from some of the key ques-\ntions are reproduced as Figure 2. The graph in the top\npanel accompanies the question to elicit likely QoL\nscores for a typical IMPROVE patient randomised to\nthe open repair strategy who did not complete the ques-\ntionnaire (Bill). At this stage, the expert has already\nbeen introduced to Alfred, a typical patient randomised\nto the open repair strategy, but who did return a com-\npleted QoL questionnaire: Alfred's score is known and\nmarked on the QoL scale. The expert is asked for the\nmost likely value of Bill's score and to indicate graphi-\ncally their uncertainty about this value: the blue curve\nchanges dynamically as the expert moves sliders. The\ngraph in the middle panel is for the corresponding ques-\ntion about a typical patient randomised to the eEVAR\nstrategy who did not complete their questionnaire\n(Chris). The bottom panel shows the feedback provided\nto the expert about the implications of their answers in\nterms of the differences between the QoL scores for\npatients who did not complete a QoL questionnaire and\nwere assigned to the OPEN (Bill) and eEVAR (Chris)\narms respectively, assuming that the elicited distribu-\ntions for the OPEN and eEVAR arms are not related.\nTo allow for the possibility that the elicited values in\nthe two arms are related, for example, a high QoL score\nin the OPEN arm makes a high score in the eEVAR arm\nmore likely, we asked about the score in the eEVAR arm\nagain, but this time provided the score for the OPEN\narm (see supplementary material for further details). By\neliciting this third distribution, we had sufficient informa-\ntion to formulate a joint prior for the two sensitivity\nparameters allowing for correlation between them.\nEliciting and synthesising expert opinion\nThe chief investigator emailed a participation invitation\nto all the experts identified as potential respondents,\nincluding a web link to the elicitation tool and the par-\nticipant information sheet. Weekly reminders were sent\nthroughout the following month and we offered a fur-\nther opportunity to complete the elicitation at The\nVascular Society Annual Scientific Meeting in\nThe sensitivity analysis approach required that the\nuncertainty in the individual responses was recognised\nFigure 2. Screen shots from the elicitation tool.\nand that these responses were then pooled. We first\nspecified individual bivariate normal prior distributions\nfor both of the sensitivity parameters using the\nresponses from each expert. Second, we combined the\nresponses across the experts using linear pooling,16\nwhich is a method of mathematical aggregation widely\nused in practice, calculating an average of the individual\ndistributions using equal weights. This was specified in\nour Bayesian models as a mixture of the bivariate nor-\nmal distributions for each expert using the WinBUGS\nsoftware.29 See supplementary material for examples\nand code.\nTo fully explore the sensitivity of the trial results to\na range of expert opinion, we formed a `community' of\npriors30 comprising three pooled priors (all experts, all\ndoctors and all nurses). To examine the sensitivity of\nthe results to the full range of diversity of opinion, we\nalso considered two individual priors according to the\n`most sceptical' expert (QoL score 0.2 higher for\nOPEN) and the `most enthusiastic' expert (QoL score\nResults\nExpert responses\nTable 1 summarises the characteristics for the 26\nexperts who completed the survey. Over half of the\nresponses were provided at the conference and almost\ntwice as many doctors as nurses responded.\nTable 2 reports the elicitation responses. Overall, for\na typical patient in the OPEN arm, the elicited QoL\nscores were lower versus the corresponding average\nscore from the observed data, with a mean difference\nof four units on the 0\u00ad100 scale. For patients with miss-\ning QoL, the mean elicited values were on average 11\nunits higher for patients in the eEVAR versus Open\nrepair arms. In general, the nurses tended to be more\noptimistic than the doctors about the expected out-\ncomes of OPEN patients (with incomplete QoL data),\nbut there was less difference in their views about the\neEVAR patients. Half the experts believed the QoL\nscores for non-respondents in the eEVAR and OPEN\narms were positively correlated, and all except one of\nthe others reported no correlation. The supplementary\nmaterial contains more detail.\nAs Figure 3 shows, for both trial arms, there is a\nwide diversity across the experts in the elicited QoL\nscores for patients with missing data. The bold black\nlines indicate the result of combining the views of all\nthe experts in each trial arm using linear pooling.\nImplications for the effectiveness of eEVAR versus\nOPEN\nThe results of our sensitivity analysis compared to the\ncomplete case and MAR analyses are reported in\nFigure 4 as (1) the posterior probability that the\neEVAR QoL at 3 months is at least 0.03 units greater\nthan the OPEN QoL, where 0.03 is the minimum clini-\ncally important difference,31 alongside (2) the posterior\ndistribution of the difference in the mean 3-month QoL\nscores between the two arms. The full posterior distri-\nbution is shown as a density strip, where the darkness\nat a point is proportional to the probability density.32\nThe estimated effect of randomised arm on average\nQoL score is generally similar across the alternative\napproaches to the missing data, but the sensitivity anal-\nysis resulted in substantially greater uncertainty about\nthis mean difference. That is, the credible intervals from\nthe MNAR are wider than following the MAR and\nTable 1. Summary of the experts' characteristics and knowledge of the IMPROVE trial results.\nAll Nursesa Doctorsb\nYears in current role: n (%c)\nFamiliarity with results: n (%c)\nReported treatment difference at\n3 months: n (%c)\naIncludes vascular nurse specialists, research nurses and a consultant vascular nurse.\nbIncludes consultant vascular surgeons, a consultant interventional radiologist and a vascular academic junior doctor acting as site trial coordinator.\ncPercentage of column total.\ncomplete case analyses. These wider credible intervals\nrecognise the variation within and across experts in the\nlikely differences in outcomes for patients with missing\nversus observed QoL data. The doctor and nurse sub-\ngroup results are broadly similar to the overall result.\nHowever, the extreme individual priors give markedly\ndifferent results and levels of uncertainty, for the `opti-\nmistic' expert the probability of a clinically important\ndifference in favour of eEVAR is 100% while the corre-\nsponding probability for the `sceptical' expert is 38%.\nDiscussion\nWe successfully developed and demonstrated a user-\nfriendly tool for eliciting the expert opinion required for\nrecommended sensitivity analysis for missing data. The\ntool uses existing open source software and can be\nadministered face-to-face or online, to elicit beliefs from\nreasonably large numbers of experts without imposing\nan undue burden. We have shown that the elicited views\ncan be converted into informative priors for the sensi-\ntivity parameters in a pattern-mixture model, allowing\nTable 2. Summary of elicited QoL scores for IMPROVE trial patients with missing versus observed data.\nAll Nursesa Doctorsb\nElicited scores: mean (SD)\nTypical OPEN arm patient, who did not return a completed QoL questionnaire\nTypical eEVAR arm patient, who did not return a completed QoL questionnaire\nDifferences in scores: mean (SD)\nCorrelation between QoL scores for non-respondents in the eEVAR and OPEN arms n(%d)\nSD: standard deviation; eEVAR: emergency endovascular strategy; QoL: quality of life.\nThe QoL scale is from 220 to 100, and mean (SD) is across experts.\naIncludes vascular nurse specialists, research nurses and a consultant vascular nurse.\nbIncludes consultant vascular surgeons, a consultant interventional radiologist and a vascular academic junior doctor acting as site trial coordinator.\ncExcludes one nurse who expressed almost complete uncertainty about the quality of life scores.\ndPercentage of column total.\nFigure 3. Individual and pooled prior distributions for patients randomised to eEVAR and open repair arms: (a) eEVAR: all experts\nand (b) OPEN: all experts.\nThin grey lines = individual priors, thick black lines = smoothed pooled priors across all experts.\nAlthough each individual prior has been elicited as a normal distribution, this restriction does not apply to the pooled priors which are a mixture of\nnormal distributions.\nfor correlation in the elicited values across the trial\narms. Trial data can then be re-analysed under different\nMNAR assumptions to explore the robustness of the\nresults.\nThis article contributes to the literature in several\nways. First, by providing a practical tool that can\nquickly elicit the views of a range of experts, this\nresearch will help make recommended approaches to\nsensitivity analyses accessible to a wide range of trial\nsettings. Second, the new tool goes further than those\ndeveloped previously, eliciting expert views about the\ncorrelation between the randomised arms in the out-\ncomes for those with missing data. Third, the article\ncontributes to knowledge about the relative effective-\nness of a potentially important intervention eEVAR\nversus open repair for patients with ruptured aortic\naneurysm. The sensitivity analysis builds on the previ-\nously published research in finding that eEVAR does\nincrease the mean QoL at 3 months post-randomisation\neven after recognising that data may be MNAR.\nA key reason that our elicitation exercise was suc-\ncessful was because it was undertaken alongside an\nactive trial and annual society meeting for the clinicians\ninvolved, which allowed for ease of access to experts\nwith the requisite knowledge about the patients with\nmissing data. Also, the study was carefully designed to\nfocus the elicitation exercise on gaining expert opinion\non the key parameters required to avoid creating an\nexercise that was too burdensome. The qualitative ques-\ntions allowed assessment of the experts' engagement in\nthe exercise, indicated a general consensus that eEVAR\npatients recovered more rapidly and provided reasons\nfor the missing outcome data according to unobserved\naspects that were therefore not accounted for in the\nMAR analysis. These included degree of physical and\npsychological recovery, personality of patient, lack of\nfamily support, financial pressures, family bereavement,\nsocial life, dislike of paperwork, forgetfulness, loss of\ninterest in the study and lack of appreciation of the\nimportance of completing the questionnaire.\nOur tool has been designed to be generally applica-\nble to RCTs with different designs and with alternative\nendpoints and can be extended in several ways. In\nIMPROVE as in other studies, there is interest in\nwhether the treatment effect is modified by subgroup,\nin this case according to age, gender and the Hardman\nindex which measures the patients' baseline severity. A\npotential extension would be to elicit expert beliefs on\nthe differences in average outcomes for the different\npatient subgroups. Similarly, in IMPROVE, as in many\ntechnology assessments, there is interest in the long-\nterm effectiveness of the intervention. While the experts\noffered the view that the gain in QoL for eEVAR ver-\nsus open repair would be maintained at 12 months\npost-randomisation, it would be helpful to extend the\nelicitation exercise to inform sensitivity analyses at mul-\ntiple time points.\nOur proposed elicitation tool is generalisable to\nother clinical trials by adapting both the set of ques-\ntions and response options. To encourage methods\nuptake, R code for implementing the expert elicitation\nis available in the online supplementary material.\nFigure 4. Difference in mean quality of life score at 3 months between randomised arm (eEVAR - open repair) for survivors.\nEach shaded rectangular strip shows the full posterior distribution of the difference in mean QoL at 3 months for survivors for one model run. The\ndarkness at a point is proportional to the probability density, such that the strip is darkest at the maximum density and fades into the background at\nthe minimum density. The posterior mean and 95% credible interval are marked.\n*the posterior probability that the eEVAR QoL at 3 months is at least 0.03 greater than the open repair QoL. 0.03 is the minimum clinically\nimportant difference.31\nFurthermore, the priors elicited from such primary\nresearch, as undertaken alongside the IMPROVE trial,\ncould be `borrowed' by future studies of the same inter-\nvention (e.g. other eEVAR trials) to explore the robust-\nness of their conclusions. Potentially, a series of\nreference priors for different disease areas could be\ndeveloped to facilitate MNAR sensitivity analysis with-\nout undertaking primary elicitation exercises.\nThe approach could be used at the design stage, uti-\nlising either previously collected priors or new priors\nelicited from the trial team. Combining these with the\nexpected level of loss to follow-up could provide an\nimproved estimate of the likely impact of missing data\non the trial's results. Hence, this approach could help\nimprove trial design, so that the study results are more\nrobust to anticipated levels of missing data.\nAn alternative approach is double sampling,33\u00ad35\nwhich seeks to collect additional information from\nthose whose data are missing. The validity of this\napproach depends on the (often untestable) assumption\nof outcome stability, and re-contacting patients may\nraise ethical and practical issues. However, in trials\nwhere the main concern is in missing data for outcomes\nother than QoL, where the assumption of outcome sta-\nbility is more plausible, it would be interesting to con-\ntrast the results of double sampling with expert\nelicitation.\nLimitations\nA potential limitation with sending by email is that\nthere might be compatibility issues between Rshiny and\nolder versions of web browsers. The flexibility of\nadministering the tool face-to-face helps address this\ndrawback.\nOur elicitation tool is intended to be widely accessi-\nble to clinical investigators, and to achieve this goal, we\nmade several simplifying assumptions. First, we\nassumed that individual expert opinion could be ade-\nquately represented as a normal distribution. The\nexperts at the pilot stage considered this assumption\nreasonable, and only two experts contributing to the\nmain elicitation indicated that the normal distribution\nwas restrictive. More generally, the elicitation literature\nsuggests that it may be preferable to avoid imposing a\nparametric distribution that artificially constrains\nbeliefs.36 In future, we therefore plan to extend our eli-\ncitation tool to allow greater flexibility in the distribu-\ntion of possible values. This restriction does not apply\nto the priors used in our models, for example, the\npooled priors are a mixture of normal distributions.\nSecond, the elicitation exercise was undertaken after\nthe primary analysis concerning the outcome of inter-\nest, QoL at 3 months post-randomisation, had been\npublished. Inevitably, experts' priors are informed by\nprior evidence and knowledge of the results of the trial\nin question, and related evidence and this may influence\ntheir views. Here, we found little difference between the\nsubgroup of experts who were aware of the published\nresults versus those who were not. Nevertheless, we rec-\nommend that this type of elicitation is carried out\nbefore the trial results are known if at all possible.\nWhen this is not possible, it is important that the analy-\nsis investigates the likely implications for interpretation\nof the results.\nFuture versions of the tool should improve on the\nwording describing the scenarios. In particular, in\nresponse to a reviewer's suggestion, we recommend\naltering the introductory sentences to the main ques-\ntions, to sharpen the distinction between the variance\nof an observation and the variance of a mean. For\nexample, before future use of the tool in Question 2, we\nwould replace the preamble wording (`the range of val-\nues which you believe are plausible') by the more pre-\ncise, correct, wording used in asking the question (`your\nopinion of the most likely quality of life score for Bill').\nWe anticipate that users of the tool will need to modify\nthe text and images. Such modifications need to main-\ntain this distinction.\nSummary\nThis article successfully demonstrates a general and\npractical approach, for eliciting expert opinion and con-\nducting sensitivity analysis to assumptions about miss-\ning data in clinical trials.\n"
}