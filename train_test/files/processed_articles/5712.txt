{
    "abstract": "Abstract\nIn 2004, Google embarked on a massive book digitization project. Forty library partners and billions of scanned pages\nlater, Google Book Search has provided searchable text access to millions of books. While many details of Google's\nconversion processes remain proprietary secret, here we piece together their general outlines by closely examining\nGoogle Book Search products, Google patents, and the entanglement of libraries and computer scientists in the longer\nhistory of digitization work. We argue that far from simply ``scanning'' books, Google's efforts may be characterized as\nalgorithmic digitization, strongly shaped by an equation of digital access with full-text searchability. We explore the\nconsequences of Google's algorithmic digitization system for what end users ultimately do and do not see, placing these\neffects in the context of the multiple technical, material, and legal challenges surrounding Google Book Search. By\napproaching digitization primarily as a text extraction and indexing challenge--an effort to convert print books into\nelectronically searchable data--GBS enacts one possible future for books, in which they are defined largely by their\ntextual content.\n",
    "reduced_content": "Original Research Article\nProducing ``one vast index'': Google Book\nSearch as an algorithmic system\nMelissa K Chalmers1 and Paul N Edwards2\n Keywords\nAlgorithmic system, digitization, algorithmic culture, Google, web search, scanning\nReading a public domain book on the Google Books\nwebsite is a mundane encounter with text on a screen.1\nIn the midst of this experience, the appearance of a hand\npresents an unsettling disruption (Figure 1). Positioned\nwithin the front matter of the Code of Procedure of the\nState of New York (1862), bright pink rubbers cover\nthree fingers. The hand bears a thick silver ring and\nmatching pink nail polish. The thumb has been partially\nerased, appearing as a brown, pixelated stripe. The words\n``Digitized by Google'' have been digitally tattooed on the\nhand's skin.\nMomentarily pulling back the curtain on Google's\ndigitization processes, the hand's presence draws atten-\ntion both to the book's print origins and to the human\nand machine labor required to transport (and transform)\nit from library shelf to laptop screen. This hand belongs\nto a contract worker hired by Google to turn the pages of\nmore than 20 million books digitally imaged through the\nGoogle Book Search Project since 2004. These fingers,\nskin, nails, and rings appear as visible traces of ongoing\nprocesses designed to obviate--and subsequently to\nerase--human intervention. The dream of automation\npersists, even as the materials resist.\nThe hand's ghostly presence also highlights the\nopacity surrounding Google's undertaking, a disjunc-\nture between the company's techno-utopian public\nrhetoric and the paucity of public access it provided\nto the technical specifics of digital conversion.\nEnvisioning a far-reaching public impact, Google\nCEO Eric Schmidt (2005) described the project's goals:\nImagine the cultural impact of putting tens of millions\nof previously inaccessible volumes into one vast index,\nevery word of which is searchable by anyone, rich and\n1University of Michigan School of Information, USA\n2Stanford University, USA\nCorresponding author:\nMelissa K Chalmers, University of Michigan School of Information, 105 S.\nEmail: mechalms@umich.edu\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License (http://\nwww.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nBig Data & Society\njournals.sagepub.com/home/bds\npoor, urban and rural, First World and Third, en toute\nlangue \u00ad and all, of course, entirely for free.\nYet the actual digitization proceeded under a cloud of\nsecrecy, leaving analysts such as ourselves to glean traces\nof the project's values and processes from public state-\nments, contracts, project webpages, blog posts, presen-\ntations, and patent applications--and sometimes from\nthe margins of the page images themselves.\nExisting research has investigated many aspects of\nGoogle Book Search (hereafter GBS), including its\ngoals, its outputs, and its intellectual property frame-\nworks (Samuelson, 2009). Scholars have considered\nGBS in the context of the corporate monopolization\nof cultural heritage (Vaidhyanathan, 2012), the history\nand future of the book as a physical medium (Darnton,\n2009), and the place of digitized books in knowledge\ninfrastructures such as libraries (Jones, 2014; Murrell,\nanalog\u00addigital conversion processes, while Google\nemployees Langley and Bloomberg (2007) and\nVincent (2007) have presented elements of Google's\ntechnical workflows to specialized technical research\ncommunities.\nHere we take a new tack, arguing that Google's\napproach to digitization was shaped by a confluence\nof technical and cultural factors that must be\nunderstood together. These include Google's corporate\ncommitment to the scalable logic of web search, partner\nselection parameters, the lingering influence of print\nintellectual property regimes, and the requirements of\nGoogle's highly standardized ``mass digitization''\nprocesses (Coyle, 2006). This article proposes an alter-\nnative descriptor, algorithmic digitization, intended to\nhighlight how the algorithms Google uses to scale and\nautomate digitization intertwine with the production\nlogic that governs GBS planning and execution.\nUnderstanding GBS as an algorithmic system\nforegrounds Google's commitment to scale, standar-\ndized processes, automation, and iterative improvement\n(Gillespie, 2016). These features must also be under-\nstood as negotiated translations of varied project, part-\nner, and corporate goals into executable workflows. We\nfirst examine how algorithms shape and structure the\nwork of digitization in GBS and consider the effects of\nalgorithmic processing on digitized books accessible to\nusers. We then explore the implications of Google's\nembrace of an algorithmic solution to the multiple tech-\nnical, material, and legal challenges posed by GBS.\nBeyond simply scaling up existing book digitization,\nGoogle's algorithmic digitization effort has had the\neffect of reimagining what the intended outcome of\nsuch a project should be--with important implications\nfor mediating digital access to print books.\nBooks as data: Digital hammer seeks\ndigital nails\nGoogle's corporate mission: ``to organize the world's\ninformation and make it universally accessible and\nuseful,'' has remained effectively unchanged since its\nfirst appearance on the company's website in late\nchiefly to web search, Google's core business. In\nDecember 2004, Google announced an extension to\nthat mission: a massive book digitization project in\npartnership with five elite research libraries.2 Since\nthen Google has worked with over 40 library partners\nto scan over 20 million books, producing billions of\npages of searchable text. In 2012, without any formal\nannouncement, Google quietly began to scale back the\nproject, falling short of its aspirations to scan ``every-\nthing'' (Howard, 2012). While it seems unlikely that\nGoogle will stop digitizing books completely or jettison\nits digitized corpus anytime soon, the project's future is\ncurrently unknown.\nFigure 1. Hands scanned by Google (New York, 1862).\n2 Big Data & Society\nTo Google, converting print books into electroni-\ncally searchable data was GBS's entire raison d'e\n^ tre.\nTherefore, Google constructed digitization as a step\nparallel to the web crawling that enabled web search.\nIn contracts with library partners, Google defined digi-\ntization as ``to convert content from a tangible, analog\nform into a digital representation of that content''\n(University of Michigan and Google, Inc., 2005). In\npractice, this conversion produced a digital surrogate\nin which multiple representations of a print book exist\nsimultaneously. Each digitized book is comprised of a\nseries of page images, a file containing the book's text,\nand associated metadata. Layered to produce multiple\ntypes of human and machine access--page images,\nfull-text search, and pointers to physical copies held\nby libraries--each of these elements was produced by\nseparate, yet related, processes.\nIntegrating human values--and labor--into\nalgorithmic systems\nAs with many Google endeavors, the company\nreengineered familiar processes at new levels of techno-\nlogical sophistication. From that perspective, Google's\nprimary innovation on libraries' hand-crafted ``bou-\ntique'' digitization models (which pair careful content\nselection with preservation-quality scanning) was to\napproach book digitization as it would any other\nlarge-scale data management project: as a challenge of\nscale, rather than kind. Susan Wojcicki, a product man-\nager for the project, contextualized Google's approach\nbluntly: ``At Google we're good at doing things at\nscale'' (Roush, 2005). In other words, Google\nturned book digitization into an algorithmic process.\nScaled-up scanning required a work process centered\nin and around algorithms.\nAlgorithms are complex sequences of instructions\nexpressed in computer code, flowcharts, decision trees,\nor other structured representations. From Facebook to\nGoogle and Amazon, algorithms increasingly shape\nhow we seek information, what information we find,\nand how we use it. Because algorithms are typically\ndesigned to operate with little oversight or intervention,\nthe substantial human labor involved in their creation\nand deployment remain obscured. Algorithmic invisi-\nbility easily slides into a presumed neutrality, and they\nremain outside users' direct control as they undergo\niterative improvement and refinement. Finally, the\nvast complexity of many algorithms--especially inter-\nacting systems of algorithms--can render their beha-\nvior impossible for even their designers to predict or\nunderstand.\nEmbedded in systems, algorithms have the power\nto reconfigure work, life, and even physical spaces\nSeaver (2013) calls for reframing the questions we ask\nabout algorithmic systems, moving away from\nconceiving of algorithms as technical objects with cul-\ntural consequences and toward the question of ``how\nalgorithmic systems define and produce distinctions\nand relations between technology and culture'' in spe-\ncific settings. Studying algorithmic systems empirically\nmay thus bring together several elements: the technical\ndetails of algorithm function; the imbrication of\nhumans (designers, production assistants, users) and\nhuman values in algorithmic systems; and the multiple\ncontexts in which algorithms are developed and\ndeployed.\nLike many contemporary digital systems, GBS\nintegrated humans as light industrial labor, necessary\nif inefficient elements of an incompletely automated\nprocess. Human labor in GBS was almost entirely phy-\nsical, heavily routinized, and kept largely out of sight;\nhuman expertise resides outside rather than inside\nGoogle's system. Partner library employees pulled\nbooks from shelves onto carts destined for a Google-\nmanaged off-site scanning facility (Palmer, 2005).\nThere, contract workers turned pages positioned\nunder cameras, feeding high-speed image processing\nworkflows around the clock (University of Michigan\nand Google, Inc., 2005). Directly supervised by the\nmachines they were hired to operate, scanning workers\nwere required to sign nondisclosure agreements but\nafforded none of the perks of being a Google employee\nbeyond the walls of a private scanning facility (Norman\nWilson, 2009). For the time being, at least, human\nlabor in book digitization remains necessary largely\nbecause of the material fragility, inconsistency, and\nvariety of print books.\nPreparing to digitize: Partnerships, goal alignment,\nselection\nMass digitization initiatives are often characterized as\noperating without a selection principle: ``everything''\nmust be digitized (Coyle, 2006). In practice, however,\npartnerships, scaling requirements, intellectual property\nregimes designed for print, and the particulars of\nbooks' material characteristics all challenged Google's\nuniversal scanning aspirations.\nobserved that cultural heritage institutions mostly\nunderstood the hows of digitization, even at moderately\nlarge scale. The main challenge, he argued, was to opti-\nmize processes. Lesk (2003) described the challenges of\nscale and efficiency more succinctly: ``we need the\nHenry Ford of digitization,'' i.e. an institution willing\nto invest vast resources in ``digitization on an industrial\nscale'' (Milne, 2008). Google stepped forward to\nassume this role.3\nChalmers and Edwards 3\nGoogle courted partners to provide content by\nincurring nearly all costs of scanning, while carefully\navoiding the repository-oriented responsibilities of a\nlibrary. Each partner library brought its own goals\nand motivations into the project. The New York\nPublic Library (2004) observed that ``without\nGoogle's assistance, the cost of digitizing our books\n-- in both time and dollars -- would be prohibitive.''\nOther partners spoke of leveraging Google's technical\nexpertise and innovation to inform future institutional\nemployed different selection criteria, from committing\nto digitize all holdings (e.g., University of Michigan) to\nselecting only public domain holdings (e.g., Oxford,\nNYPL) or special collections (later partners). Most\ndigitization contracts remained private, adding to the\nsecrecy surrounding Google's efforts.\nFull-text search quickly emerged as a kind of lowest-\ncommon-denominator primary functionality for the\nproject. Using the Internet Archive's Wayback\nMachine, we can see how Google incrementally mod-\nified language relating to the project's goals and\nmechanisms throughout its first year (Google, Inc.,\n2004b). The answer to the question ``What is the\nLibrary Project'' evolved from an effort to transport\nmedia online (December 2004) to a pledge to make\n``offline information searchable'' (May 2005) to a\nmore ambiguous plan to ``include [libraries'] col-\nlections. . . and, like a card catalog, show users informa-\ntion about the book plus a few snippets \u00ad a few\nsentences of their search term in context'' (November\nThe purpose behind these changes became clear in\nFall 2005, as the Authors Guild and the Association of\nAmerican Publishers filed lawsuits alleging copyright\ncreating a ``comprehensive, searchable, virtual card cat-\nalog of all books in all languages,'' it provided pointers\nto book content rather than access to copyright-\nprotected books. The company maintained that scan-\nning-enabled indexing constituted ``fair use'' under the\nU.S. Copyright Act (Schmidt, 2005; US Copyright\nchanged from Google Print to Google Book Search,\nreorienting users' frame of reference from the world\nof paper to the world of the electronic web (Grant,\n2005). The change attempted to correct any mispercep-\ntions that Google intended to enable access to\nuser-printed copies of books and to deemphasize the\nidea that the project was in the business of copying or\nof content ownership.\nSince December 2004, GBS has provided full access\nfor public domain books. Google consistently down-\nplayed this capability, maintaining that like a book-\nstore ``with a Google twist,'' readers would use it\nmainly to discover books rather than to actually read\nthem (Google, Inc., 2004a). Yet partners scanning\npublic domain books often referenced online reading\nas a benefit. This ambiguity perhaps contributed to\ncopyright-related concerns--and misunderstandings--\nduring GBS's early days (Carr, 2005; New York\nA means to an end: Image capture\nOnce it took custody of partner library books,\nGoogle deployed its own selection criteria. In a\n(rare) concession to the library partners tasked with\nstoring and preserving paper materials, Google used a\nnondestructive scanning technique. In patents filed in\neral high-resolution image capture systems designed\naround the logistical challenges posed by bound\ndocuments.5 The thicker the binding, for example,\nthe less likely a book is to lie flat. In flatbed or\noverhead scanners, page curvature creates skewed or\ndistorted scanned images. Book cradles or glass pla-\ntens can flatten page surfaces, but these labor-inten-\nsive tools slow down scanning and can damage book\nspines. Google addressed this page curvature problem\ncomputationally, through a combination of 3D ima-\nging and downstream image processing algorithms.\nThat decision shaped and complicated Google's\nworkflow.\nIn the patent schematic shown in Figure 2, two cam-\nsional images of opposing pages of a bound book\n(301). Simultaneously, an infrared (IR) projector\n(325) superimposes a pattern on the book's surface,\nenabling an IR stereoscopic camera (315) to generate\na three-dimensional map of each page (Lefevere and\nSaric, 2009). Using a dewarping algorithm, Google\ncan subsequently detect page curvature in these 3D\npage maps and correct by straightening and stretching\nScanning produces bitmapped images that repre-\nsent the pages of a print book as a grid of pixels\nfor online viewing. Unlike text, this imaged content\ncannot be searched and remains ``opaque to the algo-\nrithmic eyes of the machine'' (Kirschenbaum, 2003).\nAs a next step after scanning, Google might have\nadopted existing library-based preservation best prac-\ntices for imaged content. Or it could have created\nnew standards around 3D book imaging (Langley\nGoogle chose to transform the raw 3D page maps\ndescribed above--rich in information, but unwieldy\nfor end users due to file size and format--into\n``clean and small images for efficient web serving''\n4 Big Data & Society\nProducing a machine-readable index: Image\nprocessing\nFor GBS, then, imaging ultimately represented a key\nyet preliminary step toward text-searchable books on\nthe web. The project's image processing workflows thus\nacquired a dual imperative. It had to produce both (a)\ntwo-dimensional page images for web delivery, and (b)\nmachine-readable--and therefore searchable--text.\n``[O]ur general approach here has been to just get the\nbooks scanned, because until they are digitized and\nOCR is done, you aren't even in the game,'' Google\nBooks engineering director James Crawford observed\nis search. In a web search engine, crawled page content\nand metadata are parsed and stored in an index, a list\nof words accompanied by their locations. Indexing\nquickly became the key mechanism (and metaphor)\nthrough which Google sought to unlock the content\nof books for web search.\nTo produce its full-text index, Google converted\npage images to text using optical character recognition\n(OCR). OCR software uses pattern recognition to iden-\ntify alphanumeric characters on scanned page images\nand encode them as machine-readable characters.\nOriginally used to automate processing of highly stan-\ndardized business documents such as bank checks, over\nthe past 60 years OCR has become integral to organiz-\ning and accessing digital information previously stored\nThrough OCR, imaged documents gain new function-\nality, as text may be searched, aggregated, mined for\npatterns, or converted to audio formats for visually\nimpaired users.\nTanner et al. (2009) argue that by providing search\nfunctionality for large digitized corpora at low cost,\nautomated OCR systems have been a key driver of\nlarge-scale text digitization. GBS leveraged decades of\ncomputing research related to OCR. Through the\n1990s, boutique library digitization efforts had\naddressed the question of quality mainly by establish-\ning image-centric digitization standards (e.g., scanner\nspecifications and calibration, test targets, resolution)\n(Baird, 2003). Rooted in libraries' traditions of\nensuring long-term visual access to materials through\nreformatting (e.g., copying, microfilming), these prac-\ntices relied on labor-intensive visual inspection for qual-\nity control. By contrast, pattern recognition research\ndeveloped systems for algorithmically assessing quality,\nmeasured by accurate recognition of printed characters\nand document structure (Le Bourgeois et al., 2004; Lin,\nGoogle adopted this framing of digitization as a text\nextraction challenge, optimizing its processes to\nproduce the clean, high-contrast page images necessary\nfor accurate OCR. The GBS processing pipeline relied\nheavily on OCR to automate not only image processing\nand quality control but also volume-level metadata\nextraction. Google's Vincent (2007) described the\nFigure 2. System for optically scanning documents (Lefevere and Saric, 2009).\nChalmers and Edwards 5\ndigitized corpus as algorithmic ``document understand-\ning and analysis on a massive scale.''\nBooks bite back: Bookness as bug, not\nfeature\nIn their commitment to scale and standardized\nprocedure, algorithmic systems often prioritize system\nrequirements over the needs of individual inputs (e.g.,\nbooks) or users. Google's search engine, for example,\nhas come under criticism for failing to prioritize\nauthoritative or accurate search results. In December\n2016, the Guardian reported that a Google query on\n``Did the Holocaust happen?'' returned a Holocaust\ndenial website as the first result. A Google spokesper-\nson maintained that\n[w]hile it might seem tempting to fix the results of an\nindividual query by hand, that approach does not scale\nto the many different variants of that query and the\nqueries that we have not yet seen. So we prefer to\ntake a scalable algorithmic approach to fix problems,\nrather than removing these one by one. (Cadwalladr,\nGoogle's "
}