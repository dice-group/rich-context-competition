{
    "abstract": "Abstract\nHumans' ability to detect relevant sensory information while being engaged in a demanding task is\ncrucial in daily life. Yet, limited attentional resources restrict information processing. To date, it is\nstill debated whether there are distinct pools of attentional resources for each sensory modality\nand to what extent the process of multisensory integration is dependent on attentional resources.\nWe addressed these two questions using a dual task paradigm. Specifically, participants performed a\nmultiple object tracking task and a detection task either separately or simultaneously. In the\ndetection task, participants were required to detect visual, auditory, or audiovisual stimuli at\nvarying stimulus intensities that were adjusted using a staircase procedure. We found that tasks\nsignificantly interfered. However, the interference was about 50% lower when tasks were\nperformed in separate sensory modalities than in the same sensory modality, suggesting that\nattentional resources are partly shared. Moreover, we found that perceptual sensitivities were\nsignificantly improved for audiovisual stimuli relative to unisensory stimuli regardless of whether\nattentional resources were diverted to the multiple object tracking task or not. Overall, the present\nstudy supports the view that attentional resource allocation in multisensory processing is task-\ndependent and suggests that multisensory benefits are not dependent on attentional resources.\n",
    "reduced_content": "Article\nAuditory Stimulus\nDetection Partially\nDepends on Visuospatial\nAttentional Resources\nBasil Wahn and Supriya Murali\nInstitute of Cognitive Science, University of Osnabru\n\u00a8ck, Osnabru\n\u00a8ck,\nGermany\nScott Sinnett\nDepartment of Psychology, University of Hawai'i at M\nanoa, Honolulu,\nHawai'i, USA\nPeter Ko\n\u00a8nig\nInstitute of Cognitive Science, University of Osnabru\n\u00a8ck, Osnabru\n\u00a8ck,\nGermany; Department of Neurophysiology and Pathophysiology,\nUniversity Medical Center Hamburg-Eppendorf, Hamburg, Germany\n Keywords\nattentional resources, multisensory processing, vision, audition, multiple object tracking,\nmultisensory integration, load theory\nCorresponding author:\nBasil Wahn, Institute of Cognitive Science, University of Osnabru\n\u00a8ck, Germany.\nEmail: bwahn@uos.de\ni-Perception\njournals.sagepub.com/home/ipe\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sage-\npub.com/en-us/nam/open-access-at-sage).\nIntroduction\nThe environment provides far more sensory input than can be effectively processed by the\nhuman brain. Via a process called ``attention'' only the information that is currently relevant\nhas been shown that attentional selection is severely limited in resources (i.e., only a limited\namount of sensory input can be processed) within each sensory modality (Hillstrom, Shapiro,\n& Spence, 2002; Potter, Chun, Banks, & Muckenhoupt, 1998; Tremblay, Vachon, & Jones,\n2005) and that these limitations depend on the features of stimuli that are processed\n(Morrone, Denti, & Spinelli, 2002). However, a matter of debate in multisensory research\nis whether attentional resources for one sensory modality are shared with the resources for\nanother sensory modality or whether there are distinct attentional resources for each sensory\nmodality. The answer to this debate appears to be nuanced, as recent research has argued\nthat the allocation of attentional resources across sensory modalities is task-dependent (Chan\n\u00a8 nig, 2016). That is, distinct or shared attentional resources can\nbe found depending on whether tasks involve the discrimination of stimulus attributes (e.g.,\ndiscriminating pitch or color) or the localization of stimuli (Alais, Morrone, & Burr, 2006;\nwas found that attentional resources were shared when two spatial tasks are performed in\nseparate sensory modalities (Wahn & Ko\nfound to be distinct when a spatial task was performed together with a discrimination task\n\u00a8 nig, 2016), and also when two discrimination tasks were\nsuggest that spatial tasks rely on shared attentional resources regardless of whether they are\nperformed in the same or separate sensory modalities while the discrimination of stimulus\nattributes relies on distinct attentional resources for the sensory modalities. However, to date,\nhow this task dependency extends to other task types and other task combinations have yet to\nbe fully explored.\nWith regard to other types of tasks, such as a detection task, previous research on the\nphenomenon of ``inattentional blindness'' has investigated how performing a demanding\nvisual task negatively affects the ability to detect a task-unrelated visual stimulus (Mack &\nRock, 1998). However, it was also found that participants' ability to detect auditory stimuli\nwas lower when performing a demanding visual discrimination task in comparison with\nperforming a less demanding visual discrimination task, providing a cross-modal version\nof this phenomenon referred to as ``inattentional deafness'' (Macdonald & Lavie, 2011).\nSimilar results were found in a recent study (Raveh & Lavie, 2015), in which several\nexperiments consistently showed that auditory stimulus detection was affected by task\ndifficulty in a visual search task. As a point of note, a visual search task requires spatial\nattention as well as object-based attention (Eimer, 2014; Ghorashi, Enns, Klein, & Di Lollo,\n2010). That is, humans need to allocate attentional resources to locations in space (i.e., a\nspatial attention component of the task) and then discriminate whether the attended location\nis a target or distractor (i.e., an object-based attention component of the task; Eimer, 2014;\nThese findings (Macdonald & Lavie, 2011; Raveh & Lavie, 2015) suggest that attentional\nresources required for visual stimulus discrimination are shared with the resources required\nfor auditory stimulus detection. In an earlier study (Sinnett, Costa, & Soto-Faraco, 2006),\nauditory as well as visual detection ability was investigated when simultaneously performing\na discrimination task in the same or different sensory modality. In this example, when tasks\nwere matched in difficulty, it was found that auditory detection ability was better when a\n2 i-Perception\nvisual discrimination task was performed compared with performing an auditory\ndiscrimination task, suggesting that there are also distinct attentional resources for the\nvisual and auditory modalities. Taken together, previous research on detection task\nperformance suggests that auditory stimulus detection only in part relies on visual\nattentional resources. However, this research has predominantly investigated whether a\nsecondary task involving the discrimination of stimulus attributes (e.g., discriminating\ntarget from distractors in a visual search task) affects auditory detection task performance.\nAccordingly, the extent to which auditory detection task performance relies specifically on\nvisuospatial attentional resources has not been determined. That is, given a visual search task\nalso has an object-based attention component (i.e., discriminating targets from distractors), it\nhas not been explored whether performing a purely visuospatial task (i.e., without any\nrequirement to discriminate stimulus features) affects the ability to detect auditory stimuli\nto the same degree as the ability to detect visual stimuli.\nSo far, we have addressed the question of how attentional resources are allocated for\nsensory input that is processed by separate sensory modalities. Yet, sensory processing\nusually involves several modalities. Depending on how sensory input from multiple\nsensory modalities is received (i.e., whether it coincides in space or time), it is integrated,\nresulting in an enhanced perceptual sensitivity (e.g., stimulus features can be processed more\nof ongoing debate is whether the process of multisensory integration is dependent on\nattentional processes. That is, whether diverting attentional resources away from a task in\nwhich stimuli are typically integrated affects the multisensory integration process. Previous\nstudies have both found evidence for that the integration process is affected by attentional\nprocesses (Alsius, Mo\n\u00a8 tto\n\u00a8 nen, Sams, Soto-Faraco, & Tiippana, 2014; Alsius, Navarra,\nMozolic, Hugenschmidt, Peiffer, & Laurienti, 2008) or not (Gentile, Guterstam, Brozzoli,\nMacaluso, 2007), indicating several factors that potentially influence how attentional\nprocesses affect multisensory integration (Navarra, Alsius, Soto-Faraco, & Spence, 2010;\nTalsma, 2015). In particular, it has been proposed that the integration of linguistic stimuli\n(e.g., spoken and written words) is susceptible to attentional processes while the integration\nof nonlinguistic stimuli (e.g., simple tones and flashes) is not susceptible to attentional\nprocesses (Navarra et al., 2010; Wahn & Ko\n\u00a8 nig, 2015a). Similar to the task-dependency\nwith regard to the question of whether there are shared or distinct attentional resources\nfor the sensory modalities, it could be that the integration of stimuli from multiple sensory\nmodalities is also task-dependent. In particular, studies that did not find an effect of\nattentional processes on multisensory integration for low-level stimuli have predominantly\ninvestigated multisensory integration in localization tasks or discrimination tasks. Whether\nmultisensory integration in a detection task is affected when attentional resources are diverted\nto a spatial task is unknown.\nTaken together, the present study aims to investigate two research questions: (a) Are\nattentional resources shared or distinct when a detection task is performed in combination\nwith a visuospatial task? and (b) Is multisensory integration in a detection task affected when\nattentional resources are diverted to a simultaneously presented visuospatial task?\nWe investigated these two research questions using a dual-task paradigm, in which\nparticipants performed a visuospatial task (i.e., a multiple object tracking [``MOT''] task;\nPylyshyn & Storm, 1988) and a detection task alone or at the same time. In the detection task,\nparticipants responded to visual, auditory, or audiovisual targets. In the MOT task,\nWahn et al. 3\nparticipants tracked a subset of several randomly moving objects. Note that previous\nresearch has shown that the MOT task reliably taxes visuospatial attentional resources\n(Alvarez & Franconeri, 2007; Cavanagh & Alvarez, 2005; Wahn, Ferris, Hairston, &\nKo\n\u00a8 nig, 2016). With regard to the first research question, if attentional resources are\ndistinct for the sensory modalities, we hypothesized that performing the MOT task in\ncombination with an auditory detection task should lead to less interference between tasks\nthan performing the MOT task in combination with a visual detection task. Conversely, if\nattentional resources are shared for the visual and auditory modality, the interference\nbetween the MOT task and detection task should be equal regardless of the sensory\nmodality in which the detection task is performed. With regard to the second research\nquestion, if visuospatial attentional resources are required for multisensory integration in a\ndetection task, we hypothesized that the benefit from integrating stimuli (i.e., a higher\nperceptual sensitivity to detect multisensory stimuli than unisensory stimuli) would be\nlower or disappear completely when performing the MOT task at the same time with the\ndetection task compared with performing the detection task alone. Conversely, if\nmultisensory integration in a detection task is not dependent on visuospatial attentional\nresources, multisensory integration should not be affected by performing a MOT task\nsimultaneously with the detection task.\nMaterials and Methods\nParticipants\nTwenty students (M \u00bc 22.6 years, SD \u00bc 3.09 years, 14 female) were recruited from the\nUniversity of Osnabru\n\u00a8 ck to participate in the study in exchange for money or course\ncredits. The study was approved by the ethics committee of the University of Osnabu\n\u00a8 ck,\nand written informed consent was obtained from each participant.\nExperimental Setup\nParticipants wore headphones (Philips SHL3000WT 00) and were seated in a dark room at a\nmovements with a remote eye-tracking system (Eyelink 1000, monocular pupil tracking,\n500 Hz sampling rate). To calibrate eye position, we used a five-point grid. The calibration\nprocedure was repeated until the maximum error was below 0.7 visual degrees.\nStaircase Procedure Prior to the Experiment\nPrior to starting the experiment, in order to match performance levels in the detection task\nacross sensory modalities, participants' individual detection thresholds for the visual and\nauditory detection task were determined using a QUEST (Watson & Pelli, 1983) staircase\nprocedure. In the visual staircase procedure, participants saw a central black dot (0.14 visual\ndegrees) that flashed for 50 ms every second. The participants' task was to press the ``enter''\nkey on the keyboard whenever they detected the flash. A failure to respond was considered a\nmiss. Participants were instructed to press ``enter'' only when they detected the stimulus.\nDepending on whether the flash was detected or not, the contrast of the flash relative to the\nblack dot was either lowered or increased using the QUEST staircase procedure, aiming for a\n75% detection task performance. The contrast was changed using the RGB color code for the\nflash and was expressed in a percentage ranging from white (100%, RGB color code: 255,\n4 i-Perception\ncontrast was set to 30%, and participants received a total of 150 contrast changes.\nParticipants were offered a short break, after every 20th contrast change. Analogously, for\nthe auditory staircase procedure, a ``click'' sound was received every second via the\nheadphones lasting 50 ms (impulse tone; stereo sound; sampling frequency 44.100 Hz;\nsound pressure level \u00bc 3.8 dBA) that was adjusted downwards or upwards in loudness\ndepending on whether participants detected the sound or not. Matching the number of\ncontrast changes in the staircase procedure for the visual stimuli, a total of 150 sounds\nwere received. The loudness was expressed in a percentage as well and ranged from 0% to\n100% (sound pressure level \u00bc 3.8 dBA). After the staircase procedure was completed, the last\ncontrast and loudness values of the QUEST procedure were taken as the 75% detection\nthreshold for the visual and auditory detection performance. These thresholds were used\nas initial thresholds in the QUEST staircase procedures in the detection task conditions\nthat were performed in the actual experiment.\nAs a point of note, the stimuli in the staircase procedure prior to the experiment were\nreceived at a constant rate (i.e., every 1 s). Hence, participants could have predicted the rate\nof stimulus presentation and pressed the response key every second. Therefore, before the\nmain experiment, the experimenter was instructed to check the responses of the participant\nwhether there is an unusual pattern of responses (i.e., every stimulus was detected toward the\nend of the QUEST procedure). In case of such a response pattern, the participant was again\ninstructed to only respond when a stimulus was detected, and the initial staircase procedures\nwere repeated.\nIn the actual experiment, the stimulus onsets were jittered in time, leaving a minimum of\n1.7 s and a maximum of 2.5 s between stimulus onsets, and the number of stimuli received in a\ntrial was randomly varied between four and five as well.\nExperimental Procedure\nParticipants performed the detection task or the MOT task either separately or at the same\ntime in a within-subjects design. We will first describe each task (i.e., the detection task and\nMOT task) in turn and then describe how they were performed simultaneously.\nIn the detection task (Figure 1(a)), like in the staircase procedure prior to the experiment,\nparticipants either had to detect a white flash that always occurred within a black circle in the\ncenter of the screen (``VI'' condition) or a ``click'' sound (``AU'' condition) by pressing the\n``enter'' key on the keyboard using their right hand. In a third detection task condition, both,\nthe flash and click were presented simultaneously (``VIAU'') and had to be detected. In all\ndetection task conditions, the stimuli lasted 50 ms and occurred at random time points within\na trial, always leaving a minimum of 1.7 s and a maximum of 2.5 s between stimulus onsets.\nAs for the staircases described earlier, if participants did not respond within this time interval\n(i.e., indicate that they had detected a stimulus), then it was considered a miss. As a point of\nnote, after stimulus onset, we did not set a time limit after which responses would count as\nfalse alarms.\nFor each trial, the number of onsets was randomly varied between four and five. Thus,\nonsets of stimuli and the number of stimuli were not predictable to the participants. As a\ndependent measure in the detection task, we used a similar approach as in previous studies\ninvestigating attentional resources across sensory modalities (Alais et al., 2006; Arrighi et al.,\n2011). That is, while participants performed the detection task, stimulus contrast (VI),\nloudness (AU), or both simultaneously (VIAU) were again adjusted depending on whether\nparticipants detected the stimulus or not using a QUEST (Watson & Pelli, 1983) staircase\nWahn et al. 5\nprocedure. In particular, as a dependent measure, we estimated for each condition and each\nexperimental block the 75% detection task threshold using the QUEST. Note, at the start of\neach experimental block, the initial stimulus threshold was the 75% detection threshold\ndetermined in the QUEST procedure prior to the experiment for each participant.\nIn the MOT task (Figure 1(b)), participants had to covertly track a subset of three target\nobjects among 15 distractor objects. Objects were 1.06 visual degrees wide. At the beginning\nof each trial, the three targets turned gray for 2 s and then became indistinguishable from the\nother objects and started moving for 11 s. While objects were moving, they bounced off the\nscreen borders and repelled each other. Moreover, the direction and speed of movement was\nrandomly selected with a probability of 1% in each frame (average velocity was 2.57 visual\ndegrees per second and ranged between 1.71 and 3.42 visual degrees per second--the\nexperiment was run at a refresh rate of 100 Hz). Once the movement stopped, participants\nwere asked to determine which objects had originally turned gray (i.e., the targets) using the\nmouse. Feedback was given at the end of each trial.\nIn the single task conditions, only one (i.e., VI, AU, or VIAU) of the detection tasks or the\nMOT task was performed. In the dual task conditions, one of three combinations of the two\ntasks was performed. Namely, the MOT task was performed in combination with the visual\n(``MOT\u00feVI''), auditory (``MOT\u00feAU''), or audiovisual (``MOT\u00feVIAU'') detection task.\nNote, that in all dual task conditions, the detection task was performed only while the\nobjects in the MOT task were moving (i.e., simultaneously). During object motion,\nparticipants were instructed to fixate on the center of the screen. Moreover, the objects\nFigure 1. Experimental design overview. (a) Detection task: Depending on condition, participants were\nrequired to detect a white flash that always occurred within a black circle in the center of the screen (``VI'',\ntop row), a ``click'' sound (``AU'', second row), or both the flash and click when presented simultaneously\n(``VIAU'', third row), by pressing the ``enter'' key on the keyboard. In all detection task conditions, four to five\nstimuli were presented per trial that lasted 50 ms and occurred at random time points within a trial. (b)\nMultiple object tracking task: The trial logic is shown for performing the multiple object tracking task alone\n(top row), and in combination with the visual (MOT\u00feVI, second row); the auditory (MOT\u00feAU, third row); or\nthe audiovisual (MOT\u00feVIAU, fourth row) detection task.\n6 i-Perception\nnever moved through the fixation point, thus the visual detection task was spatially separated\nfrom the MOT task at all times. To keep all conditions perceptually identical, 18 objects were\nalways displayed regardless of whether participants were required to perform the MOT task or\nnot. That is, while participants performed the detection task alone, objects were still moving.\nThe experiment had a total of 21 blocks each consisting of 10 trials. After a trial was\ncompleted, participants initiated the next trial by pressing the ``space'' key on the keyboard.\nBlocks were presented in a pseudorandomized order. Within each block, participants were\nrequired to always perform the same condition. The condition was indicated at the start of\neach block. All seven conditions (VI, AU, VIAU, MOT, MOT\u00feVI, MOT\u00feAU, and\nMOT\u00feVIAU) were included in each set of seven blocks. We avoided repetitions of a\ncondition in consecutive blocks. After participants completed a set of seven blocks, we\noffered them an optional break. Prior to starting the experiment, participants were\ninstructed about the procedure of each of the seven conditions separately.\nIn total, the experiment took about 2 hr. Python was used to program and display the\nexperiment as well as extract the data.\nData Analysis\nPrior to data analysis, we excluded trials in which the participant's gaze deviated from the\ncenter by more than two visual degrees on average (total of 1.13% trials excluded, M \u00bc 3.23\nvisual degrees, SD \u00bc 1.48 visual degrees of excluded trials).\nFor the analysis, with regard to the detection task thresholds, we obtained the last\nthreshold value from the QUEST as an estimate of the 75% detection task threshold for\neach block and calculated the median threshold across all blocks. With regard to the MOT\ntask, we calculated for each trial the fraction of correctly selected targets and took the mean\nacross trials for this measure separately for each condition.\nGiven that statistical assumptions for parametric tests (i.e., normality) were frequently\nviolated, we used nonparametric tests throughout (e.g., a Wilcoxon-signed rank test) and\nboxplots to plot the data. All conducted tests are two-sided and as an effect size measure, we\nResults\nAre Attentional Resources Shared or Distinct for Sensory Modalities When a\nDetection Task is Performed in Combination With a Visuospatial Task?\nWe first separately assessed performance in the MOT and detection task. On a descriptive\nlevel (Figure 2(a)), it can be seen that MOT performance was negatively affected when\nperforming the MOT task in combination with the VI condition, while performing it in\ncombination with the AU condition did not affect MOT performance. With regard to\ndetection thresholds, to obtain a measure of how much the sensory detection thresholds\nchange between performing the detection task alone or in combination with the MOT\ntask, we divided the thresholds obtained from the QUEST staircase procedure in the dual\ntask conditions (MOT\u00feVI and MOT\u00feAU) by the thresholds obtained from the single task\nconditions (VI and AU). In this ratio, a value above 1 indicates that detection task thresholds\nincreased (i.e., performance was worse) in the dual task conditions relative to the single task\nconditions. On a descriptive level, we found that detection task thresholds increased for both\ntypes of detection task conditions (Figure 2(b)), suggesting that the MOT task interfered with\nthe ability to detect auditory as well as visual stimuli.\nWahn et al. 7\nFigure 2. Results overview. (a) Multiple object tracking performance (i.e., fraction correct of target\nselections) as a function of single task (multiple object tracking) and dual task conditions (MOT\u00feAU,\nMOT\u00feVI, and MOT\u00feVIAU). (b) Detection task ratios (i.e., threshold in the single task condition divided by\nthe threshold in the dual task condition--values larger than 1 indicate that the multiple object tracking task\ninterfered with the detection task) as a function of detection task conditions (AU and VI). (c) Interference\nbetween the multiple object tracking and detection task. For the interference, we first calculated\nperformance ratios for the multiple object tracking performance by dividing the single task condition\nperformance (MOT) separately by each of the dual task conditions (MOT\u00feVI and MOT\u00feAU). A value above\n1 indicates that the detection task interfered with performance in the MOT. To have a measure of the overall\ninterference between tasks, we calculated the differences from 1 for the detection task and MOT task ratios\nand added these differences, separately for each participant and condition. (d) Multisensory detection task\nratio (i.e., thresholds in the audiovisual detection task are divided by the unisensory detection task thresholds\n(VI and AU)). A value below 1 indicates that participants' perceptual sensitivities to detect stimuli benefits\nfrom receiving stimuli from two sensory modalities (i.e., vision and audition) compared with one sensory\nmodality (either vision or audition). Box plots are shown in all panels (1.5 of the interquartile range is used for\nthe whiskers).\n8 i-Perception\nHowever, to address the question of whether there are separate attentional resources for\nthe visual and auditory modalities, it does not suffice to look at performances of each task\nseparately as participants may devote more attentional resources to one task than the\nother. As a consequence, it could be that no decrease of performance is visible in one\ntask while a large decrease of performance is visible in the other task, making the\ninterpretation of results difficult. To counteract such an unequal distribution of\nattentional resources across tasks, we computed an overall score of interference between\ntasks. To achieve this, we first computed the relative change in MOT performance between\nthe single and dual task conditions by dividing the MOT performance in the single task\n(MOT) by the performance in the dual task conditions (separately for MOT\u00feVI and\nMOT\u00feAU). This calculation results in a ratio, in which a measure of above 1 indicates\nthat the detection task interfered with the MOT task. To compute an overall score of\ninterference, we calculated for each participant the difference from 1 for the MOT ratio\nand the detection task ratio, separately for each condition. The differences from 1 for each\nratio (i.e., MOT and detection task ratios) were then summed to have an overall score of\ninterference (for a descriptive overview, see Figure 2(c)). We found that the interference\np \u00bc .008, r \u00bc .58), indicating that tasks interfered in both dual task conditions (i.e.,\nMOT\u00feVI and MOT\u00feAU). However, the overall amount of interference between tasks is\nlarger when the MOT task is performed in combination with the visual detection task when\ncompared with the auditory detection task. Given the medians of the interferences (VI:\nMdn \u00bc 0.16 vs. AU: Mdn \u00bc 0.07), the interference is about twice as large in the VI\ncondition than in the AU condition, and a comparison between these conditions is\nsignificant (z \u00bc 2.09, p \u00bc .036, r \u00bc .47). Overall, results indicate that attentional resources\nare only partly shared when an auditory detection task is performed in combination with a\nMOT task.\nTo verify that the results given earlier are not due to a speed-accuracy trade-off, we also\ntested whether response times in the detection task differed between single and dual task\nconditions. We found no significant difference between the VI and MOT\u00feVI conditions (VI:\nsuggesting that there is no speed-accuracy trade-off between the single and dual task\nconditions. That is, participants did not achieve a higher detection sensitivity in the single\ntask condition by taking longer to respond in the single task conditions compared with the\ndual task conditions.\nAs an additional analysis, we also tested how performing the MOT task and detection task\nat the same time affected participants' ability to fixate on the center of the screen. For this\npurpose, we tested how the average deviations in eye fixation (measured in visual degrees)\nfrom the screen center differed between single and dual task conditions. With regard to how\neye fixations in the MOT task were affected by the additionally performed detection task, we\nfound that the average deviations did not significantly differ between single and dual task\nconditions when the VI detection task (MOT: Mdn \u00bc 0.98 visual degrees vs. MOT \u00fe VI:\nr \u00bc .10) was additionally performed. With regard to how fixations were affected in the\ndetection task when additionally performing the MOT task, we did not find any significant\ndifferences between single and dual task conditions as well (VI: Mdn \u00bc 0.85 visual degrees vs.\nWahn et al. 9\nIs Multisensory Integration in a Detection Task Affected When Attentional Resources\nAre Diverted to a Visuospatial Task?\nWith regard to the second research question, we first tested whether there is a multisensory\nbenefit in the single and dual task condition. For this purpose, for each participant, we first\ndivided the visual contrast threshold in the VIAU condition by the visual contrast in the VI\ncondition and the auditory loudness threshold in the VIAU condition by the auditory\nloudness threshold in the AU condition. We then averaged across these two ratios for\neach participant to have an overall estimate how the sensitivity improves in the\nmultisensory condition relative to the unimodal conditions (i.e., values below 1 would\nindicate a multisensory benefit). This procedure was also repeated for the dual task\nconditions. For a descriptive overview of these ratios, see Figure 2(d).\nWe found a multisensory benefit in both conditions (single: z \u00bc 3.47, p < .001, r \u00bc .78; dual:\nz \u00bc 3.21, p < .001, r \u00bc .72), indicating that participants' perceptual sensitivities to detect\nstimuli increased when receiving audiovisual stimuli compared with receiving only visual\nor auditory stimuli. To test whether this multisensory benefit is higher in the single than in\nthe dual task condition, we then compared whether these ratios significantly differed between\nthese two conditions. We found no significant difference between these conditions (z \u00bc 0.30,\np \u00bc .784, r \u00bc .07), suggesting that the magnitude of the multisensory benefit does not depend\non the available visuospatial attentional resources.\nWe also repeated this analysis for a more conservatively estimated ratio to assess the\nmultisensory benefit. In particular, we took the lowest estimated unisensory threshold for\neach participant (instead of the median one--as described in the ``Methods of data analysis''\nsection earlier) and then divided the multisensory threshold by these unisensory estimates.\nWith this more conservatively estimated ratio, we found a similar pattern of results: A\nsignificant multisensory benefit for the single task condition (Mdn \u00bc 0.91, z \u00bc 2.02,\np \u00bc .045, r \u00bc .45) and a trend toward significance for the dual task condition (Mdn \u00bc 0.94,\nz \u00bc 1.83, p \u00bc .070, r \u00bc .41). Comparing ratios between these conditions again yielded no\nIn addition, we also tested whether the multisensory benefit found earlier is due to the\nprocess of multisensory integration or could be alternatively explained by probability\nsummation. In the case of probability summation, participants' increased sensitivity in the\nbimodal condition could simply be due to the fact that they received two stimuli compared\nwith one of the them in the unimodal conditions, resulting in a higher sensitivity. In the case\nof multisensory integration, the bimodal sensitivity is assumed to be higher than estimated by\nprobability summation. For this purpose, we extracted the unimodal staircase sequences for\nthe VI and AU condition and took the maximum of the VI and AU responses for each\npresented stimulus. For instance, for a presented stimulus, if the participant would not have\ndetected the visual stimulus but did detect the auditory stimulus, then the (maximum)\nperformance would be that the participant still detected the stimulus. On the basis of these\ncalculated responses, using logistic regression, we fitted psychometric functions to the visual\ncontrast and loudness values, respectively, and extracted the 75% thresholds of these\nfunctions. We repeated this procedure also for the MOT\u00feVI and MOT\u00feAU conditions.\nIn addition to fitting psychometric functions to these simulated performances, we also fitted\npsychometric functions to the staircase sequences of all other conditions and extracted 75%\nthresholds. In line with the analysis earlier, we computed ratios between the unimodal\nthresholds and bimodal thresholds. In addition, we also computed these ratios for the\nsimulated bimodal thresholds. A descriptive overview is shown in Figure 3. We found that\nthe ratios for the actual data did not significantly differ from the simulated data\ncannot exclude the possibility that the multisensory benefit is due to probability summation.\nOverall, results suggest that multisensory benefits (i.e., a higher sensitivity for bimodal\nstimuli compared with unimodal stimuli) in a detection task are not affected by\nsimultaneously performing a visuospatial task.\nDiscussion\nIn the present study, we examined how attentional processes and multisensory processing are\ninterrelated. Specifically, we investigated two research questions: (a) Are attentional resources\nshared or distinct for the sensory modalities when a detection task is performed in\ncombination with a visuospatial task? and (b) Is multisensory integration in a detection\ntask affected when attentional resources are diverted to a visuospatial task?\nWith regard to the first question, we found that auditory stimulus detection is in part\ndependent on visuospatial attentional resources as the MOT task did interfere with auditory\ndetection task performance. However, results also suggest that the MOT task interfered to a\ngreater extent with performance on the visual detection task (i.e., about twice as much),\nthereby suggesting that there are at least partly shared attentional resources for vision and\naudition for the present task combination.\nThese findings dovetail with previous studies that have investigated the question of shared\nor distinct resources for these two sensory modalities using either detection or discrimination\ntasks (Alais et al., 2006; Sinnett et al., 2006). Specifically, these previous findings also\nindicated distinct attentional resources when two discrimination tasks or a detection task\nin combination with a discrimination task were performed in separate sensory modalities.\nThe present study extends these findings by showing that attentional resources are partly\nFigure 3. Multisensory benefit. Shown are multisensory detection task ratios (based on 75% thresholds\nfrom fitted psychometric functions) as a function of single and dual task conditions--separately for the actual\ndata and simulated data; 1.5 of the interquartile range is used for the whiskers in the box plots.\nshared for vision and audition when a detection task is performed in combination with a task\nthat taxes visuospatial attentional resources.\nFrom a neurophysiological perspective, the present findings align with previous studies\nthat have shown that there are more attentional resources available across sensory modalities\nthan within a sensory modality (Finoia et al., 2015; Haroush, Deouell, & Hochstein, 2011;\nRees, Frith, & Lavie, 2001). For instance, Rees et al. (2001) found neurophysiological and\nbehavioral evidence that visual motion processing was unaffected when performing either an\neasy or difficult auditory detection task. Similar to the present study, participants were\nrequired to process visual motion information while simultaneously performing an\nauditory detection task. However, in contrast to this previous research, participants in the\npresent investigation did not only passively view concurrent visuospatial information while\nperforming an auditory task but had to actively perform two tasks in two separate sensory\nmodalities. Nonetheless, we found less interference between tasks when tasks were performed\nin separate sensory modalities than within the same sensory modality, suggesting that future\nneurophysiological studies that require participants to actively and simultaneously perform\ntwo tasks (i.e., an auditory detection and a visuospatial task) would also find neural\ncorrelates of distinct attentional resources for the visual and auditory modalities.\nWith regard to the second research question, we found that the multisensory benefit (i.e., a\nbetter detection task performance in multisensory conditions than in unisensory conditions) is\nrobust against diverting visuospatial resources to a secondary task. However, we cannot\nexclude the possibility that this benefit is due to probability summation rather than the\nprocess of multisensory integration. These findings are in line with previous studies arguing\nthat for low-level stimuli (e.g., such as tones) multisensory benefits are not dependent on\nattentional resources (Wahn & Ko\nA number of potential confounds need to be addressed. Namely, one might argue that the\nbenefit of performing tasks in separate sensory modalities (i.e., vision and audition) in\ncomparison to performing them in the same sensory modality (i.e., vision) is due to\noculomotor limitations. However, it should be noted that participants were not required to\nrepeatedly switch their gaze between tasks to perform the visual detection task and the MOT\ntask at the same time--only distributing attentional resources between tasks were required as\nparticipants were instructed to fixate their gaze at the center of the screen.\nHowever, a confound that cannot be fully discounted is the requirement to continuously\npress a key in the detection task while this is not required in the MOT task. This motor\ncomponent in the detection task may cause interference between tasks that is independent of\nthe sensory modalities in which they are performed. Previous research investigating the\npsychological refractory period indeed has shown that attentional resources for performing\nactions in tasks are independent of the sensory modalities in which they are performed (Dux,\ncannot discount the possibility that the observed interference between the MOT task and the\nauditory detection task may be due to this motor component in the detection task. Given this\npossibility and if tasks without any motor component would have been performed, attentional\nresources for vision and audition could be completely distinct for the present task combination.\nFuture studies could address this point by using a dual task design that does not require\nparticipants to perform motor actions when performing two tasks at the same time.\nAnother point of note related the motor component in the task is that participants could\nhave adopted a strategy in which they estimate the rate of stimulus onsets in the detection\ntask and continuously press the key regardless of the contrast or loudness of the stimuli,\nrespectively. We did jitter the onsets and varied the number of the stimuli to counteract such\na strategy. However, we cannot fully exclude the possibility that participants nonetheless\nadopted such a strategy--also due to the fact that we did not include the possibility of\ncommitting false alarms in our experimental design. However, this limitation applies to all\nexperimental conditions that included the detection task. That is, systematic differences\nbetween conditions cannot be explained due to such a response strategy. Future studies\ncould counteract this limitation in the design by setting a time limit for responses after\nstimulus onset (e.g., 1 s) and consider responses after the time limit as false alarms.\nAs an additional point of note, previous findings have indicated that when performing two\nspatial tasks in separate sensory modalities that attentional resources are completely\noverlapping, suggesting that spatial processing is modality-independent (Wahn & Ko\n\u00a8 nig,\n2015a, b). Notably, the present detection task did not include any spatial uncertainty.\nThat is, the location where the visual or auditory stimuli would appear was kept constant\nthroughout the experiment. Future studies could investigate how the allocation of attentional\nresources across sensory modalities systematically changes with the spatial uncertainty where\na stimulus in a detection task appears.\nWith regard to the second research question, an alternative explanation for finding no\ndependency of multisensory benefit on visuospatial attentional resources could be that the\namount of attentional resources diverted from the detection task was too low. In particular, it\ncould be that a more difficult visuospatial task (e.g., a MOT task in which five targets instead\nof three targets need to be tracked) could have affected the multisensory benefit. However,\nprevious studies using a similar design (i.e., also a MOT task with three targets) in which the\noverall interference between tasks was higher also found that the increased demand of\nvisuospatial attentional resources did not affect the multisensory benefit (Wahn & Ko\n\u00a8 nig,\nAnother alternative account of our findings would be that the observed improvement in\ndetection task performance when receiving audiovisual stimuli is not due to the process of\nmultisensory integration but could instead be due to an alternation strategy. That is,\nparticipants could have chosen to always respond to the stimulus in the sensory modality\nthat they can detect more easily. However, this account seems unlikely given that detection\nthresholds were matched to equal performance levels for each sensory modality prior to the\nexperiment. Moreover, findings related to such an alternation strategy typically result in faster\nreaction times (Diederich & Colonius, 2004; Sinnett, Soto-Faraco, & Spence, 2008; Vroomen &\nGelder, 2000). In the present study, however, we found improved perceptual sensitivities--a\nperceptual benefit associated with the process of multisensory integration (Ernst & Banks, 2002;\n\u00a8 nig, 2016). That is, participants were\nable to detect audiovisual stimuli at lower stimulus intensities than in unisensory conditions.\nYet, the improved sensitivities in the audiovisual condition did not surpass our simulated\naudiovisual sensitivities which were estimated under the assumption that the auditory and\nvisual stimuli are processed independently. Hence, we cannot exclude that the visual and\nauditory stimuli were not truly integrated in the audiovisual detection task. A possible\nreason why stimuli might not have been integrated could be that they were not received from\nthe same spatial location (i.e., auditory stimuli were received via headphones, and visual stimuli\nwere presented on the screen). A future study could address this point by providing audiovisual\nstimuli from the same spatial location, using loudspeakers instead of headphones.\nOverall, the findings of the present study suggest that attentional resources required for\nauditory stimulus detection are partly shared with the attentional resources required\nfor visuospatial processing. Even though visuospatial processing resources are required for\nauditory as well as visual stimulus detection, multisensory benefits for audiovisual stimuli in a\ndetection task were not affected by diverting visuospatial attentional resources away from the\ndetection task.\nWith regard to future studies, relevant investigation could explore the extent spatial\nattentional demands in a different sensory modality than vision interfere with stimulus\ndetection. That is, in the present study, we investigated how increasing the demand of\nvisuospatial attentional resources affected auditory stimulus detection. A future study\ncould investigate to what extent a spatial auditory task would interfere with visual\nstimulus detection performance. Given that humans tend to have a processing preference\nfor visual sensory input (e.g., see the ``Colavita Effect'' Colavita, 1974; Hartcher-O'Brien,\nGallace, Krings, Koppen, & Spence, 2008; Hartcher-O'Brien, Levitan, & Spence, 2010; Hecht\nSpence, Parise, & Chen, 2012), it is possible that visual stimulus detection might be unaffected\nwhen an auditory spatial attention task is performed simultaneously. Moreover, given that\nthis visual preference effect is dependent on task demands (Chandra, Robinson, & Sinnett,\ncombinations (e.g., involving temporal order judgements) could yet yield a different pattern\nof results. For instance, performance in an auditory temporal order judgment task may be\nunaffected by visual attentional load while the reverse may not the case.\n"
}