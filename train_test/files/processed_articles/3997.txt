{
    "abstract": "Abstract\nRecently, philosophers and social scientists have turned their attention to the epistemological shifts provoked in estab-\nlished sciences by their incorporation of big data techniques. There has been less focus on the forms of epistemology\nproper to the investigation of algorithms themselves, understood as scientific objects in their own right. This article,\nbased upon 12 months of ethnographic fieldwork with Russian data scientists, addresses this lack through an investigation\nof the specific forms of epistemic attention paid to algorithms by data scientists. On the one hand, algorithms are unlike\nother mathematical objects in that they are not subject to disputation through deductive proof. On the other hand,\nunlike concrete things in the world such as particles or organisms, algorithms cannot be installed as the objects of\nexperimental systems directly. They can only be evaluated in their functioning as components of extended computational\nassemblages; on their own, they are inert. As a consequence, the epistemological coding proper to this evaluation does\nnot turn on truth and falsehood but rather on the efficiency of a given algorithmic assemblage. This article suggests that\nunderstanding the forms of algorithmic rationality employed in such inquiry is crucial for charting the place of data\nscience within the contemporary academy and knowledge economy more generally.\n",
    "reduced_content": "Original Research Article\nAlgorithmic rationality: Epistemology\nand efficiency in the data sciences\nIan Lowrie\n Keywords\nEpistemology, data science, algorithms, rationality, Russia, anthropology\nIntroduction\nContemporary data are big. In our moment, data col-\nlection is ubiquitous, and their storage and analysis are\nchallenges facing everyone from multinational indus-\ntrial conglomerates and governments to lone social sci-\nentists and would-be digital entrepreneurs. The\ncollection of data has been a central feature of govern-\nance since at least the development of statistics in the\ntion has become an increasingly digital affair since the\nemergence of operations research and computing fol-\nlowing the Second World War (Rose, 1991). However,\nthe past two decades have seen an explosive growth in\nthe sheer amount of data produced, tied to a radical\nmetastasis of modalities of their collection and manipu-\nlation. During this same period, new forms of intellec-\ntual and pragmatic attentiveness to data have emerged\nalongside the quantitative growth, and political eco-\nnomic salience, of data (Halpern, 2015). While each\ndiscipline must tarry with data in its own way, there\nhas also been the consolidation of a specific set of tech-\nniques and theories proper to the velocity, volume, and\nvariety of contemporary data. In this article, I show\nhow the collective intellectual practice of one group\nof data scientists is producing a coherent form of algo-\nrithmic rationality, irreducible to a congeries of prac-\ntices or scraps of theory borrowed from mathematics or\ncomputer science. This rationality is inextricable from\nthe emergence of big data infrastructures and deeply\nimbricated with our lately computational modernity\nforms of expertise emerge, circulate, and interact with\nRice University, USA\nCorresponding author:\nIan Lowrie, Department of Anthropology, Rice University, 6100 Main\nEmail: lowrie.ian@gmail.com\nBig Data & Society\njournals.sagepub.com/home/bds\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://\nwww.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nother forms is a crucial component of sociology of con-\ntemporary knowledge.\nThis paper is based upon 12 months of ethnographic\nfieldwork with Russian data scientists, centered on the\nnew department of computer science at the Higher\nSchool of Economics, founded with substantial intellec-\ntual and material support from Yandex, a web infra-\nstructure firm often referred to by my informants as\n``the Russian Google.'' During my time in Russia,\nI used loosely quota-based snowball sampling to\nidentify and collect interviews from around 80 data\nscientists, predominantly from Yandex and the\nHigher School, but also other universities, institutes,\nand corporations with active research programs in\ndata science. These interviews were divided more or\nless evenly between academic and industrial interlocu-\ntors, as well as across cohorts. Following IRB-\napproved protocol, the semistructured interviews were\none to two hours in length, digitally recorded (unless\nthe interviewee objected), translated and transcribed by\nthe author. Interviews were supplemented by partici-\npant observation of classroom instruction, scientific\nseminars, business meetings, and industry events, as\nwell as discourse analysis of scientific articles, publicity\ndocuments, and internal memoranda. The goal of this\ngranular, quantitative observation was to produce an\nintimate picture of the intellectual milieu, career trajec-\ntories, and daily work practices of Russian data\nscientists.\nAnthropologists are generally quite committed to the\nradical particularity of their field sites; the anthropology\nof expertise, specifically, has mostly focused on under-\nstanding the emergence of local knowledge (e.g.,\nstudy data science in Russia because of its specific\nplace within the local ecology of knowledge. Given the\nstagnation of Russia's extraction economy, elites from\nbusiness, education, and government are working fran-\ntically to build the institutional and infrastructural sup-\nports for a home-grown Russian knowledge economy.\nThis has been a difficult project, in part because of the\nhistorical privileging of the theoretical over the applied,\nand a relatively high barrier between academy and\nindustry, in virtually all sectors of the Russian science\nsystem. In this context, one venture capitalist explained\nto me that pushing the development [razvitie] of data\nscience was a clever way of transforming the ``human\nresources'' of excellence in fundamental mathematics\ninto the ``human capital'' of trained computing profes-\nsionals. Much of my fieldwork focused on the specific-\nally local institutional, pedagogical, and professional\ntransformations tied to this development project.\nWhen I asked my informants what was intellectually\nunique about Russian data science, however, I was\noften met with blank stares; when I brought this up\nin conversation, one data scientist friend helpfully\ntold me that this was likely because the question was\nboth ``rude'' and ``a little stupid.'' The academics and\nindustrial researchers with whom I worked considered\nthemselves full-fledged members of a robust, inter-\nnational scientific community (albeit somewhat self-\nconsciously marginal ones). Although I have come to\nmore or less share their belief that the operational core\nof data science is not subject to a great deal of geo-\ngraphical variation, it is important to be attentive to\nthe institutional and cultural fields within which this\ncore must inevitably become concrete. In expressing\ntheir view of themselves as deploying the epistemo-\nlogical and practical operations of data science, under-\nstood as a transnationally coherent form of inquiry, my\ninformants also articulated a fierce and particular\nstance toward that science itself, and a local vision of\nwhat it means to be a scientist. This essay, then, should\nbe read not as an exhaustive exploration of the epis-\ntemological dynamics of data science as a ready-made\nwhole, but rather as a sympathetic investigation of their\nintellectual project, of their attempts to build a coherent\nand epistemologically sophisticated investigation of\nalgorithms. That said, I think that the following ana-\nlysis of local forms of algorithmic rationality offers a\nrange of laterally portable analytics (Howe and Boyer,\n2015) capable of shedding light on the social and intel-\nlectual processes structuring data scientific work in\nother milieux.\nThere has lately been a flurry of writing in the\nhumanistic sciences about algorithms and data. Much\nof it has focused on technical transformations in the\nanalysis and storage of data. This literature has pro-\nduced critical insights into the design processes\nbehind technological infrastructures subtending big\ndata handling (Kockelman, 2013), and how these infra-\nstructures interface with broad structural changes in\ngovernance (Rouvroy and Berns, 2013), consumer cap-\nitalism (Carah, 2017), and scientific inquiry (Leonelli,\n2012). There is a complementary strand of inquiry that\nfocuses instead on how big data technologies face users,\nboth professional (Leonelli, 2014) and lay (Bucher,\n2016). However, there has been little attention paid to\nthe expertise of those who are building the epistemo-\nlogical frameworks within which these technologies are\nbeing developed and implemented. In focusing on this\nexpertise, my work is most closely in dialog with those\nworking to understand the epistemologies produced by\nand within big data architectures (Kitchin, 2014;\nbeen focused on the ways that the techniques of data\nscience restructure existing fields of inquiry. My own\nproject, by contrast, looks toward those knowledge\npractices that take algorithms, the mathematically\n2 Big Data & Society\nformalizable procedures for operating upon data, as\ntheir object of inquiry. This article, then, is not about\neveryone who calls themselves a ``data scientist'' (many\nof whom my informants felt to be simply technologists\nor analysts, concerned only with building machines or\nusing their outputs). Nor is it about the epistemological\nstatus of the predictions, classifications, or cluster ana-\nlyses produced by algorithmic assemblages. Rather, it\nfocuses on the epistemological dynamics of an inquiry\ninto algorithms themselves and the computational\nassemblages with which they are imbricated, as they\noperate among a specific group of scientists.\nIt should be said that, although they were committed\nto producing what they consistently called ``scientific''\nknowledge about algorithmic assemblages, the primary\nbusiness of most of my informants remained the iden-\ntification of structures and tendencies in data, and the\nprovision of actionable interpretations of such struc-\ntures and tendencies. While the elegance of a particular\nalgorithmic approach to a problem tout court might\nattract academic notice, even the most scholastic of\npractitioners are interested in technological questions\nof feasibility and expense, at least in terms of both\nthe practical difficulties posed by its working out in\ncode and the computational time required. Thus, the\nexpertise of most data scientists I worked with bridges\nthe conceptual and the pragmatic: to function as a\ncompetent data scientist requires familiarity with the\nmathematical operations in question, their implementa-\ntions in code, and the hardware architectures underly-\ning such implementations.\nGiven the constant pressure to minimize the\nexpenditure of processing time, the rationalization of\ncomputational and storage architectures, and the\nsystem-internal standardization of data handling proto-\ncols is as much a component of an elegant solution to a\nproblem as the algorithmic sophistication of the actual\nanalysis. This focus on expenditure and output might\nlead us to conclude that we are dealing here with tech-\nnology, which Lyotard calls ``a game pertaining not to\nthe true. . . but to efficiency,'' rather than science\nproper, which tends to remain a cultural system or\nalso Luhmann, 1996).1 Certainly, we must concede that\ndata science is dominated by the putatively technical\n``criterion of efficiency,'' that it is governed by the\nquest to find the ``best possible input/output equation''\nthat in data science a given ```move' is `good' when it\ndoes better and/or expends less energy than another,''\nrather than when it produces some ``truth'' about an\nMy informants, however, are firmly committed to a\nvision of themselves as scientists, and of their work as\nscience rather than engineering. Although they like\nit when things ``work,'' when their projects ``give\npeople something they can use,'' they most often told\nme they were interested primarily in doing ``good\nscience,'' in publishing ``serious papers,'' or in making\n``discoveries.'' Certainly, this commitment to a\n``science'' of algorithms is in part disciplinary boundary\nmaintenance, an effort to establish the intellectual basis\nfor the institutionalization of a new research field,\nseparate from the existing and locally well-\nestablished domains of mathematics, software engineer-\ning, or operations research. Their insistence that they\nwere focused on new knowledge about algorithms,\nrather than on building new machines or solving prob-\nlems, is also conditioned by a Russian intellectual\nmilieu that has long valued the abstraction of theoret-\nical science and basic research above applied science\nand engineering. It is also part of constructing and\nenacting their ``socially recognised capacity to speak\nand act legitimately. . . in scientific matters,'' which in\nthe contemporary global science system has consist-\nently meant being able to convincingly claim that one\nis producing truth rather than merely building or using\nAlthough their striving to purify data science of the\napplied and the technological is undoubtedly structured\nby these institutional and cultural forces, I saw no\nreason to doubt the sincerity of their intellectual com-\nmitment to a profound understanding of the objective\ncharacteristics of algorithms, rather than their enlist-\nment in the application of technological ``force''\ntheir science from technology, they cannot abandon\nthe criterion of efficiency altogether. This is in large\npart because, unlike the objects taken by many other\nsciences, algorithms can only be approached obliquely,\nthrough an evaluation of their functioning within\nextended computational assemblages. Further, the\ntechnical components of these assemblages themselves\noften emerge as objects of epistemic attention in their\nown right, as their function is implicated in the evalu-\nation of the assemblage as a whole. It is also, however,\ndue to the inevitable imbrication of these technical\ncomponents with not only the mathematical abstrac-\ntion of algorithms, but with real-world data and pro-\ncesses that are not fully compatible with the true/false\ncode of the science system. In short, my argument is\nthat my informants have repurposed the technological\ncriterion of efficiency as an epistemological standard in\nthe service of a new form of scientific inquiry.\nIn order to understand the forms of algorithmic\nrationality and epistemology at the core of this inquiry,\nthen, this article begins with a brief look at the concrete\nknowledge practices that comprise these data scientists'\nprofessional toolkits. With this background estab-\nlished, it moves on to a description of the epistemology\nthat dominates their investigation of algorithms, before\nturning to a discussion of data itself--the material upon\nwhich these algorithms operate.\nKnowledge practices\nWith few exceptions, the core operations of data-\nscientific inquiry are borrowed. This should not be\nparticularly surprising. Data science, situated as it is\nat the intersections of statistics, computer science, and\nmathematics, rather predictably sources many of its\ntechniques from these disciplines. It is interesting to\nnote that while most of these techniques and modes\nof expression have obtained a high degree of semantic\nabstraction and practical flexibility within data science,\nmany were in fact developed in highly specific, applied\ndomains of inquiry.\nIn taking up these techniques, however, it modifies\nthem, combines them, breaks them down, and recom-\nbines them in ways that render them strange when\nviewed from the vantage point of their home discip-\nlines. In both pedagogical and developmental contexts,\ntraditional statistical modeling techniques percolate\ntogether with a range of other mathematical\napproaches to characterization and prediction, drawn\nfrom fields such as graph theory, complex systems ana-\nlysis, and mathematical logic. The modes of inquiry\ndominant within data science, however, are drawn\nfrom a class of algorithmic approaches to classification\nand prediction known as machine learning. In machine\nlearning, to quote a definition that was widely para-\nphrased by my informants: ``a computer program is\nsaid to learn from experience E with respect to some\nclass of tasks T and performance measure P, if its per-\nformance at tasks in T, as measured by P, improves\nwith experience E'' (Mitchell, 1997: 2). To put it\nanother way, machine learning aims to build programs\nthat develop their own analytic or descriptive\napproaches to a body of data, rather than employing\nready-made solutions such as rule-based deduction or\nthe regressions of more traditional statistics. They do so\nthrough repeated trials, following each of which error is\nidentified and fed back into the system, adjusting the\napproach for each subsequent trial. (These trials are\ncostly: learning through iteration takes place in compu-\ntational time that always threatens to spiral out of con-\ntrol; this is another crucial reason that data science\ncannot eschew the criterion of efficiency.)\nToday, machine learning is a diverse congeries of\nalgorithmic approaches, software implementations of\nsuch approaches, and hardware configurations\ndesigned to handle such implementations. There is a\nwide range of algorithmic approaches to core machine\nlearning tasks, such as clusterization, classification, and\nprediction. These rest on quite diverse logical\nor mathematical foundations. Some of my informants\nat the Higher School, for example, are committed to\nadvancing a machine learning approach based on a\nrather obscure branch of mathematical logic called\nFormal Concept Analysis (e.g., Poelmans et al.,\n2013). What they all share, however, is the commitment\nto the conceptual understanding of machine learning\noutlined by Mitchell. While some data scientists are\npartisans of one or another approach to such learning,\nmost are flexible and willing to employ those techniques\nthat they feel most appropriate to the task at hand.\nThis is in part because there is another, orthogonal\nsense in which data science might be said to be a con-\ngeries of practices. While we might think of ``datafi-\ncation'' (Van Dijck, 2014) as a process of distilling\nthe complexity of the world into a homogenous digital\nmedium, the reality is that data-scientific inquiry not\nonly comprises a motley of mathematical approaches\nbut it also handles variegated forms of data. The pres-\nentation of homogeneity, I want to suggest, might\nultimately be understood not as a property of data,\nbut as an effect of the work of commensuration\n(Espeland and Stevens, 1998) done by data scientists\nto bring different forms of data together in constructing\nfunctioning algorithmic assemblages.\nThe formal differences between text and image are\nstark. Performing feature identification on a series of\nimages, for example, requires an altogether different\nconceptual and computational apparatus than per-\nforming network analysis on a series of interlinked,\ntextual objects. Although the final operations may be\nsubstantively the same (i.e. we might want to classify a\ncertain type of object or predict how individual objects\nmight change over time), my informants were clear that\nthe procedures for managing the coupling between data\nand algorithm were wildly different, requiring a fair bit\nof knowledge about the specificities of extracting\n``usable information'' from a given medium. This dif-\nference persisted, although at the conceptual rather\nthan technical level, when moving between objects\nthat, to me, initially seemed to be in the same class.\nIn a series of talks on network analysis, for example,\nparticipants would take off on flights of theoretical\nfancy, discussing the finer mathematical points of clus-\nterization, only to be grounded by the injunction to\n``speak concretely [govori konkretno]'' about the real\nthings and processes underlying the network: specific\nassertions, drawn from the graph-theoretical observa-\ntion of social networks made up of people interacting\nwith each other, simply rang absurd when applied to\nthe world of hyperlinked Wikipedia pages, and vice\nversa.\nI'd like to suggest that these different forms are not\nonly the result of the ``second-order, distinctively\nhuman activity'' of categorization or organization\n4 Big Data & Society\n(Larkin, 2015); rather, some distinctions, such as\nbetween images and text, seem to inhere in the formal\nqualities of the data themselves. At the same time,\nI agree with Larkin that these formal qualities cannot\nbe reduced to the characteristics of ``technical devices''\n(2015), but rather emerge through a series of interpret-\nive choices about how to apply such devices and struc-\nture information in ways that make it amenable to\nanalysis. In short, data are produced by specific,\nhuman methods of capturing the world, and this\ntechnological process of capture carves the world up\ninto various, more or less incommensurate forms of\ndata. This is a stronger claim than the commonplace,\nacceded to by most data scientists I spoke with, that\nthere is no such thing as ``raw'' data (Gitelman, 2013);\nform pushes us to think further about the internal\nontological and conceptual differences inherent in the\nprocess of making the word into data. As a conse-\nquence of this incommensurability as much as of its\nintellectual history, the field of data-scientific inquiry\nis also organized into clusters of purpose-built algo-\nrithms for different areas of inquiry, clustered together\nunder names such as ``computer vision,'' ``natural lan-\nguage processing,'' ``social network analysis,'' and so\non. The core epistemological principles and algorithmic\nprocedures animating such algorithms may be laterally\nquite similar, but the specific features of their imple-\nmentation in extended computational assemblages can\nbe quite distinct.\nThus, data science is structured by two cross-cutting\ndistinctions: between specific analytic approaches and\nbetween specific domains of problems. Many of the\ndata scientists I spoke with tended to specialize accord-\ning to either analytic approach or problem domain,\nleaving them more or less free to generalize along the\nother axis. That is to say, some might specialize in nat-\nural language processing, but graze freely from various\nfields of algorithmic approach. Others might be obses-\nsively committed to a certain class of algorithms, eager\nto test their applicability to a huge range of concrete\nforms and sets of data. Data scientists' competence,\nhowever, tends to extend somewhat in either direction,\nwith perhaps a preference for specialization according\nto type of data. Of course, we should not be taken to\noverstate the radicality of such specialization. Indeed,\nthe business of data science, in many cases, is finding\nways to smooth the frictions (Nafus, 2014) that build\nup as inquiry moves across different forms of data. My\ninformants devoted a great deal of time and effort to\nbuilding higher level computational abstractions cap-\nable of bringing together, for example, social network\ndata with purchasing data, or dashcam video with nat-\nural language analysis of driver speech. Indeed, they\nroutinely pointed to this ability to abstract over forms\nof data as a crucial and distinguishing component\nof their expertise; this work of commensuration is one\nof the key factors they identified as separating data\nscience from earlier forms of computational analysis\norganized around a specific field, such as computer\nvision, or mathematical approach, such as Bayesian\nstatistics.\nIn addition to these forms of scientific expertise, the\ndata scientists I met were generally more or less com-\npetent technicians and builders of infrastructure, as\nwell. They considered their professional competence\nto extend through the entire data pipeline, from collec-\ntion through analysis to visualization. The importance\nof material infrastructures for contemporary social life\nhas been well documented by anthropologists (Star,\nare a growing number of ethnographies exploring how\ndata infrastructures subtend and facilitate the deploy-\nment of expert and scientific knowledge (e.g., Leonelli,\n2014). However, there has been less focus on the emer-\ngence of infrastructural arrangements as the objects of\nscientific attention, as what Rheinberger calls epistemic\nobjects, in their own right.\nCertainly, those who considered themselves data sci-\nentists spent less time tinkering with infrastructure than\nthose who called themselves developers or engineers.\nThis is in large part because well-developed, functional\ncomputing architectures will tend to ``recede'' from\nview, to function as merely the ``technical, preparative\nsubroutines'' of data-scientific inquiry (Rheinberger,\nunderlying data scientific research also sometimes\nemerged as objects of intense scientific scrutiny in\ntheir own right. Most often, this happened when some-\nthing broke down. One researcher described his\nongoing contributions to Hadoop, an open-source\ninformation retrieval and storage framework, as the\n``selfish'' result of ``falling down the rabbit hole''\nwhen one of his algorithms failed to function properly\nwith an off-the-shelf implementation of Hadoop.\nHe just wanted to build something that would let him\n``get back to business.'' However, the emergence of the\ncomputing architecture as an object of epistemic con-\ncern does not only occur through the failure of infra-\nstructural components to function as ready-to-hand\ntools. Another of my informants, for example, set her-\nself a dissertation project focused primarily on develop-\ning big data architecture. She still considered herself to\nbe a data scientist, rather than engineer, and her\nadvisor agreed. She justified this self-identification\nthrough explicit reference to the criterion of efficiency:\nif data science is, after all, about learning how to create\nmore efficient algorithms, then part of the scientific\nproject is a ``rational'' approach to investigating the\n``relative efficiency'' of various ``technical things,\nlike storage,'' not just ``the mathematics'' behind data\nscience.\nI find this view compelling. In other disciplines,\nresearchers are sometimes disinclined toward infrastruc-\nture building precisely because it is viewed as low-status,\ntechnical work, ancillary to their primary scholarly aims\n(Leonelli, 2014). Given the high degree of integration\nbetween computational architectures and software\nimplementations of algorithmic approaches in data\nscience, however, it seems impossible to extricate the\n``scientific'' investigation of such approaches from the\n``engineering'' challenges posed by the computational\narchitecture, or even to clearly separate the algorithm\nfrom its assembly with data and computational architec-\ntures. As technical objects become objects of epistemic\nconcern, while the algorithms themselves recede from\nview, they confirm Star and Ruhleder's assertion that\ninfrastructure is ``fundamentally and always a relation,\nrefocusing, this inversion of the relationship between\ntechnical and epistemic objects did not fundamentally\ncomplicate their vision of themselves as scientists, intel-\nlectually and practically dedicated to producing new\nknowledge about the properties and function of algo-\nrithms; sometimes doing science means doing a bit of\nEpistemological standards\nThe sine qua non of mathematical reasoning, historic-\nally, has been the work of proof. Ian Hacking argues\nthat people have been drawn to mathematics in large\npart because ``they have experienced mathematics and\nThis experience, mostly, has been the experience of\nproof: proof that smacks us with the inescapability of\nits conclusions, that strikes as an intellectual coup de\nfoudre, rendering the unknown as the obvious. What\nmakes this kind of proof so exciting and so different\nfrom other forms of rationality is that it seems to\nbypass the mucky business of empirical inquiry, to pro-\nduce new knowledge directly from the mind itself.\nHowever, although it makes use of axioms and cer-\ntain procedures derived from the logical, deductive\ntraditions of mathematics, data science operates\naccording to a different set of epistemological standards\nand produces different truth experiences. The work of\nthe ideal-typical proof-building mathematician or the-\noretical physicist crescendos in a series of Aha!-\nmoments, of moments when her cognition snaps into\nclarity and rigor, when new knowledge has been pro-\nduced out of raw thought. The data scientist, however,\nmust content herself with the more pragmatic, some-\nwhat exasperated ``Finally!'' when her system begins to\nproduce useful outputs. (Anyone who has ever\ntinkered, though, will know that this somewhat paler\nform of cognition can be tied to its own, equally-intense\naffective investments.)\nIn data analysis, the goal is not proof of anything.\nThe algorithms employed either function well or they\ndo not. Whether we are engaged in classification, clus-\nterization, time series forecasting, or the visualization\nof networks, the goal is decidedly not the demonstra-\ntion of logical deduction from axioms. Neither, how-\never, are we searching for the elegant congruence of\nmathematical models to physical structures or dynamic\nprocesses that is the hallmark of a great deal of applied\nmathematics (cf. Steiner, 1998). While such forms of\nreasoning can and sometimes do figure in discussions\nabout the development of new algorithmic approaches\nto classes of problems, they ultimately play second\nfiddle to an altogether more pragmatic logic of feasibil-\nity, practicality, and computational time.\nOne way of thinking about the intellectual proced-\nures of mathematics is as the socially mediated inter-\npretation of structures (Wagner, 2010). These\ninterpretations are made and contested in mathematical\ndiscussions, which are dominated by logical criteria\nappropriate to the work of proof and refutation\n(Lakatos, 1976). Data science, by contrast, might be\nconsidered to be the collective development of algorith-\nmic approaches to data handling. Rather than through\nlogical proof and refutation, these algorithms are devel-\noped by their assembly within computational architec-\ntures and the social evaluation of their inputs and\noutputs with respect to some real-world set of tasks.\nThus, this form of inquiry cannot function except as\na by-product of the primary implementation of such\nassemblages in the real business of increasing the effi-\nciency of certain types of these tasks. This is why effi-\nciency has emerged as the operative logic of evaluation\nshared by my interlocutors working in both academic\nand industrial data science. Efficiency as a term is here\nchosen in part for its capacity and abstraction; what\ncounts as efficiency depends largely upon the concrete\nproblem space in which the data scientists is working at\nany given moment. As such, unlike proof and refuta-\ntion in mathematics, the discussions over efficiency that\nI witnessed frequently included stakeholders in the spe-\ncific domains operated upon by algorithmic\nassemblages.2\nThis is perhaps most obvious when the problems are\nthose of business: data science applications succeed\nwhen they lead to an increased rate in client retention,\nreturn business, or total cost of items in a customer's\nbasket. One informant suggested that, in such cases,\nthe scientific considerations of optimization are imme-\ndiately also business concerns, right? It's an area where\nit's hard to separate out whether you are doing\n6 Big Data & Society\nsomething in the name of doing good science, or in the\nname of doing good business, because they come\ntogether.\nIn most cases, an efficient application of industrial data\nscience is one in which the cost of the application is\noutweighed by the increased revenue in which it results.\nThis need not be by a huge margin. In sufficiently large\noperations, such as mobile telecom firms, marginal\nimprovements such as an increase in monthly client\nretention of 0.5\u00ad1% through targeted communications\ncan translate into millions of dollars in annual revenue.\nIn many academic or scientific contexts, we can draw\nsome relatively straightforward parallels to this pecuni-\nary logic. For example, we might measure efficiency by\nthe application of similarly external metrics: a decrease\nin false positives for cancer diagnoses, improvements in\nthe rate at which the clusterization of social networks\nbring out relevant features, or an increase in the rate of\nmeaningful results when topic sorting natural language\ntexts.\nUltimately, however, the logic of efficiency in both\ncases is much more immanent to the experimental\nsystem than either of these lists of examples might indi-\ncate. It turns on the cost of implementing a given algo-\nrithm in any particular software and computational\nsubstrate. Computational time costs money, but it is\nalso a finite resource to be managed by scientists\nwhose access to quality server time is often institution-\nally constrained. With this in mind, this focus on effi-\nciency might again seem to be a contingent result of the\ncurrent situation of data science within the contempor-\nary knowledge economy. We might object that, were\ndata science relieved of its requirement to be useful,\ngranted unlimited access to computational resources,\nand allowed to pursue its own ends as a ``pure'' science,\nwe might begin to see an altogether different, more\nrespectably ``scientific'' set of epistemological standards\nfor the evaluation of algorithms to emerge. This, how-\never, would be to somewhat miss the point. Efficiency is\nthe epistemological code through which data science\nproduces knowledge about algorithms. This is because\nalgorithms of data science always, constitutively,\npractical tasks.\nThe personal and professional interests of academic\ndata scientists may tend toward the purely theoretical,\nin that their object of inquiry may be the algorithmic\nprocess itself, rather than the data being manipulated at\nany given time. Indeed, as scientists, the consistency of\nthe algorithmic approach, its elegance and feasibility, is\ngenerally what is at issue in any given iteration of data\nscientists' inquiries. However, the investigation of such\nalgorithmic epistemic objects requires their working out\nin and through concrete engagements with real data.\nEngaging real data means engaging delimited problem\nspaces, and producing practical, useful results through\nsuch engagements is the only feasible way to produce\ninsight into the operations of the algorithmic propos-\nitions that ultimately form the epistemic core of such\ninquiry. Unlike the epistemic objects discussed by\nRheinberger (1997), algorithmic processes cannot be\ninstalled as the objects of an experimental system. We\ncannot substantivize and produce knowledge about an\nalgorithm as we can the protein synthesis pathways of\na given model organism. They must be approached\nobliquely, through their application as technical com-\nponents of systems of inquiry. The evaluation of their\nfunctioning as technical objects is the primary source of\nknowledge about algorithms qua epistemic objects. One\ncannot ``directly'' investigate an algorithm. Such an\ninquiry is a process that is essentially a by-product\nThis may seem quite abstract, so let us look at a\nconcrete example. Viktor3 is a Higher School professor\nwho works in computer vision, which he describes as\n``basically information extraction from images.'' This is\nhis central problem space: the development of algo-\nrithms that efficiently and reliably identifies and oper-\nates on the content of video and still images. Even more\nso than most academic data scientists that I spoke with,\nhe is deeply invested in the practical outcomes of his\ntheoretical research, one of which is road mapping:\nFor example, you have a camera on a car, and the goal\nis to map all objects, all types of objects that are observ-\nable from that car, to the map. Starting from simple\nroad infrastructure like traffic signs, and finishing to the\ntrees, poles, traffic poles, road markings \u00ad everything\nthat can influence the map.\nViktor is invested in the application of computer vision\nto a real world of problems. He thinks that mapping\nsoftware is both useful and interesting and has hopes\nfor a future with fewer car crashes and deaths on the\nroad. However, as a theoretical data scientist, his ultim-\nate goal is not this application; his output is, rather,\nimprovements to the class of algorithms which might\npotentially come to be incorporated in public or com-\nmercial mapping software. This class of algorithms\nfaces unique problems, because unlike in facial recog-\nnition, his other area of inquiry,\nThe objects seem simple, but. . . the requirements of\nprecision and reliability are much higher. For example,\nif you have a traffic maintenance service, they have to\ncheck that all traffic signs are in place and are clear and\nvisible from the road. . . Every traffic sign should be\naccounted for because if you don't. . . you open the\npossibility for some traffic accidents, and the service\nwill be held accountable. So, the precision should be\nmuch higher. And there are a lot of different looking\ntraffic signs, so the number of classes are much larger\nthan for example human face. And so the problem still\nexists. For some subtasks, for example if you want to\ndetect speed signs only, then it's ok. . . But if you have\ntwo hundred types of objects, then it's still not solved.\nProbably next, maybe, five, maximum ten years, it will\nbe solved. But right now, it's still not ready.\nHis goal, ultimately, is not to build a market-ready\nimage recognition machine, tailored to the specific\ndemands of road mapping. Rather, it is to ``get the\nalgorithms [he is] working with ready'' to tackle the\nproblem of mapping hundreds of classes of objects in\nreal, living data. It is the class of algorithms that is the\nobject of his epistemic attention. However, he cannot\nabstract the algorithms lying at the core of such appli-\ncations from those applications. He cannot remove\nthem from play and tinker with them on the side-\nlines--at least not without immediately sending them\nback out to the field. Without their ramification in\nextended, practical assemblages of code, data, and com-\nputational architecture, they would be inert objects.\nUnlike in some other branches of mathematics, where\nproofs and formulae are themselves open for discus-\nsion, here there is no internal metric by which to evalu-\nate improvements to the algorithms at hand. Instead,\nthe efficiency of a given system, measured through\naccurate identifications made in real-world data, is\nwhat Viktor uses to differentiate and evaluate various\nalgorithmic approaches to computer vision. Precision\nand efficiency, here, are standards immanent both to\nalgorithms and the practical tasks to which they\naddress themselves.\nThe companies who feed him the grist for his scien-\ntific mill in the form of real data sets are aware that\nViktor's primary goal is the investigation of new algo-\nrithmic approaches, not the development of industrial\napplications. They do not look to him either to do the\nwork of classifying the objects in any specific data set,\nor to once and for all solve the problems facing their\nown, industrial algorithmists. There are no ``formal\nrequirements'' attached to the data they send him:\n``they know that currently there is no perfect solution''\nand cannot expect immediate return on investment.\nThis does not mean that they are disinterested; rather,\nthey know that algorithmic science needs many people\nworking through many successive iterations on real\ndata sets in order to progress, and that ``if they provide\nthe data, then it's probable that the result will be sooner\nrather than later.'' More fundamentally, as stake-\nholders in the domain of road mapping, they are\nsubstantively involved in constituting the problem\nspace in which Viktor is working, and consequently\nin discussions over, ultimately, what constitutes an effi-\ncient algorithmic approach within that space. Even\nVictor, though who more than many of the academics\nI spoke to is invested in the eventual, practical applica-\ntions of his research--views practical trials as ``test-\nbeds'' for theoretical improvements, rather than as\nends in themselves. As he put it: he's a scientist.\nIn short, algorithms are the object of Viktor's\ninquiry. They are also, essentially, processes and\ntools. While we may be interested in knowing about\nthem in the abstract, about how they work, and articu-\nlating a scientific approach to their development and\nimplementation, we can only observe them in their\nfunctioning. While we can represent them in mathem-\natical language, and further elaborate them in a specific\nsoftware environment, without their enactment in a\ncomputational substrate and concrete articulation\nwith a specific set of data and its attendant problem\nspace, we cannot learn anything about them. They\nremain inert.\nHowever, I want to suggest (more speculatively) that\nthis is more robustly essential to the nature of algo-\nrithms than the preceding might imply. It seems to\nme that for data scientists, the test of an algorithm,\nof its elegance and conceptual unity, is not only\nfound in its syntactical parsimony but in its practical\neconomy. Ultimately, an algorithm is a crystallization\nand representation of cognitive processes that might\notherwise be performed by humans.4 Of course, few\nalgorithms aim to directly model human forms of cog-\nnition, although this latter provides a rich metaphorical\nrepertoire for commentary on their behavior. However,\nin confronting data, they all attempt to replace human\ncognition with cognition in a different computational\nsubstrate. There is nothing magical about algorithmic\ninformation processing: it simulates a particular form\nof searching through data. These algorithms are\nqueries. As such, it is neither scientifically interesting\nnor aesthetically satisfying if we come up with an algo-\nrithm that searches everything, that brute forces prob-\nlems by checking every possible outcome, pattern, or\nconfiguration. Rather, we're enamored by parsimo-\nnious approaches that let us search only try out a few\norganizational schemes or pathways, while still produ-\ncing reliably valuable insights. In short, algorithmic\neconomy is an evaluative standard that, like much in\ndata science, straddles ``materiality, mathematics, and\nstandard immanent to the fact of being an algorithm,\nrather than one imposed post facto by the practical\nconstraints of data science as a form of inquiry; this\nis the case to the exact extent (and no further) that\n``function'' is immanent to the fact of being an organ\nrather than being a framework imposed by the physi-\nologist or anatomist.\n8 Big Data & Society\nMaterial domain of operation\nSo far, I have been arguing that algorithms are the\nprimary objects of data-scientific inquiry. Data science\nis also, however, quite obviously about data. When I\nasked data scientists to tell me what data was, however,\nthe answers were surprisingly hollow: ``I can't tell you\nwhat data is, because I can't tell you what isn't data.''\n``Data is anything capable of being operated upon by\nan algorithm.'' The most ubiquitous answer, though,\nwas that data was just another word for ``information.''\nInformation, however, is a notoriously polysemic term\neven within specific communities of practice. Labeling\ndata as ``information'' doesn't ultimately tell us any-\nthing, doesn't do any particularly useful conceptual\nwork \u00ad which fact my interlocutors would be the first\nto admit. Empirically speaking, I think we can here at\nleast give data a synthetic definition both more con-\nstrained than and encompassing of the three versions\nI heard in my fieldwork: as they emerge within data\nscience, data are digital traces of real processes or\nobjects capable of being manipulated within computa-\ntional environments.5\nNow, data scientists already use the concept of digi-\ntal trace to describe at least a particular kind of\ndatum--namely, those that humans produce as they\nmove through computational modernity. These ``frag-\nments of past interactions or activities'' have lately fig-\nured quite prominently in the projects of consumer\ncapitalists and the national security state alike\ndescription of data, however, forces us to expand our\nconception of the trace: I want to think, with my\ninformants, about data as being composed of traces\nnot just of human activities, but of the world. That is\nto say, data can be understood as the marks that some\nsection of the world makes when it moves through\nsome recording field. The traces that emerge within\nthese fields, then, are both signs that retain indexical\nrelations of actual spatiotemporal contiguity to the\nobjects for which they stand (Peirce, 1906), and the\nmarks such objects produce within the graphematic\nspace installed at the core of data scientific assemblages\nboth ``real,'' in the sense of having definite relations to\nactual things in the world, and ``fictive,'' in the sense of\nbeing ordered and conditioned by the human-built\nsociotechnical systems within which they emerge.\nThese two faces of data are complementary, emphasiz-\ning their ontological and ontic aspects, respectively.\nIt should be immediately apparent that, given this\ndefinition, everything is always on the cusp of being\ndata. Given the vast array of techniques for shepherd-\ning analog material across the digital threshold and\nformalizing it as data, all that is required is a bit\nof money and some elbow grease. Indeed, my inform-\nants would often ask if I had ever considered applying\nalgorithmic approaches to the analysis of my own data.\nAt first, these questions struck me as rather odd, per-\nhaps not least because of my own insecurities about the\nrigor of my chosen disciplinary approach to analysis.\nThe first time this happened, I didn't really even know\nhow to respond properly.\n``I'm not really sure that I have data in the sense that\nyou mean, Sergei,'' I demurred.\n``Of course you do,'' he replied cheerily. ``It's just a\nvery small set of messy and perhaps not so useful data,\nbut we can always give it a try.''\nHe went on to explain that while my field notes were\nprobably a hopeless cause, being too far gone into the\nwilds of literary text to tell us about anything other\nthan my own idiosyncrasies, my interview data repre-\nsented a ``small'' but perhaps interesting collection of\n``natural language data, just waiting to be cleaned up\nand played around with.'' To integrate it with one of his\nongoing investigations into a new class of natural lan-\nguage processing machines, we would just have to type\nup the transcripts, tag them with a little metadata, and\nsee what we could see.6\nOther informants were less optimistic. One initially\nexpressed similar hopes for my corpus of interviews,\nuntil he found out that I was only planning on collect-\ning around a hundred or so; ``generally,'' he said: ``it's\nnot so interesting for us to work with any set of texts\nless than one or two orders of magnitude bigger than\nthat.'' (For comparison, he and his team had a current\nproject working on the set of all mathematics articles\npublished in Russian over the previous decade.)\nBeyond its form, then, the size of the data set mat-\nters. On the one hand, we can chart the growth of data\nsets quantitatively. Indeed, at the industry events that\nI attended it was de rigueur to have at least one slide\nchronicling the explosion in sources, types, and sheer\namount of data available to be manipulated over the\npast 10 or 20 years. Academics and industry people\nalike would frequently and not-so-subtly brag about\nthe magnitude of data to which they had access in\nboth presentations and informal conversation.\nHowever, as data grow, the problem space within\nwhich they may be approached does not always\nexpand linearly, but might instead undergo a series of\nrapid state changes, mutations, which signal genuine,\nqualitative shifts in their status as objects of inquiry.\nThat is to say, simple statistical approaches scale\nquantitatively with the size of the data set upon\nwhich they are operating. The reliability of the descrip-\ntions and inferences made by such approaches simply\nincreases, often linearly, as new data points are added\nto the set. For many of the algorithmic approaches\nemployed by data scientists, however, at certain levels\nof scale new operations and approaches become epis-\ntemologically and practically feasible. Once certain\nthresholds of bigness are passed, for example, machine\nlearning algorithms shift from producing nearly\nrandom clusterings or classifications to reliable, repeat-\nable identifications. New graph-theoretical approaches\nto network analysis become available as the amount of\ndata about relationships between members of the net-\nwork increases.\nOf course, the techniques that emerge as certain\nthresholds of quantity are passed could be applied to\nsmaller sets of data. As one industrial algorithmist put\nit, there was nothing preventing him from calibrating a\nneural network using a training set of 20 pictures of his\nfamily. However, the results of a machine trained on\nsuch an impoverished set would not merely be poor,\nbut profoundly ``incoherent.'' As he explained, knees\nmight be identified as faces, or the same image identi-\nfied as different people over successive iterations of the\nmachine. Conversely, the machine might ``overtrain,''\nbecoming excellent at performing categorization on his\nfamily, but incapable of extending its analysis to new\ndata. However, even in cases where such incoherencies\nresolve into coherence at a mathematically linear rate,\nthere are nevertheless hidden thresholds, subject to\ncase-by-case evaluation, at which new methods\nbecome appropriate. Part of data scientific expertise is\nbeing able to choose the appropriate algorithmic\napproach for the properties and scale of a given data\nset, precisely in the absence of firm, logical, generaliz-\nable rules.\nThese transformations speak at least partially to the\nHegelian-Marxist dictum that beyond certain thresh-\nolds, changes in quantity have the uncanny ability to\nresolve themselves into changes of state or quality (cf.\ncoherence described here diverges from Hegel's model\nin that they are emphatically not due to the ontological\nqualities of data. As I've said, data objectively scale\nlinearly and quantitatively: you have more, or fewer.\nYou know more, or less, about each datum. The trans-\nformations I observed, instead, are products of the\nsociotechnical systems in which data are embedded,\nof the questions being asked of them, the techniques\nemployed to extract answers to those questions, and\nthe forms of rationality evaluating those answers. It is\nimpossible to attribute such state changes in the prob-\nlem space adjacent to any given algorithmic assemblage\nto either epistemological or pragmatic criteria alone;\nindeed, I have been arguing that for data science\nthere is little difference between the two. In practice,\nas in the anecdote above regarding my own impover-\nished data sets, there are definite, discriminating, if ad\nhoc and disputable lines demarcating the lower\nquantitative bounds of the investigatory envelope for\nparticular classes of solutions.7 Charting the work of\nestablishing such boundaries remains a critical task for\nthe ethnography of data science.\nConclusion\nI have argued that there are two levels of inquiry opera-\ntive in my informants' project of building a properly\nscientific data science: the exploration of concrete\ndomains of applied tasks, on the one hand, and the\nepistemic inquiry into the nature and functioning of\nthe algorithmic assemblages used in such exploration.\nMy informants consistently stated their primary intel-\nlectual commitment to the latter. However, it is ultim-\nately parasitic upon the former. Practically speaking:\n``algorithms are inert, meaningless machines until\npaired with databases upon which to function''\nThese databases must come from somewhere. When\nteaching students new techniques, for example, or test-\ning out some new approach to a well-trod class of prob-\nlems, the data scientists I met preferred to work with\nmore or less public and well-characterized data sets,\nwhose properties and contours have already been estab-\nlished. Their students learned to work graph-theoretical\napproaches to network analysis, for example, on clas-\nsics such as the university karate club described by\nZachary (1977). One computer vision researcher I inter-\nviewed hired Mechanical Turk workers to clean and\nprepare training sets of images celebrity faces from\nGoogle image search; most researchers I knew, how-\never, disdained such sets, viewing them as at best teach-\ning tools, test beds for exploratory techniques still in\ntheir infancy, or as opportunities for controlled com-\nparison with colleagues' approaches to certain classes\nof problems. What they variously call ``real-world,''\n``lively,'' or simply ``interesting'' data sets almost\nuniversally entered their experimental assemblages\nfrom outside. While pedagogical activities and cer-\ntain forms of algorithmic research do employ well-\ncharacterized sets of data analogous, perhaps, to\ncertain well-characterized ``model organisms'' in\nlaboratory biology, the real work of data science is\ndone in its encounters with new data sets. There is a\nmanifold of reasons for this tendency. Perhaps the most\nsociological is, simply, that data sets come with their\nown problem spaces, and that any significantly worked-\nover data set has had its problem spaces relatively well\nplumbed. However, more epistemologically speaking,\nnovel data sets function as both the occasion for a\nrigorous test of existing algorithms and their implemen-\ntations and as a crucial, empirically chaotic ground for\nthe emergence of epistemic innovation. There is\n10 Big Data & Society\nsomething lively about the encounter between algo-\nrithms and new data.\nIt should be no great surprise, then, that data scien-\ntists, and their experimental systems, are voracious and\nomnivorous consumers of data; similarly, they are dedi-\ncated producers of structures that facilitate its circula-\ntion. Given the forms of inquiry proper to their\ndiscipline, it is also not unexpected to find these systems\ntending toward acting as quilting points for a varie-\ngated multitude of social processes. Their operations\nnecessitate certain forms of input, drawing together a\nwide range material from outside of the experimental\nsystem, which must retain its connection to that outside\nworld in order to fuel the analytic work of algorithmic\nassemblages. That is to say, the nature of this inquiry\nnecessitates a relationship with what my informants\nfrequently referred to as ``the world of applied tasks.''\nAs a consequence, they found themselves continually\nforced to develop relationships with business and gov-\nernment, to build the communicative infrastructures\n(Elyachar, 2010) that would ensure their continued\naccess to sufficiently novel data. Indeed, some of them\npursued careers in industry for no other reason that the\nease of finding new data sets and attendant problem\nspaces to explore.\nMy informants, generally, were more committed to\ntheir theoretical work than to the resolution of specific\nproblems. From this view, these communicative infra-\nstructures were part of the ground state for the investi-\ngation of the relative intellectual merits of various\nalgorithmic approaches. This perspective, however, is\nexceptionally misleading: it ignores that the criterion\nof efficiency dominating this investigation can never be\nentirely system internal. Rather, it must be cocon-\nstructed in dialog with other stakeholders in a specific\nproblem area. What constitutes successful road map-\nping, cancer diagnosis, or financial prediction is ultim-\nately an empirical question, requiring intellectual\nengagement with the social worlds of transportation,\nmedicine, or business. For this reason, it seems to me\nunlikely that my informants will ever totally succeed in\npurifying their practice of its ``technological'' or\n``applied'' character. Most of the dedicated researchers\nI met during my fieldwork, however, didn't spend a great\ndeal of time fretting about this remainder. For even the\nmost scholastic of them, efficiency is, simply and\ninescapably, the epistemological criterion best suited to\nthe collective investigation of the function of algorithms,\ntheir possible uses, and their modes of assembly.\n"
}