{
    "abstract": "Abstract\nThe present study used the two-level testlet response model (MMMT-2) to assess impact, differential item functioning (DIF),\nand differential testlet functioning (DTLF) in a reading comprehension test. The data came from 21,641 applicants into English\nMasters' programs at Iranian state universities. Testlet effects were estimated, and items and testlets that were functioning\ndifferentially for test takers of different genders and majors were identified. Also parameter estimates obtained under\nMMMT-2 and those obtained under the two-level hierarchical generalized linear model (HGLM-2) were compared. The\nresults indicated that ability estimates obtained under the two models were significantly different at the lower and upper ends\nof the ability distribution. In addition, it was found that ignoring local item dependence (LID) would result in overestimation\nof the precision of the ability estimates. As for the difficulty of the items, the estimates obtained under the two models were\nalmost the same, but standard errors were significantly different.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nArticle\nItem measures may be affected by person grouping factors\nsuch as gender, L1 background, and ethnic background,\namong others, as well as by item grouping factors such as\ncommon input or stimulus, common response format, and\nitem chaining, to name but a few. In either case, item diffi-\nculty is affected by a factor irrelevant to the main construct;\nhence, construct validity of the test composed of the items is\nthreatened. The effect of person grouping factors can be\nstudied through impact and differential item functioning\n(DIF) analysis, and the effect of item grouping factors can be\ncaptured by studying testlet effect.\nFair assessment requires invariance of measures across dif-\nferent samples within the same population. Males and females\nof comparable abilities, belonging to the same population, for\nexample, are expected to have equal probabilities of giving a\ncorrect response to any given DIF-free item. Furthermore, all\ntypes of statistical analyses in social sciences hinge upon the\nassumption of independence of observations. People or things\nthat are nested within a hierarchy tend to perform in a more\nsimilar way than people or things in other clusters, which\nresults in local person dependence (LPD). For example, the\nanswers that students in a given classroom or school give to a\nset of test items may be consistently more similar to those of\nthe students in other classes or schools. Violation of the local\nperson independence assumption leads to underestimation of\nstandard errors (SEs), which in turn results in spurious rejec-\ntion of null hypotheses (Hox, 2010). Multilevel models\n(MLMs) have been designed to handle the interdependencies\namong the data points. Interrelatedness among a set of items,\nreferred to as local item dependence (LID), can also pose\nsome problems when it is neglected. Possible causes of LID\ninclude a shared passage, response format (open-ended vs.\nmultiple choice), speededness, and practice effect. The con-\ncept of testlet has been proposed to capture LID (Wainer &\nKiely, 1987). Research has shown that neglecting testlet\neffect results in inaccurate estimation of reliability, person\nability, and item difficulty (Lee, 2004; Wainer & Lukhele,\nDifferent models have been proposed to study DIF (e.g.,\nlogistic regression, Mantel Hanzel, item response theory\n[IRT]\u00adbased models, and multiple indicators multiple causes\n[MIMIC] models). However, when local item independence\nis violated, DIF detection may be affected (Bolt, 2002).\nResearchers have taken one of the following approaches to\n1Vali-e-Asr University of Rafsanjan, Iran\nCorresponding Author:\nHamdollah Ravand, Vali-e-Asr University of Rafsanjan, Rafsanjan, Iran.\nEmail: ravand@vru.ac.ir\nAssessing Testlet Effect, Impact,\nDifferential Testlet, and Item Functioning\nUsing Cross-Classified Multilevel\nMeasurement Modeling\nHamdollah Ravand1\n Keywords\nDIF, DTLF, impact, MMMT-2, multilevel models\n2 SAGE Open\naddress LID: (a) Testlet data have been fitted to score-based\npolytomous IRT models such as the graded response model\n(Samejima, 1969), polytomous logistic regression (Zumbo,\nthese polytomous item response models, each testlet with m\nquestions is treated as an item with the total score of the\nitems within each testlet ranging from 0 to m. (b) An item-\nbased testlet response theory (TRT) model such as the two-\nparameter logistic TRT (2PL-TRT; Bradlow, Wainer, &\nWang, 1999), three-parameter logistic TRT (3PL-TRT;\nWainer, Bradlow, & Du, 2000), and the Rasch testlet model\nor one-parameter logistic TRT (1PL-TRT; W.-C. Wang &\nWilson, 2005) has been employed. (c) An item-based multi-\nlevel TRT model such as the three-level testlet response the-\nory model (MMMT-3; Jiao, Wang, & Kamata, 2005) or the\ntwo-level cross-classified testlet response theory model\n(MMMT-2; Beretvas & Walker, 2012) has been employed.\nScore-based approaches have been criticized on the fol-\nlowing grounds: (a) They do not take into account the exact\nresponse patterns of test takers to individual items within a\ntestlet; therefore, a lot of information would be lost (Eckes,\nin press); and (b) applying polytomous IRT models to cap-\nture testlet effect has been reported to lead to biased param-\neter estimates and substantial overestimation of reliability\nand test information values (Thissen, Steinberg, & Mooney,\nNeither polytomous IRT models nor TRT models permit\nsimultaneous modeling of both LID and LPD commonly\nencountered in educational assessment settings. In social and\nbehavioral sciences, typically, students are nested within\nschools, and schools are in turn nested within neighbor-\nhoods. Ignoring LPD might have consequences (e.g., biased\nparameter estimates) as serious as those of ignoring LID. An\nadvantage of item-based multilevel TRT models such as\nMMMT-3 and MMMT-2 is that besides taking LID into\naccount, they can also take into account dependencies as a\nresult of higher clustering of data. If data are item scores on\na test consisting of testlets administered to test takers in a\nschool, adding a fourth level to the MMMT-3 (e.g., Jiao,\nKamata, Wang, & Jin, 2012) or a third level to MMMT-2 can\ncapture the nested structure of the data. Another clear benefit\nof MMMT-2 and -3 is that they permit simultaneous assess-\nment of impact and DIF by adding person predictors to the\nmodel. But it is not possible to assess differential testlet func-\ntioning (DTLF) with MMMT-3. Instead, MMMT-2 by ben-\nefiting from the flexibilities of cross-classified multilevel\nmodeling can simultaneously test for impact, DIF, and DTLF.\nIn this formulation rather than having a separate level for\ntestlet, person and testlet are modeled at Level 2 using\ndummy-coded testlet indicator variables, as explained below.\nAbstruse technicalities of multilevel measurement models\n(MLMs) have prevented educational practitioners from apply-\ning these models where they are more appropriate. The present\nstudy is an attempt to demonstrate the application of MMM-2 to\nsecond-language data and discuss the implications of ignoring\nthe hierarchical structure of the data. To this end, an acces-\nsible presentation of MMMT-2 is presented. Then, the data\nare analyzed, and the implications of ignoring LID are\ndiscussed.\nIn MMM-2, items at Level 1 are cross-classified by testlets\nand persons at Level 2. For a test of q items and m testlets,\nthe log-odds of a correct response to item i for person j at\nLevel 1 is expressed in Equation 1:\nlog ( ) ( )\np\np\nX\nij\nij\nj j ij q j q ij\nq j q i\n-\n\n\n\n\n\n = + + +\n+\n- -\n+ +\n  \n\n\nj\nj q j q ij\nT j T ij Tmj Tmij\nX\n+ +\n+ + +\n- -\n\n\n\n \n( ) (m )\n,\nwhere X and T are item and testlet indicators, respectively. X\nand T are dummy-coded indicators with \"-1\" for the relevant\ntestlet and item and 0 otherwise. To identify the model, one\ndummy-coded item for each testlet is dropped. According to\nBeretvas and Walker (2008), for each testlet of q items, there\nare ( )\nq -1 dummy-coded item indicators; there is one refer-\nence item per testlet, and for the m testlets, there are m test-\nlet indicators; there is no reference testlet.\nThe Level 2 model with fixed item and testlet effects can\nbe presented as in Equation 2:\n\n \n \n \n \ndj j\nj\nq j q\nq j q\nmq j\nu\n=\n=\n=\n=\n=\n- -\n+ +\n-\n\n\n( ) ( )\n( ) ( )\n( ) (m\nmq\nT j T\nTmj Tm\n-\n=\n=\n)\n,\n \n \n\nwhere q0 to ( )\nq-1 0 represent the difficulties of Items q to\nq - 1 in the first testlet, ( )\nmq-1 0 represent the dif-\nficulties of Items q +1 to mq -1 in Testlet m ( q -1 and\nmq -1 indicate that for each testlet, one item is excluded\nfrom the model as the reference item), and T10 to Tm0\nrepresent the fixed effects of Testlets 1 to m. Therefore, the\nprobability that test taker j answers Item q in Testlet d cor-\nrectly is as follows:\np\nu\nqj\nqj j q Td\n=\n+ -\n=\n+ - - +\nexp( ) exp{ [ ( ]}\n.\n  \nAlgebraically, Equation 3 is equivalent to the Rasch model.\nPerson ability parameter, u j\n0 , corresponds to j . In MMMT-\n2, two aspects of item difficulty are modeled (Beretvas &\nWalker, 2012): difficulty as a result of the item effect, q0 ,\nand item-specific testlet difficulty, Td0. The item difficulty\nof the reference items in each testlet is equal to the relevant\ntestlet's difficulty. A notable difference in the conceptualiza-\ntion of testlet effect in MMMT-3 and MMMT-2 is that in\nMMM-3, testlet effect is person-specific, that is, it contrib-\nutes to person ability, but in MMMT-2, it is item-specific,\nthat is, it contributes to the difficulty of the items in the\nrespective testlets.\nRandom testlet effects can be modeled as presented in\nEquation 4:\n \n \n \nT j T T j\nT j T T j\nTmj Tm Tmj\nu\nu\nu\n= +\n= +\n= +\n\n,\nwhere uT j\n1 to uTmj represent random testlet effects.\nBased on fit indices of Akaike Information Criterion\n(AIC) and Bayesian Information Criterion (BIC) or signifi-\ncance of the testlet effect variance component, researchers\ncan decide whether a testlet with random effects fits better or\none with fixed effect. If the random effects for one or more\ntestlets are statistically significant, person-level predictors\ncan be incorporated into the relevant Level 2 equations to\naccount for the variance. Inclusion of Level 2 categorical\nperson predictors in the testlet effect equations is a test of\nDTLF. As Beretvas and Walker (2012) demonstrated, one of\nthe benefits of MMMT-2 is that it can simultaneously test for\nimpact, DIF, and DTLF.\nA MMMT-2 that tests for gender impact, DIF, and DTLF\ncan be expressed as shown in Equation 5:\n\n  \n \n\nj j j\nj j\nmq j mq\nT j\nFemale u\nFemale\n= +\n= +\n=\n- -\n\n\n( ) ( )\n=\n= + +\n= +\n \n \nT T j T j\nTmj Tm T j\nFemale u\nu\n\n,\nmq-1 0 represent the difficulty of Items 1 to q\nin Testlet m, T10 to Tm0 represent fixed testlet effects,\nuT j\n1 and uT j\nand T11 provide measures of gender impact, DIF, and\nDTLF, respectively. As is evident, Equation 5 tests for\nimpact, DIF, DTLF, and the random effect of testlets simul-\ntaneously. If, after the inclusion of gender, the variance for\nthe random effect of testlet is still significant, more person-\nlevel predictors can be included.\nAll DIF and DTLF effects are the result of a secondary\nfactor or dimension assessed by an item or a cluster of items\n(Zumbo, 2007). The random testlet effect uTdj in MMMT-2\ncorresponds to the secondary dimension that the testlet might\nbe measuring, which might be interpreted as construct-irrel-\nevant or as a complementary dimension. In other words, the\nrandom testlet effect is the effect of testlet on the difficulty of\nthe items within the testlet, which is due to the secondary\ndimension targeted by the testlet. However, the fixed testlet\neffect Td0 can be interpreted as the direct contribution of\nthe common stimulus to the difficulty of an item on the pri-\nmary dimension being measured by the item. In other words,\nthe fixed testlet effect represents the effect of testlet on the\ndifficulty of the items, which is due to the primary dimension\nintended to be assessed by the testlet. Therefore, as Beretvas\nand Walker (2012) argued, \" Addition of a fixed effect for\neach testlet permits more specific decomposition of a test-\nlet's contribution to an item's functioning in addition to an\nassessment of the person's testlet ability\" (p. 208).\nResearch Questions\nThe present study aimed to analyze testlet effect, impact,\nDIF, and DTLF in the reading comprehension section of the\nUniversity Entrance Examination (UEE) for applicants into\nEnglish Masters' programs at state universities in Iran. This\npart of the test measured test takers' ability to comprehend\nand respond to written academic passages. There were three\nreading passages with 7, 6, and 7 items, respectively. The\nreading texts, along with the items associated with them,\ndefined the three testlets to be analyzed.\nThe research questions in the present study were as\nfollows:\nResearch Question 1: To what extent do the reading pas-\nsages show testlet effects?\nResearch Question 2: Do the reading comprehension\nitems display gender and major impact and DIF?\nResearch Question 3: Do the reading comprehension\npassages display gender and major DTLF?\nResearch Question 4: How do the parameter estimates\nobtained under MMMT-2 compare with the ones obtained\nusing HGLM-2?\nMethod\nParticipant\nThe data for the present study came from all the applicants\nMasters' programs at state universities in Iran. UEE, as a\nsole selection criterion, is a very stringent test that screens\nthe applicants into English Teaching, English Literature,\nand Translation Studies programs at the MA level in Iran.\nThe UEE is administered once a year in February. The data\ncame from the 2012 version of the test. The participants\nwere Iranian nationals and mainly holding a BA degree in\nEnglish Literature and Translation Studies (88%). About\n12% of the participants held a BA degree other than\nEnglish.\n4 SAGE Open\nInstrument and Procedures\nThe UEE measures General English (GE) and content knowl-\nedge of the applicants. The GE section consists of 10 gram-\nreading comprehension items. The content knowledge part\nconsists of three separate parts with 60 items each. Each part\nis intended for the applicants to English Teaching, English\nLiterature, or Translation Studies programs.All the questions\nare multiple choice, and test takers have to complete both the\nGE and the content knowledge parts in 120 min, 60 min\neach. For the purpose of the present study, only the reading\ncomprehension data of the GE part were used. There were\nthree reading passages in the 2012 version of the test. The\nfirst passage was a relatively long passage of 720 words on\nnatural selection theory, followed by seven questions. The\nsecond passage, about 524 words, discussed wastes and the\nprecautions taken by governments against its harmful effects\nand was followed by six comprehension questions. The third\npassage, 584 words long, discussed the changes in the family\nstructure and function over the centuries, followed by seven\nquestions.\nData Analysis\nTo answer the research questions, MMMT-2 was estimated\nusing SAS PROC GLIMMIX (SAS Institute, 2006). PROC\nGLIMMIX is designed for generalized linear mixed models'\nestimation and is a flexible procedure commonly used for\nestimations in MMMs (Flom, McMahon, & Pouget, 2007).\nResults\nTestlet Effects\nTestlet effects obtained under MMMT-2 and MMMT-3 are\npresented in Table 1. As it was previously alluded to, under\nMMMT-3, the testlet effect variance is assumed to be con-\nstant (fixed) across all the testlets. Therefore, SAS PROC\nGLIMMIX generated one testlet effect variance for all the\nthree testlets under MMMT-3 and a separate testlet effect for\neach testlet under MMMT-2.\nRemember that the testlet effect variance indicates the\ndegree of LID among the items associated with a given testlet.\nWhen there is no LID, the testlet effect variance equals 0. The\nhigher the variance, the more the items of a testlet are locally\ndependent. In the absence of any commonly accepted criteria\nfor judging the magnitude of the testlet effect variance, it was\ndecided to follow two kinds of evidence available in the lit-\nerature: (a) evidence provided by simulation studies, wherein\nvariances lower than 0.25 are considered negligibly small\n(Glas, Wainer, & Bradlow, 2000; W.-C. Wang & Wilson,\nto empirical studies, which considered testlet effect vari-\nteria, the omnibus variance effect of 0.27, obtained under\nMMMT-3, was somewhat slightly above the criterion sug-\ngested by simulation studies and still below the guideline\nproposed by empirical studies. The interesting point is that\nthe MMMT-3 testlet effect is the average of the separate tes-\ntlet effects generated by MMMT-2 for each testlet.According\nto Table 1, the variance for Testlet 2 is negligible, judged by\ncriteria suggested by either simulation or empirical studies.\nThe variances for Testlets 1 and 3 are above the criterion sug-\ngested by simulation studies and below the guideline sug-\ngested by empirical studies, hence considered medium.\nImpact and DIF\nImpact refers to \"difference in person abilities as a function\nof some person-level predictors\" (Beretvas, Cawthon,\nLockhart, & Kaye, 2012). In the present study, two person\npredictors were added to the intercept model 0 j\n( ) in\nEquation 5 to test for impact. For gender, male test takers\nwere coded 0 and females were coded 1. As for major,\nEnglish majors were coded 0 and non-English majors were\ncoded 1. As MMMT-3 does not permit simultaneous impact,\nDIF, and DTLF tests, in what follows, the results generated\nby MMMT-2 formulation are provided. As Table 2 shows,\nthe impact of major was not significantly different from 0.\nOn the contrary, the impact of gender was significantly dif-\nferent from 0 (the estimate was at least twice its SE). The\nnegative sign of the estimate indicated that, on average,\nfemales had significantly higher ability estimates than males\n(as females were coded 0).\nDIF refers to significant difference in item difficulties\nacross different groups in the same population, which are\nmatched for ability. Differences in item difficulties and dis-\ncriminations are referred to as uniform and non-uniform DIF,\nrespectively. MMM formulations in general and MMMT-2\nTable 1. Testlet Statistics for UEE Reading Comprehension Section.\nTestlet No. of items Testlet variance SE\nNote. UEE = University Entrance Examination; MMMT-3 = three-level testlet response theory model.\nin particular permit only tests of uniform DIF. Table 2 pres-\nents major and gender DIF estimates for the items that have\nbeen flagged for gender and/or major DIF. The last item in\neach testlet was dropped as reference items (Items 7, 13, and\n20 for Testlets 1, 2, and 3, respectively), and Item 1 was not\nflagged for either gender or major DIF.\nAs Table 2 shows, all the items flagged for gender DIF\n(items with DIF estimates twice their SEs), except for Item\n12, have got negative signs, which indicate that they favored\nfemales (gender was coded 0 for females and 1 for males).\nFrom the 10 items flagged for major DIF, Items 2, 5, 6, and 8\nwith negative values favored non-English-language majors,\nand the items with positive values favored English majors\n(major was coded 0 for non-English-language majors and 1\nfor English majors).\nSources of DTLF\nThe concepts of DTLF refer to differential measurement\nproperties of a cluster of items for different groups in the\nsame population, conditioning on ability. As it was stated\nabove, testlet effect, under MMMT-2, is decomposed into two:\n(a) random effect, which corresponds to the secondary or added\ndimension assessed by a testlet, and (b) fixed effect, which rep-\nresents the contribution of the common stimulus to the diffi-\nculty of an item obtained from the primary dimension assessed\nby a testlet (Beretvas & Walker, 2012). In the MMMT-2 formu-\nlation, for identification purposes, one item is dropped per test-\nlet as the reference item, the difficulty of which is the fixed\neffect of the respective testlet. The fixed effects of the testlets\nand 3, respectively. As the estimates suggest, Testlet 1 was the\nhardest testlet followed by Testlets 2 and 3. In the present study,\nthe flexibility of MMM-2 was employed to investigate DTLF,\nDIF, and impact simultaneously. Gender and major DTLF\nresults are presented in Table 3.\nAs for major, all of the testlets were functioning differen-\ntially. Inspecting the right section of Table 3, the reader notes\nthat the most difficult testlet (Testlet 1) and the least difficult\none (Testlet 3) favored English majors, whereas the second\ntestlet was easier for non-English majors than for English\nmajors. Gender DTLF test, as shown in the left side of Table 3,\nrevealed that Testlets 1 and 3 were functioning significantly\ndifferently for males and females (the estimates are twice as\nlarge as the respective SEs). Testlet 1 was easier for females,\nwhereas Testlet 3 was slightly easier for males.\nOne of the benefits of MMMT-2 is that it permits assess-\ning DIF and DTLF in situations where the source of differen-\ntial functioning is unknown (Beretvas & Walker, 2012). This\ncan be done by including random effects for testlets and\nitems. If the random effect is significant, the researcher may\nwant to include a person-level predictor to capture the vary-\ning effect of items or testlets across test takers. The effects of\nTable 2. Gender and Major Impact and DIF Estimates.\nGender impact SE Major impact SE\nItem Gender DIF SE Major DIF SE\nNote. DIF = differential item functioning.\nTable 3. Gender and Major DTLF.\nTestlet Gender DTLF SE Major DTLF SE\nNote. DTLF = differential testlet functioning.\n6 SAGE Open\nunobserved sources of DTLF are presented in Table 1 above.\nAs the random effects of Testlet 2 were small, person-level\npredictors were added only to the testlet part of Equation 5\nfor Testlets 1 and 3. Addition of gender significantly reduced\nvariances with those in Table 1). As the new variances sug-\ngest, addition of gender reduced testlet effects to below 0.25,\nwhich are negligibly small, judged either by the criterion\nsuggested by simulation studies or by the guideline sug-\ngested by empirical studies. The amount of variation in test-\nlet effects explained by gender can be measured by computing\nhow much the testlet effect variance component has dimin-\nished between the model with unobserved and the model\nwith observed (here, gender) source of differential function-\nthe amount of variance explained can be computed for each\ntestlet as (testlet effect in the unobserved model - testlet\neffect in the model with predictors) / testlet effect in the\nunobserved model as follows:\nThese can be interpreted by stating that for the first and\nsecond testlets, 37% and 42% of the explainable variation,\nrespectively, are explained by gender.\nAddition of major as an observed source of DTLF also\nresulted in the reduction of testlet effect variances to 0.20\nand 0.29 for Testlets 1 and 3, respectively. The amount of\nvariance explained by major in each testlet is as follows:\nThe interpretation is that major accounts for 25% and 27% of\nthe explainable variance in Testlets 1 and 3, respectively. The\nrandom effect of Testlet 3 is still above the criterion suggested\nby simulation studies, implying that there are still unobservable\nsources affecting the testlet's variance, and more person predic-\ntors can be added to the model to account for the variance.\nComparison of Parameter Estimates Obtained\nUnder MMMT-2 and Two-Level HGLM Rasch\nModel\nTo investigate the effect of ignoring LID in the data, person\nability and item difficulty estimates along with their SEs\nwhich takes into account LID and the two-level HGLM\nRasch model (Kamata, 2001), which ignores LID, were\ncompared.\nA paired-samples t test was run to compare the ability\nestimates generated under HGLM-2 and MMMT-2. The\nresults showed that the mean ability estimates were not sig-\nwere divided into three groups: low, high, and mid, accord-\ning to their bachelor's grade point averages (GPAs). A sepa-\nrate paired-samples t test was run in each group to compare\nthe ability estimates produced by the two models. The results\nof the analyses are displayed in Table 4.\nAs is evident from Table 4, ability estimates generated by\nthe two models were significantly different at the two ends of\nthe ability distribution. At the lower end, there was a signifi-\ncant difference between the ability estimates produced by\nthan the estimates generated by HGLM-2, but the estimates by\nHGLM-2 showed more dispersion at this part of the ability\ndistribution. In the middle of the ability distribution, the esti-\nmates generated by the two models were almost the same. At\nthe upper end, the estimates produced by HGLM-2 (M =\nstudy was very large and with large samples even very small\ndifferences can turn out to become statistically significant,\neffect size was also calculated. Interpreted according to the\n= moderate effect, 0.14 = large effect), eta squared (one of the\nmost commonly used effect size statistic) statistics in Table 4\nshow slightly above-medium effect sizes for the difference\nbetween the ability estimates generated by the two models,\nboth at the lower and upper ends of the ability continuum.\nThe precision with which each of the two models esti-\nmated person abilities was investigated by comparing SEs of\nability estimates. As Table 5 shows, the SEs of the ability\nestimates generated by MMMT-2 and HGLM-2 were signifi-\ncantly different at all levels (lower, mid, and upper end) of\nthe ability distribution. Eta squared statistics provided in the\nlast column of Table 5 show very large effect sizes at all lev-\nels of the ability distribution.\nTable 4.Paired-Samples t Tests of Ability Estimates.\nGPA (binned) M t df\nSignificance (two-\nNote. GPA = grade point average; HGLM-2 = two-level hierarchical generalized linear model; MMMT-2 = two-level cross-classified testlet response\ntheory model.\nThe difficulty estimates produced by MMMT-2 (M =\nBut the precision with which difficulties were estimated by\nwas significantly different. The eta squared statistics (.7)\nindicated a very large effect size.\nSummary of the Results and Discussion\nThe present study investigated testlet effects within the con-\ntext of the reading comprehension section of the UEE for\nMA applicants into English programs at Iranian state univer-\nsities. Drawing on the unique benefits of multilevel measure-\nment modeling in general and MMMT-2 (Beretvas & Walker,\n2012) in particular, the present study simultaneously assessed\nimpact, DIF, and DTLF for males versus females and English\nmajors versus non-English majors, and investigated the\neffect of ignoring item clustering on the estimates of item\ndifficulty, person ability, and their associated SEs.\nThe first research question concerned the magnitude of\ntestlet effect for each of the three testlets in the reading com-\nprehension section of UEE. The result obtained under\nMMMT-3 indicated a medium testlet effect. According to the\nresults generated by MMMT-2, Testlets 1 and 3 showed\nmedium effects, and Testlet 2 showed negligible testlet\neffect. One of the interesting findings of the present study\nwas that the omnibus testlet variance obtained under\nMMMT-3 was exactly the average of the three separate test-\nlet effect variances generated by MMMT-2. As the variances\nfor the random effects of the Testlets 1 and 3 were of medium\nsize, gender and major were separately included in the model.\nInclusion of gender accounted for 42% and 37% of the\nexplainable variance in Testlets 1 and 3, respectively. Major\nexplained 25% and 27% of the explainable variances in\nTestlets 1 and 3, respectively.\nFrom among the TRT models capable of assessing testlet\nDIF, MMMT-2 uniquely permits assessing for both observed\nand unobserved (unmeasured) sources of DTLF. Along with\nthe separate addition of gender and major as measured\nsources of DTLF, the random effects of testlets were added to\nthe model, besides their fixed effects. Statistically non-sig-\nnificant random effect variances for Testlets 1 and 3 after\naddition of gender suggested that there were no latent (unob-\nserved) factors causing testlets to function differentially.\nMajor, separately added to the model, did not leave any sig-\nnificant testlet effect variance unexplained in Testlet 1 but\nleft some statistically significant variance in Testlet 3.\nTherefore, deeper inspection of Testlet 3 was needed to\nassess what other unmeasured factors might have been the\ncause of DTLF to be included in the model.\nGender and major impact assessment indicated that, on\naverage, females were significantly more able than males,\nbut English and non-English majors did not show significant\ndifference in their ability estimates. Nine items were flagged\nfor gender DIF and 10 items were flagged for major DIF. To\ntest for unobserved sources of DIF, besides fixed effect of the\nitems, their random effects could have also been included in\nthe model. To save time, it was decided not to employ this\nunique capability of MMMT-2 to test for unobserved sources\nof DIF in the items. Addition of each random effect would\nincrease processing time exponentially. As for DTLF, all the\ntestlets were flagged for major DTLF; Testlets 1 and 3\nfavored English majors, whereas Testlet 2 favored non-Eng-\nlish majors. Only Testlet 1 showed gender DTLF, which\nfavored females.\nTo answer the last research question, parameter estimates\nobtained under the model taking LID into account (MMMT-2\nmodel) and the model ignoring LID (HGLM-2) were com-\npared. The results indicated that the ability estimates obtained\nunder the two models were significantly different at the\nlower and upper ends of the ability distribution but they were\nnot significantly different in the middle. The results also\nindicated that ignoring LID would result in overestimation of\nthe precision of the ability estimates. As for the difficulty of\nthe items, the estimates obtained under the two models were\nexactly the same but SEs were significantly different, sug-\ngesting that ignoring LID results in lower SEs, hence overes-\ntimation of the precision of item difficulty estimates.\nThe findings regarding ability estimates, their SEs, and\nthe SEs of item difficulty estimates are in line with those of\nthe previous studies, which have shown that ignoring LID\nwould result in biased estimation of ability parameters at low\nand high ends of ability distribution (Ackerman, 1987;\nTuerlinckx & De Boeck, 2001) and underestimation of SEs\nTable 5.Paired-Samples t Test of Standard Errors of Ability Estimates.\nGPA (binned) M t df\nSignificance (two-\nNote. GPA = grade point average; HGLM-2 = two-level hierarchical generalized linear model; MMMT-2 = two-level cross-classified testlet response\ntheory model.\n8 SAGE Open\n(2001) also found that violation of LID leads to inflated item\ndifficulty estimates, a finding that was not corroborated by\nthe present study.\nAnother interesting finding of the present study was the\nfact that all the three items in the first testlet flagged for major\nDIF favored non-English majors, but the testlet as a whole\nfavored English majors. Had I employed a conventional dif-\nferential bundle functioning (DBF) method, the DIF and\nDTLF in Testlet 1 would have canceled each other out as they\ndisplayed differential functioning in opposite directions.\nUnlike conventional DBF, the cross-classified multilevel tes-\ntlet model (MMMT-2; Beretvas & Walker, 2012) partitions a\ntestlet's DBF into a part common to all items in a testlet (i.e.,\nDTLF) and the part that is unique to each item (i.e., DIF).\nLimitations and Suggestions for Further Research\nIn the present study, the author tested for DIF and DTLF. The\ncause of differential functioning of items and testlets was not\nfocused upon. Future research can inspect the items qualita-\ntively to find what qualities of the items or testlets might give\ntest takers of one gender or major an upper hand over the\nother. Qualitative inspection of the subskills required for\neach item along with the topic of the reading passages might\nshed some light on differential item and testlet functioning.\nThe present study investigated the effect of ignoring LID on\nperson and item parameter estimates. Future research can (a)\ncompare parameter estimates obtained under models with\nand without DIF and DTLF, (b) investigate the effect of\nignoring LID on DIF and impact detection, (c) investigate\nthe effect ignoring local person dependence on DIF and\nDTLF detection, and (d) investigate the effect of simultane-\nous ignoring of LPD and LID on DIF and DTLF detection.\nAs to the criterion for selecting the reference items, previous\nstudies have excluded the last item on a test or a testlet as the\nachieve continuity with the literature, I dropped the last item\nin each testlet in the present study. Future research can study\nthe effect of changing the reference item on parameter esti-\nmates, impact, DIF, and DTLF detection.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nAckerman, T. (1987). The robustness of LOGIST and BILOG\nIRT estimation programs to violations of local independence\n(Research Report 87-14). Retrieved from http://www.act.org/\nresearch/researchers/reports/pdf/ACT_RR87-14.pdf.\nBeretvas, S. N., Cawthon, S., Lockhart, L., & Kaye, A. (2012).\nAssessing impact, DIF and DFF in accommodated item scores:\nA comparison of multilevel measurement model parameteriza-\ntions. Educational and Psychological Measurement, 72, 754-\nBeretvas, S. N., & Walker, C. M. (2008). Use of the multilevel mea-\nsurement model for testlets for differential testlet functioning.\nUnpublished manuscript.\nBeretvas, S. N., & Walker, C. M. (2012). Distinguishing dif-\nferential testlet functioning from differential bundle\nfunctioning using the multilevel measurement model.\nBolt, D. M. (2002). A Monte Carlo comparison of parametric and\nnonparametric polytomous DIF detection methods. Applied\nBradlow, E., Wainer, H., & Wang, X. (1999). A Bayesian ran-\nBryk, A. S., & Raudenbush, S. W. (1992). Hierarchical linear\nmodels: Applications and data analysis methods--Advanced\nquantitative techniques in the social sciences. Newbury Park,\nCohen, J. (1988). Statistical power analysis for the behavioral sci-\nences. Hillsdale, NJ: Lawrence Erlbaum.\nEckes, T. (in press). Lokale Abh\u00e4ngigkeit von Items im TestDaF-\nLeseverstehen: Eine Testlet-Response-Analyse [Local item\ndependence in the TestDaF reading section: A testlet response\nanalysis]. Diagnostica.\nFlom, P., McMahon, J., & Pouget, E. R. (2007). Using PROC\nNLMIXED and PROC GLMMIX to analyze dyadic data with\nbinary outcomes. SAS Global Forum 2007. Retrieved from\nGlas, C.A.W., Wainer, H., & Bradlow (2000). MML and EAP esti-\nmates for the testlet response model. In W.J. van der Linden\n& C.A.W. Glas (Eds.), Computer Adaptive Testing: Theory\nPublishing.\nHox, J. J. (2010). Multilevel analysis: Techniques and applications.\nNew York, NY: Routledge.\nJiao, H., Kamata, A., Wang, S., & Jin, Y. (2012). A multi-\nlevel testlet model for dual local dependence. Journal of\nJiao, H., Wang, S., & Kamata, A. (2005). Modeling local item\ndependence with the hierarchical generalized linear model.\nKamata, A. (2001). Item Analysis by the Hierarchical Generalized\nLinear Model. Journal of Educational Measurement, 38, 79-\nLee, Y.-W. (2004). Examining passage-related local item depen-\ndence (LID) and measurement construct using Q3 statistics in\nan EFL reading comprehension test. Language Testing, 21, 74-\nPenfield, R. D., & Lam, T. C. M. (2000). Assessing differential item\nfunctioning in performance assessment: Review and recom-\nmendations. Educational Measurement: Issues and Practice,\nSamejima, F. (1969). Estimation of latent ability using a\nresponse pattern of graded scores (Vol. 17). Richmond, VA:\nPsychometric Society.\nSAS Institute Inc. (2006). SAS Version (9.2) [Computer Software].\nCary, NC: Author.\nSinger, F. M. (1998). Relations Lineaires entre Solutions d'une\nEquation Differentielle (with E. Compoint) [Linear relation-\nships between a differential equation solutions]. Annales des\nThissen, D., Steinberg, L., & Mooney, J. A. (1989). Trace lines\nfor testlets: A use of multiple-categorical-response mod-\nTuerlinckx, F., & De Boeck, P. (2001). The effect of ignoring item\ninteractions on the estimated discrimination parameters in item\nWainer, H. (1995). Precision and differential item functioning on\na testlet-based test: The 1991 Law School Admission Test as\nWainer, H., Bradlow, E., & Du, Z. (2000). Testlet Response\nTheory: An analog for the 3PL model useful in Testlet-\nBased Adaptive Testing. In W. Linden & G. W. Glas (Eds.),\nComputerized Adaptive Testing: Theory and practice (pp. 245-\nWainer, H., & Kiely, G. L. (1987). Item Clusters and Computerized\nAdaptive Testing: A case for testlets. Journal of Educational\nWainer, H., & Lukhele, R. (1997). How reliable are TOEFL scores?\nWainer, H., Sireci, S. G., & Thissen, D. (1991). Differential testlet\nfunctioning: Definitions and detection. Journal of Educational\nWang, W.-C., & Wilson, M. (2005). Assessment of differential\nitem functioning in testlet-based items using the Rasch Testlet\nModel. Educational and Psychological Measurement, 65, 549-\nWang, X., Bradlow, E. T., & Wainer, H. (2002). A gen-\neral Bayesian Model for testlets: Theory and applica-\nYen, W. M. (1984). Effects of local item dependence on the fit\nand equating performance of the three-parameter logistic\nYen, W. M. (1993). Scaling performance assessments: Strategies\nfor managing local item dependence. Journal of Educational\nZhang, B. (2010). Assessing the accuracy and consistency\nof language proficiency classification under compet-\nZhang, O., Shen, L., & Cannady, M.(2010, April). Polytomous\nIRT or testlet model: An evaluation of scoring models in small\ntestlet size situations. Paper presented at the Annual Meeting\nof the 15th International Objective Measurement Workshop,\nBoulder, CO, USA.\nZumbo, B. D. (1999). A handbook on the theory and methods of\nDifferential Item Functioning (DIF): Logistic regression\nmodeling as a unitary framework for binary and Likert-type\n(Ordinal) item scores. Ottawa, Ontario, Canada: Directorate\nof Human Resources Research and Evaluation, Department of\nNational Defense.\nZumbo, B. D. (2007). Three generations of DIF analyses:\nConsidering where it has been, where it is now, and where\nAuthor Biography\nHamdollah Ravand is an assistant professor at Vali-e-Asr University\nof Rafsanjan where he is teaching Masters' courses of Research\nMethods in TEFL, Language Testing, Statistics & Computer, and\nAdvanced Writing. His major areas of interest are Language Testing\n& Assessment, Cognitive Diagnostic Modeling, Structural Equation\nModeling, Multilevel Modeling, and Item Response Theory."
}