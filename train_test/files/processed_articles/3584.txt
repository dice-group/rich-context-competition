{
    "abstract": "Abstract\nMobile application design can have a tremendous impact on consumer privacy. But how do\nmobile developers learn what constitutes privacy? We analyze discussions about privacy\non two major developer forums: one for iOS and one for Android. We find that the\ndifferent platforms produce markedly different definitions of privacy. For iOS developers,\nApple is a gatekeeper, controlling market access. The meaning of \"privacy\" shifts as\ndevelopers try to interpret Apple's policy guidance. For Android developers, Google is\none data-collecting adversary among many. Privacy becomes a set of defensive features\nthrough which developers respond to a data-driven economy's unequal distribution of\npower. By focusing on the development cultures arising from each platform, we highlight\nthe power differentials inherent in \"privacy by design\" approaches, illustrating the role of\nplatforms not only as intermediaries for privacy-sensitive content but also as regulators\nwho help define what privacy is and how it works.\n",
    "reduced_content": "new media & society\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\njournals.sagepub.com/home/nms\nPlatform privacies:\nGovernance, collaboration,\nand the different meanings\nof \"privacy\" in iOS and\nAndroid development\nDaniel Greene\nMicrosoft Corporation, USA\nKatie Shilton\nUniversity of Maryland, USA\n Keywords\nApple, collaboration, Google, governance, mobile media, platforms, privacy, values in\ndesign\nCorresponding author:\nDaniel Greene , Microsoft Corporation, 1 Memorial Drive, Cambridge, MA 02142, USA.\nEmail: dgreene@microsoft.com\nArticle\nIntroduction\nTwo senior privacy engineers spoke at Apple's 2016 Worldwide Developer Conference\n(WWDC) to developers designing software applications (apps) for the company's plat-\nform. In their talk, \"Engineering Privacy for Your Users,\" the fourth slide read, \"Your\nusers\n+\nyou\n+\nprivacy\n=\n\" (Pease and Freudiger, 2016). There is an important actor\nmissing here: Apple itself. The \"you\" of this equation, developers, must produce apps\nthat pass Apple's review process. The WWDC presentation was intended to train devel-\nopers on how Apple defines privacy and how that value can be built into apps.\nApple is not alone in training app developers, who work outside the company but cre-\nate products for the platform, in its values. The Google Developers blog, for example,\nregularly posts new tools and feature sets that model Google's approach to privacy (e.g.\nKhazanie and Matias, 2016). Developer values are also governed through more quotid-\nian means. Developers learn what privacy means through their everyday work practices,\nas they interact with iOS or Android's technical features, its human representatives, and\nother developers working through similar products and problems. In turn, developers\nmust make decisions about how best to incorporate platform values into their products to\ngain the access to markets provided by both platforms. Formal presentations and publica-\ntions are just the tip of a values governance iceberg.\nThis article explores the rest of the iceberg or at least the portion devoted to mobile\napplication development. Mobile application developers (\"devs\") range from hobbyists\nto employees of multinational corporations (Stack Overflow, 2016). They develop apps\nthat collect a diverse array of personal data, such as geolocation and call logs. Devs sell\nthese apps through platform stores, such as Google's Play Store and Apple's App Store.\nDuring this process, they navigate the rules and regulations of these platforms. Since the\nintroduction of the App Store in 2008 and the Android App Market in 2009 (replaced by\nthe Google Play Store in 2012), mobile apps have become a primary means by which\nconsumers engage with digital content. Between 2013 and 2015, mobile app usage grew\nby 90% and contributed to 77% of the total increase in digital media time spent by US\nconsumers. Time on mobile devices now accounts for 2 out of every 3 minutes spent with\ndigital media, and mobile apps themselves now constitute just over half of those minutes\nThe growth of the app-based model of software, coupled with the granular personal\ndata that mobile apps can collect, makes app development an important site to explore\nthe concept of privacy. Data collected by apps are largely unregulated in the United\nStates (Rubinstein, 2011) and may be sold to advertisers, shared with strategic partners,\nor given to analytics companies. How privacy is defined and implemented--or not--in\nmobile application development has broad implications for consumers of digital media\nand the future of the internet more generally. The ways that developers understand,\ndebate, and attempt to enact privacy shape app design and use, and determine what data\nare available to other actors within the mobile application ecosystem. Devs must also\nweigh the importance of privacy against other values such as efficiency, cost, security,\nand accessibility. To understand how devs legitimate privacy as a design criterion and\ndefine it in practical terms, we conducted a critical discourse analysis of privacy discus-\nsions in Android and iOS developer forums. Our guiding questions were the following:\n1. How is privacy authorized as a design problem in contrasting development\ncommunities?\n2. How is privacy defined among mobile application developers?\n3. How do mobile platforms, through technical or regulatory means, shape these\ndefinitions?\nWe find that privacy is a frequent topic of conversation in iOS and Android developer\nforums. But developers encounter, define, and legitimate privacy in fundamentally dif-\nferent ways within these platforms. The findings illustrate how definitions of privacy\nchange as a function of developers' subjectivity (i.e. their sense of self and their values\nwithin particular contexts) and how that subjectivity is produced through platform gov-\nernance. Platform governance consists of a mix of formal restrictions and policy along-\nside informal training as developers debate the \"right\" way of doing things. It utilizes\nboth negative and positive power: platform governance discourages or prohibits devel-\nopers from making specific design decisions and encourages or teaches them to make\nother ones more in line with platform values. The centralized distribution points of app\nmarketplaces controlled by each platform and the distinct development cultures that arise\naround each platform are powerful influences.\nThe style and impacts of governance differ between platforms. In the iOS ecosystem,\nApple functions as a regulator and a gatekeeper, defining and legitimating privacy. The\nmeaning of \"privacy\" shifts as developers try to interpret and test the limits of Apple's\npolicy guidance. Developers collaborate to determine what Apple means by \"privacy,\"\nso that they can bring their app to market within Apple's gated ecosystem. In Google's\nopen-source Android ecosystem, privacy is a feature: a way for open source advocates,\nactivists, and tinkerers to respond to the unequal distribution of power in a data-driven\neconomy and mark out a role for themselves distinct from the platform for which they\nare developing software. Devs design privacy features to build identity and community,\nbut these privacy-enhancing features are largely only accessible to people with high lev-\nels of technological literacy and interest.\nPrivacy is a critical challenge for mobile development (Balebako and Cranor, 2014;\nUrban et al., 2012), and this article adds to the literature on privacy in mobile media.\nFinally, the different meanings of \"privacy\" in Android and iOS ecosystems produce dif-\nferent consequences for users: broadly, iOS encourages accessible security, while\nAndroid affords privacy creativity. Understanding these challenges can help us shape\nbetter guidelines for privacy by design, inform regulation of platforms, and broach chal-\nlenges to the adoption of privacy by design principles.\nBackground\nCurrent US approaches to mobile data protection rely on self-regulation through privacy\nby design: encouraging developers to proactively implement best-practice privacy fea-\ntures to protect sensitive data (Federal Trade Commission (FTC), 2012). Precisely what\n\"privacy\" means for design, however, is contested (Mulligan et al., 2016). Technical\nresearch has produced a range of methods and tools to give users control over data on\ntheir mobile devices (e.g. C\u00e1ceres et al., 2009; Jeon et al., 2012). However, privacy\nmeasures are particularly difficult to implement in the mobile technology space, where\nchallenges range from space restrictions that limit privacy notices (Schaub et al., 2015)\nto the conflict between consumers' privacy interests and platforms' advertising-based\nrevenue models (Leontiadis et al., 2012). More broadly, privacy scholars argue that\nmeaningful privacy protection is more complicated than enabling user control over data\n(Cohen, 2012; Nissenbaum, 2009). Nissenbaum's influential theory of contextual integ-\nrity specifies that privacy expectations depend upon complex contextual and situational\nnorms. Identifying and complying with these norms are challenges for developers.\nResearch has investigated consumers' perceptions of mobile privacy and has found\nwide gaps between consumer expectations and data-sharing realities (Lin et al., 2012;\nMartin and Shilton, 2016; Sadeh et al., 2009). As predicted by contextual integrity, con-\nsumers expect some data uses in constrained contexts; for example, they expect naviga-\ntion apps, but not games, to collect their location (Martin and Shilton, 2016). Research\nhas also shown that consumers assume platforms test and vet apps when they do not\n(Kelley et al., 2012). Simultaneously, researchers have quantified the extraordinary\namount of information sharing in the mobile economy. For example, Zang et al. (2015)\nfound that iOS and Android apps widely share personally identifiable information (PII),\nbehavior data, and location data with third parties. They also discovered clear differences\nbetween platforms in the amount and nature of data shared. Seventy-three percent of\nAndroid apps shared PII with third-party domains compared to 16% of iOS apps. While\n47% of iOS apps shared location data with third-party domains compared to 33% of\nAndroid apps. What this research does not reveal, however, is how and why developers\nmake starkly different decisions about data sharing within each platform.\nThe hows and whys of developer decision-making matter because privacy by design\npositions developers and mobile companies as ethical agents. In a loose US regulatory\ncontext subject to contested privacy definitions and conflicting values, design decisions\nbecome the political and ethical terrain on which privacy materializes (Rubinstein,\n2011). And as developers become ethical agents, platforms emerge as de facto regula-\ntors. Platforms block, promote, flag, ban, feature, and edit the content they host\n(Gillespie, 2015). Research literature on platforms has focused either on intermediaries\nfor user-produced content, such as Facebook and YouTube (Gillespie, 2010), or on more\nmaterialist approaches to the \"full stack\" of a technology's lifecycle, from design\nthrough its media representations and user experiences (Bogost and Montfort, 2008).\nWe take a middle road, focusing on the regulation of technical designs rather than on art\nor speech. We are concerned with the explicit regulation and implicit training Google\nand Apple provide to shape the design approach app developers--who do not work for\nthese firms--take in producing software for each platform. Platforms create develop-\nment ecosystems where devs, advertisers, and expert users collaborate to shape distinct\nvisions of what privacy is and how it works. In both platforms, devs must abide by\nplatform rules: in return for access to a centralized portal that provides access to cus-\ntomers and lowers distribution costs, developers must accept more centralized forms of\nThe platforms studied here are Apple's mobile-operating system, iOS, and the Google\nequivalent, Android. These are the two most popular mobile platforms in the United\nStates, accounting for 63% of the US mobile phone market (Pew Research Center, 2015).\nAndroid and iOS are not just competitors but also fundamentally different environments\nfor developers and consumers. The iPhone is the only phone running iOS; in contrast,\nAndroid is available on a variety of handsets with varying capabilities. The iPhone is a\nfamously closed development environment encouraging purpose-built software and\nminimal user tinkering in the name of curated and a unified mobile experience. Apple\nrequires an approval process for all applications in its store, and developers must comply\nwith Apple's content, privacy, and security policies to gain approval (Spencer, 2016).\nZittrain (2008) termed this environment a \"walled garden\" reminiscent of pre-Web inter-\nnet services like CompuServe and America Online (AOL). In contrast, Android is an\nopen-source platform that handset designers, devs, and hobbyists can modify as they\nplease. Although placing apps in the Play Marketplace does require developers to agree\nto Google's Developer Distribution Agreement (Google Play, 2016), which includes\nbasic privacy guidelines, the agreement explicitly states that Google does not \"undertake\nan obligation to monitor the Products or their content.\" This open approach to design was\ncrucial to Android's rapid global uptake, just as a rhetoric of \"openness\" was of critical\nimportance to Google's marketing of Android (Goggin, 2012), helping Android grow\ninto a much larger ecosystem (Stack Overflow, 2016).\nWith this literature on platform governance as our backdrop and the collaborative\nspace of developer forums as our focus, we reveal the everyday interactions between\ndevs and the platform, and describe the ways devs' privacy design decisions are shaped\nby the platform in which they work. Platforms exert not just negative power (hiding or\nblocking content or users) but also positive power: training devs on what privacy means\nand why it is important.\nMethod\nTo trace the work of privacy construction in each ecosystem, we conducted a critical\ndiscourse analysis of mobile developer forums. This is a qualitative method for analyz-\ning how participants talk about their social practices (Leeuwen, 2008). Discourse analy-\nsis looks for the ways that written texts recontextualize social practice by representing\nactors, action, legitimacy, and purpose. Critical discourse analysis indicates a focus on\nthe power differentials driving the recontextualization of social practice. Here, we are\nconcerned with two distinct fields of power. First, we are interested in devs' agency to\ndesign specific ideas of what privacy is and how it works into mobile apps. Second, we\nexamine platforms' ability to regulate devs' definitions of privacy. Platform regulation\nsurfaces through devs' descriptions of interactions with platforms' technical features and\nencounters with platforms' human representatives. Platform regulation also emerges in\nthe cultural imaginary produced by conversation among devs: their shared vision of what\nApple or Google value and where, how, and why platforms exert those values.\nWe analyzed data from two forums: iPhoneDevSDK and XDA. The iPhoneDevSDK\nforum is one of the most popular and widely used iOS developer forums. It features topics\nsuch as code sharing, programming tutorials, open discussion, and marketing guidance.\nUnlike other Apple-related forums, it is meant primarily for devs and focuses on develop-\nment advice. Unlike the official Apple developer forum, it does not require an Apple-\nissued developer key to participate. This means participants have diverse development\nexperience and purposes for participating, and that non-devs (e.g. advertising network\nrepresentatives searching for potential clients) sometimes participate. Devs liked the\nforum's independence from Apple. iPhoneDevSDK offers personalization measures (e.g.\na profile page with a record of participation and personalized icons), an emoji-based reac-\ntion system that lends character to up-votes on posts and a reputation system. Participants\nearn badges for engagement milestones (e.g. 500 posts and 5-year anniversaries) or for\nreactions from others (e.g. 25 \"Insightful\" reactions to a participant's content grant them\nan Insightful badge).\nXDA is a massive collection of forums on a variety of technology topics and\nincludes within it the largest Android developer forums on the English-language Web.\nIt features many of the same technical topics as iPhoneDevSDK but widens its partici-\npant base. Because of XDA's size and Android's open operating system, XDA was\ncharacterized by a high degree of collaboration between developers and \"hobbyists\"--\npower users who could not only critique devs'code but also often \"rooted\" their phones\nto customize the operating system or download software from alternative markets out-\nside of the Play Store (including some apps posted directly within XDA).1 Compared\nwith iPhoneDevSDK, XDA was more diverse in terms of professional background and\ngeographic location, drawing global participants with all levels of expertise and inter-\nest. The forum also supported a detailed reputation system, featuring membership\nranks based on activity.\nWe initially approached these contrasting forums because previous, unpublished pilot\nresearch indicated that developers frequently specialize in a single platform, forming\ndistinct communities of practice. A survey of mobile developers conducted by the site\nStack Overflow (2016) validates this insight. However, a limitation of our method in this\nresearch is that we are unable to track (generally pseudonymous) developers' work his-\ntories. Some developers in the forums discussed switching platforms, and others may\nwork across both platforms.\nIn each forum, we collected data based on the phenomenon of interest. We searched\nfor threads which contained the term \"privacy\" and analyzed those that included a dis-\ncussion of privacy as it applied to application development. We discarded threads where\nprivacy was used as a keyword in an advertisement for an app or job ads that promised\nprivacy for job applicants. On iPhoneDevSDK, we found 155 results in June 2015 (rang-\ning from 2009 to 2015) that fit these criteria. XDA is a much larger community, reflect-\ning the larger numbers of Android developers (Stack Overflow, 2016). To narrow our\nsearch and ensure results contained active discussions, we limited our \"privacy\" search\nto threads containing at least two replies within XDA's App Developers, Android Wear,\nor Android Development and Hacking forums. The search was performed in October\nthe smaller iPhoneDevSDK, we sampled every third result. We exported sampled threads\nfrom both forums to Dedoose for coding.\nBoth authors performed a first read of the data to generate initial thematic codes\nfocused on privacy definitions. Right away we noticed legal definitions, platform-spe-\ncific definitions, and ethical definitions of privacy. Reading XDA and iPhoneDevSDK\nthreads also revealed the distinct affordances of each site, the conversational styles in\neach forum, and distinctions in the professional concerns displayed in each. The reading\nprocess also surfaced the powerful role of the Android and iOS platforms. This drove our\ngeneration of codes that analyzed privacy discussions not just as values-driven debates\nbut as functions of the development cultures of specific platforms. Once initial codes\nwere established, we divided the data set in half and coded threads separately, reviewing\neach other's codes in weekly meetings to ensure mutual understanding and thematic\ncoherence. During this process, the code set grew to include emphasis on the \"opposite\"\nof privacy (generally data collection and personalization needs), ways that privacy was\nauthorized and legitimated, and conceptions of other actors in the ecosystem (e.g.,\nGoogle, representatives of firms producing popular software development kits [SDKs],\nand users).\nOur university's Institutional Review Board (IRB) certified that forum data qualified\nas public data and, thus, did not require further IRB review. However, we believe that\nquoting participants violates the contextual integrity of the forum space; participants\nmay not expect their posts to be used for research. To minimize this violation, we have\naltered participant handles and slightly altered quotations within this article to reduce the\nsearch ability of specific exchanges. Alterations preserve the original meaning of posts,\nand all analyses were conducted on the original, unaltered quotations. We have also\nannounced our ongoing work with posts on each forum and offered a survey to partici-\npants as part of follow-up research.\nFindings\nPrivacy discussions occurred in both ecosystems but were more complex and engaged in\nthe iOS ecosystem. In each ecosystem, devs defined privacy in ways that responded to\nregulatory moves made by the platforms, the affordances of the marketplace models\nemployed by each platform, and the distinctive culture of each ecosystem. In what fol-\nlows, we first show how devs authorized privacy, invoking third parties whose expertise,\nmoral standing, or regulatory power impressed upon devs the need to design for privacy.\nOnce privacy became a legitimate concern, it had to be operationalized in concrete pol-\nicy and design decisions. We then examined how devs in each ecosystem produced dis-\ntinct definitions of what privacy is and how it works.\nAuthorizing privacy\nDevelopers viewed a number of actors as authorities who could sanction privacy viola-\ntions, block access to markets, or even exercise moral claims. These authorities helped to\nmake privacy a legitimate design concern in both ecosystems. Discussion about these\nauthorities--the federal government, the app platform and app users--were broadly\nsimilar across iOS and Android, with the important exception of the nature of the author-\nity lent to Google and Apple.\nGovernment authority. References to government authority were rare and often non-spe-\ncific. Most references to government authority invoked privacy as an overarching politi-\ncal value to which all US devs must adhere, as in iOS dev Klicker's 2009 declaration that\n\"The founding fathers ensured we had rights to personal privacy ...\"\nOccasionally, guidance was more specific. As iOS dev bossapp wrote in 2013, refer-\nring to a quote from the Federal Trade Commission (FTC) in a popular press article,\nThe FTC quote says it all: if you're developing mobile apps, you have to give the straight story\nabout what your app can do and be transparent about your privacy practices.\nThe FTC was occasionally invoked in Android forums as well. In a thread which\n\"[GUIDE] Some incredibly simple things to protect YOUR PRIVACY!,\" CornWall\nThe FTC (Federal Trade Commission is the chief American privacy regulator) issued a report\nmobileprivacy.shtm\nGovernment authorization of privacy could work in the other direction as well; some-\ntimes the government (almost always imagined as the US federal government) was posi-\ntioned as a party to protect against. XDA developers, in particular, authorized privacy by\ninvoking government surveillance. Edward Snowden's National Security Agency (NSA)\nleaks were a popular topic of conversation, and devs often discussed building privacy\nmeasures to protect themselves. For example, in response to Cornwall's invocation of the\nFTC to authorize privacy, NY Limited said, \"Those jerks just wanna quietly scoop even\nmore data!\"\nUser authority. In both iOS and Android ecosystems, devs cited users to authorize pri-\nvacy as a legitimate design concern. User authorization was sometimes framed as instru-\nmental: users might reject privacy-invasive apps. In other cases, user authorization was\nframed as dutiful: many devs felt a charge to care for or protect users.\nProtective measures stemming from an ethic of care for users were common. This was\nmost often described as an imperative to notify users of data collection measures and\ngarner their consent. But higher standards of care also appeared, particularly in the scru-\ntiny of unfamiliar pieces of software. For example, when iOS dev uNbOuNd considered\ninstalling third-party advertising in 2010, they wrote,\nHi, I was about to put [third party ad software] in an app I'm finishing, but I have a serious\nconcern. AddressBook framework is required for having [company] ads. Excuse me?!!! I'm\nvery concerned regarding my user's privacy. I want to know why this is needed for enabling\n[company] ads in my app....\nThe same ethic of care appeared on XDA. But because of the heavy presence of\nskilled hobbyists who identified with devs, and vice versa, it was more often expressed\nas self-care: developer\u00adhobbyists had privacy rights to protect. In a 2013 thread discuss-\ning the Facebook app, corp_corp wrote,\nWhy I disabled facebook app on my phone. Don't want Facebook to track my location everytime\nI try it. We need a class action lawsuit because we have no choice to disable it ...\nConversely, user privacy expectations were sometimes characterized as obstacles to\ndevelopment. This was particularly true on XDA, where sharp lines were drawn between\nexpert technologists--hobbyist power users and developers--and the broader public. In\nIt's not like we google faces. We know Google had the ability to do this with Goggles [sic,\npresumably Glass], but didn't because of human privacy whiners.\nUser authorization for privacy could thus surface tensions between users and devs. In\na 2012 thread about software to help devs \"watch ppl on ur app,\" iOS dev k*lee wrote,\n\"This sounds good to devs, but, it obviously invades the privacy of users.\" This conflict\nwas not necessarily users' fault. Some iOS devs felt that Apple assigned authority to\nusers. In a 2011 discussion of an app rejected for privacy reasons, Carvellous wrote to the\noriginal poster, \"That's a valid point, but I doubt Apple is gonna budge, they care more\nabout users than devs.\"\nUsers were a complicated source of privacy authority. Sometimes they needed to be\nprotected, and other times merely satisfied. Some devs felt that users did not care about\nprivacy, and that notice and consent would only scare them. AS w7x posted in 2011,\nNo one cares about privacy policies. They assume the data is already protected. Just saying \"No\npersonally identifiable information (PII) leaves the phone. This greatly reduces the risk of\nprivacy problems\" makes me wonder why I would have to worry about this to begin with.\nThe original poster responded to agree that a privacy notice might set off unnecessary\nalarm bells. But he went on to justify his PII notice through the authority of a specific\ngroup of users for whom the app was designed: lawyers and legal researchers.\nPlatform authority.In the iOS forums, references to Apple's authority were the single\nmost-coded-for item. Examples frequently included privacy advice followed by a warn-\ning that noncompliance would trigger rejection from Apple's store. As davezwarez\nadvised in a 2009 thread, \"You gotta ensure that the user knows and agrees to allow you\nto collect personal data or Apple may reject your app.\" In a discussion about automati-\ncally subscribing customers who downloaded an app to an email list, iOS dev Funkicular\nput it memorably, \"Whoa, this is soooo much worse than I thought. If Apple finds out\nwhat they did, they'll get into a shit storm.\" Subsequent commenters agreed with Fun-\nkicular, with two participants suggesting that the app in question should be reported to\nApple. The dev community censured the app in question for perceived privacy violations\non Apple's authority.\nIt is difficult to overstate the importance of the App Store approval process in the dis-\ncussions on iPhoneDevSDK. The longest thread on the forum is the App Store Wait\nthread. Active since September 2008, the thread had over 257 pages of replies at the time\nof writing, each cataloging and unpacking acceptances and rejections from the App Store.\nWhile devs often individualized their own App Store rejections as the bad judgment of a\nspecific reviewer, devs as a whole recognized that Apple had an overarching privacy phi-\nlosophy with which they had to comply. It was \"Apple,\" rather than individual reviewers,\nwho requested specific notice icons, or \"Apple\" that mandated user consent to tracking.\nIn a 2012 discussion about getting rid of a location data awareness icon, Kairos warned\nthe original poster that \"Apple wants the icon there, so the user knows that the GPS is\nrunning, both for battery info and for privacy reasons. Getting rid of it is working against\nApple's design.\" Kairos'iPhoneDevSDK profile page showed over 9000 forum posts and\na Seventh Anniversary Badge. He was a repeat participant in threads about privacy. His\ncomments about Apple's requirements carried weight not because he was particularly\nskilled coder--he admitted in the thread to never having worked with jailbroken iPhones--\nbut because he had repeatedly tested the boundaries of Apple's authority through success-\nful app store submissions and could draw from those experiences.\nAlthough Apple's privacy guidelines were sometimes seen as protection--particu-\nlarly from unscrupulous third-party advertisers whose software might corrupt otherwise\nprivacy-sensitive designs--they could also serve as obstacles to development. A number\nof threads dealt with Apple's decision to deprecate the unique device identifier (UDID)\nin 2011. This discussion highlighted a challenge Apple's authority posed for developers:\nApple's guidelines could change over time. iOS devs were worried that new identifier\nsolutions would eventually face regulation. In 2012, Jouissance offered,\nI hashed the Mac address as an alternative [to UDID], and it offers the exact same uniqueness\n(and so suffers the risk of Apple fighting it later on).\nChanges to app regulation troubled iOS devs because Apple's authority was ultimate:\na denied app was a waste of devs'time and money. In a thread about rejections by Apple,\nsilvioc noted that pivoting app designs based on Apple policy changes was a resource\ndrain for small companies. Other devs in the thread were resigned to this state of affairs.\nAs C@ble posted in 2010, \"we work with Apple. Apple is the boss. So when boss\nchanged mind, then we should too.\"\nDespite the soft policy presence of Google's developer guidelines, discussion of com-\npliance with, or enforcement of, these guidelines was almost completely absent on XDA.\nGoogle was invoked to authorize privacy, but for reasons opposite of those invoked by\niOS devs. Google was a figure from which users needed to protect their privacy. Devs\ncould prevent Google's privacy violations by building the right features. In a 2010 thread\ntitled \"Privacy Concerns--Google Firmware,\" ImTheZoltan wrote,\nAt the core this is a pragmatic question: \"How do I stop Android from collecting information\nabout me?\"... If you download Google apps, that's a very different story. Those are closed\nsource and probably do collect your data for ads. If that freaks you out, get a ROM without\nthem.\nA \"ROM without them\" references alternative operating systems--modifications\nbuilt outside of Google--that come with basic apps such as an address book and calen-\ndar. ImTheZoltan suggested that these alternative systems could help concerned users\navoid Google apps'data collection.Apple authorized privacy as a design concern through\nits regulatory interventions, while Google authorized privacy through its reputation for\ncollecting user data.\nThe figures authorizing privacy for devs were largely similar across platforms, with\nthe major exception of the platforms themselves: Apple set the privacy rules for iOS\ndevs, where Android devs saw Google's behavior as justification for privacy design.\nDefining privacy in iOS\nAfter devs concluded that privacy was a legitimate design concern, they spent a large\namount of time trying to delineate what counted as privacy in their products. While the\njustifications for privacy were similar across ecosystems, the ways in which privacy was\ndefined and operationalized differed. In the iOS ecosystem, Apple's gatekeeper status\nplayed a powerful role in defining the boundaries of privacy. iOS devs debated diverse\ndefinitions of privacy but often settled on instrumental definitions reinforced by Apple's\nreview process.\nMaximalist definitions. Maximalist definitions of privacy, in which any collection of per-\nsonal data raised concerns, were one form of privacy definition in iOS. As MouseTafa\nput it in 2009, \"There is, still, the bigger ethical question of whether ANY transmission\nof ANY user data should be done without at least alerting the user.\" MouseTafa's point\nwas made in a larger thread debating whether a user analytics package was \"spyware.\"\nHe concluded that it was not, and that devs need not alert users to every piece of software\nused within their app. But the \"bigger ethical question\" shows that he and other devs\nconsider maximalist privacy definitions, and understand their work to come in tensions\nwith these definitions. \"User data\" in this case expanded beyond PII to include any infor-\nmation that could conceivably be collected about users. That devs debated maximalist\ndefinitions also hints at a recognition of conflicts between privacy and other design val-\nues, such as efficiency and profitability.\nRestrictions on data sharing.Also present in iOS were privacy definitions centered on\nprohibitions against sharing data beyond the app company. Examples include promises\nthat \"... all data will be kept private,\" and incorporated concerns about sharing data with\nMy concern is, doesApple or other companies, like [SDK C] also collect user location including\nUDID [an identifier] at the same time when our own servers collect this info? If so, is it a\nprivacy violation?\nRestrictions on data sharing reflect an element of contextual approaches to privacy\n(Nissenbaum, 2009), as devs recognize that sharing data beyond an application may\nviolate privacy expectations.\nNotice and consent. The most frequently discussed privacy definition in iOS focused on\ntransparency with users, particularly in the form of notice and consent. Devs frequently\ndefined most kinds of data collection as allowable as long as users were informed. They\nfrequently credited Apple with authorizing this particular definition. One 2013 thread\ncaptured both dynamics. The original poster asked for help designing an app that could\nlock other apps out of location services, as well as track installation patterns. Forum\nveteran Kairos, introduced above, responded to the first query with, \"Are you asking\nwhether this app can lock other apps out of iPhone location services. The answer is NO,\ndefinitely not. Apple would never allow that.\" Kairos responded to the second request\nwith, \"Are you asking if you can track the location of every user for your app? The\nanswer is yes, with the user's cooperation.\" He continued with the stipulation, \"You need\na clear privacy policy that told the user that you were going to upload their location infor-\nmation, and let them know what you were going to do with that information.\"\nThe different ways that iOS devs authorized and defined privacy illustrate a consensus\nover privacy as a concern but less consensus over what \"counts\" as privacy. Privacy func-\ntioned as an invisible fence for iOS devs: an obstacle structured by Apple's approval pro-\ncess, whose dimensions were unclear, and which devs must feel their way around during\ndevelopment. Veteran devs drew on their experience to guide novices. By working together\nin the forums, iOS devs collectively mapped the boundaries of Apple's invisible fence.\nDefining privacy in Android\nLacking the regulatory \"fence\" imposed by Apple, Android developer discourse posi-\ntioned privacy as a choice for consumers in a mobile ecosystem full of potential threats,\nincluding from Google itself. Android devs created a marketplace of privacy-enhancing\napplications, born of the loosely regulated open-source platform and the close devel-\noper\u00adhobbyist relationships within the Android ecosystem. Two broad, related themes of\napproximately equal frequency surfaced in XDA: privacy defined as a technical feature,\nand privacy defined as user control.\nPrivacy as a feature. Android devs developed privacy-centric applications to distinguish\ntheir product within the marketplace, using privacy as a feature to set their applications\napart. An example of this was a 2011 thread surveying forum participants on whether\nthey would use a new privacy-enhancing application. Groln posted,\nHi, I have just developed some privacy protection software for Android. It blocks access for\nany installed app to the following data: [comprehensive list of data types including identifiers,\nGPS, call logs, browers info]. For device ID, phone and mailbox #, SIM serial, subscriber ID\nand GPS it also lets you give custom or random values. Unlike similar apps, this doesn't cause\ncrashes when access to private data is blocked.\nResponses were enthusiastic. WindUp wrote, \"Would be v cool to use it. Its hard to\nkeep privacy without not installing apps from the Market.\" TvXY chimed in,\nI'd totally use this app on my galaxy s and tab. Especially the feature where you give apps\nrandom or custom information instead of just blocking access--that's crucial! If you want\ntesting assistance those mentioned devices just let me know. Fingers crossed you get the\npositive feedback to port and keep building this thing.\nTvXY's response highlights a technical privacy feature--fake data to fulfill app's\ndata requests--and the enthusiastic uptake of that feature by skilled hobbyists.\nTvXY's response also recalls a privacy definition found in the iOS ecosystem: restric-\ntions on sharing data with third parties. Frequently, members would tout services that did\nnot require third-party data hosting or cloud-based services like VamOOS in 2010:\nThe issue with [text-messaging app] is its 3rd party server so if ur worried about privacy, then\n[competing app] is far stronger. I use [competing app] and ... it's a direct connection so no\nprivacy concerns. Very solid.\nSimilarly, the creator of the \"Where You At\" location-sharing app promised in a 2014\nthread to update \"Privacy & Stuff,\" particularly by ensuring that location data would not\nbe stored or shared with third parties. Respondents analyzed his source code, pointed out\nvulnerabilities, and collaborated on solutions. The commonalities of data-sharing prohi-\nbitions in both ecosystems suggest that this definition might provide inroads for promot-\ning more complex, contextual notions of privacy.\nPrivacy as user control. Another prominent privacy definition in Android was of privacy\nas individual control over personal data. Products would highlight their encryption fea-\ntures, for example, emphasizing affordances for user control. As dev 67_others posted\nhighlighting their new app in 2013, \"Fynd uses end-to-end encryption to ensure that your\nlocation data is only seen by those you publish it for.\" Android devs and hobbyists often\nemphasized how important it was for privacy-sensitive users to maintain control over the\nfull mobile stack, from hardware to software, to signals. These lessons were full of cau-\ntionary tales regarding hackers, Google's data collection, or NSA surveillance. Defining\nprivacy as user control over data led to another class of privacy-preserving applications:\napps to protect interpersonal privacy. Applications like Blockage and Blockage+\n(described in a 2014 thread) locked everything but the phone's call dialer phone, block-\ning friends borrowing the phone from snooping through other applications.\nDefining privacy as user control led to a wealth of privacy-enhancing options within\nthe Android ecosystem. But privacy-enhanced applications were often developed by\nhobbyists and open-source developers working on their own. This led to a marketplace\nwith many choices for consumers, but little indication of the level of professionalization,\ntrustworthiness, or product support. Evaluating the privacy-enhanced applications dis-\ncussed in XDA required a high level of technological literacy. In a 2011 thread about ad\nblockers, participant ExDot wrote,\nxda is huge across the web but still a sort of \"niche.\" only a certain slice of those us who like\ndoing stuff to our handsets fit in. suspect most of the market has no clue what even rooting is.\nOthers recognized the problems of Android's free market approach to privacy and\nsecurity. In a thread about threats to Android security, a participant railed against Apple's\nstrict app store model. Carboncopy responded, describing \"the wild west that is\nMarketplace\" and wrote,\nTotally agree with you on Apple, that's a big part of why I picked a Desire over an iPhone, but\nthe Android approach is too far the other way IMHO. My two cents, bc hopelessly maybe just\nmaybe someone at Google is reading and thinking: hey know what, it is an accident waiting to\nhappen.\nCarboncopy appeared to be in a minority among XDA participants, however. Most\nplaced responsibility for privacy protection on users. In a 2012 thread called \"App Privacy\nViolation Concerns,\" coldbear expressed concerns about apps that \"start off w kosher\npermissions\" but gradually add permissions as they update over time. Zachrospesiak\nresponded,\nSure but the [Android] market does not auto-update the app if its permissions have changed\nwhich is really the best you can ask of Google. They COULD do the apple thing and filter every\napp that gets submitted, but I for one (As a dev making apps) would be seriously pissed if they\ndid so, apples submission process can take 3weeks while google takes at worst 1hour to appear\nin the Market [...] Google did all they can, at the end of the day its up to the user to read before\ninstalling and not just clicking around, its there fault if they install apps with more permissions\nthan they really needs\nFor devs and skilled hobbyists, Android enabled access to privacy-enhancing\napplications, limited only by skill and literacy. The charms--but also the underlying\ninaccessibility--of Android privacy were summed up by mavenz in a 2013 thread\nabout privacy problems in the Facebook app: \"Whatevs. This is why i<3 android. I\ncan just hack something better.\"\nDiscussion\nAnalysis of privacy discourse in iOS and Android forums illustrates that the ecosystem\nin which personal data are collected, processed, and shared matters to how data collec-\ntors legitimize privacy and how they ultimately define and implement privacy. Previous\nresearch has shown stark differences between Android and iOS apps with respect to\nprivacy best practices (Zang et al., 2015). Our research helps us understand why. iOS\nprivacy discussions illustrate that platforms can be powerful regulators. Apple's defi-\nnitions of privacy (primarily focused on notice and consent) are widely adapted and\nused by devs in the Apple ecosystem. Android's free market model inspired less fre-\nquent privacy discussions, but the flexibility of open-source development encouraged\na range of innovative, if sometimes inaccessible, privacy features to flourish. Critics\nhave pointed to the unregulated Android store as a nexus of privacy problems in\nAndroid (Zang et al., 2015), and that assumption holds. But the story goes deeper:\nGoogle and Apple's governance of mobile app development occurs not just at the App\nStore submission checkpoint but also in platform feedback (or lack thereof) received\nby developers, the community's negotiation of the meaning of that feedback, the train-\ning of developers through online materials and offline conferences, and the lessons\ningrained in operating systems updates and permissions protocols. Fundamentally dif-\nferent development cultures emerge on each platform, with different results for con-\nsumer privacy. The \"wild west\" of Android development means that privacy solutions\nabound for skilled hobbyists but that baseline privacy measures for the masses are\nlacking.\nSeveral challenges emerge from these findings. First, iOS devs seem to be more inter-\nested in finding the acceptable boundaries of Apple's definition of privacy than they are\nin exploring more complex meanings of the term. The current privacy requirements of\nthe App Store narrow devs' privacy imaginaries. This is problematic because privacy is\na more complex notion than reflected in Apple's current policy guidance. Apple policies\nare based on what many privacy scholars argue are outdated notions of notice and con-\nsent (Martin, 2013). Missing from both Apple's regulations and, importantly, dev con-\nversations are discussion of alternate privacy models, such as privacy as negotiation of\nidentity boundaries (Cohen, 2012) or privacy as contextual integrity (Nissenbaum,\n2009). While these definitions of privacy are increasingly important in research and law,\nthey are not widely discussed or debated among developers.\nPlatforms might use their power as regulators to address this challenge by introducing\nnew (and we would argue, better) definitions of privacy into their ecosystems. Recent\nnews coverage and the WWDC event discussed in the \"Introduction\" indicate thatApple,\nfor example, hopes to encourage differential privacy (a technique that keeps individual\nuser data inaccessible while enabling insights from group behavior (Dwork, 2008)) as a\nnew design norm within its ecosystem (Simonite, 2016). Our research indicates that\nApple may successfully encourage differential privacy through either its policies or the\ntechnical restrictions it places on its developers. If required to do so, devs will scramble\nto figure out how to operationalize differential privacy in a way that gets them to market\nwithout compromising the data collection practices on which their profits often rest.\nIt seems unlikely that Google would choose to legislate a shift in privacy definitions.\nHowever, Google can indirectly encourage innovative privacy features to become\nAndroid market choices by finding ways to make new forms of privacy protection (such\nas differential privacy) technically accessible to their developers. But the \"wild west\" of\nthe Android marketplace (as described by participants) raises the difficult issue of trust.\nHow do consumers know to trust the promises of the privacy applications they are using?\nAs Carboncopy put it in 2010, \"I have a choice right now: Trust an app or refuse to use\nit. Simple, see? No other choice.\"\nAdditionally, XDA developers were frequently not very reassuring when questioned\nabout trust by their peers. In 2013, a dev who built an alternative to the Facebook app\nwithout location tracking was asked by R3D3: \"I gotta ask, how can we know you didn't\nmod this [app] so our info gets re-routed back to you?\" The app's creator, q2rex, replied,\n\"bc im 1st: not that smart;). 2nd, Wayyyyy too lazy to work that out:P Trust me, i got\nbetter stuff to do with my time.\"\nA final challenge is a lack of transparency in how platforms regulate privacy. The per-\nvasive feeling among iOS devs that privacy was an always-moving target, and the notable\nenergy spent trying to find the invisible fence, reflects this lack of transparency. On the\nother hand,Apple can act more quickly than governments. The power ofApple as a privacy\nregulator links to calls to better understand the politics of platforms (Gillespie, 2010).\nConclusion\nMobile apps are a critical medium through which sensitive user data circulates. Thus, the\nvalues embedded in their design are an important site for research and advocacy. Because\nAndroid and iOS apps (to differing degrees) are purchased in centralized marketplaces\nand compete for user attention with thousands of similar apps, developers must adapt\ntheir design values to fit those of the platform on which they build. The political struggle\nto define how privacy works in the mobile ecosystem is often concluded before consum-\ners pick an app. That struggle takes place in a thousand tiny interactions among devs,\nhobbyists, the platform, and third-party vendors. The nature of the struggle and its results\ndiffer between platforms, but the struggle is always bounded, and those boundaries are\npoliced and adjusted by the platform and its representatives.\nMobile platforms are not just passive intermediaries supporting other technologies.\nRather, platforms govern design by promoting particular ways of doing privacy, training\ndevs on those practices, and (to varying degrees) rewarding or punishing them based on\ntheir performance. Unique technocultures emerge in the iOS andAndroid development eco-\nsystems, each producing a different set of development practices for privacy designs.\n\"Privacy by design\" is thus perhaps better termed, at least in the mobile development space,\n\"privacy by permitted design\" or \"privacy by platform.\" For privacy advocates looking to\neducate--or sanction-- mobile devs, the power of platforms must be taken into account.\nTheir influence manifests not only within the technical and policy specifications of the plat-\nform itself but also in devs' interaction with platform representatives and each other. State\nregulators should similarly recognize the power of the platform to define the boundaries of\nprivacy within their ecosystem.And we hope other researchers build on our work to explore\nthe power of platforms to bound not only the limits of acceptable speech in the public sphere\nbut also the limits of acceptable labor and design practices supporting that public sphere.\n"
}