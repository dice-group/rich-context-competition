{
    "abstract": "Abstract. To determine the size of the drug-involved offender population that\ncould be served effectively and efficiently by partnerships between courts and treat-\nment in the United States, a synthetic dataset is created by Bhati and Roman\n(2009). Because of hidden structure and aggregation necessary to create variables,\nthere exists latent variance that can not be explained fully by a normal random ef-\nfect model. Semiparametric regression is a well-known and frequently used tool to\ncapture the functional dependence between variables with fixed effect parametric\nand nonlinear regression. A new Gibbs sampler is developed here for the number\nand positions of knots in regression splines by expressing semiparametric regres-\nsion as a linear mixed model with a random effect term for the basis functions.\nOur Gibbs sampler exploits the properties of the multinomial-Dirichlet distribu-\ntion, and is shown to be an improvement, in terms of operator norm and efficiency,\nover add/delete one MCMC algorithms. We find that the Dirichlet distribution\nwith small values of the parameters results in a smaller number of knots and, in\ngeneral, good fit to the data. This approach is shown to reveal previously unseen\nstructures in the synthetic dataset of Bhati and Roman.\n",
    "reduced_content": "A Computational Bayesian Method for\nEstimating the Number of Knots In Regression\nSplines\nMinjung Kyung\n Keywords: Regression Splines, Multinomial-Dirichlet distribution, Bayesian Semi-\nparametric Regression\n1 Introduction\nIs substance abuse treatment an effective method for reducing drug-involved offenses?\nWhat treatment options are the most effective and cost-beneficial? These are important\npolicy questions for reducing crime and improving the lives of addicted individuals. A\nkey measurement objective in this literature is estimating the size of the drug-involved\noffender population that could be served effectively and efficiently by partnerships be-\ntween the courts and treatment regimes. Unfortunately, obtaining individual-level data\nDepartment of Statistics, Duksung Women's University Seoul, Korea, mkyung@duksung.ac.kr\n794 Number of Knots in Regression Splines\nthat can be usefully aggregated is a much more difficult challenge than expected since\nthere exists considerable under-reporting, privacy restrictions, and medical issues in\nsuch settings. Bhati and Roman (2009) recently approached this dilemma in a creative\nway by constructing micro-level data from three nationally representative sources to\nconstruct a 40,320 case synthetic dataset, which uses population profiles rather than\nsampled observations.\nResearchers create synthetic datasets (Rubin 1993) when the actual data in its raw\nform are either unavailable or partially restricted due to privacy concerns. However,\nit is often the case that in the aggregation process that creates synthetic data hidden\nstructures and latent information are formed in unexpected ways. What this suggests\nis that the set of parametric models in our standard toolkit, linear and generalized\nlinear models, will be inadequate for explaining the key underlying relationships in\nsuch data. Additionally, the data creation process used by Bhati and Roman (2009)\nrelies heavily on simulation models to estimate substance abuse treatment effect. The\nwork here develops a new Bayesian semiparametric regression model that balances user-\ndefined restrictions against pure data information, and is sufficiently flexible that it has\nthe ability to capture unusual or hidden features of the data that would ordinarily be\nmissed by conventional approaches. This approach requires a new variant of the Gibbs\nsampler to provide posterior estimates.\nSemiparametric regression is concerned with the flexible incorporation of nonlinear\nfunctional relationships in regression analysis. Consider the typical setup of an n-\nlength outcome variable vector Y associated with the explanatory matrix R, which is\npartitioned into two components, n \u00d7 p matrix X and n \u00d7 q matrix W, such that for\nthe ith case, i = 1, . . . , n, Yi\nis modeled as\nE [Yi\n|Ri\n] = h (Xi\n + g(Wi\nwhere g(\u00b7) is some unspecified \"smooth\" function to be estimated. This specification,\nwith known link function h(\u00b7), is therefore a form of semiparametric regression due to\nthe partitioned treatment of the covariates. Details about semiparametric regression\nmodels are found in Ruppert et al. (2003) and H\u00a8\nardle et al. (2004). Recent applications\nof these models are contained in Zhang and Davidian (2001), Zeng and Lin (2007), Yin et\nal. (2008), and Maity et al. (2009). A likelihood-based approach is developed by Ke and\nWang (2001) with a semiparametric nonlinear mixed effects model (SNMM) that extends\nthe nonlinear mixed effects models where self-modeling nonlinear regression models are\nused to fit repeated measures data. Fahrmeir and Lang (2001) proposed an approach\nfor Bayesian inference via Markov chain Monte Carlo (MCMC) methods in generalized\nadditive and semiparametric mixed models. Our approach is a modernization of this\nidea whereby the smoothing parameters of g(\u00b7) are updated on each cycle of the Gibb\nsampler.\nThe creation and use of synthetic data has dramatically increased over the last\ndecade due to heightened attention to privacy issues, more powerful computation, de-\nmand for larger datasets, a huge upswing in data mining work, and improved estimation\nalgorithms. When information is combined from multiple data sources in this process,\nprofiles are created, which are combinations of attributes that are considered in the\nsame way as actual individual observations. Thus, cases are created which are not ac-\ntual individuals but characterizations of individuals that are in the aggregate unbiased.\nIn the case of Bhati and Roman (2009) the synthetic data contains a profile for ev-\nery client permutation, therefore allowing estimation of the effect of drug treatment on\nevery combination of client attributes and characteristics. Additionally, each profile is\ntested against four differing treatment regimes to understand their efficacy in reducing\nsubstance abuse. The application of the Bayesian semiparametric regression model de-\nveloped in this work shows that Bhati and Roman miss key features present in their\nsynthetic data, and that these features substantially change the policy interpretation of\nthe results. Further evidence is supplied here to show that the adaptive semiparametric\nalgorithm reveals latent structure in benchmarking nonparametric datasets suggesting\nthat this approach is useful in very general contexts as well.\n1.1 Relationship to Mixed Models\nA semiparametric model can be expressed as a set of penalized regression splines, and,\nmore generally, as a linear mixed model. To allow the flexibility in the estimation of\na unknown function, constraints are widely used, typically with a roughness penalty,\nwhich leads to parametric statistical models. In a classic work, Wahba (1977) derived\ntheoretical details showing that a nonparametric regression model can be rewritten in\nthe form of a linear combination of the fixed effect (trends) and the random effects (the\nsmall-scale variations). Silverman (1985) then showed that spline smoothing provides a\nnatural and flexible approach to curve estimation and the smoothing parameter deter-\nmines the degree to which the data are smoothed to produce the estimate. Stone (1985)\nalso showed that the closed additive approximation to a nonparametric regression model\ng with explanatory variables is h (\u00b7) = \u00b5 + J\nf (\u00b7), which has been chosen subject\nto the constraint that Df\nj\n= 0 for j = 1, . . . , J, minimizes mean squared error. French\net al. (2001) argued that good low-rank approximations (reduced knot, K < n) exist\n796 Number of Knots in Regression Splines\nwith the mixed model representation of smoothing splines by showing the equivalence of\nthe BLUP coefficient estimator of low-rank smoothing splines and to the exact smooth-\ning splines. These results allow for mixed model software solutions to perform the entire\nfitting algorithm and for inference within the mixed model framework.\nFor the ease of notation, we consider again the following linear semiparametric re-\ngression model\nYi\n|Ri\n= Xi\n + g(Wi\n) + i\nwhere i\n iid N 0, 2 for i = 1, . . . , n. Here, g is a unknown function and needs\nto be estimated. The model (2) can be treated as a semiparametric regression when\nthe covariate is measured with error. We can consider smoothing splines to estimate\nthe unknown function g, but smoothing splines become less practical when n is large,\nbecause they can use up to n knots. Thus, an alternative approach to spline fitting is\npenalized splines, which are given by\ng(Wi\n) =\nK\nk\nBk\n(Wi\nwhere K < n is the number of knots with the degree of the B-spline or the degree of\npolynomial, and Bk\ns denote basis functions. Here we define the order of positions of K\nknots as K\n, . . . , K\n) . Thus, penalized spline fitting can be written generally as\nmin\n\n||y - X - S||2 +  D (4)\nwhere  > 0 is the penalty parameter, S is an n \u00d7 K smoother matrix of Bk\nwith\ncoefficient vector , and D is a symmetric positive semidefinite matrix.\nFor the semiparametric regression in exponential families with suitable link function\nof h(\u00b7) in (1), we need to investigate a new parameterization of the hierarchical model\nto derive a Gibbs sampler that more fully exploits the structure of the model. Also, for\nthe multivariate semiparametric regression, we can consider generalized additive models\n(GAM) by an additive model predictor, interaction models with tensor products of spline\nbases and bivariate radial basis functions, and the bivariate smoothing models with thin\nplate spline family of smoothers. These issues are left to further research in this area.\nThere are various basis functions that have been used in this context. The truncated\npower functions of degree p, natural cubic splines (Green and Silverman 1994), B-spline\nbasis function (Eilers and Marx 1996), and the radial basis functions (Ripley 1996) are\nthe most popular choices. Eilers and Marx (1996) showed that B-spline basis functions\nprovide more stable numerical properties than truncated power functions. They have\nlocal support, thus with different positions of knots with the same number of knots,\nB-spline basis functions will have flexible forms.\nFor the degree of a spline p, Ruppert et al. (2003) discussed that if one is using a\nlinear spline with enough knots so that increasing the number of knots has no appreciable\neffect on the penalized fit, then increasing the degree of the spline is also unlikely to\nhave a noticeable effect. In this paper, we do not restrict or prefix the degree of a spline.\nAlso, we do not restrict to a specific set of basis functions, as the following methods can\nbe applied to all basis functions.\nKauermann et al. (2009) discussed asymptotic properties of generalized penalized\nspline smoothing if the spline basis increases with the sample size. They argue that the\nequivalence of penalized spline fitting and generalized linear mixed models is asymptot-\nically justified only if the Laplace approximation holds. Also, they make use of a fully\nBayesian viewpoint by imposing prior distributions on all parameters and coefficients,\nand show that a fully Bayesian formulation of the model yields approximately the same\nresults as a Laplace approximation even for growing dimensions of the spline basis. This\nmeans that even though we express the semiparametric model as a form of mixed model,\nthese models share the same theoretical properties.\nFurthermore, one of the merits of semiparametric models is that the Gauss-Markov\ntheorem generally holds, as in the linear regression model. There are various versions\nof Gauss-Markov theorems for linear random effect models (Harville 1976) and for a\nheteroscedastic linear model (Carroll 1982). Also, Pfeffermann (1984) discussed exten-\nsions of the Gauss-Markov theorem to the case of stochastic regression coefficients and\n\nN consistent version of semiparametric regression. More\nrelevantly, for semiparametric models, Huang and Lu (2001) and French et al. (2001)\ndiscuss Gauss-Markov theorems for fixed effect coefficient estimators by re-expressing\nthe nonparametric mixed effects model as a linear combination of the fixed effects and\nthe random effects by describing the space of fixed effects and the space of random\neffects via subspaces of certain Reproducing kernel Hilbert spaces (RKHS) (Aronszajn\nFor better estimation and fitting of models to a given dataset with basis functions,\nwe need to carefully consider the number of knots K and the order of positions K\n=\n, . . . , K\n) . Early discussions about the penalized linear spline models with smaller\nnumber of basis functions than sample size (K  n) can be found in Parker and Rice\nand Marx (1996) and Ruppert and Carroll (2000). More discussions in recent years are\n798 Number of Knots in Regression Splines\nIn a regular penalized linear model (4), K, K\nand  will control the smoothness of\nthe model, and how much variational information can be captured in random effects\nterms. The penalty parameter  is a parameter that controls the bias and variance of\nthe random effects. Also, in practice, the number of knots K and their position in K\nare unknown, so we need to estimate them.\nThere has been much written on the choice of K and K\nbased on generalized cross\nvalidation (GCV) using various smoothing methods (Friedman and Silverman 1989;\nthere are knot selection methods based on stepwise selection (Stone et al. 1997) and on\na componentwise boosting algorithm with radial basis functions (Leitenstorfer and Tutz\n2007). For Bayesian methods, reversible jump MCMC has been widely used (Denison\nRecently, Claeskens et al. (2009) provided a theoretical justification that, depending\non the number of knots, sample size and penalty, the theoretical properties of penalized\nregression spline estimators are either similar to those of regression splines or to those\nof smoothing splines, with a clear breakpoint distinguishing the cases. They prove that\na smaller number of knots leads to a smaller averaged mean squared error. Also, they\nshowed that using truncated polynomial basis functions leads to an optimal rate of\nconvergence independent of the assumption made on the number of knots.\nStochastic methods that move simultaneously in model space and parameter space\nallow us a limitless range of possibilities for the choice of K and K\nBayesian methods, since the dimensionality of the parameter space generally changes\nwith the model, reversible jump MCMC (RJMCMC) proposed by Green (1995), is some-\ntimes used. However, the RJMCMC algorithm usually results in a slow mixing chain\nbecause empty components arise and the sampler retains these for extended periods of\ntime.\nIn this paper, we develop an overtly Bayesian method to find the number of knots\nand the position of knots in a Gibbs sampling scheme. For this we need to consider\nvarious candidates for the position of knots. Typically, the observed data points have\nbeen used as candidates for knots. If we consider a different point of view, the position\nof knots can be thought of as a changepoint problem in the data sense (Moreno et al.\non et al. 2007). This means that before and after a knot, the curve changes\ndirections or amount of curvature, or it drops or goes up suddenly. Thus, we propose\na method for the choice of K and K\nbased on the standard changepoint problem by\nusing Bayes Factors (BF) for the update instead of the likelihood function. In this\nprocess, we apply a Dirichlet prior on the process of choosing the number of knots, K\nand the order of positions, K\n, and instead of , a Dirichlet prior on a changepoint\nsetup that controls the bias and variance of the random effects. Thus, in our model,\nwe don't need to consider the penalty part with D and  in a regular penalized linear\nmodel (4). More details about the MCMC sampling schemes for the regression splines\nare provided in the following sections.\n2 A Bayesian Approach to Knot Selection\nWe now outline a Gibbs sampler for the semiparametric model, which is represented as\na linear mixed model where, at each iteration, we generate a length n vector K\nand\nrecover the knot size through marginalization for K and K\n.\nTo make the sampling scheme easier, let W1\n \u00b7 \u00b7 \u00b7  Wn\n. Given the num-\nber of knots K and their positions K\n, the linear mixed model representation of the\nsemiparametric model with basis functions can be written as\ny = X + S + ,\nwhere S is an n \u00d7 (K + 1) smoother matrix with K knots in the positions of K\n=\n, . . . , K\n) . Thus, Sik\n, the element in the ith row and kth column of S, is Bk\n(Wi\n)\nin (3), which are basis functions only related to K knots. Also,\n Nn\n.\nHere, for W  [a, b] with a = 0\n< \u00b7 \u00b7 \u00b7 < K\n= b, we define\nnk\n= number of observations in (k\n] , k = 0, . . . , K,\n+ \u00b7 \u00b7 \u00b7 + nK\n= n, and we use\n\n, . . . , nK\n}.\n800 Number of Knots in Regression Splines\nThe given structure can also be considered as a subclustering of W's with subcluster\n, . . . , nK\n. This means that the position of knots will determine the subgroups\nand the subgroup size. Because the W's are assumed to be in increasing order, it is\neasy to see that 1\n, . . . , and K\nnk\n. Thus, we are\nmarginalizing n knots into K knots in a manner similar to that of Kyung et al. (2010).\nThe joint likelihood for regression parameters  and K\nwith normal assumptions can\nbe written as:\nLK\n=\n|\n\n.\nWith a flat prior on K\nand K, such as  (K)  c and  (K\n|K)  c, and with\npriors on the regression parameter () , we get the joint posterior distribution as:\n(, K\n| y) =\nLK\n(, K\n|y)()\n\nKJ\nLJ\n(, J\n|y)()d\n,\nwhere\nKK\n= {all possible K knots : (1\n, . . . , K\n)}\n, . . . , nK\n) : all possible nk\n's such that n0\n+ \u00b7 \u00b7 \u00b7 + nK\n= n} .\nThus, the full conditional posteriors of  and K\nare\n( | K\n, y) =\nLK\n(, K\n|y)()\n\nLK\n(, K\n|y)()d\n(K\n| , y) =\nLK\n(, K\n|y)\nKJ\nLJ\n(, J\n|y)\n.\nWe add the following uniform, normal, and inverted gamma (IG) priors:\n\u00b5   (\u00b5)  c, - < \u00b5 < \n),\nwhere b = (\u00b5, 0, . . . , 0) , a1\n> 0 is the shape parameter, b1\n> 0 is the scale parameter,\nand c, d > 0 are constants. Here,  = , b, , 2 . For the priors on K and K\n, we\nadd:\n (K) =\nn\nand  (K\n(nj\n),\nwhere w is a weight. Alternatively for the prior on K, we can consider a truncated\nPoisson distribution with hierarchical parameter  that can be specified with a small\nnumber for higher probability on smaller value of K. As stated in DiMatteo et al.\n(2001), posteriors appear not to be sensitive to the precise specification of the prior on\nK.\nWe now outline a Gibbs sampler that will generate from the conditionals by gener-\nating a length n vector K\nand recovering the knot size through marginalization.\nFor t = 1, . . . T, at iteration t\n1. Starting from (t), (t)\nK\n, with (t)\nK\n= (n(t)\n, . . . , n(t)\nK\n)\ndraw : (t+1)    | (t)\nK\ndraw : q(t+1) = (q(t+1)\n, . . . , q(t+1)\n)\n Dirichlet (t)\n, . . . , (t)\nK\n, . . . , n-1\nwhere (t+1)\nK\n wK +1m(y|\nK\n)\nn\nn\n\u00b7 \u00b7 \u00b7 n\nj\n]n\nand n\n+\u00b7 \u00b7 \u00b7+n\n= n with K +1 of the n\nj\n> 0, and m(\u00b7) is the marginal distribution.\nWe note that the length of (t+1) is K = K(t+1).\nSampling of the model parameters  in (6) is straightforward (Appendix 6), so we\nwill concentrate on the sampling of K\nand q, a vector of probabilities that decides the\nnumber of knots and the order of positions.\nThe marginal distribution of K knots in positions K\nwill have the form of\nm (y|MK\n)  X I - S S S + -1\n\n\n+\n\n\n\n\ny\n-(n-p\n)\n802 Number of Knots in Regression Splines\nwhere MK\nis a model with K knots in positions K\n\n= I-S S S + -1\n\nThus, the Bayes Factor of a model with K knots in positions K\nand a model with no\nknots can be written as\n(y) =\nm (y|MK\n)\n)\n\nX I - S S S + -1\n\n\n\u00d7\n\n\n\n\n\n\ny\ny I - X (X X)-1 X y\n\n\n-(n-p\n)\nTherefore, the posterior distribution of K and K\nis\n (K\n|y, K) =\nK\n(nj\n)|\n(y)\nK\nKK\nK\n(nj\n)|\n(y)\n=\nK\n(nj\n)|\n)\nK\nKK\nK\n(nj\n)|\n)\nand\n (K|y) =\n(nj\n)|\n)\nK\nKK\nK\n(nj\n)|\n)\nHere, |\nnk\n.\nThe transition kernel of Markov chain in (7) is\nk ((, K\n) , ( , \nK\n)) =  ( | K\n, y)\nQ\nP(\nK\n| q,  )f(q | K\nwith\n|q, ) =\n(y) n\nqnj\nj\nK\nKK\n(y) n\nqnj\nj\nand\nf(q|K\n) =\nj\n)\n(j\n)\nqj\nj\n.\nIf we take j\n= nj\n+ 1 for all j = 0, . . . , n - 1 with nK+1\n= \u00b7 \u00b7 \u00b7 = nn-1\nj\n)\n(j\n)\nqj\nj\n=\nn!\nn\n\u00b7 \u00b7 \u00b7 nK\nK\nqnj\nj\n.\nThus, with this choice, the transition kernel has (, K\n|y) as its stationary distribution.\nThe verification of the stationary distribution is the same as in Kyung et al. (2010) with\nBF instead of the likelihood function.\nFor the estimation of w, we consider a gamma prior. Escobar and West (1995) use a\ngamma prior on w and update Gibbs with an auxiliary variable method for a Bayesian\ndensity estimation. Also, Blei and Jordan (2006) discussed that in the stick-breaking\nrepresentation, the gamma distribution is conjugate to the stick lengths. In the stick-\nbreaking representation, the mixing proportions are given by successively breaking a\nunit length \"stick\" into an infinite number of pieces. The size of each successive piece\n(stick length), proportional to the rest of the stick, is given by an independent draw from\na Beta(1, w) distribution with our notation. This means that the gamma distribution\nis a good candidate for a prior on w. For all cases, w is a parameter that controls\nthe number of mixtures or the smoothness of the model in Dirichlet. However, in our\napproach, the number of mixtures or the smoothness is controlled by K\n. Thus w is\nthen insensitive to choosing the number of knots and their positions.\n3 Generating the Positions of Knots\nIn this section we show how to generate the positions of knots according to (7). Then we\nexamine convergence rates, and establish that our sampler is an improvement, in terms\nof operator norm and efficiency, over commonly used one knot add/delete algorithms.\n3.1 Dirichlet Distribution\nTo generate the positions of knots based on (7), we use the Dirichlet distribution with\nparameters . The Dirichlet distribution is the multivariate generalization of the beta\ndistribution with density given by\nf(q) =\nj\n)\n(j\n)\nqj\nj\nwith i\n> 0 for all j, and\nqj\nFirst consider the simple case, (p, 1 - p), with  = (1\n) . Then, (p, 1 -p) has a beta\ndistribution with 1\n> 0. For the beta distribution with 1\nif p  0, then 1 - p  1 with high probability and vice versa. However, p  1 - p will\noccur with small probability. This means that one of (p, 1 - p) will be much larger than\nthe other. For the case with 1\n 1, the probability of p  1 - p is high.\n804 Number of Knots in Regression Splines\nThus, p and 1 - p will tend to be close to each other.\nNow we consider the more general case with  = (1\n, . . . , m\n) . Due to the restric-\ntion that m\nqj\n< 1 for all j = 1, . . . , m, some qj\n's will have larger values\nthan the others. This means that in the Gibbs sampler (7), if j\n< 1 for all j = 1, . . . , n,\nthe number of knots tends to be smaller. However if we consider j\n 1, the number of\nknots tends to be close to n/2.\nThe general goal of the penalized linear spline models is good prediction with a\nsmaller number of basis function and knots than sample size, K < n. If we believe that\nthe original number of knots should be small, instead of K  n/2, then we update the\nGibbs sampler for the number of knots and the position of these knots with\n(t)\nj\n=\nn(t)\nj\nn\nfor j = 0, . . . , K\nn\nfor j = K + 1, . . . , n - 1.\nIn the following section, we discuss how the positions of knots with the number of knots\nare generated based on a Dirichlet prior.\nWe follow the methods described in Kyung et al. (2010) for the number and the positions\nof knots, updating the positions of knots with the marginalization of a multinomial-\nDirichlet distribution using a Metropolis-Hastings algorithm. In other words, based on\nthe value of the qj\nin (7), we generate a length n vector of indices of subclusters from the\nmultinomial, grouping the objects with the same indices, then remove the subclusters\nwith no elements. For example, let a candidate vector of indices for subclusters from the\nof indices, 2, 6, . . . , 10, and sum over the frequencies of indices. Then, n0\n+ \u00b7 \u00b7 \u00b7 + n3\n= 10. Therefore, the updated \nK\n)\nwith 3 knots (K = 3). The details about the marginalization of multinomial-Dirichlet\ndistributions are provided in the Appendix of Kyung et al. (2010).\n3.3 Convergence Properties\nGiven K\n, the sampling of the model parameters from (|K\ny) is straightforward.\nThus, in investigating convergence we only need to be concerned with the convergence\nof the Markov chain on the number and position of knots.\nIf we ignore the model parameters for now, then we are concerned only with conver-\ngence of the chain to the stationary distribution, that is\n(K\n, . . . , nK\n) =\n(w)\n(n + w)\nK\n(nj\n),\nwith the expected value of K,\nE(K) =\nn\nw\nThe full conditionals, ignoring the model parameters, are given by\nP(a = j|n0\n, . . . , nK\n) =\n\n\n\n\n\nnj\nfor j = 0, . . . , K\nw\nfor j = K + 1.\nwhere a is an index. With a similar argument, the full conditionals from the chain in\nP(a = j|n0\n, . . . , nK\n) \n\n\n\n\n\n\n\nnj\nfor j = 0, . . . , K\nw\nfor j = K + 1.\nNotice that for qj\n= j\n(j = 0, . . . , n - 1) (the normalization is not important), we see\nthat the one-knot add Gibbs sampler (15) is the same as (16). Based on Hobert and\nMarchev (2008), it can be shown that for j\n= nj\n+ 1 for j = 0, . . . , K and j\nj = K+1, . . . , n-1, the kernel of (16) dominates the kernel of (15) with smaller variance\nfor any square-integrable function with any m > 0. Details about the derivation of full\nconditionals and the dominated convergency are in Kyung et al. (2010).\nHowever, if j\n's have the form in (13), we are not able to compare the transition\nkernel of (16) to the kernel of (15), because the transition kernel of our chain has a\ndifferent stationary distribution. If we consider more details about the full conditionals\nin (16), we know from the properties of the Dirichlet distribution that\nE (qj\n) =\nj\nj\nfor j = 0, . . . , n - 1.\nAlso, we know that qj\n's determine K and K\n. Thus, the j\n's have an important role\nin the Markov chain to update the number of knots, K and the positions of knots, K\n,\nsimilar to m in (14).\n806 Number of Knots in Regression Splines\nHalpern (1973) proved that when the locations of all possible knots are assumed to\nbe known but the subset of these which are the actual knots is unknown, the posterior\ndistribution of the Bayesian spline regression is proper for natural conjugate and vague\npriors. The subset of all possible knots, that is, the set of actual knots, forms a model.\nThus, with conjugate priors of the form of a marginal distribution on the index of the\nmodel, or with non-informative priors which are a function of number of elements in\neach subset of knots, the posterior distribution of the Bayesian spline is proper. For the\nlocation of the knot, a prior probability based on a discrete distribution is assigned to\neach subset of the possible locations, and the posterior probability has been calculated to\nchoose the model with highest posterior probability. Also, the optimal predictor based\non the marginal distribution for the model and the mean vector for the coefficients is\nderived for a loss function which is a generalization of that appearing in Lindley (1968).\nThis loss function is squared error loss plus the amount of shrinkage which controls the\nnumber of knots.\nOur model setting fits Halpern's Bayesian spline setting with conjugate priors except\nit has one more step to generate the number and positions of knots. A uniform distri-\nbution is assigned as a prior and our chain is updated to a new number and position\nof knots with higher posterior probability. Specifically when we generate the number\nand positions of knots, we start from n knots then marginalize over the empty sets; this\nimplies conditionally consistent multivariate proper normal priors on coefficients . We\ncan then combine the parametric part X to a component of the nonparametric part.\nTherefore, from Lemma 2 of Halpern (1973), any sample from the posterior distribution\nof our proposed model is proper and the set of multivariate normal posteriors on coeffi-\ncients will be conditionally consistent with the chosen number (K) and positions (K\n)\nof knots. Also, with non-empty intervals between knots in our setting, from Theorem\n1 of Halpern (1973), the posterior mean of coefficients  in our model is the optimal\npredictor for the expected loss function which is an addition of the expected squared\nerror loss and the risk of wrong number (K) and position (K\n) of knots.\n4 Simulations and Data Analysis\nTo illustrate the proposed nonparametric approach and required Gibbs sampler, we\npresent a simulation study and two data analyses. The parameter w is known to be\ninsensitive to the choice of data, thus we fix w = 1 because the number of knots and the\nposition of knots will be controlled by  in the Dirichlet distribution. We compare small\nand large parameter values over  in the Dirichlet distribution. In these applications,\na Poisson distribution with mean  as a prior on the number of knots K is tested. As\ndiscussed in Section 2.1, the posteriors appear insensitive to the value of , thus we fix\n = 3 for convenience.\nWe also compare our proposed Gibbs sampler to recently proposed nonstationary\nmethodologies that couple stationary Gaussian processes (GP) with treed partitioning\ndone in Gramacy and Lee (2008). GP regressions accommodate prior knowledge in the\nform of covariance functions. The covariance term is considered with the correlation\nfunction of a neighbor. Tree GP regression uses treed partitioning through RJMCMC\nand in each partition (branch of the tree), independent local GP regressions are ap-\nplied. The tgp package for R is developed for Bayesian nonstationary, semiparametric\nnonlinear regression and design by treed Gaussian processes with jumps to the limiting\nlinear model by Gramacy (2007). GP models are well known for effectively fitting ar-\nbitrary functions or surfaces. The GP sampling proposed by Gramacy and Lee (2008)\ncommences with RJMCMC which allows a simultaneous fit of the tree and the GPs at\nits leafs. In this paper, we consider GP regression and tree GP regression models to\ncompare to our proposed semiparametric models.\nWe begin with the simple smooth test functions:\nand\nSimulated data are drawn from the rescaled function with support in the unit interval.\nThe first function comes from Denison et al. (1998). In the original setting, it is evalu-\nated at 200 regularly spaced points with normally distributed noise having mean 0 and\nstandard deviation  = 0.4 but in our simulation, we evaluated at 200 regularly spaced\npoints with  = 0.3. The second function comes from DiMatteo et al. (2001). Origi-\nnally, it has been evaluated at 101 regularly spaced points with  = 0.3 in DiMatteo et\nOur implementation focuses on Gibbs sampling with the marginal distribution,\nm (y|MK\n). We computed the mean squared error for our Bayes method and GP models\n808 Number of Knots in Regression Splines\nFunction 1. Function 2.\nAMSE Number of knots AMSE Number of knots\nTable 1: Average mean squared errors with standard errors on 50 samples. Small\nand large valued parameters of Dirichlet distribution in MCMC, and GP and tree GP\nregressions are compared. (Number of knots for tree GP regressions is the number of\nchange points to fit independent GP models.)\nas:\nn\nn\n^\nf(xi\n) - f(xi\n)\n,\nwhere ^\nf(xi\n) is an estimate and f(xi\n) is the true function. The Bayesian estimates of\nE [f(x)|y] are found from our Markov chain Monte Carlo process with runs of 5000\nHere, with MCMC, we compare:\n a large valued parameter in the Dirichlet distribution\nj\n=\nnj\n+ 1 for j = 0, . . . , K\n1 for j = K + 1, . . . , n - 1\n versus a small valued parameter in the Dirichlet distribution\nj\n=\nnj\nn\nfor j = 0, . . . , K\nn\nfor j = K + 1, . . . , n - 1.\nThe average mean squared error (AMSE) with standard errors based on 50 samples\nof data, and the average number of knots are reported in Table 1. We observe that the\nsemiparametric model as a linear mixed model fits well for all examples shown by small\nmean squared error (MSE). For generated data from Function 1, the MSE and number\nof knots from a small valued parameter in the Dirichlet distribution are smaller than\nthese from a large valued parameter. For this function, the number of knots has a big\nX\nf.X\nData\nSmall Valued Parameter\nLarge Valued Parameter\nGP Regression\nTree GP Regression\nX\nf.X\nData\nSmall Valued Parameter\nLarge Valued Parameter\nGP Regression\nTree GP Regression\nFigure 1: Left panel is the rescaled smooth Function 1 and right panel is the rescaled\nsmooth Function 2.\neffect on the fit with a preference on a small number of knots. However, for Function\n2, the number of knots does not have a big effect on the fit.\nCompared to GP and tree GP regressions, our semiparametric model with a small\nvalued parameter in the Dirichlet distribution tends to recover precision on GP regres-\nsions. AMSEs are not different numerically for Function 1 and Function 2. Tree GP\nhas been fitted with three independent local GP regressions. From Figure 1, we clearly\nobserve three partitions if we consider local regressions for Function 1 and Function 2.\nGraphically, in Figure 1, we also observe that Function 1 has a small number of change\npoints, but Function 2 shows a large number of change points. Also, the estimated curve\nwith small valued parameter and the estimated curves of GP and tree GP regressions\nare close to the true function , but the estimated function with large valued parameter\nis close to the data points.\nWe now consider one-dimensional simulated data which is partly a mixture of sines\nand cosines, and partly linear that is considered in Gramacy (2007):\nf(x) =\nsin x\nx\n- 1 otherwise.\nFrom the true model itself, we expect that the independent local regression in each\npartition, tree GP regression, will show superiority among others. However, in Figure\n810 Number of Knots in Regression Splines\nX\nf.X\nData\nSmall Valued Parameter\nLarge Valued Parameter\nGP Regression\nTree GP Regression\nFigure 2: Mixture of sines and cosines and partly linear function.\nSmall valued Large valued GP regression tree GP\nTable 2: Average mean squared errors with standard errors on 50 samples. Small\nand large valued parameters of Dirichlet distribution in MCMC, and GP and tree GP\nregressions are compared. (Number of knots for tree GP regressions is the number of\nchange points to fit independent GP models.)\n2, we observe that our semiparametric regression with a large valued parameter in\nthe Dirichlet distribution also tends to capture wiggly curves well. The average mean\nsquared error (AMSE) with standard errors based on 50 samples of data, and the average\nnumber of knots are reported in Table 2. The AMSEs of GP regressions are smaller\nthan AMSEs of our models. Tree GP regressions fit two independent GP regressions\non each partition and the partition point is the given degenerate point x < 9.6. In\nour model, the number and position of knots depend on the subsample sizes instead\nof the correlation with neighbors. Thus, our method might not recover the true model\nas much as tree GP regression, but without considering all possible partitions, the\nsemiparametric regression tends to fit the appropriate smoothed curve on data. In\nother words, the proposed semiparametric model is simpler and faster, while capturing\nthe main features.\n4.2 Data Analysis: Nitrogen Oxides in Engine Exhaust\nOriginally, Brinkman (1981) conducted an experiment of a single-cylinder engine with\nethanol or indolene to see how the nitrogen oxides (NOx\n) concentration in the exhaust\ndepended on the compression ratio (C) and the equivalence ratio (E). There were 88\nruns with ethanol; for these runs, E varied from .535 to 1.232, C took one of five values\nranging from 7.5 to 18, and the values of E and C were nearly uncorrelated. There were\n22 runs with indolene; for these runs, C took just one value, 7.5, and E ranged from 0.665\nto 1.224. In this example, we only consider the data with ethanol. This data has been\nanalyzed with various methods, but Cleveland and Devlin (1988), Breiman (1991) and\nGu (2002) used smoothing methods. Cleveland and Devlin (1988) applied the locally\nquadratic smoother based on an adaptation of Mallows's Cp\n, the M plot and argued that\nan additive fit of E and C is inappropriate because of a substantial interaction. Breiman\n(1991) used a product method of multivariate functions for estimating an underlying\nsmooth function of noisy data by a sum of products of the univariate functions. For\nthe NOx\ndata, it has been argued that 2 knots are enough to estimate the function of\nE and there is a non-removable interaction in these data. In these analysis, an NO1/3\nx\ntransformation has been used, but Gu (2002) made a log-transformation because the\nconcentrations are positive with some near-zero readings. As it has been argued,\nthe effect of equivalence ratio was dominant, but the compression ratio had little impact.\nGu (2002) used a cubic spline fit with estimating smoothing parameter  estimated after\nseeing a rough cross-validation fit.\nWe use a log-transformation on NOx\nand we compare our methods to the cubic\nspline fit of Gu (2002), GP regression and tree GP regression based on the residual sum\nof squares (RSS)\nn\nn\n{^\nyi\n- yi\nwhere ^\nyi\nis an estimate from the cubic spline and yi\nis the observation. Two models\nhave been applied to this data previously:\nModel 1. log\n(NOx\nE + g (E) +\n(NOx\nE \u00d7 C + g (E) + .\nThe compression ratio, C, has five distinct values; it could have been treated as an\n812 Number of Knots in Regression Splines\nNOx in Engine Exhaust\nEquivalence ratio\n(NOx\n)\nLarge Valued\nSmall Valued\nCubic Spline\nGP Regression\nTree GP Regression\nNOx in Engine Exhaust\nEquivalence ratio\n(NOx\n)\nLarge Valued\nSmall Valued\nCubic Spline\nGP Regression\nTree GP Regression\nFigure 3: Left panel is a scatterplot of NOx\nwith estimated curves of Model 1 and right\npanel is with estimated curves of Model 2.\nordinal discrete variable and interaction has been considered. The numerical results are\ndisplayed in Table 3 and the graphical functional fits are in Figure 3.\nFrom Table 3, we observe that the number of knots which has been detected by the\nproposed methods with small valued parameters in Dirichlet distribution is the same\nas Breiman (1991). With C and interaction between E and C (Model 2), the RSS is\nsmaller than the RSS of Model 1, because as it has been argued, there exists a non-\nremovable interaction in these data. The curve estimation with cubic splines tries to\nsmooth out the dataset by reducing a function of RSS (cross-validation); thus the cubic\nspline curve fit has smaller RSSs. GP regression behaves similar to cubic spline models,\nand tree GP has the smallest RSS among the techniques. From Figure 3, we observe\nthat an estimated curve based on small valued parameter in the Dirichlet distribution\nis showing smaller estimated valued in the highest peaks of data, but bigger estimated\nvalued in the tails of the data. From the left panel in Figure 3, for Model 1, we observe\nthat compared to the cubic spline curve, GP regression and tree GP regression, the\nsmall valued parameter curve has a lower peak with thick tails. It might be the reason\nwhy RSS of the small valued parameter curve is larger than the RSS of the cubic spline\ncurve. As discussed with the simulations, the large valued curve is more sensitive to the\ndata points, but less smooth. The right panel in Figure 3 is based on the Model 2. C is\nan ordinal discrete variable, thus the estimated curves are not as smooth as in the left\npanel. However, if we consider C and interaction in the model, the estimated curves are\nRSS Number of knots RSS Number of knots\nTable 3: Residual sum of squares and number of knots from NOx\nconcentration data.\nFor Model 1 and Model 2, small and large valued parameters of the Dirichlet distribution\nin MCMC are compared. Also, cubic spline fit of Gu (2002), GP regression and tree\nGP regression are compared. (Number of knots for tree GP regressions is the number\nof change points to fit independent GP models.)\nmore sensitive to the data points compared to the model with E only. In this data set,\nthere are more data points in the right hand side tail; thus a small valued parameter\ncurve tries to give more weight onto the right tail with somewhat less weight on higher\npeaks. The cubic spline curve is attempting to smooth out the data points based on the\npoint value and GP regressions are trying to smooth out the data points considering\nthe correlation with neighbor points. However, the proposed semiparametric regression\nis also attempting to smooth out the data points with the basis functions which are\ndetermined based on the subsample sizes n0\n, . . . , nK\n. This technique is a somewhat\ndifferent setup for positions of knots compared to others and it might be the reason for\nthe above results.\n5 Crime Reducing Benefits In Treating Drug Involved Of-\nfenders\nReturning to our primary data interest, we look at the synthetic dataset problem de-\nscribed in Section 1. Empirical investigation of \"Going to Scale\" in drug interventions\nthe size of the drug-involved offender population that could be served effectively and\nefficiently by partnerships between the courts and treatment regimes. From micro-level\ndata of three nationally representative sources, a dataset of 40,320 cases which is de-\n814 Number of Knots in Regression Splines\nBox Plots of Estimated Number of Arrestees by Race and Gender\nFigure 4: Box Plots of Estimated Number of Arrestees by Race and Gender\nfined using population profiles was constructed. The principal investigators combined\nand the Arrestee Drug Abuse Monitoring (ADAM) Program in the United States, 2003\n(USDJNIJ 2004) to estimate the likelihood of drug addiction or dependence problems\nand develop nationally representative prevalence estimates. They used information in\nDHHS 2010) to compute expected crime reducing benefits of treating various types of\ndrug involved offenders under four different treatment modalities.\nIn this dataset, age, race, gender, offense, history of violence, history of treatment,\nco-occurring alcohol problem, criminal justice system status, geographic location, arrest\nhistory, and a total of 134 prevalence and treatment effect estimates and variances are\nall included. Moreover, the principal investigators obtained estimates of crime reducing\nbenefits for all crimes as well as select sub-types. The four different treatment modalities\nconsidered from DATOS were Long-Term Residential Treatment (Modality 1), Short-\nTerm Inpatient Treatment (Modality 2), Outpatient Methadone Treatment (Modality\n3) and Outpatient Drug-Free Treatment (Modality 4).\nAmong these 145 variables, in this example, the estimated number of arrestees\n(ESIZE) by weighting on the prevalence in the population of interest is considered as the\ndependent variable Y . In the original data, ADAM provides information on the number\nof times individuals were arrested in the year prior to the current booking. Using the\nempirical similarity between synthetic profiles and ADAM sample members, Bhaty and\nRoman computed the expected number of arrests for particular profiles. Thus, by rescal-\ning based on the prevalence in the population of interest, ESIZE has been created. For\nindependent variables, we consider age (AGE), current offense status (OFFENSE) - Violent,\nDrug, Property, or Other, history of substance abuse or dependent treatment (THIST) -\nYes or No, geographical location (GEO) - Rural, Urban and Suburban areas, number of\nprior arrests (AHIST) - 0, 2, 5, 10, or 20 , and eight abuse/dependence variables which\nare estimated based on the four treatment modalities;\nHere subscript i is an index of Modality, and j = 0 is for ABUSE and j = 1 is for\nThis is a large dataset (n = 40, 320), so all variables are trivially significant at  =\n0.05 with a standard GLM. Figure 4 shows box plots of ESIZE by race and gender. For\nthe BLACK MALE case, it is showing skewness to the right with large values of ESIZE.\nCompared to distributions of other race and gender combinations, the distribution of\nthe black male case shows broadly skewed large variance. For these data, a regular\nGLM might not be enough to capture the large variance. Thus, instead of considering\nthe whole dataset, in this paper, we used part of the dataset which is for black males\nwith violence and alcohol history under criminal justice status. There are n = 840 cases\nmeeting this new criterion.\nThese data are used to detect significant factors with controlling hidden structures\nand latent information. GP regression and tree GP regression implement Bayesian\nregression models of varying complexity and allow for the explicit estimation of predic-\ntive uncertainty when considering neighbor points. Thus GP regression and tree GP\nregression are not meaningful for the purpose of the data analysis in this section.\nFor this smaller dataset, we consider a linear mixed effect model (LMEM) and semi-\nparametric model. For LMEM, we use TEC10\nas a random effect term. We observe from\nLMEM that most demographical variables are significant at  = 0.05, but 8 prevalence\nand treatment effect estimated variables are not significant. Modality 1 is strongest\nabuse/dependence treatment; thus we might expect that it decreases the number of\ndrug-involved offenders. However, if you fit LMEM to the dataset of black males with\nviolence and alcohol history under criminal justice status, Long-Term Residential Treat-\nment has no statistically reliable effect with ESIZE at the 0.95 level. Also, other treat-\n816 Number of Knots in Regression Splines\nRSS Number of knots\nTable 4: Residual sum of squares and number of knots from crime reducing benefits\ndata. Small and large valued parameters of the Dirichlet distribution in MCMC are\ncompared. Also, linear mixed effect model (LMEM) with random effect TEC10\nand\nnatural cubic spline fit on TEC10\nare compared.\nment methods are not statistically reliable either. It might be due to large variances\n(various variabilities of estimation or scaling) for these 8 estimated variables. Thus,\nwe consider a semiparametric model with natural cubic spline (NCS) on TEC10\n. For\nthe semiparametric fit, we consider the regular NCS method based on generalized cross\nvalidation and our proposed Gibbs sampling with large and small valued parameters in\nthe Dirichlet distribution.\nFrom Table 4, we observe that the RSS of LMEM is smaller than the RSSs of the\nregular NCS semiparametric model and our proposed models. Coefficients of LMEM\nare estimated based on the restricted maximum likelihood method to reduce variance\nin this dataset, thus we might expect the RSS of LMEM to be smallest compared to\nother models. For these data, the number of knots has little effect on the fit, because\nRSS with the small valued parameter of the Dirichlet distribution is close to the RSS\nwith the large valued parameter. However, from Figure 5, we observe that our pro-\nposed semiparametric model with a Dirichlet prior distribution has uniformly smaller\n95% highest posterior density (HPD) intervals than the standard NCS semiparametric\nmodel and LMEM. This variance reduction property in linear Dirichlet random effects\nmodels has been proved theoretically by Kyung et al. (2009). They showed that if the\ndata vector does not consist of a contrast within observations between each position of\nknots, the mean of the posterior distribution of the variance from the Dirichlet random\neffect model is smaller than that of the regular normal random effects model. However,\nin most cases we might not be able to find such contrast. The left panel in Figure\n5 shows 95% intervals for the demographical variables, AGE, OFFENSE, THIST, GEO and\nAHIST. HPD intervals of the proposed semiparametric model (small and large) are no-\nticeably smaller than intervals from regular models (LMEM and NCS). The right panel\n95% Intervals for Coefficients of Demographic Variables\nLarge Parameter=Black, Small Parameter=Red, LMEM = Pink, NCS = Green\nAHIST\nGEO\nTHIST\nOFFENSE\nAGE\n95% Intervals for Coefficients of Estimated ACAR of Abuse or Dependence\nLarge Parameter=Black, Small Parameter=Red, LMEM = Pink, NCS = Green\nFigure 5: 95% highest posterior density (HPD) Intervals. The HPD intervals for\nLMEM and NCS models are given in Pink and Green, respectively, and intervals for the\nproposed semiparametric model with small valued and large valued parameter models\nare given in Red and Black, respectively.\nin Figure 5 shows 95% intervals of eight estimated abuse/dependence variables based\non the four different treatment modalities. The TEC10\nvariable is used in the smoothing\nmethod, so the credible intervals for the coefficients include zero for all models. How-\never, all other coefficients in our proposed semiparametric model are reliably different\nfrom zero. Therefore, our proposed method gets rid of unexpected and hidden variance\nand correlation under the data structure efficiently and provides a smoother curve fit\nwith small variance compared to other regular methods.\nFrom Figure 5, we observe that for black males with violence and alcohol history also\nunder criminal justice status, if the current offense case is violence (OFFENSE=VIOL)\nand if drug abuse/dependent has been treated (THIST=YES), the number of arrestees\n(ESIZE) tends to decrease compared to drug offense (OFFENSE=DRUG) and no treat-\nment history (THIST=NO). Also, as the number of previous arrests (AHIST) increases,\nthe estimated number of following arrests tends to decrease. Considering four differ-\nent drug abuse/dependent treatments, treatment for drug abuse in each of the domains\n) substantially reduces the number of arrestees. Specif-\nically, outpatient methadone treatment (Modality 3) is shown to be the most efficient\nmethod to reduce the number of arrests. However, for those at greatest risk of drug de-\n), reverse treatment in each of the modalities tends\n818 Number of Knots in Regression Splines\nto create an increase in the number of arrests. This might be the reason that, compared\nto abuse, the drug dependence treatment is not intensive, and the standard categoriza-\ntion method of abuse or dependence is not restricted. Thus, by trying more intensive\ndrug abuse/dependence care, drug-involved recidivism should be reduced. When we\nuse a standard method to fit a nonlinear model, we are not able to detect this hidden\ninformation, but by using the proposed method, we reduce the variance in the data\nstructures and find better fitting models with greater predictive qualities. Note that\nusing the Dirichlet distribution to generate the positions of the knots gives us a powerful\nimplementation of a Stein-rule like estimator.\n6 Discussion\nFor synthetic data, in the aggregation process hidden structures and latent information\nare created in unexpected ways. What this suggests is that the usual set of parametric\nmodels will be inadequate for explaining the key underlying relationships in such data.\nThus we developed a new Bayesian semiparametric regression model that balances user-\ndefined restrictions against pure data information, and which is sufficiently flexible that\nit has the ability to capture unusual or hidden features of the data that would ordinarily\nbe missed by conventional approaches. For the analysis of synthetic data of crime-\nreducing benefits in treating drug-involved offenders who are black males with violence\nand alcohol history under criminal justice status, when we used standard methods to\nfit a nonlinear model, we were not able to detect this hidden information. However by\nusing our proposed method, we remove more variance in the data structure and we find\na more smooth and informative model for better prediction.\nTo control the number of knots, we use the Dirichlet distribution with large valued\nparameter for a large number of knots and with small valued parameter for a small\nnumber of knots. For the function with a small number of change points, the number of\nknots has a big effect on the fit with a preference on a small number of knots; therefore\na Dirichlet distribution with a small valued parameter for the number and positions\nof knots in MCMC performs better compared to that with a large valued parameter\nbased on the mean squared error. From simulation, we observe that the estimated curve\nwith a small valued parameter is close to the true function, but the estimated function\nwith large valued parameter is close to the data points. Also, from data analysis, we\ncould observe that the large valued curve is more sensitive to the data points, but less\nsmooth. This may lead to the poor smooth for the large valued parameter seen in\nFigure 2. Comparing to a full nonstationary model with coupled stationary GP with\ntree partitioning, a new Bayesian semiparametric regression is not much less effective for\nfitting arbitrary functions or surfaces and it is simpler and faster. One of the advantages\nof the proposed semiparametric model is that it removes more hidden variance in the\ndata structure and also provides better estimates of model parameters for the smoother\ncurve fit by giving us a powerful implementation of a Stein-rule like estimator.\nIf we believe that there are not a large number of knots, in other words the distri-\nbution of data is smooth and not degenerated, we consider a Dirichlet distribution with\nsmall valued parameter for the number and positions of knots using MCMC. However,\nif there exist many change points (partition points) in the data, the independent local\nregression in each partition will be the best curve fit method. A semiparametric regres-\nsion spline with a Dirichlet distribution with large valued parameter for the number\nand positions of knots is not as effective as a semiparametric model with small valued\nparameter for the smoother curves. However, if the distribution of data is skewed or\na mixture of curves with degenerated points, our proposed semiparametric regression\nwith a Dirichlet distribution with large valued parameter tends to fit the smooth curve\non data faster without considering all possible partitions. Also, if our intention for curve\nfitting is better prediction over the sample space, our proposed semiparametric models\nwith a Dirichlet distribution prior will provide a more informative model with simpler\nand faster implementation, even with a Dirichlet distribution with large valued param-\neter. For prediction, the choice of valued parameters (large or small) in the Dirichlet\nprior should be considered differently depending on the data.\nThe convergence properties of the proposed Bayesian semiparametric regression are\ndiscussed based on the Halpern's Bayesian spline model setting with conjugate priors\n(Halpern 1973). For the number and positions of knots, a discrete distribution is as-\nsigned as a prior and our chain is updated to a new number and position of knots with\nhigher posterior probability. This implies conditionally consistent multivariate proper\nnormal priors on coefficients  of the basis functions; then the posterior is proper and\nthe set of multivariate normal posteriors on coefficients will be conditionally consistent.\nAlso, with this setting, the posterior mean of coefficients  in our model is the optimal\npredictor under the chosen number (K) and positions (K\n) of knots.\nWe have provided a new semiparametric regression approach but we can extend our\nmethod to more of a nonparametric regression strategy. We express a semiparamet-\nric model as an addition of parametric and nonparametric parts. If we consider the\nparametric part as a smoother, semiparametric regression is an additive model of non-\n820 Number of Knots in Regression Splines\nparametric regressions. With a linear mixed model representation, a semiparametric\nregression has the same form as a nonparametric regression, but with more parameters\nin the fixed effects. Thus, we can easily use the proposed sampler with fully nonpara-\nmetric regression.\nGeneralization of the proposed semiparametric regression can be considered with\nregression in exponential families with multivariate nonparametric structure. However,\nthese are open issues within the proposed semiparametric regression and we leave these\nfurther explorations as a part of our future research.\nFrom synthetic data analysis, we observe that a linear mixed effects model and a\nregular semiparametric model with natural cubic spline are not enough to explain effects\nof treatments for the number of arrestees among black males with violence and alcohol\nhistory under criminal justice status. However, our proposed semiparametric models\nwith a Dirichlet distribution prior remove the unexpected and hidden variabilities and\ncorrelations under the data structure efficiently and provide a smoother curve fit with\nsmall variance compared to other regular methods. Thus, with uniformly smaller 95%\nhighest posterior density intervals for the demographical variables than the standard\nNCS semiparametric model and LMEM, our methods provide that by trying more\nintensive drug abuse/dependence care, drug-involved recidivism could be reduced. By\nusing our described method, we could remove more variance in the data structure and\nwe could find a smoother model for better prediction.\nAppendix\n1. Generating the Model Parameters\nThe joint posterior distribution can be written as\n , b, , 2, K, K\n|y\n\nexp -\n|\n\u00d7 exp -\n( - ~\n) \n\n) exp -\n - ~\n\n - ~\n\n\u00d7 exp -\nb - ~\nb\nb - ~\n\u00d7 exp -\n\n\n\n\ny ,\nwhere\n\n\n\n~\n = S S + -1\n\nS (y - X)\n\n= I - S S S + -1\n\nS\n\n=\nd\n\nX, ~\n =\nd\n\nX\nd\n\ny\nb\n= I -\nd\nd\n\nX\n~\nb = I -\nd\nd\n\nX\nd\n\nX\n\ny.\nThen for fixed K and K\n, a Gibbs sampler of (, b, , 2) is\n|, b, 2, K, K\n~\n\n|, b, 2, K, K\n, y  Np\n~\nb|, , 2, K, K\n, y  Np\n~\n2|, , b, K, K\n, y  IG\nn + K + p\n,\n|y - X - S|2 +\n\n +\n.\n822 Number of Knots in Regression Splines\nReferences\nAronszajn, N., 1950. \"Theory of Reproducing Kernels.\" Transactions of the American\nBhati, A. S. and Roman J., 2009. Empirical Investigation of \"Going to Scale\" in Drug\nArbor, MI: Inter-university Consortium for Political and Social Research [distributor],\nBiller, C., 2000. \"Adaptive Bayesian Regression Splines in Semiparametric Generalized\nBlei, D. M. and Jordan, M. I., 2006. \"Variational Inference for Dirichlet Process Mix-\nBreiman, L., 1991. \"The Method for Estimating Multivariate Functions from Noisy\nBrinkman, N. D., 1981. \"Ethanol Fuel - A Single Cylinder Engine Study of Efficiency\nCarroll, R., 1982. \"Adapting for Heteroscedasticity In Linear Models.\" The Annals of\nClaeskens, G., Krivobokova, T., and Opsomer, J. D., 2009. \"Asymptotic Properties of\nCleveland, W. S. and Devlin, S. J., 1988. \"Locally Weighted Regression: An Approach to\nRegression Analysis by Local Fitting.\" Journal of the American Statistical Association\nDenison, D. G. T., Mallick, B. K., and Smith, A. F. M., 1998. \"Automatic Bayesian\nDiMatteo, I., Genovese, C. R., and Kass, R. E., 2001. \"Bayesian Curve-Fitting with\nEilers, P. H. C. and Marx, B. D., 1996. \"Flexible Smoothing with B-splines and Penal-\nEscobar, M. D. and West, M., 1995. \"Bayesian Density Estimation and Inference Using\nFahrmeir L. and Lang, S., 2001. \"Bayesian Inference for Generalized Additive Mixed\nModels Based on Markov Random Field Priors.\" Journal of the Royal Statistical\nFrench, J. L., Kammann, E. E. and Wand, M.P., 2001. Comment on \"Semiparametric\nNonlinear Mixed-Effects Models and Their Applications\" by Ke and Wang. Journal\nFriedman, J. H. and Silverman, B. W., 1989. \"Flexible Parsimonious Smoothing and\nGir\u00b4\non, F. J., Moreno, E., and Casella, G., 2007. \"Objective Bayesian Analysis of Mul-\ntiple Changepoints for Linear Models.\" Bayesian Statistics 8 (J. M. Bernardo, J. O.\nBerger, A. P. Dawid, D. Heckerman, A. F. M. Smith and M. West, eds.) Oxford\nGramacy, R. B., 2007. \"tgp: An R Package for Bayesian Nonstationary, Semiparametric\nNonlinear Regression and Design by Treed Gaussian Process Models.\" Journal of\nGramacy, R. B. and Lee, H. K. H., 2008. \"Bayesian Tree Gaussian Process Models\nWith an Application to Computer Modeling.\" Journal of the American Statistical\nGreen, P.J., 1995. \"Reversible Jump Markov Chain Monte Carlo Computation and\nGreen, P.J. and Silverman, B.W., 1994. Nonparametric Regression and Generalized\nLinear Models: A Roughness Penalty Approach. Chapman & Hall, London. 796\nHalpern, E. F., 1973. \"Bayesian Spline Regression When the Number of Knots is Un-\n824 Number of Knots in Regression Splines\nH\u00a8\nardle, W., M\u00a8\nuller, M., Sperlich, S. and Werwatz, A., 2004. Nonparametric and Semi-\nparametric Models, Springer. 794\nHarville, D., 1976. \"Extension Of The Gauss-Markov Theorem To Include The Estima-\nHastie, T. J., 1996. \"Pseudosplines.\" Journal of the Royal Statistical Society, Series B\nHastie, T. J. and Tibshirani, R. J., 1990. Generalized Additive Models, Chapman &\nHobert, J. P. and Marchev, D., 2008. \"A Theoretical Comparison of the Data Augmen-\ntation, Marginal Augmentation and PX-DA Algorithms.\" The Annals of Statistics\nHolmes, C. C. and Mallick, B. K., 2001. \"Bayesian Regression with Multivariate Linear\nHolmes, C. C. and Mallick, B. K., 2003. \"Generalized Nonlinear Modeling With Mul-\ntivariate Free-Knot Regression Splines.\" Journal of the American Statistical Associ-\nHuang, S. Y. and Lu, H. H.-S. (2001). \"Extended Gauss-Markov theorem for nonpara-\nKauermann, G., Krivobokova, T., and Fahrmeir, L., 2009. \"Some Asymptotic Results\non Generalized Penalized Spline Smoothing.\" Journal of the Royal Statistical Society,\nKe, C. and Wang, Y., 2001. \"Semiparametric Nonlinear Mixed-Effects Models and Their\nKelly, C. and Rice, J., 1990. \"Monotone Smoothing with Application to Dose-Response\nKyung, M, Gill, J,, and Casella G, 2009. \"Characterizing the Variance Improvement\nin Linear Dirichlet Random Effects Models.\" Statistics and Probability Letters 79,\nKyung, M, Gill, J,, and Casella G., 2010. \"Estimation in Dirichlet Random Effects\nLeitenstorfer, F. and Tutz, G., 2007. \"Knot Selection by Boosting Techniques.\" Com-\nLindley, D. V., 1968. \"The Choice of Variables in Multiple Regression.\" Journal of the\nMaity, A., Carroll, R. J., Mammen, E., and Chatterjee, N., 2009. \"Testing in Semipara-\nmetric Models with Interaction, with Applications to Gene-Environment Interaction.\"\nMoreno, E., Casella, G., and Garcia-Ferrer, A., 2005. \"An Objective Bayesian Analysis\nof the Change Point Problem.\" Stochastic Environmental Research and Risk Assess-\nO'Sullivan, F., 1986. \"A Statistical Perspective on Ill-Posed Inverse Problems\" Statis-\nParker, R. L. and Rice, J. A., 1985. Discussion of \"Some Aspects of the Spline Smoothing\nApproach to Non-parametric Regression Curve Fitting\" by Silverman. Journal of the\nPfeffermann, D., 1984. \"On Extensions of the Gauss-Markov Theorem to the Case of\nStochastic Regression Coefficients.\" Journal of the Royal Statistical Society, Series B\nRipley, B.D., 1996. Pattern Recognition and Neural Networks. University Press, Cam-\nRobinson, P. M., 1988. \"Root-N-Consistent Semiparametric Regression.\" Econometrica\nRubin, D. B., 1993. \"Discussion: Statistical disclosure limitation.\" Journal of Official\nRuppert, D., 2002. \"Selecting the Number of Knots for Penalized Splines.\" Journal of\nRuppert, D. and Carroll, R. J., 2000. \"Spatially-Adaptive Penalties for Spline Fitting.\"\nRuppert, D., Wand, M. P. and Carroll, R. J., 2003. Semiparametric Regression, Wiley,\n826 Number of Knots in Regression Splines\nSilverman, B. W., 1985. \"Some Aspects of the Spline Smoothing Approach to Non-\nparametric Regression Curve Fitting.\" Journal of the Royal Statistical Society, Series\nStone, C. J., 1985. \"Additive Regression and Other Nonparametric Models.\" The Annals\nStone, C. J., Hansen, M. H., Kooperberg, C., and Truong, Y. K., 1997. \"Polynomial\nSplines and Their Tensor Products in Extended Linear Modeling.\" The Annals of\nUnited States Department of Health and Human Services., 2006. Substance Abuse and\nMental Health Services Administration. Office of Applied Studies. National Survey\nInter-university Consortium for Political and Social Research [distributor], 2006-10-\nUnited States Department of Health and Human Services., 2010. National Institutes of\nHealth. National Institute on Drug Abuse. Drug Abuse Treatment Outcome Study\nMI: Inter-university Consortium for Political and Social Research [distributor], 2010-\nU.S. Dept. of Justice, National Institute of Justice., 2004. ARRESTEE DRUG ABUSE\nfile]. ICPSR version. Washington, DC: U.S. Dept. of Justice, National Institute of\nJustice [producer], 2004. Ann Arbor, MI: Inter-university Consortium for Political\nWahba, G., 1977. \"Practical Approximate Solutions To Linear Operator Equations\nWoods, S., 2006. Generalized Additive Models: An Introduction with R, Chapman &\nYin, G., Li, H., and Zeng, D., 2008. \"Partially Linear Additive Hazards Regression\nWith Varying Coefficients.\" Journal of the American Statistical Association 103,\nZeng, D. and Lin, D. Y., 2007. \"Maximum Likelihood Estimation in Semiparametric\nRegression Models with Censored Data.\" Journal of the Royal Statistical Society,\nZhang D. and Davidian, M., 2001. \"Linear Mixed Models with Flexivle Distributions of\n"
}