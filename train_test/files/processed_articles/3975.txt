{
    "abstract": "Abstract\nThis paper reports on the results of an experiment that investigated the response rate and data quality effects of two\nalternative methods for making initial contact with potential respondents in a sample survey. The study was conducted in a\nuniversity setting and selected students were initially contacted about their participation in the survey either thru an e-mail\nmessage or thru their own customised university portal. Survey findings revealed that the e-mail alternative resulted in a\nsignificantly higher overall response rate. However, the findings also revealed that there were significant differences in the\nresponses of the two groups with respect to the evaluations given by students regarding their academic and social well-being\nat the university. Specifically, sample members in the portal group gave significantly more positive evaluations regarding\ntheir academic and social well-being than did members of the e-mail group. However, there were no significant differences\nbetween the responses of the two groups in terms of their evaluations of either the universitys food court or the campus\nrecreational centre. Additionally, despite efforts within both groups to achieve a high response rate and a representative\nsample, neither alternative was able to produce a sample that had a high response rate or a sample that was representative of\nthe population in terms of either students gender or grade point average. Implications of these findings for conducting\nsurveys, especially those of student populations, are also discussed.\n",
    "reduced_content": "Correspondence: F. Wiseman, Professor of Statistics and Marketing Research, College of Business Administration,\nNortheastern University, Boston, MA 02115 USA. Email: f.wiseman@neu.edu\nThe Effects of the Initial Mode of Contact on the Response Rate and\nData Quality in an Internet-Based College Satisfaction Survey\nFrederick Wiseman*\n*College of Business Administration, Northeastern University\n Key words: Internet, survey, methodology, non-sampling error, representativeness, data quality\nIntroduction\nIn recent years, marketing and survey researchers have investigated how various data collection strategies have\naffected response rates and data quality in online surveys (see, for example, Marcus, et al., 2007; Taylor et al.,\nstudies is relatively small compared to those involving other frequently used modes of data collection, the\ninternets speed, efficiency, and low cost have all spurred researchers in a variety of disciplines to search for\nimproved data collection strategies. This paper reports on an experimental study conducted in a university setting\nwhere it was possible to survey students online either by notifying them about the survey through an e-mail\nmessage or by having them receive notice about the survey on their customised university portal. These\ncustomised portals allow the university to communicate with designated students on numerous topics and issues,\none of which being their selection in a sample for a university sponsored survey.\nThe study was undertaken in response to the universitys decision to greatly reduce the use of e-mail messages\n(including requests for survey participation) as a means for communicating with their students. Members of the\nstudent body had complained about the wide spread use of what they perceived to be \"junk e-mail\" messages that\nwere being sent by university officials. Because this decision impacted requests for survey participation and\nbecause this decision was made without prior consideration of response rate or data quality implications, it was\nsubsequently decided to conduct an experimental study in order to determine whether this change in procedure\nwould, in turn, impact the quality of sample data obtained in future online surveys conducted by the university.\nSpecifically, the objectives of the experimental study were (i) to determine whether there was a significant\ndifference in the response rate between the group that received a request for participation through an e-mail\nmessage and the group that received a request for participation through their customised portal, (ii) to determine\nwhether each alternative (e-mail and portal) was able to produce a sample of respondents that was representative\nof the population from which it was drawn, and (iii) to determine whether survey results varied depending upon\nwhich method was used to initially contact potential respondents.\nPorter and Whitcomb (2003), in their review of the literature, found that there had been a paucity of research that\nexamined the effects of contact type on web survey response rates. Noting the importance of this area as\nevidenced by the rich literature on this topic for other modes of data collection, the authors conducted an\nexperimental investigation that examined how various factors such as personalisation of the salutation, setting a\ndeadline for participation and the statement of sample selectivity affected survey response rates. Their research\nfindings showed that while personalisation had little impact on the response rate, setting a deadline for\nparticipation and informing sample members of their selectivity did have a positive effect. The finding on\npersonalisation contradicted results of a previous study based upon a meta-analysis of sixty-eight web surveys\nwhich found that the use of personalisation in the initial contact did increase the response rate (Cook et al., 2000).\nIlieva et al. (2002), following up on the work of Schaefer and Dillman (1998), strongly advocated the use of\nmixed modes of data collection (for example, online and mail) especially when conducting international\nmarketing research surveys. They argued that researchers using such an approach were much more likely to obtain\na representative sample than they would be if only a single mode of data collection were used. More recently,\nPorter and Whitcomb (2005, 2007) extended their research to examine the effects of mixed mode contacts and\nalternative e-mail subject lines on response rates. Here the authors found no significant differences if survey pre-\nnotification and reminders were sent to sample members by either paper or an e-mail message. Past research,\nconducted by Schaefer and Dillman (1998) and Kaplowitz, et al. (2004), had produced contradictory evidence on\nthis subject with the former finding e-mail correspondences preferred to paper correspondences, while the latter\nreaching an opposite conclusion. For e-mail subject lines, the findings revealed that the effects were a function of\nthe level of involvement that sample members had with the survey sponsor.\nThe present research study examines another area related to how the nature of the initial contact affects response\nrates and data quality. The study also looks at other factors that must be investigated before researchers will be\nable to have a high degree of confidence that their survey findings are representative of the population from which\ntheir sample was drawn.\nMethods\nIn the present study, the population of 7,220 students at a large American university was randomly divided into\ntwo groups (portal and e-mail) of 3,610 students each. Prior to sample selection, the population was stratified by\ngender and class year so that each of the two experimental groups mirrored the population in terms of these two\ncharacteristics. Once the sample selection had been made, a subsequent analysis revealed that the groups were also\nidentical in terms of their mean grade point average (GPA).\nThe survey questionnaire was comprised of four sections. The first asked participants to evaluate various items\nthat were directly related to their academic and social well-being on campus. This section was followed by one in\nwhich students evaluated the universitys food court and its recreational centre on a number of different\ndimensions. The next section asked students to indicate the extent to which they believed that five potential\nproblems areas (academic dishonesty, alcohol abuse, drug abuse, racism and sexual harassment) were present on\ntheir campus. The last section contained classification questions. Sample members were told in the introductory\nremarks to the survey that their participation would help university officials to improve students overall\neducational experience.\nStudents received the actual survey announcement either through an e-mail message which included a direct link\nto the questionnaire or by a notice with an accompanying survey link posted on their customised portal. Two\nfollow-up reminders were also sent by similar methods to increase the surveys response rate within each group.\nFurther, as an incentive for participation, all students who completed the survey questionnaire were entered into a\nraffle for book store gift certificates.\nHypotheses\nIt was hypothesised that the student group which received an e-mail message that introduced them to the\nsurvey would have a higher overall response rate than the student group that received notice of the survey\ndirectly from their portal. There are two reasons for this. First, the e-mail message sent to each student\nincreases the perceived importance of the survey compared to the portal alternative. This is analogous to the\nuse of first class postage in a mail survey compared to that of a bulk mailing of survey questionnaires. Second,\nwith the e-mail message, students are more likely to become aware of the survey than they would be from the\nuniversitys portal. This is because students are more likely to check their e-mail messages than they are to\ncheck the portal alternative and, even if they were to check their portal, there would be other items competing\nfor their attention.\nIt was further hypothesised that neither the obtained e-mail sample nor the obtained portal sample would be\nrepresentative of the population from which it was drawn on three characteristics \u00ad gender, class year and\nGPA. Past studies, have shown that (i) females and (ii) freshmen and sophomores were more likely to\nparticipate in internet surveys than their male and upper-class counterparts (Porter, 2004). In addition, it was\nhypothesised that the GPA of participating students would be higher than the non-respondents since those\nwith the most positive academic experiences and better grades would be the ones most likely to participate in\nthe survey due to these positive factors.\nFinally, it was believed that student participation in the survey would be correlated with the ratings given to\nthe academic and social life items with those students who evaluated these items most positively also being\nthe ones most likely to complete and return the questionnaire. Because of this, it was further hypothesised that\nthe smaller sample of respondents in the portal group would give higher and more positive ratings to these\nitems compared to the larger number of sample respondents in the e-mail group. However, this was\nhypothesised only for the academic and social life items and not for the food court, recreational centre items\nand problem area items which were hypothesised to have little, if any, correlation to survey participation.\nResults\nResponse rate\nThe student group that received the request for participation directly from an e-mail message had a response\nrate of 28% compared to a response rate of 21% for the group that received its request directly from their\ncustomised portal. As shown in Table 1., this difference of seven percentage points was significant at the .01\nlevel and provided confirmation of the previously stated hypothesis. In percentage terms, the e-mail group's\nresponse rate was one-third higher than the portal group's response rate. On the other hand, these rather low\nresponse rates occurred despite using data collection strategies in both groups that were designed to increase\nsurvey participation.\nTable 1. Survey Response Rate by Experimental Group\nNumber Number Response\nGroup completed selected ________ ratea\nSample representativeness\nOne critical consideration in any survey is whether the obtained sample of respondents is representative of the\npopulation from which it was drawn. In this study, the population characteristics with respect to three\nvariables (gender, class year and grade point average) were known. These known characteristics were\ncompared to the obtained sample characteristics of those in both the e-mail and portal groups. The results of\nthis comparison, shown in Table 2., revealed that neither sample was representative of the population from\nwhich it was drawn on either gender or GPA. The groups were, however, representative of the population with\nrespect to class year.\nThe differences between sample and population characteristics were large in the portal group for both gender\nand grade point average. While the overall population was evenly split between males and females, the\nfemales in the portal group were far more likely to participate in the survey compared to their male\ncounterparts (64% vs. 36%). Additionally, for grade point average, the GPAs for sample members in the\nportal group were significantly higher than the actual GPAs of students in the population. For example, 58%\nof the survey participants claimed to have a GPA of at least 3.25 compared to a known population percentage\nof 46%. Further, only 11% of the participants in the portal sample claimed to have a GPA under 2.75\ncompared to the known population percentage of 23%. One note of caution, since the sample participant\nGPAs were self-reported, the differentials that were found between sample participants and the population\ncould also be due to measurement error rather than to non-response error. In terms of class year, there was a\nslightly higher rate of participation among freshmen and sophomore students compared to those in their upper\nclass years. These differences, though, were too small to be statistically significant.\nTable 2. Sample versus Population Characteristics of Portal and E-mail Groups\nPortal E-mail ___\nCharacteristic Sample Population Difference Sample Population Difference\nGender\nGrade point average\nClass year\nSignificant differences were also found for gender and GPA between the e-mail sample of participants and the\npopulation. However, the difference for gender was considerably smaller. For the e-mail group, the sample\nwas overrepresented by females by just fourteen percentage points (57% vs. 43%) compared to a twenty-eight\npercentage point differential in the portal group. Again, for class year, statistically significant differences did\nnot exist between the sample respondents and the population.\nThe previous analysis compared each groups sample of participants to their respective populations. We now\nlook at this problem in a slightly different fashion in order to determine whether the actual respondents in the\ne-mail group differed from the actual respondents in the portal group in terms of gender, class year and GPA.\nResults are presented in Table 3. and indicate that the two groups differed only with respect to gender. There\nwas a significantly highly percentage of female respondents in the portal group compared to the percentage of\nfemale respondents in the e-mail group. This difference was significant at the .01 level. Significant differences\ndid not exist for either GPA or class year.\nTable 3. Portal and E-mail Group Comparisons\nCharacteristic Portal E-mail Difference\nGender\nGrade point average\nClass year\nAnalysis of survey responses\nThe next step in the analysis was to determine whether the survey responses regarding various aspects of a\nstudents life on campus varied depending upon whether the request for participation came directly from an e-\nmail message or from a notice on a students portal. We turn first to items involving the evaluations of\nparticular aspects of a students academic and social life. In this part of the questionnaire, students were asked\nto give their evaluation (1, 2, 3, 4, 5) on fifteen different dimensions. The actual scale was: Poor 1 2 3 4 5\nExcellent.\nTable 4. presents the differences in the average evaluation ratings for participants within each of the two\ngroups and the associated t-values, degrees of freedom and one-sided p-values. Consistent with the previously\nstated hypothesis, on every one of these fifteen items, the smaller number of participants in the portal group\ngave a higher and more positive evaluation rating than did the larger number of participants in the e-mail\ngroup. For seven of these fifteen items (including the overall evaluation rating), the differences were\nsignificant at the .01 level and, for two other items, the differences were significant at the .05 level.\nGiven the above results, a subsequent analysis was conducted to determine whether the differences that were\nobtained between the portal and the e-mail groups were more a function of gender than a function of the mode\nof initial contact. This was necessary since there were a far higher percentage of females in the portal group\nthan there were in the e-mail group. If females were substantially more positive in their ratings than males,\nthen the differences could be attributed more to gender differences than to differences in the mode of initial\ncontact. However, this turned out not to be the case as the subsequent analysis revealed that males and females\nvaried little in terms of their ratings to these fifteen items in both the e-mail and in the portal groups.\nTable 4. Mean Differences in Evaluations between Portal and E-mail Groupsa\nMean Evaluation\nItem Portal E-mail Diff. t df p-value\nStudents were asked for their opinions concerning the university's food court and its recreational centre. Once\nagain, a five-point evaluation scale was used. As hypothesised and unlike the previous set of items, there were\nno significant differences in the ratings of survey members in the portal and e-mail groups. This can be seen in\nFinally, students were asked to indicate the degree to which they believed that five potential problem areas\nwere present at their university. As shown in Table 6., on only one of these potential problem areas, academic\ndishonesty, was there a significant difference at the .05 level in the average ratings of the portal respondents\nand the e-mail group respondents. Here, those in the portal group were significantly more likely to believe that\nacademic dishonesty was a problem at the university. A possible explanation for this, in part, may be due to\nthe fact that female students, who were overrepresented in the portal group, were also significantly more\nlikely than their male counterparts to say that academic dishonesty was a problem on the campus. The next\ntwo largest differences were for alcohol abuse and drug abuse with portal members, once again, believing that\nthese were more a problem than those in the e-mail group, but the differences that were found were not large\nenough to be statistically significant.\nTable 5. Mean Differences for Food Court and Recreation Centre Items between Portal and E-mail\nGroups\nMean Evaluationa\nItem Portal E-mail Diff. t df p-value\nFood Courta\nRecreation Centre\naScale: Very dissatisfied 1 2 3 4 5 Very satisfied.\nbScale: Strongly disagree 1 2 3 4 5 Strongly agree.\nTable 6. Mean Differences in Perception of Campus Problems between Portal and Email Groups\nMean differencea\nProblem area Portal E-mail Diff. t df p-value\nDiscussion\nThe results of this experiment revealed that a significantly higher response rate was obtained in the e-mail\ngroup than in the portal group, that portal group sample members gave consistently more positive evaluations\nabout their academic and social life on campus than did their counterparts in the e-mail group, and that neither\nthe e-mail nor the portal alternative produced a sample that was representative of the population from which it\nwas drawn in terms of either gender or GPA. Taken together these three results indicate that the universitys\ndecision to switch its initial mode of contact from an e-mail message to students customised portal could\nhave potentially serious data quality implications in future surveys.\nIt appears that students who are frequent visitors to their portals are the ones who are most engaged with their\nuniversity and evaluate their experiences most positively.\nA sample obtained exclusively from the portal will clearly not provide a sample that is representative of the\nstudent body.\nAnother area for concern is the large difference in response rates between males and females, both overall and\nwithin each of the two groups. The reason for the concern is that there are likely to be many areas in which\nmales and females differ in their opinions, although this was not the case in the present study. While the\nreasons for these response rate differences are unknown, this particular problem requires future research since\nextensive efforts and resources are likely to be required to secure acceptable response rates and representative\nsamples. Serious consideration must also be given in future studies to conducting follow-up surveys of non-\nrespondents to determine the nature and extent of any biases that might exist.\nIn conclusion, there have been numerous studies that have looked at the incremental effects of alternative data\ncollection strategies in online surveys. Such studies are valuable for helping researchers in their attempts to\nobtain samples that are representative of the population. However, this study clearly suggests that online\nsurveys may have their own unique problems and researchers must be especially careful that the samples that\nthey obtain are, in fact, representative of their target populations. The findings also lend support to the\narguments made by researchers who have strongly advocated the use of mixed modes of data collection.\nReferences\nCook, C., Heath, F and Thompson, R. (2000) ,,A Meta-Analysis of Response Rates in Web- or Internet-Based\nDeutskens, E., de Ruyter, K., Wetzels, M. and Oosterveld, P. (2004) ,,Response Rate and Response Quality of\nInternet-based Surveys: An Experimental Study, Marketing Letters 15(1): 21-36.\nIlieva, J., Baron, S. and Healey, N.M. (2002) ,,Online surveys in marketing research: pros and cons,\nKaplowitz, M., Hadlock, T. and Levine, R. (2004) ,,A Comparison of Web and Mail Survey Response Rates,\nManfreda, K., Zenel, B. and Vehovar, V. (2002) ,,Design of Web Survey Questionnaires: Three Basic\nExperiments, Journal of Computer-Mediated Communication 7(3): Article can be found at:\nhttp://jcmc.indiana.edu/vol7/issue3/vehovar.html.\nMarcus, B., Bosnjak, M., Lindner, S., Pilischenko, S. and Schutz, A. (2007) ,,Compensating for Low Interest\nand Long Surveys: A Field Experiment on Nonresponse in Web Surveys, Social Science Computer Review\nPorter, S. (2004) ,,Raising Response Rates: What Works, New Directions for Institutional Research 121: 5-\nPorter, S. and Whitcomb, M. (2003) ,,The Impact of Contact Type on Web Survey Response Rates, Public\nPorter, S. and Whitcomb, M. (2005) ,,E-Mail Subject Lines and Their Effect on Web Survey Viewing and\nPorter, S. and Whitcomb, M. (2007) ,,Mixed-Mode Contacts in Web Surveys, Public Opinion Quarterly\nSchaefer, D. and Dillman, D. (1998) ,,Development of a Standard E-Mail Methodology, Public Opinion\nSheehan, K. (2001) ,,E-mail Survey Response Rates: A Review, Journal of Computer-Mediated-\nCommunication 6(2): Article can be found at: http://jcmc.indiana.edu/vol6/issue2/sheehan.html.\nTaylor, H. (2000) ,,Does internet research \"work\"? Comparing on-line survey results with telephone surveys,\nTaylor, H., Krane, D. and Thomas, R.K. (2005) ,,Best Foot Forward: Social Desirability in Telephone vs.\nOnline Surveys, Article can be found at: http://www.publicopinionpros.com/from_field/2005/feb/taylor.asp."
}