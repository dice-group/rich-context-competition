{
    "abstract": "Abstract\nThis investigation introduces a novel tool for identifying conscientious responders (CRs) and random responders (RRs) in\npsychological inventory data. The Conscientious Responders Scale (CRS) is a five-item validity measure that uses instructional\nitems to identify responders. Because each item instructs responders exactly how to answer that particular item, each\nresponse can be scored as either correct or incorrect. Given the long odds of answering a CRS item correctly by chance\nalone on a 7-point scale (14.29%), we reasoned that RRs would answer most items incorrectly, whereas CRs would answer\nthem correctly. This rationale was evaluated in two experiments in which CRs' CRS scores were compared against RRs'\nscores. As predicted, results showed large differences in CRS scores across responder groups. Moreover, the CRS correctly\nclassified responders as either conscientious or random with greater than 93% accuracy. Implications for the reliability and\neffectiveness of the CRS are discussed.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nArticle\nWhen a self-report psychological inventory1 is administered,\nthe expectation is that respondents follow testing instructions\nand answer its items as honestly and accurately as possible.\nThat is to say they respond conscientiously and thereby\ninfuse their responses with meaning about their inner psy-\nchological workings. Unfortunately, not all of the data that\nrespondents produce are generated consciously and, there-\nfore, not all data are valid. Some individuals, for example,\npurposefully distort their responses to be perceived more\npositively or negatively than they really are. This is known as\nfakinggoodandfakingbad,respectfully(Butcher,Dahlstrom,\nRandom responding is another form of data distortion in\nwhich respondents endorse items indiscriminately. Random\nresponders (RRs) answer items without regard for what they\nmean. For example, within a single inventory, a RR might\nanswer \"true\" to an item like \"I am taller than most people,\"\nbut because he or she is answering indiscriminately, answer\n\"false\" to a similar item such as \"I am tall.\" Resulting from a\nnumber of factors such as carelessness, fatigue, psychopa-\nthology, and low intelligence (Bentler, Jackson, & Messick,\n1971), the prevalence of random responding in self-report\ndata has been estimated to be up to 5% in non-disordered\npopulations (Clark, Gironda, & Young, 2003; Pinsoneault,\ntions (Archer, Handel, Lynch, & Elkins, 2002; McNulty\nFor clinical and applied psychologists, the presence of\nrandom data in their supposed valid data sets may lead them\nto make erroneous conclusions, diagnoses, and/or predic-\ntions about their clients (Ben-Porath & Waller, 1992). Bruehl,\nLofland, Sherman, and Carlson (1998) showed this possibil-\nity in a clever study using a widely used pain inventory. They\nconcluded that if the measure was administered to a group of\nRRs in a clinical setting and their random responding went\nunidentified, 35% of them would be classified as having\nelevated levels of interpersonal distress and another 35% as\nbeing highly adaptive copers. For researchers, random\nresponding poses different problems. Primarily, it increases\nmeasurement error, making it more difficult to identify sig-\nnificant relations when they are present in data. In other\nwords, it increases the likelihood of making Type II errors\nand otherwise jeopardizes the validity of one's results\n(Osborne & Blanchard, 2011). In a recent study, Cred\u00e9\n(2010) showed that even low rates of random responding\n(e.g., 5%) can have meaningful moderating effects on the\n1Thompson Rivers University, Kamloops, British Columbia, Canada\n2York University, Toronto, Ontario, Canada\nCorresponding Author:\nZdravko Marjanovic, Department of Psychology, Thompson Rivers\nUniversity, 900 McGill Road, Kamloops, British Columbia, Canada V2C\nEmail: zmarjanovic@tru.ca\nThe Conscientious Responders Scale:\nA New Tool for Discriminating Between\nConscientious and Random Responders\nZdravko Marjanovic1, C. Ward Struthers2, Robert Cribbie2, and\nEsther R. Greenglass2\n Keywords\nrandom responding, validity scale, personality, inventory, psychometric\n2 SAGE Open\nsize and direction of correlations, even increasing the likeli-\nhood of making Type I errors.\nOld Approaches to Detecting Random\nResponding\nHistorically, there have been two types of validity scales that\nhave been effective at identifying RRs in self-report inven-\ntory data: infrequency scales and inconsistency scales. An\ninfrequency scale is composed of absurd item content--\nitems that are endorsed so infrequently (e.g., <10% of con-\nscientious responders [CRs] answer \"true\" to the item \"I\ndrink 10 glasses of milk a day\") that it is reasonable to inter-\npret responses in the infrequent direction as highly unusual.\nIf a respondent endorses too many infrequency items in the\nunusual direction, it strongly indicates the presence of ran-\ndom responding and, therefore, an invalid test profile. An\nexemplar of an infrequency scale is the Minnesota\nMultiphasic Personality Inventory\u00ad2's (MMPI-2) F Scale\nThe logic behind an inconsistency scale is that if individu-\nals are paying attention to item content, they should respond\nconsistently to items that are semantically similar and incon-\nsistently to items that are semantically dissimilar. For exam-\nple, a person who answers \"true\" to an item like \"I am a\nhappy person\" should also answer \"true\" to the item \"I am\nhappy.\" An inconsistency scale is created by identifying\npairs of items that are usually answered in the same way,\nGiven this, if a person responds inconsistently to several\nsuch item pairs, it strongly indicates the likelihood of ran-\ndom responding and that the responder's data are invalid.\nExemplars of inconsistency scales are the Variable Response\nInconsistency (VRIN) and True Response Inconsistency\n(TRIN) scales of the MMPI-2 (Butcher et al., 1989).\nAlthough effective, researchers rarely use or develop\nthese scales due to their extensive costs. First, embedding\nan infrequency scale in a questionnaire makes it longer to\nadminister, making it costly in terms of fatigue for the test\ntaker and labor intensive for the test administrator. More\nimportantly, because researchers must expect that a small\nproportion of CRs answer these items truthfully but in the\ninfrequent direction (e.g., some individuals really do drink\n10 glasses of milk a day), extensive normative testing is\nfirst necessary to establish base rates of infrequent respond-\ning in samples of CRs. In addition, normative testing is\nrequired to establish base rates for different categories of\nresponders (e.g., disordered, non-disordered) and the vari-\nous settings in which these scales are likely to be adminis-\ntered (e.g., vocational, forensic). For example, in a\ndisordered population, one should expect that mean infre-\nquency scale scores are higher than they are in a non-disor-\ndered population (see Archer et al., 2002; McNulty et al.,\n2003). This is partly due to the fact that in a disordered\npopulation, CRs are more likely to endorsee infrequency\nitems in the infrequent direction. An item like \"I talk to\ndead people\" may be affirmatively endorsed because the\nresponder actually believes he or she is communicating\nwith the dead, not because they are answering indiscrimi-\nnately. This has been a long-standing criticism of infre-\nquency scales--that infrequency scale scores can sometimes\nconfound random responding with psychopathology (Arbisi\nIn contrast, inconsistency scales have the advantage not\nhaving to embed additional items in an inventory because\nthey are made up of an inventory's existing items. They do,\nhowever, still require an extensive amount of research to cre-\nate and validate. First, a psychologist must identify a pool of\nhighly correlated items within a measure from which to cre-\nate its inconsistency scale. Subsequently, the psychologist\nmust generate normative data to determine the optimal cutoff\nscore that most effectively discriminates between CR and\nRR. Similar to the above concerns, some cutoff scores may\nbe more appropriate to a particular responder population and\nsetting than others. Consequently, the entire process is unap-\npealingly laborious, time-consuming, and complex.\nUnfortunately, apart from these costly types of infre-\nquency- and inconsistency-validity scales, there are currently\nno practical means for psychologists to differentiate between\nCR and RR in self-report inventory data (see Meade & Craig,\n2012, for an evaluation of item-based and statistically based\nindices). Researchers and applied psychologists alike there-\nfore stand to benefit from a simple, reliable, and flexible tool\nthat can effectively identify RRs in inventory data without all\nof the above mess associated with traditional validity scales.\nThe development and evaluation of such a tool was the pri-\nmary goal of this investigation.\nUsing Instructional Item Content to\nIdentify Random Responding\nThe tool we developed for this investigation is called the\nConscientious Responders Scale (CRS; see the appendix),\nwhich is a five-item variant of a traditional validity scale.\nThe main advantage of the CRS over standard infrequency\nand inconsistency scales is that it does not require extensive\nnormative testing to establish its cutoff scores.2 This is\nbecause the CRS is made up of instructional item content.\nInstructional item content directs responders how to answer\neach particular item (e.g., CRS item 3, \"To respond to this\nquestion, please choose option number five, `slightly\nagree'\"); thus, unlike typical psychological inventory items,\nthere is an objectively correct response for every item. Each\ncorrect response is given a score of 1 and incorrect response\na score of 0. A CRS total score is generated by summing up\nall of a respondent's correct responses. Thus, scores range\nfrom 0 (all incorrect responses) to 5 (reflecting all correct\nresponses).\nMarjanovic et al. 3\nDepending on how many response options a responder\nhas to choose from, the likelihood that a RR will answer an\nitem correctly by chance alone can be estimated a priori\nusing probability theory. In our investigation, we used a\n7-point response-option scale for all measures.\nConsequently, the probability of a RR answering a CRS\nitem correctly was 14.29% (i.e., 1/k, where k is the number\nof response options). Because the probability of answering\nseveral items correctly is great deal lower, a high CRS\nscore was very likely the result of conscientious respond-\ning. This raised the following question: How high a CRS\nscore is necessary to reliably discriminate between CR and\nCalculating A Priori CRS Cutoff Scores\nUsing the binomial distribution, we were able to determine\nthat only 2.33% of RRs would be able achieve a CRS score\nof 3 or higher by chance alone (percentage answering 3\nwhereas 15.19% would be able to answer 2 or more cor-\nrectly by chance alone (percentage answering 2 correct =\n12.86%). In fact, most RRs would only be able to generate\nalmost no items correctly. Using the ubiquitous critical\nprobability value of p < .05 as our guide, we settled on a\n2/3 cutoff score for the purposes of this investigation. That\nis, because fewer than 5% of RRs can be expected to\nachieve CRS scores of 3, 4, or 5 by chance alone, we are\nconfident that these scores will be reflective of conscien-\ntious responding. Thus, responders with a CRS score of 3\nor higher will be labeled \"conscientious responders.\" In\ncontrast, responders with CRS scores of 2 or lower (i.e., 0,\n1, or 2), which are statistically indistinguishable from\nscores generated by random responding, will be labeled\n\"random responders.\"3\nThe Present Investigation: Purpose,\nDesign, and Strategy\nThe purpose of this investigation was to evaluate the effec-\ntiveness of the CRS for discriminating between CR and RR\nin self-report inventory data. We evaluated our new measure\nin two identical experiments in which a single 89-item ques-\ntionnaire was administered to university students either\nusing a paper-and-pencil format (Experiment 1) or over the\nInternet (Experiment 2). The question of interest was\nwhether CRS scores could reliably discriminate between\nRR and CR across these two widely used means for data\ncollections.\nAccording to recent studies, Internet collected data are\nequivalent to data collected through traditional methods,\nsuch as paper-and-pencil data questionnaires, in terms of\npsychometric properties such as factor structures, inter-scale\ncorrelations, means, and standard deviations (Johnson,\n2005). Importantly, Internet data have not yet been heavily\nscrutinized in terms of data distortion tendencies such as ran-\ndom responding. In one such study, Pettit (2002) found that\npaper-and-pencil responders actually produced slightly\nhigher rates of random responses than their Internet counter-\nparts. In that study, however, the Internet participants were\nall self-selected and therefore probably highly motivated to\nparticipate from the outset. The extent to which the average\nundergraduate student completes questionnaires conscien-\ntiously has long been a source of doubt and controversy\namong psychologists (Sears, 1986). Because students are\noften compelled to participate in psychological research as a\nmeans to fulfill program requirements, the typical student is\nprobably less motivated to participate conscientiously than\nwe expect him or her to be. Given that the Internet provides\nparticipants with greater anonymity than traditional forms of\ndata collection, unmotivated students may take advantage\nand produce more random responding than they normally\nwould on a paper-and-pencil questionnaire. Experiment 2\nwas conducted to replicate the findings of Experiment 1 in a\ntypical online administration of a psychological question-\nnaire. We also included a traditionally developed infrequency\nscale in our questionnaire to serve as a comparative measure\nagainst which we could evaluate the effectiveness of the\nData were collected in both experiments using the ana-\nlog design (e.g., Clark et al., 2003). Participants were ran-\ndomly assigned to one of two experimental conditions. In\nthe CR condition, participants were instructed to answer\nitems as honestly and accurately as possible. In the RR con-\ndition, participants were given a questionnaire with no\nactual item content, only blank response options, and\ninstructed to complete the response-option sheet as ran-\ndomly as possible.\nIn each experiment, the statistical strategy for evaluating\nCRS was threefold. First, group differences on CRS and\nPettit Random Responding Scale (PRRS) scores would be\nassessed using independent-samples t tests. Hypothesis 1\nstates that participants assigned to the CR group will produce\nsignificantly greater scores on the CRS and PRRS than par-\nticipants in the RR group. Second, because the CRS and\nPRRS purportedly measure the same construct (i.e., consci-\nentious responding), we conducted a correlation analysis to\nreveal whether these measures were indeed related.\nHypothesis 2 states that the CRS will be strongly and posi-\ntively correlated with the PRRS. Finally, in order that the\nCRS proves its worth as a tool for identifying CR and RR,\nHypothesis 3 states that it will correctly label participants in\nthe CR group as \"conscientious responders\" and participants\nin the RR condition as \"random responders\" at an average\nclassification accuracy rate of 80% (cf. Clark et al., 2003).\nThis analysis was also conducted on the PRRS for the pur-\nposes of comparison.\n4 SAGE Open\nMethod\nParticipants\nExperiment 1. A total of 68 participants were recruited from\na second-year psychology class in exchange for being entered\ninto a draw for $50 (CAD). In total, 33 students were ran-\ndomly assigned to the CR condition and 35 were randomly\nassigned to the RR condition. Three participants were\nremoved from the CR sample due to missing data. The final\n5.61). As we expected to find a \"large\" effect of CRS scores\nacross responder groups (Cohen's d  .80), this sample size\nwas more than adequate to detect statistical significance and\navoid Type II errors.\nExperiment 2. A total of 412 participants were recruited from\nan undergraduate research participant pool in exchange for\ncourse credit. Thirty-two participants (7.8%) were removed\ndue to missing data. The final total sample (N = 380, CR =\nindividuals who did not report their sex. The mean age of the\nMeasures\nThe same questionnaire was administered in both experi-\nments. In addition to the CRS and PRRS, all of the measures\nincluded were selected based on acceptable levels of internal\nconsistency and breadth of content, such as perfectionism\nand ethics. The subject matter and validity of each scale was\nirrelevant to their selection. All of the questionnaire's 89\nitems, including the CRS and PRRS items, were presented in\na scrambled, random order. All items were answered on a\n7-point Likert-type scale, ranging from 1 = strongly disagree\nto 7 = strongly agree, with 4 = neither agree nor disagree at\nthe midpoint. The questionnaire consisted of the following\nmeasures.\nSelf-Esteem Scale (SES).The 10-item SES is a widely used\nself-report measure of trait self-esteem (Rosenberg, 1965). It\nhas acceptable internal consistency across a variety of sam-\nples and has been extensively used in psychological research\n(Blascovich & Tomaka, 1993). Higher scores reflect greater\nlevels of trait self-esteem. A sample item is \"I wish I could\nhave more respect for myself.\"\nRight-Wing Authoritarianism\u00adShort Form (SRWA). The 14-item\nSRWA (Manganelli Rattazzi, Bobbio, & Canova, 2007) was\ninto two subscales measuring Authoritarian Aggression and\nSubmission (SRWA-A) and Conservatism (SRWA-C). Each\nsubscale has acceptable reliability and correlates highly with\nthe original 30-item RWA scale (Bobbio, Canova, & Man-\nganelli, 2010). Higher scores on either subscale reflect\ngreater levels of RWA. A sample item from the authoritarian\naggression and submission scale is \"The situation in our\ncountry is getting so serious, the strongest methods would be\njustified if they eliminated the troublemakers and got us back\nto our true path.\"\nMultidimensional Perfectionism Scale (MPS). The 35-item MPS\n(Frost, Marten, Lahart, & Rosenblate, 1990) is a scale that\nassesses six factors of trait perfectionism. Its subscales are\nConcern Over Mistakes (MPS-CM), Organization (MPS-O),\nParental Criticism (MPS-PC), Personal Standards (MPS-\nPS), Doubts (MPS-D), and Parental Expectations (MPS-PE).\nThe MPS has acceptable internal consistencies, with esti-\nmates for its subscales ranging between .73 and .93 (Frost et\nal., 1990), and has been used extensively in perfectionism\nresearch (Parker & Adkins, 1995). Higher scores reflect\ngreater levels of perfectionism on all subscales. A sample\nitem from the personal standards subscale is \"I set higher\ngoals than most people.\"\nEthics Position Questionnaire (EPQ).The 20-item EPQ (For-\nsyth, 1980) measures the philosophical framework from\nwhich individuals justify their decisions and behaviors. It\ncontains two subscales, Idealism (EPQ-I) and Relativism\n(EPQ-R), which previous research has shown to be internally\nconsistent measures (e.g., Davis, Andersen, & Curtis, 2001).\nHigher scores reflect greater levels of both ethical idealism\nand relativism. A sample item from the relativism subscale is\n\"Whether a lie is judged to be moral or immoral depends\nupon the circumstances surrounding the action.\"\nPRRS. The PRRS is a 10-item infrequency scale containing\nscale, items endorsed in the infrequent (statistically unusual)\ndirection are scored as 1s, whereas items endorsed in the fre-\nquent direction are scored as 0s. We reversed this scoring\nsystem so that higher PRRS sum scores reflect greater con-\nscientious responding, not random responding. The original\nmeasure's cutoff score had also to be altered because of this\nscaling change, such that the original cutoff score of 2/3 was\nchanged to 7/8. Put another way, only responders who\nachieved a high score of 8, 9, or 10 were labeled \"conscien-\ntious responder.\" Low scorers (i.e., 7 and less) were labeled\n\"random responder.\"\nThe original scale was validated on a large Internet sam-\nple using a dichotomous response scale and its psychometric\n7-point scale was used in this investigation, the likelihood of\na RR answering an item correctly by chance alone was\nPRRS' 7/8 cutoff score a more difficult standard for a RR to\nmeet. A sample PRRS item is \"Sometimes I feel warm or\ncool,\" to which answering anything but \"strongly agree\" is\nan empirically infrequent response and is assigned a score\nMarjanovic et al. 5\nCRS. The CRS is a variant of a traditional infrequency scale\nthat relies on instructional item content to identify CRs and\nRRs (see the appendix). The CRS is made up of five items\nthat direct the responder exactly how to answer that particu-\nlar item, such that each item has only one possible correct\nresponse. Thus, the number of the measure's items, as well as\nthe number of response options, can be used to generate\neffective cutoff scores using probability theory. For the pur-\nposes of this investigation, we adopted the p < .05 critical\nvalue as our standard. Using the binomial distribution, we\ncalculated that fewer than 5% of RRs would be able to\nachieve a CRS score of 3, 4, or 5, and were most likely to\nachieve a score of 0, 1, or 2. Consequently, 2/3 became our\ncutoff score to discriminate between CR and RR. Higher\nscorers (3 and above) were labeled \"conscientious responder\"\nand low scorers (2 and below) were labeled \"random\nresponder.\"\nProcedure\nWhether recruited in a second-year psychology class to com-\nplete an in-class paper-and-pencil questionnaire (Experiment 1)\nor from an undergraduate research participant pool to com-\nplete an online questionnaire of the same length (Experiment\n2), participants were randomly assigned to complete one of\ntwo versions of the 89-item questionnaire. Participants\nassigned to the CR group received standard questionnaire\ninstructions with some additional language that prepared\nthem for the instructional nature of the CRS items: \"Some of\nthe items will ask you to answer them in a particular way . .\n.\" Their questionnaire contained all of the measures listed\nabove. In contrast, participants assigned to the RR group\nreceived a questionnaire with no items inside, only a 7-point\nLikert-type scale they had to endorse for each missing item.\nThese participants were instructed to \"respond on the scales\nbelow as randomly as possible, but do this in such a way that\nit will not be apparent that this is what you did.\" Thus, in an\neffort to simulate reality, participants tried not to make their\nrandom responses so obvious that they would be easily iden-\ntified by a visual inspection of the data. These instructions\nhave similarly been used in other validity scale studies that\nused the analog design (e.g., Clark et al., 2003). In the\ndebriefing, no CR participants reported having any difficulty\nunderstanding the instructions or completing the items in the\nquestionnaire.\nResults\nDescriptive statistics for all of the measures across responder\ngroups and experiments are presented in Table 1. Our first\nanalysis involved conducting independent-samples t tests to\ntest Hypothesis 1. In Experiment 1, as predicted, the CR\nTable 1. Descriptive Statistics Across Responder Groups and Experiments..\nExperiment 1 Experiment 2\n Responder group Responder group\nRR group\nMeasure M SD  M SD M SD  M SD\nNote. Cronbach's alpha () was calculated in the unconfirmed conscientious responder group only. CR = conscientious responder; RR = random\nresponder; CRS = Conscientious Responders Scale; PRRS = Pettit Random Responding Scale; SES = Self-Esteem Scale; SRWA = Right-Wing\nAuthoritarianism\u00adShort Form (A = Authoritarian Aggression and Submission; C = Conservatism); MPS = Multidimensional Perfectionism Scale (CM =\nConcern Over Mistakes; PS = Personal Standards; PE = Parental Expectations; PC = Parental Criticism; D = Doubts; O = Organization); EPQ = Ethics\nPositions Questionnaire (I = Idealism; R = Relativism).\n6 SAGE Open\ngroup produced significantly larger CRS scores (M = 4.53,\n2, results were much the same. The CR group produced sig-\n3.02, respectively. That is, across experiments, CRs were\nmuch more likely to answer CRS items correctly and PRRS\nitems in the frequent direction than were RRs. Hypothesis 1\nwas therefore fully supported.\nWe next calculated zero-order correlations to test\nHypothesis 2, that the CRS and PRRS would be strongly\npositively related because they both purportedly measure the\nsame construct. This hypothesis was also fully supported by\nhighly on the CRS, they were also very likely to have scored\nhighly on the PRRS.\nIn the final stage of the analysis, we examined CRS scores\nin the RR and CR groups to assess the effectiveness of our\ntheoretically derived, a priori cutoff score (see Table 2 for\nCRS scores across responder groups and experiments). In the\nRR group in Experiment 1 (n = 35), as expected, 0 partici-\npants answered four or all five CRS items correctly, and only\n1 participant answered three items correctly by chance alone.\nThus, as expected, fewer than 5% of RRs were capable of\nproducing a CRS score of 3, 4, or 5. In contrast, in the CR\ngroup, 27 participants produced CRS scores of 5, answering\nall CRS items correctly. The remaining 3 participants' scores\nfell below the 2/3 cutoff at 1, 0, and 0, answering nearly all\nof the items incorrectly. Altogether, our a priori 2/3 cutoff\nproduced a classification accuracy rate (i.e., number of cor-\nrectly labeled responders to their responders groups/total n\nof that responders group) of 90.00% in the CR group (mak-\nof 35), and 93.85% averaged across both groups, in full sup-\nport of Hypothesis 3. The PRRS similarly correctly labeled\n35 RR responders as \"random\" (100%), and produced an\noverall classification accuracy rate of 93.85%.\nIn Experiment 2, results were highly similar for the CRS.\nAltogether, our a priori 2/3 cutoff produced a classification\nand 93.42% averaged across both groups, in full support of\nHypothesis 3. In contrast, results were substantially worse\nfor the PRRS. The PRRS correctly labeled 47.12% in the CR\n100% in the RR group as \"random responders\" (0 errors of\naged across both groups. Thus, the CRS produced a similar\nresult, again exceeding the 80% classification accuracy cri-\nterion, whereas the PRRS failed to meet that standard.\nGiven that the PRRS results were so jarringly different\nacross the experiments, we reasoned that the problem was\nlikely due to the imposition of the a priori 7/8 cutoff score on\nthe data. Although it suited the Experiment 1 data just fine, in\nExperiment 2 it was too conservative, leading to too many\nCR participants being labeled as \"random responders.\" To\nexplore this hypothesis further, we conducted binary logistic\nregression analyses for each of the CRS and PRRS measures\nin both sets of experimental data. Specifically, we sought to\nexamine whether empirically derived cutoffs generated by\nthe logistic regression models would be different and more\neffective than the a priori cutoff scores we imposed on the\ndata.\nIn both sets of data, two binary logistic regressions were\nconducted in which the criterion variable (responder\ngroup: RR or CR) was regressed on the either the CRS or\nPRRS as the predictor variable. As expected, results of all\nfour regressions were significant. For the CRS, results\nshowed that 2/3 was the best empirically based cutoff to\naccurately differentiate between CR and RR--the same as\nthe theoretically derived cutoff. In contrast, results from\nTable 2. CRS Scores Across Responder Groups and Experiments.\nExperiment 1 Experiment 2\nCRS score\nScore\nfrequency\nScore\npercentage\nCumulative\npercentage\nScore\nfrequency\nScore\npercentage\nCumulative\npercentage\nScore\nfrequency\nScore\npercentage\nScore\nfrequency\nScore\npercentage\nScore\nfrequency\nScore\npercentage\nNote. CRS scores = larger scores (i.e., 5, 4, and 3) reflect a greater rate of conscientious responding. Cumulative percentage = % of sample to score at or above the CRS score.\nCRS = Conscientious Responders Scale; CR = conscientious responder; RR = random responder.\nMarjanovic et al. 7\nthe PRRS logistic regressions were significant, but showed\nthat the a priori 7/8 cutoff was not the optimal cutoff in\neither data set. In Experiment 1, 4/5 was shown to be a bet-\nter empirical cutoff (correctly classifying 100% of CR par-\nticipants and 100% of RR participants), whereas in\nExperiment 2, the best cutoff was 3/4 (correctly classifying\nproducing an average accuracy rate of 93.93%). The\nsmaller 3/4 cutoff was better in Experiment 2 because CR\nparticipants in that study produced a lower mean score\nthan in Experiment 1.\nIn sum, these logistic regression data showed that an\neffective CRS cutoff score can be generated a priori using\nprobability theory and applied reliably across data sets. In\ncontrast, an effective a priori PRRS cutoff cannot be reliably\napplied across data sets. Rather, an empirical cutoff score\nneeds to be generated for every data set it is used to optimize\nits discriminative power.\nDiscussion\nThe purpose of this investigation was to evaluate the effec-\ntiveness of a novel tool for identifying CR and RR in self-\nreport inventory data. The CRS is a five-item variant of a\ntraditional validity scale, which uses instructional item con-\ntent and theoretically derived cutoff scores as its means to\nidentify responders. Because CRs are assumed to follow test-\ning instructions diligently, answering items as honestly and\naccurately as possible, we expected them to answer all of the\nCRS items correctly. In contrast, because RRs answer items\nindiscriminately, we expected them to account for all of the\nincorrect responses in the data, and only a very small propor-\ntion of items answered correctly that were due to chance. In\nour questionnaire, we used a 7-point scale; thus, the chance\nof a RR answering an item correctly was 14.29%. Given this\nrationale, we hypothesized that CRs would produce CRS\ntotal scores near the ceiling of the scale's range (i.e., 5) and\nRRs near the floor (i.e., 0). The large gap in expected scale\nscores would make it easy to discriminate between individ-\nual cases of conscientious and random responding. The other\nunique advantage of instructional items over the traditional\nvariety used in infrequency scales stems from the fact that\nthey can be objectively scored as correct or incorrect.\nBecause of this difference, effective cutoff scores can be\ngenerated using probability theory and therefore eliminate\nthe need for extensive normative testing. This would save\ntest developers the laborious task of having to validate every\nsingle validity measure they create, and also allow test\nadministrators the flexibility of being able change the CRS\nformat depending on their particular testing requirements\n(e.g., by increasing or decreasing the number of its items or\nthe size of its response-option scale). These were the main\nideas behind the CRS when we designed it. This investiga-\ntion was conducted to assess whether these lofty specula-\ntions were realistic.\nOverall, the findings of this investigation were positive\nfor the discriminative power and validity of the CRS. As pre-\ndicted, CRs produced significantly larger CRS scores than\nRRs across experiments and these group differences were\nlarge in magnitude. The PRRS, a traditionally developed\ninfrequency scale, which was administered alongside the\nCRS for comparative purposes, correlated positively and\nstrongly with the CRS. For both measures, CRs produced\nscores toward the ceiling of the measures' scale range (5 for\nthe CRS and 10 for the PRRS), whereas RRs produced mean\nscores near the scale floors (0 for both measures). Because\nthe PRRS contains twice the number of items the CRS has,\nthe average difference between the CR and RR groups' sum\nscores was larger for the PRRS and this produced a larger\neffect size. In Experiment 2, this difference was largely elim-\ninated because CRs produced PRRS scores nearer the middle\nof the measure's scoring range.\nThe implication of this consistency is positive for the\nCRS. Given that across testing situations one can reliably\nexpect CRs to produce a score near the ceiling and RRs near\nthe floor, an a priori cutoff will be consistently effective at\nidentifying responders. In these data, the theoretically\nderived 2/3 cutoff accurately discriminated between CR and\nRR about 93% of the time. Additional analyses with binary\nlogistic regression models showed that the theoretically\nderived cutoff was identical to empirically derived cutoff\nscores from both sets of data. This agreement boosts the\nvalidity of our probability-based approach to generating cut-\noff scores.\nResults for the PRRS were good, but less positive. Like\nthe CRS, the PRRS produced large group differences\nbetween CR and RR, making distinguishing between them a\nfairly easy task. However, unlike the CRS, the size of the\ngroup difference in PRRS scores was inconsistent across\nstudies. Moreover, the optimal empirical cutoff score\nchanged from Experiment 1 to Experiment 2 and in neither\nstudy agreed with the a priori cutoff score of 7/8. The conse-\nquence of this was nicely demonstrated by the dramatic loss\nof discriminative power across studies. In Experiment 1, the\na priori PRRS cutoff score produced an average classifica-\ntion accuracy of 93%, whereas in Experiment 2, its accuracy\nfell to just above 73%, failing to even meet the 80% classifi-\ncation standard. Consequently, one has to seriously doubt the\nutility of an a priori PRRS cutoff score, like the one we used\nin this investigation. The PRRS worked best using empiri-\ncally based cutoffs. This means that after collecting PRRS\ndata, an administrator should generate an equally large set of\nrandom data and run statistical analyses to identify the best\nempirical cutoff score. In sum, the effort required to use the\nPRRS effectively is far greater than it is to effectively use the\nCRS. With the CRS, a theoretically derived cutoff score can\nwork reliably and effectively in a greater variety of testing\nscenarios. One has only to tally responders' scores and then\nassign them their appropriate responder labels. No normative\ntesting is required.\n8 SAGE Open\nAppendix\nConscientious Responders Scale (CRS)\n1. To answer this question, please choose option number four, \"neither agree nor disagree.\"\n2. Choose the first option--\"strongly disagree\"--in answering this question.\n3. To respond to this question, please choose option number five, \"slightly agree.\"\n4. Please answer this question by choosing option number two, \"disagree.\"\n5. In response to this question, please choose option number three, \"slightly disagree.\"\nNote. In this investigation, the CRS was administered using a 7-point Likert-type scale. To use the CRS effectively, embed its items randomly throughout\nthe length of a questionnaire, not all in a row or cluster. To prevent responders from being surprised or confused by the instructional nature of the CRS\nitems, we added a line to our questionnaire's instructions that warned, \"Some of the items will ask you to answer them in a particular way . . .\"\nLimitations and Future Research\nAlthough the findings of this investigation were straightfor-\nward, producing large CRS score differences across\nresponder conditions and nearly identical results across\nexperiments, the utility of the CRS should be further evalu-\nated using a wider variety of study designs and settings in\nwhich inventories are commonly used. For example, the\nCRS effective in forensic and psychiatric settings, where\nrates of random responding are highest (Archer et al., 2002;\nMcNulty et al., 2003), may be somewhat different than it was\nin this investigation with student, non-disordered samples.\nGiven that the CRS only requires respondents to follow sim-\nple instructions, at this point we believe that the CRS is safe\nfor use in non-disordered samples and for research purposes.\nCaution should be exercised when using it outside of these\ngroups or for individual assessment. In addition, the classifi-\ncation accuracy of the CRS should be examined at finer gra-\ndients of random responding, for example, in identifying\nresponders who engage in random responding in only 25%\nof a questionnaire's items versus 100% of them. Research on\nthe prevalence of random responding suggests that this form\nof intermittent random responding may account for the bulk\nof all random responding cases, as most responders admit to\nresponding randomly to at least some of a questionnaire's\nitems, but few report doing it to all of them (e.g., Baer,\nAlso, the classification accuracy of the CRS should be\nevaluated against multiple standards of comparison, such as\nthe highly regarded validity scales of the MMPI series (the F\nScale, and VRIN and TRIN scales; Butcher et al., 1989), and\nperhaps the completion times of online-administered ques-\ntionnaires. In an online question-naire, for example, it is rea-\nsonable to assume that a 100-item inventory completed in 1\nmin is not the result of conscientious responding. We predict\nthat responders who produce these and the types of abnormal\nresponding patterns would also produce very low CRS\nscores.\nFinally, there is an off chance that embedding validity\nscales like the CRS or PRRS in a questionnaire may exacer-\nbate random responding by lowering the questionnaire's face\nvalidity. Face validity is defined as the extent to which item\ncontent seems appropriate for the purposes of a given testing\nsituation (Holden & Jackson, 1979). For example, when\nassessing sadism, an item like \"I enjoy hurting others\" has\nhigher face validity than the item \"I would enjoy the occupa-\ntion of a butcher.\" Because the CRS items instruct one how\nto respond, which is very different from what people expect\nto find in an inventory, their odd nature may sap the motiva-\ntion of some responders to participate conscientiously.\nPerhaps, even the notion of being told what to do may moti-\nvate some individuals to respond incompliantly in a fit of\npsychological reactance (Miron & Brehm, 2006). With infre-\nquency scales, their item content may seem so absurd in\nsome cases that responders may feel ridiculous and put off in\nhaving to respond to them, which similarly may sap their\nmotivation to act conscientiously. As far as we are aware,\nthis hypothesis that random responding scales exacerbate\nrandom responding has not been experimentally examined\nand perhaps should be pursued in future research. We specu-\nlate, however, that given the prevalence and utility of some\nlow-face-validity inventories (e.g., the MMPI series), if there\nwas an effect here to find, it would be negligible in size and\nfully compensated by the positive effects of validity scales\n(i.e., being able to discriminate between CRs and RRs).\nConclusion\nDue to the many costs associated with random responding\nscales, basic and applied psychologists rarely use them when\nadministering inventories. This investigation aimed to rem-\nedy this situation with the introduction and preliminary vali-\ndation of the CRS, a five-item measure that uses instructional\nitem content to achieve this goal. Results across two experi-\nments were compelling in that effect sizes were large, results\nwere consistent across samples, and the CRS was accurate in\nclassifying responders about 93% of the time. Simply put, by\nembedding the CRS items randomly throughout a question-\nnaire, researchers can use endorsements of the CRS items as\nreliable indicators of whether data were generated conscien-\ntiously and should be retained or whether they were pro-\nduced randomly and should be deleted.\nMarjanovic et al. 9\n"
}