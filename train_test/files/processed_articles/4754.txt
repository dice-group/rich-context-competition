{
    "abstract": "Abstract\nThis study examined data from 96 schools in a Southeastern U.S. state participating in training and/or coaching on School-\nWide Positive Behavioral Interventions and Supports (SWPBIS) provided by the State Personnel Development Grant (SPDG)\nin their state. Schools studied either received training only (\"non-intensive\" sites) or training and on-site coaching (\"intensive\"\nsites). Fidelity of implementation was self-evaluated by both types of schools using the Benchmarks of Quality (BOQ). Some\nschools were also externally evaluated using the School-Wide Evaluation Tool (SET), with those scoring 80% or higher\ndetermined \"model sites.\" Using an independent sample t-test, analyses revealed statistically significant differences between\nintensive and nonintensive schools' Quality of Distribution Index (QDI) scores and between model sites and nonmodel sites\non QDI scores. Correlations were performed to determine whether the fidelity of implementation of SWPBIS as measured\nby the BOQ was related to any of the state's accountability measures: performance classification, QDI, or growth.\n",
    "reduced_content": "sgo.sagepub.com\nArticle\nThe evolving dynamics in today's fast-paced life has brought\nabout varied changes, especially in the field of education.\nThe use of a more rigorous curriculum and demanding\naccountability measures are salient factors educators and\nschool districts across the country consider. Meeting these\ncurricular and accountability demands is negatively impacted\nby disruptive behavior inside the classroom. This kind of\nbehavior threatens to diminish the quality and the amount of\ntime devoted to the academic instruction students receive\ninside the classroom (Cotton, 1990; Oliver, Wehby, &\nof instructional time due to recurrent classroom disruptions\nhas been cited as a factor negatively affecting the scores of\nU.S. students in Reading and Math (Lassen, Steele, & Sailor,\nSchool-Wide Positive Behavior Interventions and\nSupports (SWPBIS) have been used in schools for more than\na decade to improve student behavior, and ultimately student\nperformance (Sugai & Horner, 2001). With increased atten-\ntion focused on student outcomes through accountability\nrequirements of laws such as No Child Left Behind (NCLB,\ntices designed to identify and remediate students' academic\ndifficulties early (Sugai & Horner, 2006). At the same time,\nthere is movement across the nation to make state standards\nmore uniform, and comparisons among state's performance\nmore easily done (Common Core State Standards, 2013). In\nlight of these distinct, yet related educational reforms, it is\nimportant to investigate the relationships between the imple-\nmentation of Positive Behavior Interventions and Supports\n(PBIS) and the accountability measures used in this\nSoutheastern state to evaluate schools and school districts.\nThe Southeastern state in which this research took place\nhas endured a long cycle of poverty (Noss, 2012) and low\nacademic attainment compared with other states (Mississippi\nDepartment of Education, 2010). The socioeconomic com-\nposition of the population in the region may make the student\npopulation vulnerable to socioeconomic factors (Davis-\nwho live in poverty have been underserved in educational\n2010), which may impede the chances of educational\n1University of Southern Mississippi, Hattiesburg, MS, USA\nCorresponding Author:\nAdriana M. Marin, University of Southern Mississippi, 118 College Drive\nEmail: adriana.marin@eagles.usm.edu\nThe Relationship Between\nImplementation of School-Wide Positive\nBehavior Intervention and Supports and\nPerformance on State Accountability\nMeasures\nAdriana M. Marin1 and Hollie Gabler Filce1\n Keywords\npositive behavior interventions and supports, accountability, professional development, mentoring\n2 SAGE Open\nsuccess. Paolella (2009) noted that children in poverty and\nminority groups may be more predisposed to exhibit disci-\npline and/or behavioral issues in the classroom.\nIncreasing student performance is no easy feat, particu-\nlarly in states where poverty, mobility due to catastrophic\nnatural events, and low academic attainment are prevalent\n(Smith, Fien, & Paine, 2008). The characteristics of the\npopulation in the region, unsatisfactory results of the stu-\ndents in the state tests, high dropout rates, and low gradua-\ntion rates called for the implementation of strategies to help\nminimize class disruptions and maximize instructional time\n(Sugai & Horner, 2006). States in the southern United\nStates have long struggled to overcome circumstances such\nas these through educational initiatives (Berry & Fuller,\nplace has the lowest household income in the country\nIn 2005, the State Board of Education adopted the Three-\nTier Instructional Model (MDE, 2012) to meet students'\nneeds. Tier 1 refers to the quality of the classroom instruction\nbased on the state's Curriculum Frameworks (MDE, 2012),\nTier 2 refers to the focused supplemental instruction, whereas\nTier 3 deals with intensive interventions to meet the students'\nindividual needs that include instructional and/or behavioral\nneeds. While there are several initiatives taking place in the\nstate, the primary support structure for schools implementing\nSWPBIS is the State Personnel Development Grant (SPDG).\nOriginally developed prior to the current accountability\nmandates and measures, this state's SPDG was designed to\naddress students' behavioral needs to decrease dropout rates\nand increase graduation rates in the state (MDE, 2010). The\nstate's SPDG goal was to provide the training necessary for\nteachers to improve classroom management that may lead to\na better classroom climate and school climate (Komro, Flay,\n& Biglan, 2011). There is a plethora of research about the\ninfluence of PBIS on academic achievement that may result\nto the emerging literature on the relationship of SWPBIS\nimplementation fidelity measures and accountability out-\ncomes of participating schools.\nAccountability\nIn 1965, U.S. president Lyndon Johnson declared war on\npoverty with the implementation of The Elementary and\nSecondary Education Act (ESEA; Public Law 89-10). The\nreform provided financial support to local education agen-\ncies serving children who came from socioeconomically dis-\nadvantaged homes (Irons & Harris, 2007) in an effort to\nimprove academic achievement and thus close the achieve-\nment gap (Lassen et al., 2006). When state and local educa-\ntional agencies accept money from the U.S. Department of\nEducation as authorized by Title I of the ESEA legislation,\nthey also obligate themselves to following regulations\nimposed by those laws, including increasingly demanding\nthorization of ESEA, also known as No Child Left Behind\n(NCLB, P.L. 107-110), highlighted school districts' account-\nability for students' achievement, or lack of it. Since it is\nrequired to assess students and monitor academic achieve-\nment, states have adopted various indicators of educational\noutcomes in an effort to meet these demands (Vaughn, Bos,\nNo Child Left Behind's required accountability systems\nnot only prompted schools to look more closely at the aca-\ndemic outcomes of their students, but also the underlying\nfactors that supported or impeded academic achievement\n(NCLB, 2002). The measure promoted the creation of the\nstates' own accountability systems to evaluate school dis-\ntricts, schools, and teachers (Irons & Harris, 2007). At the\nsame time, the Response to Intervention (RtI) movement\ngave a way to monitor progress toward accountability targets\nas stated by Carney and Stiefel (2008). These systems pro-\nvide a framework for schools to provide interventions and\nsupports to students as they encounter difficulty in the class-\nroom (MDE, 2010). Most states' RtI models include aca-\ndemic and behavioral components, recognizing that these\ntwo aspects are interrelated and must be addressed when\nattempting to facilitate maximal student achievement\nThe Southeastern state's RtI model, where the study took\nplace, is designed to provide students with the academic and\nbehavioral supports required for students to succeed in the\nclassroom (MDE, 2010). The model may help schools and\nschool districts promote academic achievement that may\nlead to students' higher scores on the state's high-stakes test.\nThe test is administered to students toward the end of the\nschool year and the scores are used to determine if schools\nand/or school districts met the state's accountability require-\nments. The state's accountability system, composed of state\nand federal components (MDE, 2010), began implementing\nschool year. QDI is calculated using data from the MCT2\n(Mississippi Curriculum Test, Second Edition) language arts\nand mathematics tests, SubjectArea Testing Program (SATP)\ndata from the Algebra I, Biology I, English II, and U.S.\nHistory tests, and the results from the language arts and\nmathematics sections of the MississippiAlternateAssessment\nof Extended Curriculum Frameworks (MAAECF; MDE,\n2013) . The resulting score is then utilized to rank schools\nand school districts as follows: A, Star School; B, High\nPerforming; C, Successful; D, Academic Watch; and F, Low\nPerforming, At-Risk of Failing, and Failing. The use of both\nPerformance Classifications allows districts, schools, and\nparents to understand how the former classification, used\nschool year (MDE, 2013). The Performance Classification\nsummarizes the performance of schools and school districts\nMarin and Filce 3\nafter all the state's accountability measures have been\naccounted for.\nAccording to the Southeastern State Department of\nyears, the QDI range for districts and schools without a\nQDI range for districts and schools without a 12th grade that\napplied to Low-Performing and At-Risk of Failing districts/\nschools, whereas the latter is applied to Failing districts/\nschools.\ntricts/schools with a 12th grade was the same used for dis-\ntricts/schools without a 12th grade as High School\nCompletion Index (HSCI) was factored in separately. For\nQDI calculations: 5-year graduation rate for the state and\n4-year graduation rate as mandated by NCLB. The QDI\nrange for districts/schools with a 12th grade and a 5-year\ngraduation rate/HSCI meeting Growth is as follows: A\n(0-99). The QDI range for districts/schools with a 12th and\na 5-year graduation rate/High School Completion Index\nletter grade is applied to Low-Performing and At-Risk of\nFailing districts/schools, whereas the latter is applied to\nFailing districts/schools. The 4-year graduation rate calcu-\nlation for districts/schools with a 12th grade comprises the\ndistricts/schools' QDI plus the graduation rate. The QDI\nrange for districts/schools that meet Growth is as follows: A\n(0-169), whereas the QDI range for districts/schools that do\napplied to Low-Performing and At-Risk of Failing districts/\nschools. The latter is applied to Failing districts/schools.\nThe accountability model also includes schools' and\nschool districts' Growth status, in which a district and/or\nschool's actual achievement is compared with the expected\nachievement to determine whether Growth has been met\n(MDE, 2013). Graduation rates (4-year graduation rate\nrequired under NCLB and 5-year graduation rate required for\nthe state component), High School Completion Index\n(HSCI), Annual Measurable Objectives (AMO) for Reading\nand Math, and a third indicator referred to as \"Other\nAcademic Indicator\" that for schools without a Grade 12 is\nthe attendance rate and for schools with a Grade 12 is the\ngraduation rate, complete the state's accountability mea-\nsures. The new Performance Classification, A-F, accounts\nfor an increase of districts obtaining higher letter values and\na decrease of districts obtaining the lowest letter values\nfell in the D and F categories (MDE, 2012).\nSatisfactory results on the report card of school districts\nguarantee the continuous infusion of federal money into the\npublic school systems (Vaughn et al., 2010). Modifications\nhave been made in school districts across the state to meet\nthe requirements imposed by NCLB and the allocation of\nTitle I funds. The increased interest in the state in regard to\nstudents'academic achievement has brought attention to cur-\nriculum, instruction, and assessment (English & Steffy,\nneeded on the behavioral issues negatively impacting class-\nroom instruction (Crone, Horner, & Hawkin, 2004).\nSchool-Wide Positive Behavior\nInterventions and Supports\nTeachers are expected to meet the academic and behavioral\nneeds of their students (Crone et al., 2004; Muscott, Mann, &\nLeBrun, 2008) to deliver appropriate instruction and to\nensure optimal student achievement (Oliver et al., 2011;\nand behavior disruptions affect the quality and quantity of\ninstruction inside American schools. The author stated that\n\"approximately one-half of all classroom time is taken up\nwith activities other than instruction, and discipline problems\nare responsible for a significant portion of this lost instruc-\ntional time.\" (p. 1)\nIn an effort to preserve instructional time in the class-\nrooms, school districts across the country have long tried dif-\nferent approaches for discipline and classroom management.\nSWPBIS movement has been around since the 1990's (Sugai\ndefined Positive Behavior Interventions and Supports as the\nmeasures created and put in place in the classrooms and at\nschools sites to deal with undesirable behaviors and to pro-\nmote optimal conditions conducive to learning.\nSWPBIS is intended to minimize and/or prevent class-\nroom disruptions to protect instructional time (Sugai &\nSimonsen, 2012). Researchers believe that the approach\nmight advance students' performance in the classroom\nresulting in high scores in the state tests (Jia et al., 2009;\nHorner (2001), the codirectors of the Office of Special\nEducation Program (OSEP) Technical Assistance Center on\nPBIS, noted the importance of implementing school-wide\nand district-wide PBIS to create a nurturing, inclusive, and\nsafe learning environment. Jia et al. (2009) and Rowe and\nStewart (2009) reported that the school environment affects\nstudents' academic performance in negative or positive\nways. Komro et al. (2011) noted that \"positive school envi-\nronments help students feel connected to school, which is\nassociated with improved academic achievement\" (p. 120).\nThe benefits of the implementation of PBIS are recurrent in\nthe literature.\nPeshak and Kincaid (2008) noted that many schools\nacross the country implement some type of SWPBIS seeking\nto address students'behavior at schools. The authors reported\n4 SAGE Open\nthat the first step in the implementation of SWPBIS is the\nestablishment of a school leadership team that provides the\nvision, the leadership, and the resources necessary for the\nsuccessful execution of the strategies at school level.\nResearch has shown that the appropriate implementation of\nSWPBIS strategies at schools and school districts might\nhave positive outcomes that in turn might improve the cli-\nmate inside the classroom (Sugai & Horner, 2001).\nTeachers are in the capacity of delivering instruction\nSugai & Horner, 2006) when they have a classroom environ-\nment with few distractions in which all the students are able\nto learn. When teachers have to deal with constant class dis-\nruptions not only valuable class time is lost (Cotton, 1990;\nWalker et al., 2005) solving a behavioral issue, but there is\nalso the risk that this negative behavior might be replicated\nUnfortunately, many of the approaches to class disruptions at\nschool finalize in the writing of a discipline referral that\nmight get the student In School Suspension (ISS) or Out of\nSchool Suspension (OSS). Costenbader & Markson (1998)\nand Fenning and Rose (2007) argued that the measure might\njeopardize students' return to the educational setting increas-\ning the likelihood of being part of the judicial system.\nRosch and Iselin (2010) also noted that school suspen-\nsions may not be the answer to the behavioral problems that\nteachers encounter at schools. Suspension, as stated by\nDupper, Theriot, and Craun (2009), may temporarily allevi-\nate teachers'and administrations'frustrations toward the dis-\nruptive behavior, but may not provide a permanent solution\nto the antecedents leading to the misbehavior. The authors\nasserted that an increase in parental involvement may be a\npositive consequence of the measure however, Costenbader\nand Markson (1998) claimed that students need to be in the\nclassroom under the supervision and/or influence of appro-\npriate role models that may impact students' lives in a posi-\ntive way. Some researchers have suggested that school\nsuspensions may promote truancy (Fenning & Rose 2007).\nDawson (1991) reported that suspended students are more\nlikely to be unsupervised at home, especially students who\ncome from single-parent households. Suspension might not\ndecrease undesired behaviors in the classroom; it may esca-\nlate them.\nSchool Improvement Efforts\nSome schools in the Southeastern state where the research\nwas implemented have worked collaboratively with the State\nDepartment of Education and the SPDG to receive training\non the implementation of PBIS on the schools' sites. The\nstate's SPDG personnel have multifaceted responsibilities\nrelating to training, coaching, and information dissemina-\ntion. The SPDG staff develops training content for the Two-\nDay New Team Training based on the work of the National\nCenter on Positive Behavioral Interventions and Supports, as\nwell as the emerging research-based literature in the area.\nMaterials are then tailored to the needs of audiences in the\nstate, incorporating examples and required processes in the\ntraining.\nThe SPDG staff in the state provides feedback to high-\nintensity support sites by interpreting the readiness checklist\nand baseline BOQ (Benchmarks of Quality), reviewing,\ngiving feedback, and providing assistance with the develop-\nment of action plans, assisting with compiling quarterly data\nreports, and coaching on interpreting data from quarterly\ndata reports, or earlier reports if available, to use results to\nupdate action plans. The SPDG also assists with problem-\nsolving implementation of action plans and in the annual\ndata reporting (BOQ). Team leaders serve as a liaison among\nSPDG staff and their school/site, use school/site data to mon-\nitor progress and effectiveness of interventions (Big 5 for\nuniversal, more individualized for advanced tiers), review\ndata with school/site team, and facilitate conversations about\nprogram improvement. They also relay data to larger school/\nsite community (teachers, students, families), and serve as a\nliaison to building-level administration (i.e., principals) to\nensure SWPBIS is embedded throughout school improve-\nment strategies. At the time of the data being reported, there\nwere two full-time training and technical assistance provid-\ners serving, in addition to a part-time SPDG director.\nThe SPDG staff and the Southeastern state's Department\nof Education collaboratively identify the schools receiving\nintensive supports. Particular attention is given to ensuring\nthat during any given year, there are schools with higher sup-\nport needs. Selection is based on school-level data, which\nmay include office discipline referrals; total number of sus-\npensions, for students with and without disabilities; total\nnumber of expulsions for students with and without disabili-\nties; attendance; students placed in an alternative school;\nnumber of students adjudicated; number of students referred\nto special education; disproportionate representation of\nminority students, and so on. In addition to the criteria\ndescribed above, the SPDG specifically recruits schools not\nmeeting expectations on the state's monitoring systems\nwhose noncompliance is in areas of the federally required\nState Performance Plans, which are relevant to the SPDG.\nThese schools are required by the state to implement a\nCorrective Action Plan (CAP; MDE, 2013) to address identi-\nfied needs. It is important to note that at Level 2 status dis-\ntricts must engage a consultant to assist with the Corrective\nAction Plan, and at Level 3 federal funds are withheld. It is\nthe intent of the SPDG to assist districts that may success-\nfully complete their CAP with assistance of the SPDG. It is\nnot the role of the SPDG to remediate all districts out of\ncompliance.\nSchools selected to receive intensive supports enter into a\nMemorandum of Understanding (MOU), which outlines\nroles and responsibilities of SPDG and participating schools.\nThe MOU outlines expectations for participation and data to\nbe provided to the SPDG, including the development of an\nMarin and Filce 5\nannual plan of action, submission of quarterly data reports\n(suspensions, expulsions, and Office Discipline Referral\n[ODR] data), and annual evaluations (BOQ). The MOU also\ndescribes services and supports provided by the SPDG.\nBecause the SPDG cannot provide intensive supports to all\nschools in the state, it provides opportunities for other inter-\nested schools to receive the same training as the schools\nreceiving intensive supports. Schools not receiving intensive\nsupports are required to complete a Commitment Form,\nReadiness Checklist, and provide baseline BOQ (SWPBIS,\nTier I) prior to attending the free training events offered by\nthe SPDG. During the training, site-based teams develop\nindividualized action plans for implementation. They are\nasked to submit annual BOQs and quarterly ODR data; how-\never, not all schools follow through with this data request.\nAll professional development concludes with the devel-\nopment of an action plan based on the critical elements of\nSWPBIS. Action steps likely to lead to implementation mile-\nstones are clearly defined, with timelines and persons respon-\nsible for each step. SPDG staff monitors and supports\nimplementation of these plans at intensive support schools;\nnonintensive support schools, self-monitor implementation.\nAs a prerequisite to registration for training, participating\nschools complete a pretraining self-assessment (BOQ), com-\nplete Commitment Form, and return it to the SPDG, and\ncomplete a SWPBIS School Readiness Checklist.\nAt the beginning of training, participants view a SPDG-\nproduced video on implementation of SWPBIS in the state\nand review current behavior data, that is, ODRs, suspen-\nsions, expulsions, and so on. Then SPDG trainers/coaches\nreview pretraining self-assessment data and alter training\ncontent, if necessary, to meet the needs of the audience.\nDuring training, each school team participates in learning\nexercises relating to the critical elements. At the end of each\nsection, the team develops an action plan to implement after\nthe training.\nThe SPDG anticipated offering 6 Two-Day New Team\nally offered trainings that were required for schools receiving\nintensive supports, but that were also opened to any school\nwishing to attend that was willing to provide the required\nprerequisite information already described. The SPDG will\nalso provide training to schools requesting the training if\nstaff is available and if the prerequisite criteria are met.\nTeam Trainings which were attended by 855 individuals. It is\nimportant to note, however, that the personnel from schools\nincluded in the study may have received training prior to the\nThe Southeastern state's SPDG provides assistance to\nschools selected for intensive, on-site assistance for at least 2\nyears. Participants are engaged in systems improvement,\nincorporating evidence-based strategies to provide intensive\nintervention to youth with high levels of behavior support\nneeds. The goal is to successfully transition these students\ninto less-restrictive environments. SWPBIS training and\ncoaching incorporates evidence-based strategies including\nsystemic change/renewal, school\u00adcommunity collaboration,\nsafe learning environments, family engagement, professional\ndevelopment, and individualized instruction. All work with\nschools, districts, and centers is captured within improve-\nment plans that include specific, measureable outcome data\nthat are analyzed by the school/district and SPDG staff.\nThe SPDG staff conducts on-site visits (at least monthly),\nfrequent phone conferences, and email exchanges to provide\nsupport to intensive schools. SPDG model strategies and sup-\nport school staff as they implement their individual improve-\nment plans, which are developed annually. The state's SPDG\npersonnel attend team meetings at high-intensity districts as\nwell as provide formative feedback and guidance. This assis-\ntance is faded over time, with more responsibility transferred\nto the site-based team leader. The SPDG staff coaches high-\nintensity districts using a team-based model that is guided by\nsite-specific action plans using the critical elements of\nSWPBIS framework. All SWPBIS training, coaching, and\nevaluation revolves around the Critical Elements. By using\nnationally validated instruments (BOQ, SET), sites may self-\nassess implementation and may also be externally evaluated\nfor fidelity.\nSome schools participating in this study were evaluated\nwith the School-Wide Evaluation Tool (SET), while others\nwere not due mainly to financial and time constraints.\nBecause of the cost of completing the SET, all the schools\n(intensive and nonintensive) that submitted the BOQs with\nscores of 80% or higher were invited to be externally evalu-\nated using the SET. Schools scoring 80% of higher are con-\nsidered SWPBIS Model Sites. Model Sites are expected to\ncontinue to be evaluated annually using the SET and to sub-\nmit quarterly ODR data.\nThe level of training and coaching regarding the imple-\nmentation of SWPBIS varied in this study with some schools\nreceiving training only (\"non-intensive\") and some receiving\ntraining and on-site coaching (\"intensive\"). The schools also\ndiffered in the levels of implementation fidelity as measured\nby the BOQ; an instrument schools used to self-report the\nfidelity in the execution of SWPBIS.\nTraining only has shown to be beneficial (Joyce &\nShowers, 2002). Training and coaching, however, may pro-\nvide a better structure for the implementation of SWPBIS\ninside the classrooms (Sugai & Horner, 2006). Joyce and\nShowers (2002) noted that training and coaching may help\nteachers not only to change the structure of the classrooms\ndue to a change in teachers' beliefs, but also to help teachers\ndeal with the discomfort that the new set of procedures may\nimply. Training and coaching may also provide the emo-\ntional support teachers need when implementing the newly\nlearned set of procedures in the classrooms.\n6 SAGE Open\nTable 2. Schools Classification According to BOQ and SET\nScores.\nTotal BOQ SET scores\nNonmodel and nonintensive M 0.85 \nNote. BOQ = Benchmarks of Quality; SET = School-Wide Evaluation\nTool.\nMethod\nThis study investigated relationships among the various types\nof training and coaching received by 96 schools in a Southern\nstate in the United States and their performance on state\naccountability measures. Training and coaching were sup-\nported by the SPDG funded by the U.S. Department of\nEducation's OSEP. In addition, the data were analyzed to\ndetermine if implementation fidelity of SWPBIS was related\nto performance on those accountability measures. While sev-\neral program evaluation measures are used for reporting\nresults to OSEP, this study was undertaken to begin to inves-\ntigate potential relationships among SPDG-specific efforts\nand the larger accountability measures of the state. The aim of\nthis research is to determine (a) if the level of training and\ncoaching received by the schools was related to the schools'\nQDI; (b) if the schools' classification into \"model sites\" or\n\"non-model sites\" based on the results of the SET instrument\nwas related to the schools' QDI; (c) if the levels of training\nand coaching and the results of the SET instrument that clas-\nsified the schools into \"model sites\" and \"non-model sites\"\nwere related to the schools' QDI, and (d) if the level of\nSWPBIS implementation fidelity, BOQ, was related to the\nschools' performance classifications, QDI, or Growth status.\nParticipants\nThe sample drawn for this study came from 96 schools in the\ntargeted state who received training, coaching, or both from\nElementary, Lower Elementary, Upper Elementary, Middle\nSchools, High Schools, and Attendance Centers composed\n1 displays the information related to the participating schools\nthat belong to 41 out of 152 school districts in the area.\nMeasures\nData were gathered from several sources for this analysis.\nFirst, a list of schools that had participated in training and/or\ncoaching by the SPDG was compiled by its director. Those\nschools were then coded as either training only or training\nand on-site coaching. Next, self-reported scores on the BOQ\nwere obtained by the director of the SPDG for each school.\nThese were used to code each school as self-reported imple-\nmentation fidelity (80% or higher) or no self-reported imple-\nmentation fidelity. The schools that reported implementation\nfidelity (80% or higher) were invited to be externally evalu-\nated using the SET. Finally, a list of schools that had scored\n80% or higher on the SET and were listed on the SPDGs\nwebsite as model sites was obtained. The data set was then\nupdated to include coding for external implementation fidel-\nity or no external implementation fidelity. Table 2 displays\nthe mean and standard deviation BOQ and SET scores for\neach classification group. There is no SET data in regard to\nthe nonmodel and nonintensive schools as they did not report\nimplementation fidelity. .\nNext, publicly available accountability data for the 2011-\n2012 school year was obtained from the state's website and\neach school's performance classification, QDI score, and\nGrowth score were added to the data set.\nThe data collected regarding the BOQ, the intensity of the\ntreatment, and the ranking of the schools into \"model site\" or\n\"non-model site\" came from the information compiled by\nthe SPDG in a local university. Performance classification,\nQDI, and Growth were calculated by the State Department of\nEducation based on the state's cut-off points used to deter-\nmine the improvement of the schools.\nAnalysis\nDifferent statistical tests were conducted to address the four\nresearch objectives posed in this research using an alpha\nlevel of significance of  = .05. An independent sample t-test\nwas conducted to determine whether the schools that received\ntraining and on-site coaching (\"intensive\") differ from the\nschools that received training only (\"non-intensive\") in\nTable 1. Schools Participating in the Study.\nType of school Frequency\nLower elementary 1\nUpper elementary 6\nAchievement center 1\nAttendance center 9\nCareer & technology center 1\nSpecialty school 1\nMarin and Filce 7\nregard to QDI. A second independent sample t-test was con-\nducted to determine whether the schools that were consid-\nered \"model sites\" differ from the schools that were\nconsidered \"non-model sites\" in regard to QDI. Then, an\nANOVA was conducted to determine whether the type of\ntraining and coaching (\"intensive\" or \"non-intensive\"), and\nthe results of the SET that classified schools into \"model\nsites\" and \"non-model sites,\" were related to the schools'\nQDI. The schools were grouped as follows: (a) \"model site\"\nand \"intensive\" (training plus on-site coaching), (b) \"model\nsite\" and \"non-intensive\" (training only), and (c) \"non-model\nsite\" and \"non-intensive\" (training only). Finally, correla-\ntions were performed to establish if the level of SWPBIS\nimplementation fidelity, as determined by the self-adminis-\ntered BOQ, was related to the schools' performance classifi-\ncations, QDI, or Growth status.\nOut of the 96 schools included in this study, there is acces-\nsible data for 91 schools on the number of years of SWPBIS\nimplementation as follows: 10 schools began SWPBIS\nschools' QDI scores prior to SWPBIS implementation, there\nis QDI information on the last three cohorts as follows: the\nschool year had a mean QDI of 167 prior to SWPBIS imple-\nSWPBIS implementation and the participating schools dur-\nto SWPBIS implementation. It is important to note that the\nstate where this study took place adopted QDI as an account-\nResults\nThe first research objective sought to determine whether\nthe intensity of the training was related to the schools'\nQDI. The results of the t-test indicated that the schools\nthat received training only (\"non-intensive\") had a mean\nthat received training and on-site coaching (\"intensive\")\n(p = .017) indicated a violation of homogeneity of vari-\nance. Therefore, the Equal variances not assumed, t(25) =\ndifference between the means of the two samples. It can\nbe inferred that schools that received training plus on-site\ncoaching (\"intensive\") had higher QDIs than the schools\nthat received training only (\"non-intensive\").\nThe second research objective sought to determine\nwhether the schools that were considered \"model sites\"\ndiffer from the schools that were considered \"non-model\nsites\" upon completion of the SET in regard to QDI. The\nresults of the t-test indicated that the schools that were\nconsidered \"model sites\" had a mean QDI of 169.36 (SD\n= 21.72) compared with the schools that were considered\nhomogeneity of variance. Therefore, the Equal variances\nthere is a significant difference between the means of the\ntwo samples. It can be inferred that schools considered\n\"model sites\" had higher QDIs than the schools consid-\nered \"non-model sites.\"\nFor the third research objective, an ANOVA test was con-\nducted to determine whether the levels of training and coach-\ning, and the results of the SET, that classified schools into\n\"model site\" or \"non-model site\", had any effect on the\nschools' QDI. Table 3 displays the statistics for the groups in\nwhich it is observed that Group 1 (Model and Intensive) and\nGroup 2 (model and nonintensive) have smaller sample sizes\nthan Group 3 (nonmodel and nonintensive).\nLevene's F test showed a violation in the assumption of\nhomogeneity of variance (p = .035); therefore, Welch's F(2,\ntically significant difference between the levels of training\nand coaching, and the results of the SET, that classified\nschools into \"model site\" or \"non-model site,\" in regard to\nthe schools' QDI. Due to unequal variances among the\ngroups, the Games-Howell post hoc test was conducted to\ndetermine where the difference was between the pair-wise\ncomparisons. The results revealed that the schools that were\nconsidered \"model sites\" and that received training and\nhigher QDI than the schools that were considered \"non-\nmodel sites\" and received training only (\"non-intensive\")\nFinally, for the fourth research objective, two-tailed\nSpearman's correlations were performed to establish if the\nlevel of SWPBIS implementation fidelity, the BOQ, was\nrelated to the schools' performance classifications, QDI, or\nGrowth status. The results indicated there is a positive rela-\ntionship and a medium effect between the BOQ and perfor-\nmance classifications with r\nS\npositive relationship and a medium effect between the BOQ\nand QDI with r\nS\nTable 3. Quality of Distribution Index.\n n M SD\nLower\nbound\nUpper\nbound\nModel and\nnonintensive\nNonmodel and\nnonintensive\nNote. M = mean; SD = standard deviation; CI = confidence intervals.\n8 SAGE Open\npositive relationship and a small effect between the BOQ and\nGrowth with r\nS\nthere is a positive bivariate relationship among QDI, perfor-\nmance classifications and Growth; the accountability mea-\nsures used to evaluate schools and school districts in the\nstate.\nDiscussion\nThe aim of this research was to determine (a) if the level of\ntraining and coaching received by the schools was related to\nthe schools' QDI; (b) if the schools' classification into\n\"model sites\" or \"non-model sites\" based on the results of\nthe SET instrument was related to the schools' QDI; (c) if\nthe levels of training coaching and the results of the SET\ninstrument were related to the schools' QDI, and (d) if the\nlevel of SWPBIS implementation fidelity, BOQ, was related\nto the schools' performance classifications, QDI, or Growth\nstatus.\nThe overall findings were consistent with past studies.\nFirst, the schools that received training and coaching\n(\"intensive\") had higher QDI than the schools that received\ntraining only (\"non-intensive\"). This finding is consistent\nwith the literature of the benefits of training and coaching\nwhen implementing PBIS at schools (Joyce & Showers,\nthe schools that were classified as \"model sites\" based on\nthe results of the SET had higher QDI than the schools that\nwere considered \"non-model sites.\" This finding provides\nevidence of the impact that SWPBIS has on academic\nachievement (Rowe & Stewart, 2009; Sugai & Horner,\nin the schools in the Southeastern state where the study was\nconducted may be beneficial to keep students inside the\nclassroom; a measure that may decrease dropout rates and\nincrease graduation rates in the area. Besides, higher QDI\nfor schools in the state may indicate better results on the\nschools' and school districts' report cards, and thus the con-\ntinuous infusion of federal money into the school districts\nThird, the schools that received training and coaching\n(\"intensive\") and were considered \"model sites\" based on\nthe results of the SET instrument presented higher QDI than\nthe schools that received training only (\"non-intensive\") and\nwere considered \"non-model sites.\" This illustrates the need\nfor the continuing training and coaching of the schools in\nthe state in regard to the implementation of SWPBIS. It may\nbe that if schools are knowledgeable of the techniques\nneeded to implement SWPBIS and apply those with fidelity,\nthe benefits are exponential. Not only may the schools\nimprove the performance classification obtained in the\nstate's accountability measure, but in the long run the state\nmay see a decrease in dropout rates and an increase in grad-\nuation rates. This may be especially true for at-risk students\nin the area.\nFourth, the level of SWPBIS implementation fidelity, the\nBOQ, was related to the schools' performance classifica-\ntions, QDI, and Growth status. There was a positive relation-\nship and a medium effect between the BOQ and the schools'\nperformance levels and between the BOQ and QDI. A pos-\nsible explanation for these results may lie in the fact that\nonce the schools are knowledgeable and confident on how to\nimplement SWPBIS, the likelihood of fidelity to the measure\nincreases, thus the academic and behavioral issues in the\nclassroom may decrease. The more fidelity to the measure\nschools exercised, the better the results the schools may\nobtain in regard to the state's accountability measures.\nTherefore, future researchers and/or external coaches for\nSWPBIS may want to work with school personnel closely to\nhelp develop SWPBIS fidelity implementation. It was also\nobserved that the BOQ had a small effect with the Growth\nstatus of the schools.\nFirst, contributing to the emerging literature on the rela-\ntionship of SWPBIS fidelity implementation tools and\naccountability measures is one of the strengths of the present\nstudy. Second, the internal (BOQ) and external (SET) fidel-\nity implementation tools currently used in the state where\nthis research took place were separately analyzed in relation\nto QDI to determine relationships before being analyzed\ntogether in regard to QDI. Third, the scores reported in the\nBOQ were examined in light of the state's accountability\nmeasures: performance classification, QDI, and Growth. The\nfindings of the study showed the benefits that training and\ncoaching has on the schools that implement SWPBIS in the\nregion. This may encourage the State Department of\nEducation in conjunction with the SPDG to expand the train-\ning to other schools and/or school districts in the region.\nThe results of the study also suggested that improving\nacademic achievement may be possible. Teachers that\nreceived training and on-site coaching seemed to be more\nknowledgeable than teachers that received training only in\nregard to the behavioral strategies needed to deal with class-\nroom disruptions. The literature has shown that the less time\nteachers invest in dealing with classroom disruptions the\nmore instructional time is gained and the better results stu-\ndents may obtain when faced with high-stake tests. Districts\nor states trying to improve performance on state accountabil-\nity measures may want to consider adding the coaching com-\nponent to the training school personnel may receive on\nSWPBIS. Training and coaching has shown to be beneficial\nwhen it comes to SWPBIS implementation.\nAlthough this study contributes to the preliminary inves-\ntigation of potential relationships among SPDG-specific\nefforts and the larger accountability measures of the state, it\nhas limitations. The first one is the use of the BOQ, a self-\nreport measure that may or may not be an accurate report on\nthe fidelity of the schools when implementing SWPBIS on\nsite. Second, it would be advisable to collect a larger sample\nsize for future research to have more statistical power in the\nMarin and Filce 9\nanalysis. The groups identified in the third analysis had\nuneven sample sizes that may have contributed to the unequal\nvariances in the groups. Sample size may have also accounted\nfor the non-normally distributed data used in this study.\nFinally, it is important to remember this study does not show\ncausation, only a relationship between SWPBIS implemen-\ntation fidelity tools and the state accountability measures in\nwhich higher levels of implementation fidelity correlated\nwith higher ratings on accountability measures.\nReducing behavioral issues in the classroom in an attempt\nto improve academic achievement is a major factor in today's\neducational reforms. Research has shown that classroom dis-\nruptions account for loss of instructional time (Cotton, 1990;\nimplementation of measures designed to prevent and/or con-\ntrol disruptions such as SWPBIS may be the key to advance\nacademic achievement in our classrooms. It would be benefi-\ncial to keep exploring the relationship of SWPBIS imple-\nmentation fidelity instruments and accountability measures\nto contribute to the emerging literature on the topic as well as\nto explore the benefits of training and coaching when it\ncomes to the implementation of PBIS at schools and inside\nthe classrooms.\nAuthors' Note\nThe views expressed herein do not necessarily represent the posi-\ntions or policies of the Department of Education. No official\nendorsement by the U.S. Department of Education of any product,\ncommodity, service, or enterprise mentioned in this publication is\nintended or should be inferred.\nDeclaration of Conflicting Interest\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) disclosed receipt of the following financial support\nfor the research and/or authorship of this article: This research was\npartially supported by the U.S. Department of Education, Office of\nReferences\nBerry, B., & Fuller, E. (2008, January). Final report on the\nMississippi project CLEAR Voice Teacher Working Conditions\nSurvey. Center for Teaching Quality. Retrieved from: http://\nwww.teachingquality.org/sites/default/files/Final%20\nSurvey Briefs. The United States Census Bureau. Retrieved\nCarney, K., & Stiefel, G. (2008). Long-term results of a prob-\nlem solving approach to response to intervention. Learning\nCommon Core State Standards. (2013). Implementing the common\ncore state standards. Available from http://www.corestan-\ndards.org/\nCostenbader, V., & Markson, S. (1998). School suspension: A study\nwith secondary school students. Journal of School Psychology,\nCotton, K. (1990). Schoolwide and classroom discipline (School\nimprovement and research series, Close-Up No. 9). Portland,\nOR: Northwest Regional Educational Laboratory.\nCrone, D. A., Horner, R. H., & Hawkin, L. S. (2004). Responding to\nproblem behavior in schools. New York, NY: Guilford.\nDavis-Kean, P. (2005). The influence of parent education and fam-\nily income on child achievement: The indirect role of parental\nexpectations and the home environment. Journal of Family\nDawson, D. (1991). Family structure and children's health and well-\nbeing: Data from the 1988 national health interview survey on\nDuncan, G. J., & Magnuson, K. A. (2005). Can family socioeco-\nnomic resources account for racial and ethnic score gap? The\nDupper, D., Theriot, M., & Craun, S. (2009). Reducing out-of-\nschool suspensions: Practice guidelines for school social work-\nEnglish, F., & Steffy, B. (2001). Deep curriculum alignment.\nCreating a level playing field for all children on high-stakes\ntests of educational accountability. Lanham, MD: The\nScarecrow Press.\nEvans, G. W. (2004). The environment of childhood poverty.\nFenning, P., & Rose, J. (2007). Overrepresentation of African\nAmerican students in exclusionary discipline: The role of\nGassman-Pines, A., & Yoshikawa, H. (2006). The effects of\nantipoverty programs on children's cumulative level of pov-\nIrons, J., & Harris, S. (2007). The challenges of no child left behind.\nUnderstanding the issues of excellence, accountability and\nchoice. Lanham, MD: Rowman & Littlefield Education.\nJia, Y., Way, N., Ling, G., Yoshikawa, H., Chen, X., Hughes,\nD., . . .Lu, Z. (2009). The influence of student perceptions of\nschool climate on socioemotional and academic adjustment:\nA comparison of Chinese and American adolescents. Child\nJoyce, B., & Showers, B. (2002). Student achievement through\nstaff development (3rd ed.). Alexandria, VA: Association for\nSupervision and Curriculum Development.\nKomro, K., Flay, B., & Biglan, A. (2011). Creating nurturing envi-\nronments: A science-based framework for promoting child\nhealth and development within high-poverty neighborhoods.\nLassen, S. R., Steele, M. M., & Sailor, W. (2006). The relationship\nof school-wide positive behavior support to academic achieve-\nment in an urban middle school. Psychology In The Schools,\nManzo, K. K. (2000). The state of curriculum. In Staff of Education\nWeek (Eds.), Lessons of a century: A nation's schools come\nEducation.\nMississippi Department of Education. (2010). Mississippi statewide\naccountability system. Retrieved from https://districtaccess.mde\n.k12.ms.us/Accountability/_layouts/OSSSearchResults.aspx?k\nDocuments\nMississippi Department of Education. (2012). Tier process.\nRetrieved from http://www.mde.k12.ms.us/curriculum-and-\ninstruction/curriculum-and-instruction-other-links/response-\nto-intervention-teacher-support-team\nMississippi Department of Education. (2013). Correction action\nplans. Retrieved from https://www.mde.k12.ms.us/mississippi-\nboard-of-education/board-of-education-policy-manual/policy-\nMississippi Department of Education. (2013). Mississippi statewide\naccountability system. Retrieved from https://districtaccess.mde\n.k12.ms.us/Accountability/_layouts/OSSSearchResults.aspx?k\nDocuments\nMississippi Department of Education. (2013). Recommendations for the\nfrom https://districtaccess.mde.k12.ms.us/Accountability/Public%\nMississippi Department of Education. (2013). Understanding the\nand districts . Retrieved from https://districtaccess.mde.k12\nMuscott, H., Mann, E., & LeBrun, M. (2008). Positive behav-\nioral interventions and supports in New Hampshire: Effects\non large-scale implementation of schoolwide positive behav-\nior support on student discipline and academic achievement.\nStates Census Bureau. Retrieved from http://www.census.gov/\nOliver, R., Wehby, J., & Reschly, D. (2011). Teacher classroom\nmanagement practices: Effects on disruptive or aggressive stu-\ndent behavior. Campbell Systematic Reviews. Advance online\nPaolella, K. (2009). Positive behavior support and student response\nto the behavior education program (Social Work Student\nPapers. Paper, 40). Retrieved from http://digitalcommons\n.providence.edu/socialwrk_students/40\nPeshak, H., & Kincaid, D. (2008). Building district-level capacity\nfor positive behavior support. Journal of Positive Behavior\nRosch, J., & Iselin, A. (2010). Alternatives to suspension (North\nCarolina family impact seminar). Center for Child and\nFamily Policy, Duke University. Retrieved from http://www\n.childandfamilypolicy.duke.edu/pdfs/familyimpact/2010/\nAlternatives_to_Suspension.pdf\nRowe, F., & Stewart, S. (2009). Promoting connectedness through\nwhole-school approaches: A qualitative study. Health\nSimonsen, B., Eber, L., Black, A. C., Sugai, G., Lewandownki,\nH., Sims, B., & Myers, D. (2012). Illinois statewide positive\nbehavioral interventions and supports: Evolution and impact\non student outcomes across years. Journal of Positive Behavior\nSmith, J. M., Fien, H., & Paine, S. S. (2008). When mobility dis-\nSugai, G., & Horner, R. (2001). Features of an effective behav-\nior support at the school district level. Positive Behavior, 3,\nSugai, G., & Horner, R. H. (2006). A promising approach for\nexpanding and sustaining school-wide positive behavior sup-\nSugai, G., & Simonsen, B. (2012). Positive behavioral interven-\ntions and support: History, defining features, and miscon-\nceptions. OSEP Technical Assistance Center on Positive\nBehavioral Interventions and Supports. Effective Schoolwide\nInterventions, Retrieved from http://www.pbis.org/common/\nTobin, T., Lewis-Palmer, T., & Sugai, G. (2002). School-wide and\nindividualized effective behavior support: An explanation and\nVaughn, S., Bos, C. S., & Schumm, J. S. (2010). Teaching excep-\ntional, diverse, and at-risk students in the general education\nclassroom (5th ed.). Boston, MA: Allyn & Bacon.\nWalker, H. M., Ramsey, E., & Gresham, R. M. (2005). Antisocial\nbehavior in school: Evidence-based practices (2nd ed.).\nBelmont, CA: Wadsworth/Thomson Learning.\nWamba, N. G. (2010). Poverty and literacy: An introduction.\nAuthor Biographies\nAdriana M. Marin is a doctoral candidate in the Department of\nCurriculum, Instruction, and Special Education at the University of\nSouthern Mississippi. Her research interests are in the areas of Positive\nBehavior Interventions and Supports, state accountability measures,\nand academic achievement of at-risk students in K-12 settings.\nHollie Gabler Filce is an Associate Professor and the Coordinator\nof Special Education Programs at the University of Southern\nMississippi. She also serves as the Director of the Mississippi State\nPersonnel Grant, overseeing implementation of Positive Behavioral\nInterventions and Supports across the state."
}