{
    "abstract": "Abstract\nAustralia has reflected an international shift toward public participation in governance and science. Researchers have critiqued\nthis shift as insufficient. Meanwhile, studies of how research funds are allocated also found room for improvement. This\nexperiment tested a way to add value to the effort researchers put into research proposals by using them for deliberative\npublic engagement. Three Australian events tested a model of deliberative participation in decision-making about science\nfunding. These events were shorter than most deliberative processes, based on a model tested in the United Kingdom.\nAlthough recruitment was aimed at broad representation, participants had more formal education than Australia's average.\nVoting decisions were most influenced by potential benefits to society of the planned research, as well as participants'\nunderstanding of plans presented. Some reported that their decisions were influenced by whether benefits would happen\nlocally. Results suggested that participants' voting decisions were more influenced by the research plans than who presented\nthem. However, unconscious biases cannot be ruled out as factors in decision-making. Participants reported they would be\nkeen to participate in such a process again; however, this enthusiasm was linked to a meal incentive. The impact of brevity\non deliberative decision-making is discussed, along with potential modifications for future experiments.\n",
    "reduced_content": "sgo.sagepub.com\nArticle\nIntroduction\nMany Australian organizations are attempting to move from\na deficit model of public engagement with science toward\nmore participatory models. Researchers have criticized a\nlack of genuine public participation in Australian processes\nfor regulating biotechnology (Schibeci & Harwood, 2007)\nand nanotechnology (Lyons & Whelan, 2010). TheAustralian\nGovernment has supported workshops to try and address this\ners have called for a participatory technology assessment\nagency such as those trialed in Europe (Russell, Vanclay,\nAlthough science policy most easily comes to mind as a\nresponsibility of government, research organizations also\nhave policies, such as their priority areas for research and\nhow they communicate about research with the public.\nPublic engagement can play a role in guiding research priori-\nties, while also meeting obligations to publicly communi-\ncate. Given these obligations are already present in funding\nagreements, most publicly funded research in Australia\nincludes some funds for public communication.\nMeanwhile, researchers are also criticizing decision-mak-\ning processes used to allocate funds for research. Recently\nthe journal Nature published critique of how much time\nAustralians waste on grant application processes (Herbert,\nBarnett, & Graves, 2013). An earlier study challenged\nCanada's policies about how funding decisions are made\n(Gordon & Poulin, 2009). This found it would cost the same\nto give all researchers a grant automatically as to continue\nthe established process for funding decisions. Such studies\nsuggest a need to improve processes for decision-making\nabout science funding, irrespective of the push for greater\npublic engagement with science. This experiment tested a\nway to add value to the effort researchers put into research\nproposals by using these proposals as a basis for deliberative\npublic engagement.\nResearchers concluded that funding allocations for\nAustralian research were somewhat random and unreliable\nand so, could be improved (Graves, Barnett, & Clarke, 2011).\nThey suggested an improvement may be allowing panels to\nclassify grants into three categories: certain funding, certain\nrejection, and funding based on a random draw. Public\nengagement activities could prioritize those proposals that\ncould otherwise face random draw. These proposals represent\n1Australian National University, Canberra, Australia\nCorresponding Author:\nCobi Smith, Australian National University, Physics Link Building 38a,\nScience Road, Canberra, Australian Capital Territory, 0200, Australia.\nEmail: cobi.smith@anu.edu.au\nPublic Engagement in Prioritizing\nResearch Proposals: A Case Study\nCobi Smith1\n Keywords\npublic participation, deliberative, engagement, decision-making, science, policy, funding, allocations, agricultural science,\nAustralia\n2 SAGE Open\nchances for public involvement to genuinely influence fund-\ning decisions. Some public engagement is critiqued as con-\nsultation that aims to rubber-stamp policymakers' foregone\nthan focusing on the top 10% that certainly gets funding, pub-\nlic engagement could prioritize among those facing random-\nness or uncertainty in the current system. The pilot described\nin this article could enhance public participation in science\nwhere it may otherwise be left to chance. If randomness is\narguably preferable to the current norm, then public participa-\ntion could be better still, if only for enhancing participants'\nknowledge and experience. It would also allow scientists\nleading these uncertain proposals to understand public per-\nceptions and preferences about their research. If unfunded\nfollowing deliberations, future iterations of the proposal\nshaped by public feedback might be more successful.\nThe same researchers (Graves et al., 2011, p. 4) said anec-\ndotal evidence suggested researchers skilled at winning\nfunding complete most of the research before applying for\nfunding. This goes against ideals of upstream public partici-\npation in science policy decisions (Pidgeon & Rogers-\nfurther indicates little opportunity for publicly funded sci-\nence to be shaped by public deliberations in the current sys-\ntem. The experiment described in this article served as a\n\"proof of concept\" for one possibility of how public partici-\npation could be used to prioritize research proposals for\nfunding in Australia. There are many possibilities; context\nexplaining why this method was chosen to pilot follows.\nEnhancing participants' civic and scientific knowledge is\ntypically an aim of deliberative public processes in science.\nIn addition to academic policy reasons outlined above, there\nshould be practical benefits for individuals participating in\ndeliberations, whether as scientists or laypeople. These prac-\ntical benefits were drivers for this particular experiment.\nFrustration of scientists at the Australian Centre for Plant\nFunctional Genomics about the state of public dialogue\nabout biotechnology meant they were open to participating\nin experiments with public engagement. So 2008 saw a shift\nfrom media-focused communications and lecture-based pub-\nlic interactions toward experiments with deliberative pro-\ncesses. These experiments aimed to demonstrate that\ndeliberative public engagement could be useful for scientists\nas well as participating members of the public. The public\ncould gain knowledge and experience with deliberative deci-\nsion-making about science, whereas scientists could better\nunderstand other citizens' perspectives and see how their\nresearch is perceived when presented firsthand, rather than\nthrough media filters. The U.K. model modified for use in\nAustralia came from the Institute for Food Research, linked\nto the John Innes Centre, another center doing plant genom-\nics research. This connection helped to legitimize this method\nof deliberative public engagement as a starting point for par-\nticipating scientists and management in Australia.\nMethod\nThe method used was brief in comparison with many types\nof deliberative public engagement that run for more than\n1 day, which may require participants to consider informa-\ntion before and between events. In contrast, this method took\nno more than 3 hours of participants' time. This has benefits\nfor both participants and organizers, as well as shortcomings,\nwhich will be discussed later.\nThe event format, which was repeated 3 times across\n2 cities, is as follows:\n0:00 Participants arrive and have time to choose a seat,\nread the research project information sheet, fill in the\npre-event survey, and interact with other participants.\n0:20 The facilitator welcomes the participants and intro-\nduces the project, emphasizing the importance of par-\nticipation and introducing the presenting researchers.\n0:25 The first scientist presents his or her proposal.\n0:40 The facilitator asks participants to discuss potential\nquestions with each other.\n0:45 Questions relevant to the specific project are\nanswered by the scientist; those the facilitator consid-\ners relevant to all of the projects are recorded for later.\n1:55 Questions that were recorded earlier are addressed,\nwith each scientist given right of reply to each ques-\ntion, and other participants given the chance to ask\nfollow-up questions.\n2:20 The formal deliberative component is finished.\nPublic participants record their private vote on slips of\npaper, which are collected by volunteers. They are\nasked to fill in the second survey while the votes are\nbeing counted, and to give these surveys to circulating\nvolunteers once completed.\n2:30 After the votes are counted twice, the outcome of vot-\ning is announced. Participants are invited to stay and\nchat with the scientists and each other over a drink, so\ndiscussions can continue informally and unrecorded.\nDifferences in Event Formats\nThere were some differences in the format of the Australian\nand U.K. events. The U.K. version involved recorded\nendorsements from local celebrities for the different proj-\nects; these were not used in the Australian events as they\nwere of limited value (Rowe, Rawsthorne, Scarpello, and\ntheir research plans rather than four as only three suitable\nscientists were available for all three events. In addition, all\nthree presented their genuine research plans; there was no\ndeliberately dubious research project as there was in the U.K.\nevent. No one voted for the fake project in the U.K. event\nAustralian model.\nIn the U.K. event, it was implied that the public vote\nwould lead to funding for the winning project. The author of\nthis article was a public participant at the U.K. event, where\nthere were discussions among participants about the novelty\nof genuinely making a decision about what science is funded.\nAfter voting, it was revealed that the event was an experi-\nment to learn what the public thought about research fund-\ning, rather than an event that would actually allocate funding\nbased on public preferences. In the Australian events, partici-\npants were informed from the beginning that they were par-\nticipating in a research project testing a model of deliberation,\nalthough the facilitator expressed hope that such pilot events\ncould lead to genuine allocations in future. Australian par-\nticipants were asked to imagine they would be awarding\nabout whether the deception affected the outcomes of the\nU.K. study, so it was omitted from the Australian events.\nRepetition and Location\nAnother difference between the U.K. and Australian events\nwas repetition: There were three in Australia across two\nstates, compared with one event in the United Kingdom.\nAdelaide, the capital of South Australia, hosted the largest\nevent, whereas Canberra, the nation's capital, hosted two\nsmaller events. To capitalize on repetition, the order of the\nAustralian presenters was changed on each occasion.\nEach presenter had the opportunity to present first, second,\nand last, to counter possible effects of presentation order.\nHowever, it is worth noting that although the same scientists\npresented the same research project on each occasion, there\nwere inevitably subtle differences in each presentation and\nthe manner in which it was presented as the presenters gained\nexperience. Furthermore, it is likely that confidence levels\namong the presenters changed after the first event's vote,\nwhich may have influenced presentations and voting.\nThe events were held at public venues, away from where\nthe research projects under deliberation would happen, to\nhelp foster open-ended discussion as suggested by Powell\nand Colin (2008). In Adelaide, the event happened at the\nNational Wine Centre in the city center, which is owned by\nthe University of Adelaide but used as a public space. In\nCanberra, the two smaller events happened at The Front\nGallery and Cafe in Lyneham, an informal venue with no\nresearch or university links, apart from hosting National\nScience Week events. Canberra is renowned for its lack of a\ndominant city center, so the events happened in a typical sub-\nurban hub.\nResearch Proposals Presented\nAs in the U.K. event, proposals all related to agricultural\nand food science. In Australia, all three presented projects\nrelated to research happening at the Australian Centre for\nPlant Functional Genomics. Each presenting scientist\nreworked an existing research proposal into a 10-min pre-\nsentation (with 5 min leeway in reality). Each had experi-\nence in public science communication and consulted with\nprofessional science communicators beforehand to ensure\nthat information presented would be publicly accessible.\nThe presentations and format were piloted with a local high\nschool science class before National Science Week, after\nwhich presenters had the opportunity to tweak their presen-\ntations further.\nAll proposals had two things in common: They involved\nsome research using genetic modification (GM) and each\ndiscussed at least one potential environmental benefit. The\ntype of GM and its role in final outcomes of the research\nvaried, as did research applications. One involved moving\ngenes from one type of cereal into another to enhance salt\ntolerance. Another involved manipulating existing genes in\nbarley known to be involved in producing beta-glucan, with\napplications for human health and biofuel production.\nAnother involved investigating the genomes of different\ncorn varieties with the aim of finding genes relevant to nitro-\ngen use efficiency.\nThe presenters were selected for diversity as well as sci-\nence communication talent. There was a female senior\nresearcher, Dr Rachel Burton, who had recently published\nresearch in the journal Science. A male senior researcher,\nDr Trevor Garnett, was leading a project with funding from a\nprivate company as well as government. A male graduating\nPhD student, Darren Plett, was working on a collaborative\nproject with the University of Cambridge. Each scientist was\nencouraged to share information such as the above about col-\nlaborations and past research achievements, as this type of\ninformation is used to inform decision-making in current\nfunding models.\nPublic Participants\nThere were 85 formal participants in total across the three\nevents. The majority, 57, participated in Adelaide, whereas\nthe two events in Canberra had 20 and 8 formal participants,\nrespectively. Participants were considered formal if they reg-\nistered prior to the event and participated in the voting and\nsurvey processes. There were several informal participants\nwho did not complete surveys, mostly at the smaller venue in\nCanberra.\nAt the larger Adelaide event, participants were seated\nat round tables of eight, which facilitated discussion\namong those on the table about presentations and ques-\ntions. At the more intimate Canberra venue, participants\nwere spread on sofas and chairs around a room; conversa-\ntion among them was limited to those nearby. In addition\nto the public participants, presenting scientists and facili-\ntator, event volunteers, and hospitality staff participated\ninformally.\n4 SAGE Open\nSurvey Method\nParticipants completed surveys before and after the events,\nwhich were designed to take 10 min to complete. The design\nand text of the surveys are openly available online (Smith,\ncovering demographics, political participation, participants'\nown areas of expertise, and science funding issues. The post-\nevent survey included 13 questions asking for the partici-\npant's voting decision, their ratings of the presentations on\n11 criteria, and feedback about the event. The survey ques-\ntions were designed to facilitate comparison with data from\nthe U.K. event, detailed analysis of which is beyond the\nscope of this article.\nIn questions rating the presentations, survey respondents\ncould rate all presenters the same. These ratings were used to\ndeduce rankings in analysis. These quantitative questions\nwere complemented by open-ended questions, notably, for\nexample, asking why people voted for the proposal they did.\nParticipants also filled in a separate voting slip, the results\nfrom which were announced at the end of the event. The sur-\nveys before and after were collected for later analysis.\nSelecting Participants\nRowe, Rawsthorne, Scarpello, and Dainty (2009) noted that\na shortcoming of their event was the lack of participant rep-\nresentativeness. Longstaff and Burgess (2009) discussed in\ndetail challenges in recruiting participants. A common prob-\nlem in public engagement with science is that the same audi-\nences are repeatedly attracted, whereas other types of people\nare rarely engaged. Left to self-selection, participants are\nmore likely to be middle class and well educated, to be mem-\nbers of political parties or lobby groups, and to have previ-\nously interacted with their local government than the average\ncitizen, thus already having greater chance for input into\npolicy than other members of the public (Adams, 1989).\nThus, fair recruitment should be an important consideration\nfor organizers of public engagement with science, as Nagel\nIf policymaking were always just a matter of finding neutral,\ntechnical solutions to common problems, then disproportionate\ninvolvement of educated people would be desirable because of\ntheir superior competence. More often, however, policy choices\ndepend on interests and values not universally shared. Education\nis statistically associated with higher income and occupational\nstatus, as well as with distinctive cultural tastes. Thus,\nparticipation based on intensive, deliberative forms of citizen\nparticipation will usually neglect the needs and desires of more\nplebeian members of the population, unless the process is\ncarefully structured to counteract the normal bias in favour of\nGiven such problems, a method of counteracting this\nbias was tested by actively seeking participants without a\nbackground in science. Events were publicized in community\nservices such as event listings in media, as well as through\nuniversity services. Social networks were used to share infor-\nmation, where people were encouraged to invite friends with-\nout a background in science, emphasizing it was designed for\nthem. On registering interest, prospective participants were\nsent an information sheet explaining what would happen at\nthe event and why; the preference for people without science\ndegrees was reiterated. People with science degrees who\ninquired about participating were put on a waiting list.\nParticipants were able to specify seating preferences and\nregister to participate in a group. Allowing participants to\nregister in groups, in line with snowball sampling methods\n(Atkinson & Flint, 2001), drew individuals unlikely to attend\nsuch an event normally. It is probable that \"seed\" participants\nwho drew others to participate were more actively or confi-\ndently involved in deliberation at the event; so although such\nrecruitment draws a wider range of people, power imbal-\nances in participation may result. Given that snowball sam-\npling recruits participants through social networks, isolated\nindividuals were unlikely to be engaged using this method.\nResults\nRaw responses from before and after surveys were collected\nalong with voting slips. There were 85 participants who par-\nticipated in the surveys and voting processes across the three\nevents. Response rates to the whole surveys were analyzed to\nassess participants' engagement. Individual responses to\nopen questions were analyzed and processed to determine\nword frequency. Quantitative data were inputted to Microsoft\nExcel then analyzed using SPSS. The relatively small sample\nsize in this pilot study limits significance of quantitative\nresults, without supporting evidence from qualitative results.\nResearchers interested in meta-analysis or similar use of the\nquantitative anonymized data are encouraged to contact the\nauthor. There is limited discussion in this article of quantita-\ntive data in isolation, given the sample size.\nWith What Aspects of the Surveys and Format\nDid People Engage the Most?\nA few participants responded to open-ended and Likert-type\nquestions but not rating questions. Of the 85 formal partici-\npants, 80 consistently responded to questions quantitatively\nrating the presentations, demonstrating the majority's inter-\nest and capacity to rate research proposals in the requested\nmanner.\nParticipants were asked to vote for only one of the three\npresenters, reflected in the question wording, which of the\nthree projects did you vote for? Although in theory partici-\npants could have written \"all\" or \"none\" or could have\nabstained from marking the vote paper, all 85 participants\nresponded in the manner requested with a single, valid vote.\nThis 100% response rate to the voting question suggests that\nparticipants valued the voting process.\nWho Participated?\nParticipants were asked about their attitudes to science and\npolitics in the pre-event survey. When asked about how they\nwould rate their knowledge of science, 37% said they had\naverage knowledge and the rest were split between rating\nthemselves either above (30%) or below (28%) average.\nOnly 5% reported below average interest in science.\nRegarding political involvement in Australia, 34% of people\nconsidered themselves average, 35% considered themselves\nmore, and 25% less than average.\nParticipants were asked in the pre-event survey to list\nthree areas of their own expertise, in an open-ended response.\nThis was done instead of asking directly about profession for\ntwo reasons. First, people with science qualifications can\nmove into other professions. People without qualifications in\nscience may also develop expertise through their experi-\nences, as patients, for example. Second, asking participants\nto consider their own areas of expertise ahead of interacting\nwith scientists was designed to promote feelings of compe-\ntence and the concept of lay expertise. A higher than average\nlevel of education (Australian Bureau of Statistics, 2010a) is\ninferred from the number of responses in areas including law,\npolicy, education, and information technology (IT). Such\nprofessional areas of expertise were more commonly listed\nfirst, with topics such as travel or sport listed second or third.\nThis inference about above-average education levels was\nsupported by quantitative questions. People were asked to\ntick their educational experiences, more than one if applica-\nble. A third (33%) had postgraduate experience, while only\none person reported not completing high school. Some par-\nticipants chose not to answer the education question. Of 85\nparticipants across three events, 48 were women and 32 were\nmen; 5 people didn't specify their gender. People born\nwith Australia's general population (Australian Bureau of\nStatistics, 2010b), comprising more than 40% of the partici-\nIn Canberra, events did not reach capacity, so some people\nwith science degrees were invited from the waiting list to par-\nticipate. TheAdelaide event reached capacity with people who\nhad not indicated they had science degrees. Calling for partici-\npants without science qualifications seemed effective, with\nsome exceptions. Assumed exceptions included people who\nlisted an area of expertise as epidemiology or microbiology.\nWhen Did People Participate?\nThe smallest event was held in Canberra over lunchtime; this\nevent had the highest number of drop-ins. At least 6 people\nwere noted coming and going during this event,\ntypically sitting to watch parts of the presentations, without\nparticipating in the surveys, voting, or asking questions.\nEvening events were better attended than lunchtime events,\nas 77 people participated in the evening as opposed to 8 dur-\ning the day--that is 90.5% participating in the evening.\nHowever, people were more likely to stop by during the day,\nso less formal events that do not require committed participa-\ntion may suit lunchtimes better.\nWhich Research Proposal Was Preferred?\nVoting slips counted at the end of the events were compared\nwith reported votes on the second surveys later. Votes on the\nslips and surveys were consistent. Across all three events,\nregardless of presentation order, one project consistently\nreceived the most votes. At each of the three events and over-\nall, the research proposal about increasing salinity tolerance\nreceived the most votes (52 in total). The nitrogen use effi-\nciency proposal received one more vote (17) than the beta-\nglucan proposal (16).\nHow Did Participants Rate the Research\nProposals?\nSimilar to the U.K. study, participants were asked to rate\neach of the three presentations on 11 criteria. Rating was on\na scale between 1 and 5, in which 1 represented excellent and\n5 bad. The criteria are listed in Table 1.\nThese criteria reflect those used in the U.K. study (Rowe\net al., 2009), covering benefits to society or environment;\nlikeability,trustworthinessorpersuasivenessoftheresearcher;\nwhether participants found the talk understandable, interest-\ning, or personally relevant; and whether the research would\nhave timely outcomes, be innovative, or profitable.\nParticipants' ratings of the research proposals on the 11\ncriteria listed in Table 1 were aggregated. Averages (means)\nof the ratings for each research proposal were calculated.\nThese were sorted into a list with the highest rating for a\nresearch proposal on a given criteria at the top, shown in\nTable 1. Criteria on Which Participants Were Asked to Rate\nthe Research Proposals.\n1. My understanding;\n2. Benefit to society;\n3. Benefit to environment;\n4. Personal relevance;\n5. Speaker persuasiveness;\n6. Research innovation;\n7. Likeability of researcher;\n8. Interesting talk;\n9. Speaker trustworthiness;\n10. Timely outcomes; and\n11. Potential for profit.\n6 SAGE Open\nTable 2 shows the most popular proposal about salinity\nwas rated equally best for \"my understanding\" and \"benefit\nto society.\" The nitrogen proposal rated the best on \"benefit\nto environment,\" which ranked third highest overall, fol-\nlowed by \"interesting talk\" for the salinity project. The nitro-\ngen proposal rated the best on \"speaker trustworthiness\" and\n\"speaker persuasiveness,\" despite this proposal not receiving\nas many votes as the salinity project.\nThe ratings were accompanied by qualitative questions,\none of which directly asked participants why they voted the\nway they did. Analyzing the frequency of words in this quali-\ntative data revealed that the 10 words most used in describ-\ning decision-making were benefit (12), research (8), salinity\n(8), relevance (7), issues (7), Australia (6), important\n(6), global (5), interesting (5), and problem (5).\nAs these indicate, words with the highest frequency\nrelated to the proposal content rather than the presenter.\nHowever, the presence of \"interesting\" in the top 10 could\nrelate to presentation style rather than substance. Qualitative\nresponses suggested influence of a factor in participants'\ndecision-making that was unaddressed in the rating criteria.\nLocation of benefits was something people reported as influ-\nencing their voting decisions, reflected in the frequency of\nrespondents using the words \"Australia\" and \"global\" in\ncontext with the word \"relevance.\"\nDid the Presenter Influence Decision-making?\nVoting outcomes from both the U.K. and Australian studies\nrevealed the most junior male researchers garnered the most\npublic support and the most senior female researchers the\nleast. The poor result for the most experienced female in the\nU.K. study was confounded by the fact that she presented a\ndeliberately dubious project; a factor that was removed from\nthe Australian experiment. Small sample sizes limit the sig-\nnificance of this finding. However, it is worth flagging given\nevidence about barriers facing women in science (Clark\nnicity biases research funding decisions (Ginther et al., 2011;\nViner, Powell & Green, 2004). Interestingly in the Australian\nexperiment, the most successful proposal was pitched by a\nscientist with a foreign (North American) accent, whereas in\nthe U.K. study (Rowe et al., 2009), the most successful pro-\nposal was pitched by a Black scientist (not discussed in the\ncited article, but known from author participation in that\nstudy). Although voters consciously justified decisions by\ndiscussing the content of proposals, unconscious bias based\non characteristics of presenting scientists cannot be ruled\nout.\nHow Did Participants Evaluate the Event?\nParticipants were asked to rate the event they attended with\nthree Likert-type questions and one open-ended question\n(Smith, 2013b). They were asked about enjoyment and like-\nlihood of attending again. They were also asked their likeli-\nhood of attending again without food and drink as incentives.\nThe open-ended question requested suggestions for improve-\nment. Feedback was positive; 87% of the participants rated\ntheir enjoyment as above average and no one reported below\naverage. The same percentage said they were likely to attend\na similar event again, although two people were unlikely to\nattend again. However, when asked whether they would\nattend without the meal incentive, less than half of the par-\nticipants (46%) said they would be likely to attend; 12%\nwould be unlikely to; 11% would not.\nParticipants were asked how useful they thought such a\nprocess would be for making real funding decisions. More\nthought it would not be of much use and 5% thought it would\nbe useless. Of those who did not think it would be useful,\nconcerns were that the event was too brief and information\nTable 2. Average Criteria Ratings for Each Project, From\nHighest Vote to Lowest.\nCriteria for each research project\nScore (1 = excellent,\nMy understanding: Salinity project 1.71\nBenefit to society: Salinity project 1.71\nBenefit to environment: Nitrogen project 1.85\nInteresting talk: Salinity project 1.89\nMy understanding: Nitrogen project 1.98\nBenefit to society: Nitrogen project 2.01\nBenefit to environment: Salinity project 2.03\nInteresting talk: Nitrogen project 2.03\nLikeability of researcher: Salinity project 2.04\nSpeaker trustworthiness: Nitrogen project 2.05\nPotential for profit: Salinity project 2.06\nLikeability of researcher: Nitrogen project 2.06\nSpeaker trustworthiness: Salinity project 2.08\nBenefit to society: Beta-glucan project 2.1\nSpeaker persuasiveness: Nitrogen project 2.13\nSpeaker trustworthiness: Beta-glucan project 2.21\nPotential for profit: Nitrogen project 2.23\nBenefit to environment: Beta-glucan project 2.23\nSpeaker persuasiveness: Salinity project 2.24\nLikeability of researcher: Beta-glucan project 2.25\nResearch innovation: Salinity project 2.31\nPotential for profit: Beta-glucan project 2.34\nResearch innovation: Nitrogen project 2.36\nTimely outcomes: Salinity project 2.4\nResearch innovation: Beta-glucan project 2.41\nMy understanding: Beta-glucan project 2.46\nInteresting talk: Beta-glucan project 2.48\nPersonal relevance: Salinity project 2.55\nTimely outcomes: Nitrogen project 2.58\nPersonal relevance: Nitrogen project 2.69\nTimely outcomes: Beta-glucan project 2.7\nSpeaker persuasiveness: Beta-glucan project 2.74\nPersonal relevance: Beta-glucan project 2.8\ntoo shallow for people to make an informed decision. Some\nwere concerned that presentation style might influence deci-\nsions. Feedback from those who thought it would be useful\nreported what they learnt from the process. They also dis-\ncussed the importance of democracy and giving everyday\npeople voices in decision-making, as well as how the process\npromoted a sense of community.\nDiscussion and Recommendations\nThis pilot \"proof of concept\" experiment highlights several\nareas of improvement and further research. The criteria par-\nticipants rate proposals on could be modified. Different\nrecruitment methods could improve representation. The\nimpact of presenter diversity on decision-making could be\nfurther explored. Venues could be compared, with informal\nparticipation in mind. Deliberations among publics and\nexperts or policymakers could be compared. The impact of\ndeliberation length and depth could be explored, drawing on\nresearch in psychology, political science, and behavioral\neconomics.\nBenefits Based on Location\nParticipants were asked to quantitatively rate proposals for\npersonal relevance but not about relevance for people in dif-\nferent places or societies. Qualitative survey feedback as\nwell as discussions during the events suggested that deci-\nsions were influenced by where benefits would happen. So a\nnovel criterion about which participants could be asked to\nrate proposals would be location of benefits. Doing so could\nlink this type of research with that regarding public prefer-\nRecruiting People Without Science Degrees to\nImprove Representation\nAlthough successful in attracting people without a formal\nbackground in science, participants were more educated than\naverage, and particular age groups were underrepresented.\nFuture experiments to refine the method could recruit par-\nticipants with no tertiary qualifications to better overcome\nthe dominance of professionals. This experiment lacks repre-\nsentativeness in contrast to stratified random sampling, a\nmethod used for example in prioritizing health spending\n(Mitton, Smith, Peacock, Evoy, & Abelson, 2009).\nPresenter Diversity\nThis experiment sought diverse presenters to showcase the\ndiversity of people who work in science and seek funding\nfor research. Participants' reasons for their decisions\nappeared to be based on the content of proposals, consistent\nwith reasoning used to make real research funding deci-\nsions. However, research shows that biases affect decisions,\noften unconsciously (Burgess, Ryn, Dovidio, & Saha, 2007;\nFiske, 2002; Green, Pallin, Raymond, Iezzoni, & Banaji,\nexperiment did not explore the affective nature of decision-\nmaking (De Martino, Kumaran, Seymour, & Dolan, 2006).\nEmotional factors are likely stronger when proposals are\ncommunicated by a person rather than in writing. Future\nexperiments could group demographically similar research-\ners together to present their proposals, reducing variables of\ngender, age, and ethnicity. To test whether such variables do\naffect decision-making in this model, three labs with inter-\nnal diversity could work together to present a research pro-\nposal from each lab. The same proposals could be presented\nby different researchers within the respective labs at differ-\nent events. Regardless of how this affects voting outcomes,\nparticipating scientists may enhance their science communi-\ncation skills through seeing the same proposal presented and\nreceived via diverse perspectives. Actively acknowledging\ndiversity and its role in decision-making may also improve\nparticipants' satisfaction (Abdel-Monem, Bingham,\nIncluding Informal Participants\nVolunteers and venue staff can also benefit from participa-\ntion. At one event, a waitress became demonstrably engaged\nin the event, actively seeking out presenting scientists after\nformal deliberations to discuss a scientific question. Powell\nand Colin (2008) said that public engagement events should\nhappen away from research centers so they are less intimi-\ndating to those generally disengaged from science. This also\npotentially benefits venue staff who may have little exposure\nto science, as opposed to those working in science centers\nwho are exposed daily.\nComparing Deliberations of Different Groups\nThis experiment was a proof of concept for the idea that\nmembers of the public can make decisions about funding\nusing similar values to those of experts or policymakers. A\nvariation could involve using the same method to elicit deci-\nsions from groups of policymakers and experts, as well as\ngroups of public participants. The acceptability and credibil-\nity of such a public method for making decisions could be\ninfluenced by understanding similarities or differences\nbetween voting outcomes depending on who decides.\nThe Impact of Brevity\nParticipation for a few hours during a meal is more realistic\nfor time-poor participants than multi-day jury processes.\nEven busy people need to eat. Researchers have discussed\nthe role of incentives in participatory processes (Powell &\nColin, 2008); providing a meal during the event is a form of\nthis. Mansbridge (1973) observed that the time spent\n8 SAGE Open\nin participatory decision-making alienates many people,\nparticularly when there is little social incentive. Kleinman,\nDelborne, and Anderson (2009) assessed events using a\nfree meal or paid child care as incentives and found they\ncan strongly influence participation.\nEvent organizers are also likely to benefit from shorter\nevents. The costs of hiring a venue, arranging catering, and\ncoordinating speakers, participants, and staff are minimized.\nDietrich and Schibeci (2003) questioned the value of consen-\nsus conferences, arguing the cost and effort involved can be\nprohibitive, particularly for reiterative processes. Elster\n(1998) said in his book Deliberative Democracy,\nWhereas scientists can wait for decades and science can wait for\ncenturies, politicians are typically subject to strong time\nconstraints, in two different senses. On the one hand, important\ndecisions tend to be so urgent that one cannot afford to discuss\nthem indefinitely. On the other hand, less important decisions do\nnot justify lengthy deliberations. As I observed earlier, the\nimportance of time in political life implies that, in addition to\ndeliberation, voting as well as bargaining inevitably has some\npart to play. (p. 9)\nDecision-making about science funding, at the interface of\nscience and politics, does not wait for decades or centuries.\nSo exploring the practicalities of how to make such decisions\nin cost- and energy-efficient timeframes is worthwhile.\nHowever, voting events during a meal allow minimal time\nfor deliberation and limited consideration of new facts and\nviewpoints. There is a trade-off between accessibility and the\nbenefits of a more in-depth deliberation process. An empiri-\ncal study of making decisions using the Delphi technique\nfound that four rounds of questions and feedback were gen-\nerally the best; two iterations rarely achieve a stable outcome\n(Erffmeyer, Erffmeyer, & Lane, 1986). The optimal amount\nof time and available information for decision-making is\ndebatable and depends on context.\nSatisfaction with information and decisions does not\nalways correlate with good decisions (O'Reilly, 1980;\nStumpf & Zand, 1981). Participants have differing percep-\ntions of time's value (Elster, 1998). The amount of time peo-\nple have to make decisions affects how much information\nthey can handle before feeling overloaded (Buchanan &\nresearch suggests having too much information can nega-\ntively affect decision-making (Lyengar & Lepper, 2000;\nWilson & Schooler, 1991). People may make better deci-\nsions subconsciously than they do with conscious delibera-\ntion (Dijksterhuis, 2004). There may be inconsistencies\nbetween which methods participants prefer and which result\nin best decisions (Erffmeyer & Lane, 1984; Tjosvold &\nField, 1983). The amount of information or time preferred\nfor decision-making may vary with age (Cassileth, 1980) or\nculture (Gambetta, 1998), suggesting different deliberative\nprocesses may favor different demographics. There are\ndiverse academic and practical perspectives on ideal lengths,\ndepths, and types of deliberations, as well as how these vari-\nables shape decision quality. Given this, concise deliberative\nprocesses have a place in typologies of public engagement\nConclusion\nThree events experimented with a model of deliberative par-\nticipation in decision-making about science funding. These\nevents were much shorter than most deliberative processes,\nwhich had benefits for participants and organizers in costs in\nmoney and time, but penalties regarding the depth of delib-\nerations. Recruitment was targeted at those without science\ndegrees, but participants were still more educated than\nAustralia's average. Results were consistent across three rep-\netitions of the event format with different publics.\nParticipants'reports of their decision-making suggested their\ndecisions were influenced by potential benefits of the planned\nresearch, as well as their understanding. Research proposals\nthat were better understood were rated higher. Participants\nindicated concern for where the benefits of research flow, for\nexample, whether the benefits have an impact on their own\ncommunity or internationally. Although participants were\nnot unanimous in thinking the method would be useful for\nreal funding decisions, most enjoyed their experience and\nwould participate again. However, without the same meal\nincentive, many would not participate again. This experi-\nment demonstrated participants' capacity to rate research\nproposals on the type of criteria used in deciding what\nresearch proposals governments fund. Quality of delibera-\ntions and unconscious factors may have affected decision-\nmaking in unobserved ways. However, these biases have\nbeen shown in existing decision-making processes. Other\nresearchers have shown inefficiencies in research funding\nprocesses, even suggesting some randomization of funding\nallocations as an improvement. Given that participants val-\nued this deliberative process and participation in decision-\nmaking, it is arguably preferable to randomization. Civic\nbenefits from public involvement in prioritizing research\nproposals may improve the value of grant allocation pro-\ncesses in democracies.\n"
}