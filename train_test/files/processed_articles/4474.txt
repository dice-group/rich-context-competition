{
    "abstract": "Abstract. A recent study reported evidence of \"wishful seeing,\" where observers reported seeing a\ndesired object as being closer than other objects. A statistical analysis of the experimental findings\nreveals evidence of publication bias in the study, so the existence of wishful seeing remains unproven.\n1 ",
    "reduced_content": "a Pion publication\nThe same old New Look: Publication bias in a study of\nwishful seeing\nGregory Francis\nDepartment of Psychological Sciences, Purdue University, West Lafayette, IN, USA; email: gfrancis@purdue.edu\n Introduction\nBalcetis and Dunning (2010) reported five experiments with evidence that desirable objects were\njudged closer than other objects; an effect they termed \"wishful seeing.\" Every experiment rejected\nthe null hypothesis, thereby indicating evidence of the effect, and such replication across experiments\nis often taken as evidence that an effect is robust. However, this interpretation is valid only if the\nexperiments have high statistical power, which is the probability that an experiment will reject the null\nhypothesis. If all experiments reject the null hypothesis despite having relatively low power, then the\ncorrect interpretation is that there was a publication bias that over-reports positive findings (Ioannidis\n2 Power analysis\nTable 1 lists the sample sizes, standardized effect size, and power of each experimental finding in\nBalcetis and Dunning (2010) that investigated wishful seeing. The application of a meta-analytic\nmethod (Hedges and Olkin 1985), which pools the effect sizes across the experiments, reveals that\nthe best estimate of the effect of wishful seeing is g* = 0.537. The last column of Table 1 shows the\npower of each experiment to detect this pooled effect size. It is noteworthy that the two studies with\nthe smallest samples sizes have power values less than one half.\nTable 1. Statistical properties of the Balcetis and Dunning (2010) experiments on wishful seeing. Effect sizes\nwere computed from the reported t-tests.\nDescription N1 N2 Effect size Power from pooled ES\nThe sum of the power values (3.11) is the expected number of times these experiments should re-\nject the null hypothesis. The probability that all five experiments would reject the null hypothesis is the\nproduct of the power values (0.076), which is below the 0.1 threshold that is frequently used to indicate\nevidence of publication bias (Begg and Mazumdar 1994; Ioannidis and Trikalinos 2007). Another way\nto describe this finding is that the reported experiments are not self-consistent. Given the reported ef-\nfect and sample sizes, it is not believable that there would be so many rejections of the null hypothesis\nif the experiments were run properly and reported fully. The proper interpretation of the experimental\nfindings is that they are non-scientific or anecdotal.\nIt might be tempting to argue that the probability of the Balcetis and Dunning (2010) experiments\ni-Comment\nis not much below the criterion, so maybe there is hope that future experiments could make the findings\nmore believable. Although it is mathematically possible, such a situation is unlikely because Ioannidis\n(2008) notes that most experiments overestimate the true effect size, so the above analysis probably\noverestimates the true power of the experiments. Even if this were not true, new experiments are un-\nlikely to change the conclusion of publication bias. If a new experiment rejects the null hypothesis with\na similar effect size, then the product of the power values for all experiments can only be less than the\nproduct for the experiments in Table 1. If a new experiment fails to reject the null hypothesis, it will\nusually have a smaller effect size than what is shown in Table 1. The pooled effect size across experi-\nments will be smaller, which will reduce the power of all experiments. Once publication bias has been\nfound, it is difficult to remove.\n3 Interpretation\nThere are two broad explanations of how publication bias could have contaminated the findings in\nBalcetis and Dunning (2010). First, they may have run, but not reported, additional experiments that\ndid not reject the null hypothesis. This type of \"file drawer problem\" could happen because the authors\ndeliberately suppressed some findings or because reviewers or the editor insisted that the null/negative\nfindings be removed from the manuscript. Something similar to the file drawer problem can also occur\nfor experiments that have multiple measures but report data from only a subset of the measures.\nThe second broad explanation is that the experiments in Balcetis and Dunning (2010) were run\nimproperly in a way that caused an elevated rejection rate for the null hypothesis. One invalid approach\nis to start with a relatively small set of subjects and run a hypothesis test. If the null hypothesis is not\nrejected, additional subjects are recruited and the test is repeated. This procedure is continued until\nthe null hypothesis is rejected or the experimenter gives up. It may seem like good scientific practice\nto gather data until a research question is settled, but analyzing such data sets as if they were gathered\nwith a fixed sample size leads to a dramatic increase in the rejection of the null hypothesis, regardless\nof whether it is true or false (Strube 2006). There are several other experimental methods that also\nproduce too many rejections of the null hypothesis. Simons, Nelson and Simonsohn (2011) describe\nhow some of these techniques can ensure that almost every experiment rejects the null hypothesis,\nregardless of whether it is true or false. This too frequent rejection of the null hypothesis will show up\nas publication bias.\nThere is no way of telling which of these broad approaches, and it could be both, were used by\nBalcetis and Dunning (2010). In a similar way, now that the data are known to be contaminated with\npublication bias, there is no way to determine whether the null hypothesis is true or false. Researchers\ninterested in wishful seeing are advised to ignore the findings in Balcetis and Dunning (2010) and run\nnew experiments without bias.\nA third possible explanation of the pattern of data in Table 1 is that the studies measured different\neffect sizes, in which case the meta-analytic pooling is improper. For example, some experiments\nmeasured estimates of distance, while experiment 3a measured accuracy of distance related actions\n(tossing a beanbag to a target). The effect of wishful seeing as expressed by the accuracy of tosses might\nalter the reported effect size. However, the reported effect sizes are inconsistent with this explanation.\nThe action of tossing a beanbag might scale the overall magnitude of the measurement variable, but it\nwill also introduce an additional noise term to the experimental measurements, which will increase the\nstandard deviation. A change in scale will not alter the standardized effect size, but a larger standard\ndeviation will decrease the effect size. Based on this analysis, one might expect that experiment 3a will\nhave a smaller standard deviation than the other experiments, but Table 1 shows that experiment 3a has\nthe largest effect size of all of the experiments.\n4 Conclusions\nThe study of Balcetis and Dunning (2010) is one of several new studies (eg, Balcetis and Lassiter\n2010) that have revived ideas of the New Look theorists from the 1950s. The New Look approach\nargued that an observer's motivations and desires could alter perceptual experience, but the New Look\nfindings were ultimately rejected because of poor methodology. If the publication bias in Balcetis and\nPublished under a Creative Commons Licence a Pion publication\nDunning (2010) is present in similar studies, then the empirical efforts to revive the ideas of the New\nLook theory may suffer from variations of the methodological problems of the past.\nReferences\nBalcetis E, Dunning D, 2010 \"Wishful seeing: More desired objects are seen as closer\" Psychological Science\nBalcetis E, Lassiter G D, 2010 Social psychology of visual perception (New York: Psychology Press)\nBegg C B, Mazumdar M, 1994 \"Operating characteristics of a rank correlation test for publication bias\"\nFrancis G, 2012 \"Too good to be true: Publication bias in two prominent studies from experimental psychology\"\nFrancis G, in press \"Publication bias in \"Red, Rank, and Romance in Women Viewing Men\" by Elliot et al.\n(2010)\" Journal of Experimental Psychology: General\nHedges L V, Olkin I, 1985 Statistical methods for meta-analysis (New York: Academic Press)\nIoannidi, J P A, Trikalinos T A, 2007 \"An exploratory test for an excess of significant findings\" Clinical Trials 4\nSimmons J P, Nelson L D, Simonsohn U, 2011 \"False-positive psychology: Undisclosed flexibility in data\ncollection and analysis allows presenting anything as significant\" Psychological Science 22 1359\u00ad1366\nStrube M J, 2006 \"SNOOP: A program for demonstrating the consequences of premature and repeated null"
}