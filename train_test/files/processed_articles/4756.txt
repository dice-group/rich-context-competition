{
    "abstract": "Abstract\nSound-shape associations involving consistent matching of nonsense words such as `bouba' and\n`kiki' with curved and angular shapes, respectively, have been replicated in several studies. The\npurpose of the current study was to examine the robustness of previously noted sound-shape\nassociations when shape variations (angular and curvy) are embedded in schematic expressions of\nemotions (sad and happy). Results revealed consistent matching tendencies based on sound-\nemotion expression mapping irrespective of the particular shape of the expressions. We\nsuggest that internally simulating the facial expressions/oral gestures may have played a\nsignificant role in driving the matching preferences.\n",
    "reduced_content": "Short and Sweet\nThe Bouba-Kiki Phenomenon\nTested via Schematic\nDrawings of Facial\nExpressions: Further\nValidation of the Internal\nSimulation Hypothesis\nSethu Karthikeyan and Bianca Rammairone\nCommunication Sciences and Disorders Program, Pace University,\nVijayachandra Ramachandra\nDepartment of Communication Sciences and Disorders, Marywood\nUniversity, Scranton, PA, USA\n Keywords\nbouba-kiki, sound-shape mapping, cross-modal activation, mirror neurons\nRecent studies that reveal consistency in different kinds of sound-meaning associations, and\nthe increasing scope of multimodal activations in the brain raise questions about the extent of\narbitrariness in language (e.g., see review in Schmidtke, Conrad, & Jacobs, 2014). When\nangular and rounded shapes are presented with nonsense words such as bouba and kiki,\n95% of adults match the rounded shape with bouba and the jagged shape with\nkiki--the bouba-kiki phenomenon (Ramachandran & Hubbard, 2001; for original work,\nCorresponding author:\nSethu Karthikeyan, Communication Sciences and Disorders Program, Pace University, 163 William Street, NY 10038, USA.\nEmail: skarthikeyan@pace.edu\ni-Perception\nipe.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (http://www.\nus.sagepub.com/aboutus/openaccess.htm).\nsee Ko\n\u00a8 hler, 1929), an effect that has been replicated cross-linguistically (e.g., Bremner et al.,\n2012). As well, toddlers and infants display similar matching tendencies to those of adults\n(Maurer, Pathman, & Mondloch, 2006; Ozturk, Krehm, & Vouloumanos, 2012) suggesting\nthat the matching biases may be innate and not learned via experience. Whereas a few studies\nhave found consonant-driven matching patterns (e.g., Nielsen & Rendall, 2013), Spector and\nMaurer (2013) found that even when the consonant environment was kept constant, toddlers\ndemonstrated consistent vowel-shape matches of /i/ (as in beet) and /o/ (as in boat) with\nangular and curvy images, respectively.\nConsider also that the articulatory gestures for /i/ and /o/ are similar to the lip movements\nin a smile and a frown (vocal tract shortening and lengthening) leading to comparable\nacoustic characteristics--raising and lowering of filtered frequency components referred to\nas formants (e.g., Raphael, Borden, & Harris, 2011). Listeners are able to accurately identify\nspeech samples spoken with a smile and those spoken with a frown (Tartter & Braun, 1994).\nAdditionally, the vowels /i/ and /o/ have been associated with pleasantness and gloominess,\nrespectively (e.g., Newman, 1933). These findings point to the possibility that nonsense words\nwith vowel sounds /i/ and /o/, as those mentioned earlier, may be non-arbitrarily linked to\nsmile and frown expressions. The purpose of the current study was to examine the robustness\nof previously noted sound-shape associations when shape variations were embedded in\nschematic expressions of emotions (see Figure 1).\nRamachandran and Hubbard (2001) proposed a mirror neuron-based cross-modal\nactivation hypothesis to explain the bouba-kiki effect; the internal simulation of the\nappropriate articulatory gesture of the auditory stimulus is mapped onto specific\nphonemic inflections, which then are non-arbitrarily linked with specific shapes or\nimages. In the current study, while the facial expression of a smile or a frown may be\ninternally simulated in addition to the motor patterns of the aurally presented words, the\nexact shape of the expressions may not be amenable to simulation because angularity\nvariations such as the ones examined in the study are not naturally occurring or socially\nrelevant (e.g., Oberman & Ramachandran, 2007). The resulting matching tendencies,\ntherefore, must reflect consistencies between the corresponding acoustic characteristics of\nthe emotional expression and of the auditory stimulus. For example, a sad angular/curvy\nface matched with words containing the /o/ vowel sound. If, on the other hand, simulations\nof facial expressions of emotions do not occur, the variations of the facial expressions in\nthe angularity curviness dimension must influence the matching preferences in a similar\nmanner to the established matching tendencies between sounds and random angular-curvy\nshapes in earlier studies. For example, a sad/happy angular face matched with words\ncontaining the /i/ vowel sound.\nEach of the four computerized schematic drawings of faces was presented individually\nthree or four times with randomly chosen word pairs (see Table 1). The aural presentation of\nthe word pairs was counterbalanced across the four faces for the order of /i/ and /o/ vowel\nsounds and for the consonant environment. The institutional review board approved the\nmatch the happy/sad face with one of the words in the presented word pair by clicking on\nthe appropriate text options: WORD 1 or WORD 2.\nAs Figure 2 shows, the happy angular face and the happy curve face was each matched\nmore frequently with words containing the /i/ vowel sound than with words containing the /\no/ vowel sound. The preference for the /i/ vowel sound did not differ between these two faces\n(paired t test, t(49)\n\u00bc 1.59, p > .05). The sad curve face and the sad angular face was each\nmatched more frequently with words containing the /o/ vowel sound. The preference for the /\no/ vowel sound did not differ between these two faces (paired t test, t(49)\nTable 1. Word Pairs Created Using Cepstral David (Swifttalker) Separated\nConsonant environment 1 Consonant environment 2\nbibi-bobo fifi-fofo\nbobo-bibi fofo-fifi\ndidi-dodo kiki-koko\ndodo-didi koko-kiki\nlili-lolo titi-toto\nlolo-lili toto-titi\nmimi-momo zizi-zozo\nmomo-mimi zozo-zizi\nFigure 1. Schematic drawings presented via SuperLab 5; (a) happy angular, (b) sad angular, (c) happy curve,\nand (d) sad curve.\nKarthikeyan et al. 3\nIf the acoustic characteristics associated with the exact shape of the facial expressions were\nbeing mapped onto the acoustic characteristics of the words presented aurally, the shape\ndifferences within each emotional category (curved vs. angular versions of happy and sad\nfaces) would have resulted in a notable difference in the choices of vowel sounds; however, a\ndifference did not emerge. We cannot rule out the potential influence of emotional contagion\n(e.g., Lundqvist & Dimberg, 1995), in that the happy ``feeling'' on seeing a happy face may\nhave been mapped onto the ``pleasant'' sounding /i/ and the sad feeling on seeing a sad face\nwas matched with the gloomy sounding /o/ (e.g., Newman, 1933).\nIn sum, the current study in conjunction with the previous ones on sound-symbolism\ndemonstrate that cross-modal matching may allow for non-arbitrary associations of the\nvocal\u00adverbal signal with aspects of inanimate and animate entities (including the self and\nothers). When two or more aspects co-occur, these may compete with one another or one\nmay supersede the others in guiding the associations; in the current study, the facial\nexpressions took precedence over the angularity or curviness of the expressions. Also,\nthere exists the possibility of covert imitation of the oral gestures alone (retraction of lips\nand rounding or protrusion of lips), devoid of any emotional meaning, to aid this kind\nof auditory visual matching task (see Studdert-Kennedy, 2000, 2002, for a discussion on\nthe evolution of vocal imitation as a step toward promoting arbitrary linkages between\nsignals and messages or referents). Considering the larger scheme of things, it is reasonable\nto deduce that a communication system that is based heavily on non-arbitrariness or iconicity\ncould lead to ambiguity (e.g., Pinker & Bloom, 1990), and limited expressive scope (e.g.,\nStuddert-Kennedy, 2000, 2002), a few possible factors that may have favored arbitrariness in\nthe evolution of the human capacity for language.\n"
}