{
    "abstract": "Abstract Important theoretical questions in survey research over the past\n50 years have been: How does bringing in late or reluctant respondents af-\nfect total survey error? Does the effort and expense of obtaining interviews\nfrom difficult-to-contact or reluctant respondents significantly decrease the\nnonresponse error of survey estimates? Or do these late respondents intro-\nduce enough measurement error to offset any reductions in nonresponse\nbias? This study attempts to address these questions by examining nonre-\nsponse and data quality in two national household surveys: the Current\nPopulation Survey (CPS) and the American Time Use Survey (ATUS).\nResponse propensity models were developed for each survey, and data\nquality in each survey was assessed by a variety of indirect indicators\nof response error, for example, item-missing-data rates, round value reports,\nand interview-reinterview response inconsistencies. The principal analyses\ninvestigated the relationship between response propensity and the data-\nquality indicators in each survey, and examined the effects of potential\ncommon causal factors when there was evidence of covariation. Although\nthe strength of the relationship varied by indicator and survey, data quality\ndecreased for some indicators as the probability of nonresponse increased.\nTherefore, the direct implication for survey managers is that efforts to re-\nduce nonresponse can lead to poorer-quality data. Moreover, these effects\nremain even after attempts to control for potential common causal factors.\nIntroduction\nImportant theoretical questions in survey research over the past 50 years have\nbeen: How does bringing in late or reluctant respondents affect total survey\nScott Fricker is a Research Psychologist at the U.S. Bureau of Labor Statistics, Washington, DC,\nUSA. Roger Tourangeau is a Research Professor at the Institute for Social Research at the Uni-\nversity of Michigan, Ann Arbor, MI, USA, and the Director of the Joint Program in Survey Meth-\nodology at the University of Maryland, College Park, MD, USA. *Address correspondence to Scott\nFricker, U.S. Bureau of Labor Statistics, 2 Massachusetts Ave. NE, Room 1950, Washington, DC\n\u00d3 The Author 2011. Published by Oxford University Press on behalf of the American Association for Public Opinion Research.\nAll rights reserved. For permissions, please e-mail: journals.permissions@oup.com\nerror? Does the effort and expense of obtaining interviews from difficult-to-\ncontact or reluctant respondents significantly decrease the nonresponse error\nof survey estimates? Or do these late respondents introduce enough measure-\nment error to offset any reductions in nonresponse bias?\nEvidence from some recent studies suggests that efforts to reduce nonre-\nsponse rates have little effect on nonresponse error (Curtin, Presser, and Singer\ngible differences between monthly estimates of consumer confidence derived\nfrom a full survey dataset and those derived from a dataset in which hard-\nto-interview respondents had been removed. Similarly, Keeter et al. (2000)\nand Merkle and Edelman (2002) found little correlation between low response\nrates and nonresponse bias.\nMuch less attention, however, has been given to the relationship between\nresponse propensity and survey measurement error. In part, this neglect may\nreflect the assumption that the causes of nonresponse and measurement error\nare independent. Nonresponse typically is seen as a function of motivational\nvariables (e.g., interest in the survey topic, time spent away from home),\nwhereas measurement error is considered primarily a function of cognitive fac-\ntors (such as respondent ability, excessive recall demands imposed by the ques-\ntions, poor question wording, and so on). This assumption of independent\ncausal factors may be untenable, however, because the same motivations that\naffect participation decisions also may affect performance during the interview.\nTo the extent that individuals\u00d5 response propensities are positively correlated\nwith the level of effort that they give during the response process, bringing\nreluctant individuals into the respondent pool will increase measurement error\nThe few empirical studies that have examined the association between non-\nresponse and data quality suggest that the relationship depends on the statistic of\ninterest, how measurement error is operationalized, and the type of nonresponse\n(noncontact vs. noncooperation). Some studies that have examined indirect\ndata-quality indicators (e.g., item nonresponse, response completeness) have\nfound that late responders and initial refusers are more likely than early\nresponders and those not requiring refusal conversion to skip items; give\nshorter, less informative answers to open-ended questions; and provide ``don't\nknow,'' ``not applicable,'' or ``no opinion'' responses (e.g., Friedman, Clusen,\nindicators of data quality (e.g., acquiescence, nondifferentiation) were unrelated\nto response propensity, or were negatively correlated with it; that is, low-\npropensity groups produced better data quality than high-propensity groups.\nSeveral studies that have looked at direct estimates of measurement error\n(e.g., those based upon discrepancies between survey responses and adminis-\ntrative records) have found that low-propensity respondents tend to provide\nNonresponse Propensity and Data Quality 935\nworse data than high-propensity respondents. For example, Cannell and Fowler\n(1963) found that individuals who responded at the end of the survey field\nperiod were 10\u00ad15% less accurate in their reports of the number and duration\nof their hospital stays than those who responded earlier (see also Kalsbeek et al.\nrespondents were less likely than ``uncooperative'' respondents to drop out\nof a panel survey and to make reporting errors. More recently, Olson (2006) ex-\namined the separate impact of contact and cooperation propensity on several\nvariables related to marital dissolution (e.g., time since divorce, length of mar-\nriage), separating out the unique contributions of measurement error bias and\nnonresponse bias. She found that including reluctant respondents increased mea-\nsurement error for some estimates, but that bringing in hard-to-contact respond-\nents actually led to decreases in measurement error and overall error (see also\nVoigt et al. 2005). For most of the estimates in her study, however, the resulting\nchanges in measurement error were nonmonotonic across propensity strata and\nwere very small relative to the size of the estimates. A much stronger link\nbetween response propensity and reporting errors was found in a recent study\nby Tourangeau, Groves, and Redline (2010), which examined the role of topic\nsensitivity on nonresponse and measurement errors. They found that individuals\nwho were asked to participate in a survey that raised social desirability issues for\nthem were both less likely to respond to the survey and more likely to provide\ninaccurate answers to the sensitive questions if they did.\nThe results of these studies suggest that there may be a relationship between\nresponse propensity and data quality, but the nature of that relationship and its\ncausal mechanisms are not well understood. At the very least, these findings\nchallenge the traditional assumption that nonresponse and measurement error\nare independent. One explanation for covariance between response propensity\nand data quality is that the relationship results from a cause (or vector of causes)\ncommon to both (Groves 2006). The identification of appropriate common\ncausal factors depends in part upon the particular survey protocol and respon-\ndent pool, but several candidates seem likely to apply to a broad range of sur-\nveys. For example, topic interest is a possibility. Interest in the survey topic may\ndispose individuals to agree to a survey request and also stimulate careful pro-\ncessing of the survey items. In general, any factor that broadly increases the\nmotivation to take part in surveys and to respond carefully might lead to a rel-\natively general relationship between response propensities and response quality.\nFor example, higher levels of social capital could activate stronger norms of\ncooperation (producing higher response propensities) and those same norms also\ncould influence respondents\u00d5 willingness to engage in more careful processing of\nthe survey questions. At the other end of the spectrum, busyness or time-stress\ncould produce a general disinclination both to participate in surveys and to re-\nspond accurately if interviewed. Identifying and statistically controlling for such\nshared explanatory factors would eliminate the relationship between response\npropensity and data quality and provide a means for removing bias.\nThe purpose of this study was to explore the relationship between nonre-\nsponse and data quality, and where evidence of covariation emerged, to exam-\nine potential common causal factors underlying the relationship. These issues\nwere investigated using data from two national household surveys: the Current\nPopulationSurvey(CPS)andtheAmericanTimeUseSurvey(ATUS).Datafrom\neachsurveywereusedtodevelopresponse-propensitymodelstopredictthelikeli-\nhood of nonresponse. The next section of this article provides an overview of the\nCPS and ATUS and briefly describes the process of creating the datasets, propen-\nsity models, and data-quality indicators used in this study. The remainder of the\narticle presents the results of the principal analyses investigating the relationship\nbetween response propensity and data-quality indicators in each survey.\nData and Method\nThe Current Population Survey: The CPS is the primary labor-force survey in\nthe United States and is conducted by the U.S. Census Bureau for the Bureau of\nLabor Statistics. Each month, the CPS surveys approximately 60,000 house-\nholds in 792 sample areas across the country on issues such as employment,\nearnings, and hours worked. Demographic and labor-force questions are asked\nabout the respondent and the other household members. Each CPS housing unit\nis sampled on a rotational basis so that any given month includes eight different\nrotation groups. Housing units within a given rotation group are sampled for\nfour consecutive months, are out of the sample for eight months, and then return\nto the sample for another four consecutive months. This rotation pattern makes\nit possible to match information on housing units monthly across their entire\nCPS life cycle. The sampling unit in the CPS is the housing unit (that is, the\nresidence), not the household (the residents).\nThe data used to create the CPS propensity scores cover a two-and-a-half-\nwere eligible for all eight CPS waves. To create a longitudinal data file of\nCPS sample units, records from each month were matched based on household\nID, person ID, month-in-sample (MIS), and year. Housing units that were in-\neligible to participate in the CPS in any round by virtue of being vacant, demol-\nished, nonresidential, etc., were excluded, as were units in which all the\nprevious month's residents had moved and been replaced by an entirely differ-\nent group of residents.\nThe resulting CPS dataset contained information on 251,000 individuals.\nNonresponse in the CPS is a household-level phenomenon, so a household-\nlevel dataset was created that included basic household information--e.g.,\nthe number of household members by age group, race, employment status, ed-\nucation level, etc.--as well as information about the main household respon-\ndent. The main respondent was the household member who was the most\nfrequent CPS respondent over the household's eight waves. Over 95 percent\nof households had a person who responded to the CPS four or more times,\nNonresponse Propensity and Data Quality 937\nand many of them responded to all eight interviews.1 After collapsing to the\nhousehold level, the resulting CPS data file had household-level and main re-\nNonresponse in the CPS is relatively low compared to other national household\nsurveys,andtypicallyisworstinround1(whennoncontactishighest)andagainin\nround 5 (when the household is returning to the CPS sample for the first time in\neight months). The average response rate for the period covered in our analyses\npercent, respectively, and the remaining rounds had an average response rate of\n93.7 percent (American Association for Public Opinion Research [AAPOR], re-\nsponse rate 3). Since the vast majority of CPS households respond to all eight\ninterviews, the CPSpropensity model developed for these analysesuseddata from\nthe first two waves to predict nonresponse at any wave in months three through\neight.A singleestimate ofoverall nonresponse propensity(notseparatingout non-\ncontact and noncooperation) was obtained for each household using a logistic re-\ngressionmodelpredictingtheprobabilitythattheunitwouldbeanonrespondentin\nanyofthelastsixCPSrounds.Predictorsinthismodelincludedlevelofeffort(e.g.,\ncall attempts) and demographic control variables, as well as variables related to\nbusyness and social capital constructs. Table 1 lists the variables that reached sig-\nnificance in the final model. This model fit the data with a residual chi-square of\nOn the basis of their predicted probabilities of nonresponse, households were\ndivided into propensity quintiles that ranged in average nonresponse propensity\nfrom one percent for the lowest nonresponse propensity group (Group 1) to 30\npercent for the highest (Group 5). Each CPS propensity quintile consisted of\nThisstudyexaminedfourindicatorsofCPSdataquality:itemnonresponse,round\nvalue reports, classification errors that potentially reflect spurious changes in\nrespondents\u00d5 answers between roundsofthe survey (e.g.,changesinreportedrace),\nand interview-reinterview response variance.2 Eight CPS demographic items were\nselected for inclusion: sex, age, race, ethnicity, educational attainment, homeown-\nership,telephone status,and family income. Theseitemswere chosenbecausethey\nwere asked of every household member at least once, were variables commonly of\ninterest to researchers, and offered a range of potential response errors. In addition,\neight of the most frequently asked labor-force items were examined. Data were\n1. If two or more people in the household responded an equal number of times, person-level data\nwere retained for whichever person was the most closely related to the CPS reference person.\n2. Each month, CPS attempts to conduct a response error reinterview on a one-percent subsample of\nresponding households. The reinterviews consist of the entire set of labor-force questions; house-\nhold membership is dependently verified, and no reconciliation is conducted. An effort is made to\nreinterview the person who responded to the original interview, but interviewers are allowed to\nconduct the reinterview with other knowledgeable household members (U.S. Census Bureau\nTable 1. Significant Predictors of Nonresponse in Waves 3\u00ad8 of CPS\nConstruct Variable Construct Variable\nControls\nRespondent age, sex, race, origin\nSocial Capital\n# of Non-family/relatives present\nUrbanicity x region Household size\nCitizenship\nSeason of CPS wave 1 interview\nHH ownership (own vs. rent)\nLevel of Effort/Reluctance CPS nonresponse in wave 1 or 2\nRacial diversity (county)\n# of contact attempts in wave 1\nEducational attainment (county)\nFamily income item nonresponse in wave 1\nMedian family income (tract)\nBusyness Hours worked (wave 1)\nIncome inequality (county)\nEmployment status\nAll working adults in HH work\nMarital status\nPresence of young child(ren)\nNonresponse Propensity and Data Quality 939\naggregated across household members, variables, and survey waves to obtain an\noverall value for each data-quality indicator for each household.\nThe percent household item nonresponse was calculated as follows:\n\u00bc\nP\nW\n\u00f0\nP\nPj\nmijk\nmijk\n\u00fenijk\n\u00de, where PJ INR is the percent item nonresponse\nfor household j, mijk\nis the total number of missing responses for person i in\nhousehold j in wave k, and nijk\nis the total number of non-missing responses\nfor person i in household j in wave k, summing across all members (Pj\n) of house-\nhold j for all waves (W) in which the household responded to the CPS. The\npercent round value reports and percent between-round classification changes\nwere calculated in a similar manner, except classification changes were summed\nover wave pairs rather than waves.3 The percent of inconsistent responses\nbetween the main CPS and the CPS reinterview was created for the 3,851\nhouseholds in the dataset that participated in the CPS reinterview program.\nEach of these data-quality indicators then was examined to see if it was related\nto likelihood of CPS nonresponse. We first analyzed the four indicators across\npropensity strata to assess the relative size and direction of the association. We\nthen explored the extent to which controlling for potential common causal var-\niables affected the association between indicators of data quality and nonre-\nsponse propensity. Finally, we repeated these analyses using CPS sample\nmembers\u00d5 actual response status in rounds three through eight--i.e., whether\nthey participated in all six rounds or were a nonrespondent in at least one of those\nrounds--to examine the relationship between observedCPS nonresponse (rather\nthan respondents\u00d5 modeled nonresponse propensity) and CPS data quality.\nAmerican Time Use Survey (ATUS): The ATUS is a cross-sectional, computer-\nassisted telephone survey that is carried out by the U.S. Census Bureau for the\nBureau of Labor Statistics. Its primary purpose is to provide national estimates of\nhow Americans spend their time. The ATUS sample is drawn from CPS house-\nholds that have completed their eighth CPS interview. A single household mem-\nber from each responding CPS household is randomly selected to participate in\nthe ATUS interview two months after the eighth CPS interview. The designated\nperson is assigned a specific reporting day of the week (e.g., Monday); substi-\ntutions are not allowed either for the designated ATUS respondent or for the\nassigned reporting day. If the interview cannot be completed on the designated\nday during the first week of the interviewing period, subsequent interview\nattempts are made on the designated day each week for up to eight weeks.\nTo create the ATUS dataset used in these analyses, records from the January\u00ad\nDecember 2003 ATUS public-use files were merged with the ATUS Call History\nFile. These files contained information about the respondent (e.g., updated de-\nmographic and labor-force data), the household (e.g., composition,\n3. A classification change indicates that a respondent provided different answers to the same ques-\ntion asked in adjacent waves. The variables examined for this indicator were race, educational at-\ntainment (restricted to individuals 30 years of age or older), housing tenure, and family income.\ndemographics, weight), the time-use activities of the designated ATUS respon-\ndent, the interview process (e.g., interview outcome codes), and ATUS call his-\ntories (e.g., outcome codes for individual call attempts). The files were merged\nand matched to the CPS file, resulting in a final ATUS data file with 25,778\nrecords.\nThe dependent variable for the ATUS propensity model was the interview out-\ncome (i.e., response vs. nonresponse). The respondent, household, and commu-\nnity predictors included in the ATUS model were identical to those used in the\nCPS analyses, with four exceptions. Three ATUS interview-process variables\nwere added: the number of call attempts made to ATUS sample members over\nthe eight-week ATUS fielding period, a variable indicating whether the desig-\nnated ATUS respondent was the same person identified as the wave 8 CPS re-\nspondent, and the time of day during which the majority of ATUS call attempts\nwere made. The ATUS model also included an indicator of CPS nonresponse\nAs in the CPS analyses, a logistic regression model was used to estimate\na nonresponse propensity score for each ATUS sample member. Table 2 lists\nthe variables that reached significance in the final ATUS model. The model\na Max-rescaled R-squared of 0.3708. We then grouped ATUS respondents into\nquintiles based on these propensity scores and ordered them from lowest non-\nresponse propensity (average nonresponse propensity of 10.6 percent in the\nlowest group) to highest (average propensity of 54.9 percent), with approxi-\nFor each of the 20,698 individuals in the dataset who participated in ATUS,\nwe created four data-quality indicators: (1) total number of diary activities\nreported; (2) missing diary reports of basic daily activities;5 (3) round values\nfor activity durations; and (4) item nonresponse on ATUS labor-force ques-\ntions. As in the CPS analyses, we began by examining the means for the four\nindicators across propensity strata and then assessed the effects of controlling\nfor potential share explanatory factors. We then conducted parallel analyses to\nexamine how ATUS data quality varied as a function of nonresponse in the CPS\nand ATUS refusal conversion. We also analyzed the association between CPS\n4. The use of the overall CPS response-propensity score resulting from multivariate logistic regres-\nsion models run on the CPS predictors was evaluated as an alternative to the raw measure of re-\nsponse status in CPS waves 3\u00ad8, but was found to be less predictive. Therefore, the raw measure was\nused instead. This measure of CPS nonresponse in Rounds 3 through 8 replaced the measure of CPS\nnonresponse in Rounds 1 or 2.\n5. In a given day, most people sleep, eat, and perform personal-care activities (e.g., grooming,\ndressing, going to the bathroom). When diaries do not contain one or more of these basic activities\nin the 24-hour period, it may be an indication that respondents intentionally omitted some behaviors\nor simply did not try to report their activities accurately. We coded the number of times these basic\nactivities were reported, and flagged cases for which there was no data. A surprisingly large number\nof people (31.6 percent of ATUS respondents) failed to report at least one of these activities.\nNonresponse Propensity and Data Quality 941\nTable 2. Significant Predictors of ATUS Nonresponse\nConstruct Variable Construct Variable\nControls\nRespondent age, race, origin\nSocial Capital\n# of Non-family/relatives present\nFamily income\nLevel of Effort/Reluctance\nCPS nonresponse in waves 3\u00ad8\nEmployment status\n# of call attempts\nATUS respondent same as CPS\nMarital status\nFamily income item nonresponse\nin CPS\nPresence of young child(ren)\nBusyness\nPercent of HH adults who work\nMedian family income (tract)\nOccupation type (executive/professional,\nservice, support/production,\nnot in labor force)\nRacial diversity (county)\nDiversity x region\ndata-quality indicators and ATUS response status to see if poor response quality\non the CPS was associated with ATUS nonresponse.\nResults\nCPS: Figure 1 presents the relationship between the CPS data-quality indica-\ntors and CPS nonresponse propensity. The graph displays five nonresponse-\npropensity strata, with likelihood of nonresponse increasing from left to right\nalong the x-axis. In addition, the figure presents data-quality indicators that have\nbeen standardized into standard-deviation units in order to make it easier to\ncompare the relative strength of each measure\u00d5s association with propensity.\nWe ran regression models (regressing the individual indicators on nonresponse\npropensity) to obtain slope estimates and significance tests (ANOVA models\nalso were run to check for nonlinear trends).\nThere are two main points to take away from this figure. First, the overall\nquality of CPS reports appears to decrease across nonresponse-propensity\nstrata. Taking the mean data-quality score within each strata (by averaging\nacross the four indicators), we see that there is a monotonic increase in error\nas nonresponse propensity rises. Second, the strength of the covariance between\npropensity and error is highly dependent upon the type of data-quality indicator.\nThe relationship is strongest for item nonresponse (\u00df \u00bc .17, p < .001): House-\nholds with the highest probability of nonresponse had item-missing rates that\nwere almost a full standard deviation (or about six percentage points) higher\nthan households with the lowest nonresponse propensity. Round value reports\nalso were significantly related to nonresponse propensity, though the strength\nof the association was about two-thirds that of item nonresponse (\u00df \u00bc .11,\np < .001). The highest nonresponse-propensity households provided about\n10 percent more round value reports than the lowest-propensity households.\nIn contrast, nonresponse propensity was only weakly associated with the\npercent inconsistent reports between the basic CPS and reinterview (\u00df \u00bc\n.07, p \u00bc .021) and in fact was slightly negatively correlated with the measure\nof between-wave classification changes (\u00df \u00bc \u00c0.05, p < .001).6\nWhy might item nonresponse and round value reports be related to the level\nof nonresponse propensity? One possibility is that this relationship may result\nfrom a shared explanatory factor (or factors). If the shared factors model is cor-\nrect, and the model is correctly specified with the appropriate explanatory var-\niable(s), then the relationship between response propensity and data quality will\nbe eliminated once the shared explanatory factors are statistically controlled.\n6. Given the relatively small sample size of the reinterview dataset and the fact that the between-\nwave classification change estimate itself likely had significant error (since some ``true'' change\ncould occur between waves for some of the variables used in this measure), it is not surprising\nthat these two indicators proved less strongly and consistently related to nonresponse propensity.\nNonresponse Propensity and Data Quality 943\nTo test the shared explanatory factors hypothesis, we examined several fac-\ntors that potentially could contribute to both the likelihood of unit nonresponse\nand measurement error. Busyness and social capital, as discussed earlier, are\ntwo possible relatively general common causal candidates, and we included\nthem in the present analyses. A third possibility is survey burden. In a panel\nsurvey like the CPS, the level of burden respondents\u00d5 experience in one wave\nmay affect both their likelihood of response in subsequent waves and their will-\ningness to answer fully and accurately if they do participate. Since we did not\nhave direct measures of these three factors, we examined a number of indicators\nfor each construct. Hours worked and commute time served as indicators of\nbusyness. For social capital, we examined marital status, homeownership,\nthe presence of children in the household, and educational achievement in\nthe community. Item burden (i.e., the number of items asked during the first\ntwo CPS waves) served as the measure of survey burden.\nWe began by looking at the effects of each of these variables individually on\nthe association between nonresponse propensity and the two indicators of data\nquality that showed the strongest association with nonresponse propensity (item\nnonresponse and round value reports). If the covariance evident in figure 1 is the\nresult of one of these common causal variables, then we would expect the co-\nvariance to diminish or go to zero after controlling for that variable. However,\nwe found no evidence that busyness, social capital, or survey burden (at least as\noperationalized here) had any mediating effect on the relationship between pro-\npensity and data quality. This is illustrated in figure 2 for the item nonresponse\nFigure 1. Relationship of CPS Data-quality Indicators (in Standard Devi-\nation Units) to CPS Nonresponse Propensity.\nmeasure. To give a better sense of the magnitude of the effects, we present the\nraw item-nonresponse rates in figure 2, not the standardized measure. This fig-\nure reveals that the level of reporting error continued to covary with nonre-\nsponse propensity even after we took into account measures of busyness\n(top-left panel), survey burden (top-right panel), and social capital (bottom\ntwo panels). The shapes of the curves in this figure are essentially the same\nas those found in figure 1, and this finding also was true for the other common\ncausal variables (not presented here) that we examined. The same pattern was\napparent when we examined the effect of common causal variables on the re-\nlationship of CPS nonresponse propensity and round value reports. Figure 2\nalso underscores the practical impact of bringing in difficult-to-contact or re-\nluctant respondents. The levels of missingness for most items in the CPS\nare quite low--generally less than three percent. But respondents in the\nhigh-nonresponse-propensity group often have item-missing rates that are\ntwo or three times that level.\nFigure 2. Effects of Potential Common Cause Variables on the Relation-\nship Between CPS Item Nonresponse and Unit Nonresponse Propensity.\nNonresponse Propensity and Data Quality 945\nWe next ran a simple regression model that used nonresponse propensity to\npredict item nonresponse (or round value reports). We compared the results of\nthis model to those from a series of models that also included one of the shared\nexplanatory variables. The results of this analysis confirmed what was evident\nin figure 2; that is, controlling for individual common causal variables had little\neffect on the size or direction of the relationship between nonresponse propen-\nsity and data quality. Moreover, this relationship was evident even when more\ncomplex, multivariate models were run that controlled for multiple common\ncausal variables simultaneously.\nThe preceding analyses revealed a positive relationship between CPS non-\nresponse propensity and measurement error, but we can also look to see whether\nmeasurement error varied as a function of actual CPS nonresponse. And, in\nfact, it did. The effects of CPS nonresponse mirror those from the nonre-\nsponse-propensity analyses. Item nonresponse (\u00df \u00bc .75, p < .001) and to a les-\nser extent round value reports (\u00df \u00bc .10, p < .001) were significantly and\npositively related to actual CPS nonresponse in rounds 3 through 8, whereas\nchanges in classifications between CPS waves (\u00df \u00bc \u00c0.17, p < .001) and basic\ninterview-reinterview response inconsistencies (\u00df \u00bc \u00c0.06, p \u00bc .244) were neg-\natively related to nonresponse. Moreover, controlling for potential common\ncausal variables--both individually and in multivariate analyses--had little ef-\nfect on the associations between CPS unit nonresponse in waves 3 through 8\nand CPS data quality across waves. This finding is illustrated in table 3, which\npresents the results of a multivariate regression analyses predicting CPS item\nnonresponse from CPS unit nonresponse and a set of shared explanatory var-\niables. It reveals that this association remains when the common causal vari-\nables are statistically controlled, though this model accounted for less variance\nthan the corresponding models with CPS nonresponse propensities (R-square \u00bc\nATUS: We examined the relationship between CPS data-quality indicators\nand ATUS response status to see if the CPS quality measures could be used\nas a predictor of ATUS unit nonresponse. Table 4 presents the weighted mean\npercents of the CPS data-quality indicators for ATUS respondents and nonres-\npondents, the associated t-values, and zero-order correlations between ATUS\nresponse status and each of the CPS measures. The strongest effect was for CPS\nitem nonresponse--ATUS nonrespondents had significantly higher CPS item-\nmissing-data rates than ATUS respondents. ATUS nonrespondents also had\nsignificantly more round values in their CPS answers than ATUS respondents,\nbut the relative difference between these groups in reporting of round values\nwas quite small given the large amount of round reporting overall, and the cor-\nrelation of round reporting with ATUS response status was considerably\nsmaller than that for item nonresponse. In addition, there was a small, negative\ncorrelation between ATUS response status and CPS between-wave changes in\nclassification--ATUS nonrespondents had fewer between-wave changes than\nATUS respondents. When we examined the small number of ATUS cases that\nalso had been in the CPS reinterview program, there were no differences be-\ntween ATUS respondents and nonrespondents in the level of CPS interview-\nreinterview response inconsistencies.\nFigure 3 presents the relationship between ATUS data-quality indicators and\nATUS nonresponse propensity. As before, the graph shows nonresponse\nTable 3. Multivariate Regression Model Predicting CPS Item\nNonresponse (standardized) from CPS Unit Nonresponse in Waves 3\u00ad8\nand Potential Common Causal Variables\nEffect Estimate t F Sig\nTable 4. Relation of CPS Data-quality Indicators to ATUS Outcome\nCPS Data-\nquality\nMeasure\nATUS\nRespondent\nATUS\nNonrespondent\nCorrelation\nwith ATUS\nNonresponse\nItem-missing\nrate\nRound value\nreports\nChange in\nclassifications\n(basic CPS)\nInconsistent\nreports(CPS\nreinterview)\nNonresponse Propensity and Data Quality 947\npropensity increasing from left to right along the x-axis and the data-quality\nindicators are presented in standard-deviation units. For three of the measures--\nactivity durations reported as round values, missing activity reports, and labor-\nforce-item nonresponse--points above the zero-deviation line indicate poorer\ndata quality; points below the zero-deviation line indicate better data quality.\nFor the total number of diary activities reported, however, this is reversed--\npoints above the zero-deviation line indicate that respondents reported more\nthan the average number of activities; points below the line indicate that they\nreported less than the average.\nAs can be seen in the figure, there is a linear trend between nonresponse\npropensities and overall data quality in the ATUS. If we aggregate the standard-\nized scores from the four data-quality indicators within each nonresponse strata\n(after flipping the signs for the total activity measure), we see that error\nincreases with nonresponse propensity (see figure 4). However, the size of this\neffect is small--less than 0.2 standard deviations separate the lowest- and high-\nest-propensity groups.\nThis small difference reflects the relatively weak correlations between non-\nresponse propensity and the individual data-quality measures. Although each\nis positively (and significantly) related to nonresponse propensity, the only\neffect with any practical significance is for the total number of diary activity\nreports (\u00df \u00bc .05, p < .001). Respondents in the highest nonresponse-propensity\ngroup reported about three fewer diary activities than respondents in the\nlowest-propensity group, which amounts to roughly 15 percent of the typical\nnumber of activities reported (20). When we couple this fact with the finding\nFigure 3. Relationship of ATUS Data-quality Indicators (in Standard\nDeviation Units) to ATUS Nonresponse Propensity.\nthat respondents in the high nonresponse-propensity group also are more likely\nthan other sample members to report activities in round time blocks, neglect to\nreport basic daily activities, and provide incomplete data on ATUS labor-force\nitems, it raises questions about the impact of including these individuals in\nATUS estimates.\nDoes the significant relationship between nonresponse and total activity\nreports disappear when we control for the effects of potential common causal\nfactors? We examined the impact of several potential common causal variables\nand present some representative results in figure 5. None of the common causal\nvariables examined (including those not presented here) significantly weakened\nthe covariance between nonresponse propensities and the ATUS data-quality\nindicators. The overall magnitude and direction of the relationship between\nnonresponse and total activities was very similar to that shown in figure 3.\nRegression analyses that controlled for the common causal variables individ-\nually and then multivariately corroborate these findings; nonresponse continued\nto be significantly related to the total number of items reported in the ATUS\ntime diary even when busyness, social capital, and survey burden variables\nwere taken into account. Fewer activities were reported by respondents with\nhigh nonresponse propensities and for those without children; the number of\nactivity reports also was negatively correlated with hours worked and positively\ncorrelated with educational attainment.\nWe carried out additional analyses that examined the association between\nATUS data quality and other indicators of ATUS response propensity: CPS\nunit nonresponse and refusal conversion in the ATUS. To examine the relation-\nship between CPS unit nonresponse and ATUS data quality, we classified\nFigure 4. Average ATUS Diary Error (in Standard Deviation Units) by\nATUS Nonresponse Propensity Group.\nNonresponse Propensity and Data Quality 949\nATUS respondents into three groups: those who participated in all eight rounds\nof CPS (92.7 percent), those who failed to participate in one CPS round (5.5\npercent), and those who failed to participate in two or more rounds of the CPS\n(1.8 percent).7 Figure 6 shows the relation of the CPS response-status variable\nto the four ATUS data-quality indicators. No difference was found in data qual-\nity between ATUS respondents who participated in every CPS interview and\nthose who were nonrespondents in a single CPS round. However, ATUS\nrespondents who failed to participate in two or more rounds of CPS provided\npoorer ATUS data--more round duration reports, missed diary activities, more\nitem nonresponse on ATUS labor-force questions, and fewer diary reports over-\nall--than those who always participated in CPS or those who were only\nFigure 5. Effects of Potential Common Cause Variables on the Relation-\nship Between the Number of ATUS Diary Reports and ATUS Nonre-\nsponse Propensity.\n7. All ATUS respondents participated in wave 8 of the CPS, by definition.\nnonrespondents in one round. Regression analyses run on the individual data-\nquality indicators revealed that only labor-force-item nonresponse and total\nreported activities were significantly related to CPS unit nonresponse. As be-\nfore, data quality and nonresponse covaried even after controlling for potential\ncommon causal variables.\nWe also examined whether there were differences in data quality between\nATUS respondents who were refusal conversions and those who were not. Ap-\nproximately 20 percent of ATUS sample members were flagged as a refusal at\nleast once during the fielding period, and about five percent of ATUS respond-\nents were successfully converted. Consistent with previous findings, figure 7\nshows that data quality was worse for refusal conversion cases than those that\nnever refused, though regression analyses revealed significant effects only for\nthe total number of diary activities and missing diary reports measures. Again,\ncontrolling for various potential shared explanatory variables did not elimi-\nnate the relationship between refusal conversion status and the data-quality\nindicators in ATUS.\nDiscussion\nThe purpose of this study was to explore the relationship between response\npropensity and survey data quality. There were three main findings from these\nanalyses. First, data quality decreased as the probability of nonresponse in-\ncreased. Second, the strength of this relationship varied by data-quality\nFigure 6. Relationship of ATUS Data-quality Indicators (in Standard\nDeviation Units) to the Amount of CPS Unit Nonresponse.\nNonresponse Propensity and Data Quality 951\nindicator and by survey. The effects were stronger in the CPS, where nonre-\nsponse propensity was most strongly and positively related to item nonresponse\nand round value reports on continuous variables (e.g., hours and earnings). In\nATUS, the relationship of nonresponse propensity to three of the four data-qual-\nity indicators had essentially no practical significance. There was, however,\na moderate, positive, and monotonic association between the total number\nof reported diary activities and the likelihood of nonresponse. Third, when data\nquality and nonresponse did covary, controlling for potential shared explana-\ntory variables related to busyness, social capital, and survey burden did not\nweaken the relationship. Data quality continued to decline as nonresponse pro-\npensity rose, though there were main effects for some of the potential common\ncausal variables.\nThere were several limitations in our approach to assessing the link between\nnonresponse propensity and data quality. Our study was hampered by our use of\nrelatively indirect indicators of social capital, busyness, survey burden, and data\nquality. In the absence of better frame and validation data, we relied on the\nindirect indicators that researchers tend to use in these types of analyses,\nand brought in additional measures that are less frequently incorporated\n(e.g., social-environmental variables in our propensity models, round value\nreports as a data-quality measure). In the end, the fit of our nonresponse pro-\nFigure 7. Relationship Between ATUS Refusal Conversion Status and\nATUS Data-quality Indicators (in Standard Deviation Units).\noutstanding, was comparable to or exceeded similar models used to examine\nMaitland, and Bianchi 2006). Given our reliance on indirect indicators, the fact\nthat we observed some clear connections between these variables and response\npropensities and reporting errors was encouraging (see, for example, tables 1\nStill, our approach to modeling response propensities and the unique design\nfeatures of both surveys may limit the generalizability of our results. We are in\nfact modeling conditional nonresponse or forms of attrition, since we used\nresponses from the first two CPS waves to predict nonresponse in subsequent\nwaves and because the ATUS sample units come from households rotating\nout of the CPS. Our results therefore may tell us little about predictors of\nnonresponse propensity for the initial CPS wave, or more generally for\ncross-sectional surveys and random-digit-dialing samples. That said, virtually\neveryone participates in the first CPS interview, and most stick with the survey\nfor all eight rounds.\nThese analyses have implications for surveys and survey organizations\nthat strive for the highest response rates possible. Often, extraordinary per-\nsuasive efforts are made to bring difficult-to-contact or reluctant sample\nmembers into the respondent pool. The assumption is that these efforts\nare compensated by reductions in the total mean square error of survey sta-\ntistics. Recent work by Curtin, Presser, and Singer (2005), Keeter et al.\n(2000), and others casts some doubt on this assumption, at least with respect\nto nonresponse error. The present analyses extend this work in two distinct\nways. On the one hand, it demonstrates that bringing in low-propensity\nrespondents may also introduce statistically significant increases in mea-\nsurement error. If nonresponse error is not significantly increased by exclud-\ning low-propensity cases (as the studies by Curtin, Presser, and Singer and\nKeeter et al. suggest) and these cases also are likely to add measurement\nerror (as we see here), then survey organizations may more comfortably\ndivert resources away from recruitment of difficult respondents and focus\ninstead on other error-reduction techniques. On the other hand, evidence\nof covariation between measurement error and nonresponse may call into\nquestion previous investigations of nonresponse bias. The results of this\nstudy suggest that significant measurement error in late/difficult cases\nmay in fact be concealing nonresponse bias undetected when examining\nrespondent means (Groves 2006). Such cases may be candidates for methods\nthat remove measurement error bias from the observations in order to assess\nnonresponse bias more accurately (see, e.g., Biemer 2001). If lower-propen-\nsity individuals also are more likely to produce noisy data (increasing the\nvariance of the statistic), then it becomes more difficult to detect whether\nthese individuals are different from higher-propensity respondents; that\nis, it becomes more difficult to determine the effects of excluding these indi-\nviduals on nonresponse bias.\nNonresponse Propensity and Data Quality 953\nReferences\nAbraham, Katherine G., Aaron Maitland, and Suzanne M. Bianchi. 2006. ``Nonresponse in the\nAmerican Time Use Survey: Who Is Missing from the Data and How Much Does It Matter?''\nBiemer, Paul P. 2001. ``Nonresponse Bias and Measurement Bias in a Comparison of Face-to-face\nBollinger, Christopher R., and Martin H. David. 2001. ``Estimation with Response Error and Non-\nresponse: Food Stamp Participation in the SIPP.'' Journal of Business and Economic Statistics\nCannell, Charles F., and Floyd J. Fowler. 1963. ``Comparison of a Self-enumerative Procedure and\na Personal Interview: A Validation Study.'' Public Opinion Quarterly 27(2):250\u00ad64.\nCurtin, Richard, Stanley Presser, and Eleanor Singer. 2000. ``The Effects of Response\nRate Changes on the Index of Consumer Sentiment.'' Public Opinion Quarterly 64(4)\n------. 2005. ``Changes in Telephone Survey Nonresponse over the Past Quarter Century.'' Public\nDixon, John, and N. Clyde Tucker. 2000. ``Modeling Household and Interviewer Nonresponse Rates\nfrom Household and Regional Characteristics.'' Paper presented at the International Workshop\non Household Survey Nonresponse, Budapest. Available at http://www.fcsm.gov/committees/\nihsng/buda5.pdf.\nFriedman, Ester, Nancy A. Clusen, and Michael Hartzell. 2003. ``Better Late? Characteristics of Late\nRespondents to a Health Care Survey.'' Proceedings of the American Statistical Association, Survey\nResearch Methods Section [CD-ROM]. Alexandria, VA: American Statistical Association.\nGroves, Robert M. 2006. ``Nonresponse Rates and Nonresponse Bias in Household Surveys.''\nGroves, Robert M., and Mick P. Couper. 1998. Nonresponse in Household Interview Surveys. New\nYork: John Wiley.\nGroves, Robert M., Stanley Presser, and Sarah Dipko. 2004. ``The Role of Topic Interest in Survey\nParticipation Decisions.'' Public Opinion Quarterly 68(1):2\u00ad31.\nKalsbeek, William D., Steven L. Botman, James T. Massey, and Pao-Wen Liu. 1994. ``Cost-\nefficiency and the Number of Allowable Call Attempts in the National Health Interview Survey.''\nKeeter, Scott, Andrew Kohut, Carolyn Miller, Robert Groves, and Stanley Presser. 2000. ``Con-\nsequences of Reducing Nonresponse in a Large National Telephone Survey.'' Public Opinion\nMerkle, Daniel, and Murray Edelman. 2002. ``Nonresponse in Exit Polls: A Comprehensive Anal-\nysis.'' In Survey Nonresponse, ed. R. Groves, D. Dillman, J. Eltinge, and R. Little. New York:\nOlson, Kristen M. 2006. ``Survey Participation, Nonresponse Bias, Measurement Error Bias, and\nTourangeau, Roger, Robert M Groves, and Cleo D. Redline. 2010. ``Sensitive Topics and Reluctant\nRespondents: Demonstrating a Link Between Nonresponse Bias and Measurement Error.'' Public\nTriplett, Timothy, Johnny Blair, Teresa Hamilton, and Yun Chiao Kang. 1996. ``Initial Cooperators\nvs. Converted Refusers: Are There Response Behavior Differences?'' Proceedings of the Amer-\nican Statistical Association, Survey Research Methods Section [CD-ROM]. Alexandria, VA:\nU.S. Census Bureau and Bureau of Labor Statistics. 2002. Current Population Survey: Design and\nMethodology. Technical Paper 63RV: Washington, DC.\nVoigt, Lynda, Denise M. Boudreau, Noel S. Weiss, Kathleen E. Malone, Christopher I. Li, and Janet\nR. Daling. 2005. ``Letter to the Editor: RE: \u00d4Studies with Low Response Proportions May Be Less\nBiased Than Studies with High Response Proportions.\u00d5'' American Journal of Epidemiology.\nWillimack, Diane K., Howard Schuman, and James M. Lepkowski. 1995. ``Effects of a Prepaid\nNonmonetary Incentive on Response Rates and Response Quality in a Face-to-face Survey.''\nYan, Ting, Roger Tourangeau, and Zac Arens. 2004. ``When Less Is More: Are Reluctant Respond-\nents Poor Reporters?'' Proceedings of the American Statistical Association, Survey Research\nMethods Section [CD-ROM]. Alexandria, VA: American Statistical Association: 4632\u00ad51.\nNonresponse Propensity and Data Quality 955",
    "reduced_content": ""
}