{
    "abstract": "Abstract\nIn addition to the use of grade point average and academic background to assess candidates for admission into professional\ngraduate programs, many university programs today use structured interviews to further assess candidates' suitability. The\nMaster of Occupational Therapy program at the University of Manitoba has in recent years adopted a standardized interview\ndesigned to capture specific psychometric characteristics of applicants considered relevant for scholarship in Occupational\nTherapy professional program. This study applied the Rasch Analysis Model to test the reliability and validity of the structured\ninterview to determine whether the tool is invariant and fits the Rasch probabilistic model. A three-cohort interview data\nfrom 258 applicants were analyzed. The result indicates that the tool has high reliability (person separation index [PSI] =\n0.8715), and was invariant across the participants. This Rasch analysis result supports the use of structured interview as an\nadditional tool for students' admission.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nIntroduction\nThe process of selecting students into health professional\neducation programs has become increasingly competitive.\nThe admission committees of these programs have to con-\ntinuously re-examine their selection criteria and procedures\nto ensure that the best qualified candidates are selected\nTraditionally, admission committees for graduate programs\nselect students based on their grade point average (GPA)\nobtained at the undergraduate or prior entry degree/certifi-\ncate program. However, most selection processes for graduate-\nlevel programs today consider a wider range of criteria,\nwhich include entry GPA, a brief written statement, profes-\nsional reference letters, and an individual interview (Dowell,\nLynch, Till, Kumwenda, & Husbands, 2012; Roberts et al.,\n2014). However, with the increasing number of qualified\napplicants and limited number of spaces in programs, the\ntask has become more challenging (Salvatori, 2001; Timer &\nIn the health profession, there are many stakeholder groups\nwith diverse expectations for health care graduates. For\ninstance, regulatory and professional bodies have a set of com-\npetencies they expect graduates entering into the profession to\npossess. Program graduates have to sit for professional exami-\nnations to further assess their competencies before they can be\nlicensed to practice. Also, governments and society at large\nexpect certain levels of professionalism and skills from health\nprofession graduates. In the context of these diverse expecta-\ntions, admission committees have to ensure that they select\nstudents who do not only have high GPAs but are most likely\nto succeed in the program and become successful clinicians\nClauson, 2011). Hence, the selection criteria are designed to\nreflect expectations of a wide range of stakeholders, which\nincludes the university, employers, professional bodies, gov-\nernments, and society at large (Puddey & Mercer, 2014).\nHealth profession programs strive to ensure that students\nadmitted into the programs have the potential to graduate with\nhigh GPA, and demonstrate the competencies for effective\nclinical practice as established by their respective professional\n1University of Manitoba, Winnipeg, Canada\nCorresponding Author:\nNelson Ositadimma Oranye, Department of Occupational Therapy,\nUniversity of Manitoba, 106-771 McDermot Avenue, Winnipeg, Manitoba,\nEmail: Nelson.oranye@umanitoba.ca\nThe Validity of Standardized Interviews\nUsed for University Admission Into\nHealth Professional Programs: A Rasch\nAnalysis\nNelson Ositadimma Oranye1\n Keywords\nstandardized interview tools, school admission, professional education programs, Rasch model, reliability, differential item\nfunctioning\n2 SAGE Open\nexpectations have necessitated the emergence of stringent\nadmission procedures intended to select the best qualified\ncandidates.\nThe GPA is probably the most widely used admission cri-\nteria, and there are several reasons for this. The most obvious\nreason is the predictive value of the GPA for post admission\nperformance. A study by Wilkinson et al. (2008) found the\nGPA, admission tests, and interview scores to be modest pre-\ndictors of performance in graduate medical school courses,\nwith GPA being the strongest predictor followed by inter-\nview score and admission test results. Other studies\nthat, although the GPA scores were stronger predictors of\nacademic performance, the relative predictive strength\ndiminishes as a course progresses. Similarly, Lamadrid-\nFigueroa, Castillo-Castillo, Fritz-Hern\u00e1ndez, and Maga\u00f1a-\nValladares (2012) have reported that general admissions\ncriteria (GPA, interview score, and letters of recommenda-\ntion) were strong predictors of academic grades but not of\ngraduation. Some studies, such as Puddey and Mercer\n(2014), have shown interview and admission test scores to be\nrelatively weak as predictors of a student's academic perfor-\nmance. However, most studies (Eva, Rosenfeld, Reiter, &\nshown that previous academic performance, primarily mea-\nsured in terms of the GPA, remains a consistent predictor of\nacademic success.\nIn an effort to assess valued personal characteristics of\ncandidates into graduate programs, Lamadrid-Figueroa et al.\n(2012) have noted that individual interviews, unstructured,\nand non-standardized interview tools are commonly being\nused today, in addition to entry GPA. However, some pro-\ngrams have made efforts to develop standardized interview\ntools that are robust and relevant. For instance, the McMaster\nUniversity developed the multiple mini-interview (MMI),\nwhich measures the non-cognitive attributes of candidates,\nand is widely used in medical schools (Eva et al., 2004;\nRoberts, Rothnie, Zoanetti, & Crossley, 2010). Several stud-\nies have indicated that the MMI is reliable (Peeters, Serres,\n(Roberts, Zoanetti, & Rothnie, 2009), and has high accept-\nAlthough some of the criteria may be objective, scholars\nhave raised concerns about some level of subjectivity in the\nMMI process, especially with respect to examiner judgment,\nerror, and bias, despite the level of training and experience\nthe examiner may possess (Jones & Forister, 2011; Lamadrid-\nFigueroa et al., 2012). The difference in evaluation style\namong groups of examiners has equally been identified as a\nfactor that could influence student selection processes\nExisting evidence shows that different schools and pro-\ngrams have adopted an admission model that combines some\nof these tools as a strategy for selecting the best candidates\nfor their graduate-level programs; however, there is insuffi-\ncient evidence on the predictive validity of these admission\nextent of objectivity embedded in these processes (Lamadrid-\nFigueroa et al., 2012). Also, several studies have looked at\nthe standardization of the admission process among graduate-\nlevel programs, but much has yet to be explored in terms of\nthe reliability and validity of constructs used to measure the\ncompetencies or the components of standardized admission\ninterview questionnaires (Dowell et al., 2012; Jones &\nobserved the lack of research evidence to support the criteria\nused in creating scales to assess admission candidates and\nwhether such criteria, if met, do in fact predict success in\nacademic and clinical practice.\nThe Rasch analysis model is \"a means of achieving con-\njoint measurement for non-physical attributes by examining\ncandidates' knowledge using tests\" (Peeters & Stone, 2009,\np. 210). The Rasch model relates person's ability to one or\nmore parameters, including item difficulty (Hissbach,\nity of an individual attaining a correct response (Peeters &\nthe relationship that exists between person ability and item\ndifficulty is able to capture the relationship of the item and\nperson (Coe, 2008; Wilmot, Schoenfeld, Wilson, Champney,\n& Zahner, 2011). The Rasch model is able to provide a rigor-\nous and detailed methodology to identify the psychometric\nproperties of an instrument at the item level (Pomeranz,\nByers, Moorhouse, Velozo, & Spitznagel, 2008; Tran et al.,\n2010). It has proven to elevate test design to a level of sophis-\ntication not otherwise possible when using only raw scores to\ndetermine an individual's ability (McAllister, Lincoln,\nFerguson, & McAllister, 2010; Potgieter, Davidowitz, &\nVenter, 2008). The Rasch model is able to provide details of\nthe validity and reliability of instruments by focusing spe-\ncifically on rating scales, items, persons, and other facets like\nContext of the Current Study\nTo ensure equal opportunity to applicants selected into the\nMaster of Occupational Therapy (MOT) program at the\nUniversity of Manitoba, the Admissions Committee has\ndeveloped a set of selection tools, which include prior GPA\nand a standardized individual interview. Each applicant is\ninterviewed by three different people (a student, a clinician,\nand a faculty member). Every interviewer attends an orienta-\ntion, and receives training specific to the admissions inter-\nview process and content. Each interviewee is scored on five\ninterview domains, communication skills, and overall suit-\nability. To control for bias, each domain is scored individu-\nally and independently. Also, the interviewers were not\nallowed to discuss their interviews with each other. An aver-\nage interview score is calculated based on the three\ninterviews for each applicant. The Admission Committee\nthen considers the total scores obtained from the three-\nstation standardized interviews and the candidates' average\nGPA from the last 60 credit hours of study prior to entering\nthe program.\nThe purpose of this study is to conduct an analysis of the\nadequacy of the student selection criteria into the MOT pro-\ngram at the University of Manitoba using the Rasch analysis\nmodel. The specific objectives of this research are to deter-\nmine whether Differential Item Functioning (DIF) exists in\nthe student selection criteria, and if the interview scale is reli-\nable. Previous studies, such as Puddey and Mercer (2014)\nhave explored DIF and its role in identifying variables out-\nside the measured domain that can affect the result of the\nmeasurement. The ability of test results to be unaffected by\nfactors outside the primary trait being measured, such as\ngender, age, and socioeconomic background, is crucial in\nproviding fair and ethical evaluations (Blackman &\nindividuals selected were the most qualified based on the set\nof objective criteria.\nMethod\nThis is a retrospective cohort study designed to examine the\nadequacy of the interview method in selecting students into\nthe MOT program. A set of anonymized data were obtained\nfrom the Department, with the approval of the Admissions\nCommittee of the Occupational Therapy Program at\nUniversity of Manitoba. All components of the study were\nreviewed and approved by the Health Research Ethics Board\n(HREB) of the University of Manitoba, as conforming to\ninternational ethical standards for research involving human\nparticipants.\nParticipants and Materials\nThis study used data from 258 candidates who were inter-\nviewed for selection into MOT Program within a 3-year\ninclude anonymized applicant records. The analytical data\nset included the year the interview was conducted, appli-\ncants' interview scores, entry GPA, age, gender, application\ncategory (whether from Manitoba, other Canadians,\nAboriginal, or International), type of entry degree, and the\nuniversity attended. For the Rasch analysis, the variables on\ninterview year, GPA, age, gender, and interviewer (clinician,\nfaculty, and student) were used. The interview data contained\na set of 21 items, derived from three-stage interviews. Each\ninterview session lasted for 20 minutes and assessed seven\nareas, with the first five questions focusing on some indi-\nvidual characteristics around knowledge, attitudes, personal-\nity traits, relevant experiences, and abilities considered\nimportant for a successful participation and graduation from\nthe program. The sixth question evaluates communication\nskills, while the last question provides the opportunity for the\ninterviewer to assess the overall suitability of the candidate\nfor the profession. The specific questions are protected\nbecause they are used annually for selecting candidates into\nthe program. As such, it is not possible to provide sample\nquestions in this article.\nProcedures\nThe research team negotiated access to the admissions data\nwith the Department of Occupational Therapy. The data were\nanonymized by an authorized support staff, who removed the\napplicants' personal information such as names, candidate\nID number, address, telephone number, email address, and\npostal codes to avoid the identification of individual partici-\npants. The participants' dates of birth were converted to age\nat time of interview, which was then categorized into age\ngroups to facilitate comparison across age. Each candidate\nwas assigned a unique study identifier number.\nData Analysis\nlyze the data. The Rasch analysis was performed using the\nselected parameters, to determine the presence of DIF, reli-\nability, unidimensionality of the tests, and fit to the model.\nThe SPSS 22 provided additional statistical tools for regres-\nsion, ANOVA, and multiple comparisons.\nResults\nDescriptive Statistics\nThe majority of participants were female (89.9%).\nParticipants in the interview were from diverse disciplines,\nspanning from Arts to science degrees. The Table 1 shows\ninterview mean score across interviewer level and by other\ncharacteristics such as gender, type of degree, and GPA\nlevel. The interview mean score varied by interviewer and\nacross the interview sessions. The participants' GPA, based\non their last 60 credit hours, ranged from 3.3 to 4.4, with an\nlevel and small amount of random error indicate that the\nmajority of the candidates had a high academic perfor-\nmance. The GPA was categorized into four, with \"Below\nas High, and \"3.96 and above\" as Very High GPA. Table 1\nshows that the mean of participants' interview score\nincreased as their GPA increased, from Lower GPA to Very\nHigh GPA. The positive trend in the relationship between\ninterview scores and GPA is an important indicator that the\ninterview is a good measure of participants' ability, and as\nsuch a good screening tool for identifying participants'char-\nacteristics relevant to academic and clinical performance in\nthe occupational therapy program.\n4 SAGE Open\nThe person-item distribution in Figure 1 reveals a lack of\nconvergence between the scale items and person scores.\nAlthough the scale measured several traits located more to\nthe left, majority of the participants' scores were located\nmore to the right, which implies that the persons' mean abil-\nity level was higher than the scale mean. However, the graph\nshows that the scale has wide targeting, which means that it\ncovered a wide range of the candidates'ability level. There is\na high risk of ceiling effect in this scale, with the person loca-\nReliability and Validity\nReliability of the interview tool was measured using the\nCronbach's alpha and person separation index (PSI)\nprovided by the Rasch analysis. The PSI = 0.8715 and\nCronbach's alpha = .8790 demonstrate that the interview\ntool has a high reliability. The face validity of the tool was\ndetermined through interviewers' feedbacks and reviews\nthat have occurred over the years. In this study, the validity\nwas further determined by the significant statistical correla-\ntion between the interview scores and candidates' prior\nGPA. Academic GPA is a well-established objective mea-\nsure of a student's ability (Lamadrid-Figueroa et al., 2012;\nWilkinson et al., 2008). The Pearson's correlation between\ntotal interview score and prior GPA was significant, r =\n.214, p = .001. The relationships and the interaction between\nlevels of GPA and the interview scores were further\nexplored using linear regression and ANOVA, with post\nhoc comparative analysis.\nTable 1. Descriptive Statistics for Some Participants Characteristics.\nLevel\nInterview 1 Interview 2 Interview 3\nn M SD n M SD n M SD\nNote. GPA = grade point average.\naCombined interview mean.\nFigure 1. Distribution of person-item threshold.\nRegression Analysis\nA regression analysis was performed to determine which of\nthe factors were significant in predicting the outcome variable,\nwhich is the interview score. The R2 indicates that the linear\nregression is able to explain 5.6% of the variation in the inter-\nview performance. Table 2 provides information on the regres-\nsion coefficients of the predictor variables. Based on the\nregression table, the only factor that significantly explains the\noutcome variable is the GPA,  = .217, p = .001. This statistic\ncorroborates the Rasch analysis result that the GPA is an\nimportant determinant of candidates' interview performance,\nwith interview scores increasing as the GPA increased.\nThe result of a one-way ANOVA shows a significant dif-\nference in interview scores across the GPA, F = 4.235, p =\n.006, supporting the evidence from Table 1 that interview\nscores increased with higher GPA levels. In Table 3, the mul-\ntiple comparison result indicates that candidates with Lower\nGPA (below 3.66) were significantly different from other\ncandidates whose GPA were higher. Although the average\ninterview score increased with GPA (see Table 1), there was\nno significant difference between groups whose GPA were\n3.66 or above. Also, there was no significant difference\nbetween interview scores by year of the interview (2012,\nTable 2. Regression Analysis of Predictors of Interview Performance.\nPredictor\nvariables\nUnstandardized\ncoefficients\nStandardized\ncoefficients\nt Significance\nB SE  Lower bound Upper bound\nApplication\ncategory\nType of entry\ndegree\nuniversity\ncategories\nNote. Application category refers to whether candidate was from Manitoba, Other Canadians, Aboriginal, or International. r = .214, p = .001.\nCI = confidence interval; GPA = grade point average.\nDependent variable: Total score from all the three interviews.\nTable 3. Multiple Comparisons by Fisher's Least Significant Difference (LSD) Method.\nDependent\nvariable (I) GPA (J) GPA\nMean difference\n(I - J) SE Significance\nLower bound Upper bound\nTotal score from\nInterview 1\nTotal score from\nInterview 2\nTotal score from\nInterview 3\nTotal score from\nall interviews\nGPA = grade point average.\naThe mean difference is significant at the .05 level.\n6 SAGE Open\nstrongly reinforces the association between interview score\nand entry GPA, and the fact that candidates whose GPA were\nbelow 3.66 scored significantly lower in the interviews than\nthose with a higher GPA. This association between interview\nscores and GPA attests to the validity of the interview tool.\nAnalysis of Fit\nThe Rasch result indicates an excellent power of analysis of\nHowever, the overall model fit analysis using the mean and\nchi-square statistic indicates some problems with the model\nfit. First, the Rasch analysis of person-item fit statistics indi-\ncates that, on average, person scores are located above the\nitem mean (Item mean = 0.0, Persons mean = 1.17). In Rasch\nanalysis, the chi-square statistic is generally used to evaluate\nwhether the hierarchical ordering of items is consistent\nacross increasing levels of the person traits. The item\u00adtrait\ninteraction based on 19 items, after two items with extreme\n.0004, which suggests a significant difference between the\nmeasure and the Rasch model expectation. Also, the root\nmean square error approximation (RMSEA) = 0.069 sup-\nports the evidence from the chi-square statistic of a misfit to\nthe Rasch model. This misfit is most likely due to the pres-\nence of several items on the scale located on the extreme left,\nmeasuring traits at levels below the ability levels of the can-\ndidates. Depending on the purpose of the measure, in this\ncase primarily screening, it may be better to eliminate these\nitems from the scale.\nAn item by item analysis shows that 19 of the items in the\ninterview had appropriate fit residual, except for Item 16\nwith an extreme fit residual of 2.965. The mean residual for\nthe items and the persons was located around zero, and the\nTable 4. Summary of Fit Statistics.\nItem Person\n Location Fit residual Location Fit residual\nNote. PSI = person separation index.\naCorrelation between location and standardized residual.\nTable 5. Analysis of Individual Item Fit.\nItem Location SE Fit residual df 2 df p\nstandard deviations were approximately within 1.18 (see\nTable 4). This suggests that both the items and persons are\nwithin acceptable range of the Rasch model expectation. A\nfurther examination of each item and individual statistics\nindicated that although Item 13 has an acceptable fit residual\nof -2.261, it has a significantly high variation compared with\nthe final Rasch model. Table 5 shows the fit statistics for the\n19 items included in the final Rasch analysis. This improved\nUnidimensionality Test\nThe interview scale was originally designed as a multidi-\nmensional test to measure different aspects of a candidate's\nability that were relevant to academic and professional prac-\ntice in occupational therapy. However, there has not been any\nformal testing of this characteristic of the tool. The Rasch\nanalysis test of dimensionality was performed, using the t\ntest. The result shows that 35.3% of the estimates were sig-\nnificantly different, thereby confirming that the scale was not\nunidimensional. This is acceptable, as the interview items\nwere designed to measure different dimensions of partici-\npants' characteristics considered important for admission\ninto the MOT program.\nDIF Analysis\nThe data were also analyzed to determine whether the tool\nwas invariant, in other words, if there was any evidence of\nDIF for any of the person factors. The results show there was\nno evidence of DIF for all the person factors considered in\nthe analysis. However, Item 13 consistently showed signifi-\ncant DIF by class interval for year of interview, gender, GPA,\nand degree. As noted above, this item was removed from the\nfinal model. The lack of a uniform DIF in the tool suggests\nthat it is unbiased and as such provides all participants with a\nfair assessment of their abilities.\nThe Rasch's ANOVA shows no significant difference in\nthe participants' interview performance with respect to all\nthe person characteristics, except for the GPA (see Table 6).\nThis suggests that the only factor related to participants'\ninterview score was academic ability measured by their prior\nGPA. This was consistent with all the other statistical results,\nand further confirms lack of bias in the interview tool. Hence,\nthe interview process complemented the GPA as an objective\nmeasure of candidates' ability.\nDiscussion\nTypically, admissions committees of graduate-level pro-\ngrams select relatively high-performing graduates who have\nachieved a specified GPA during their undergraduate or prior\nentry studies (Puddey & Mercer, 2014). The admissions pro-\ncess into these programs is highly competitive, with more\napplicants than available spaces (Roberts et al., 2014; Timer\n& Clauson, 2011). Existing evidence shows that the task of\nselecting students with relevant knowledge and attitudes\nfrom a large pool of applicants has been very difficult for\nadmission committees of health profession education pro-\nearlier, there are many stakeholder groups with diverse\nexpectations for health care graduates. The admission com-\nmittees have to ensure that they select students who are most\nlikely to meet the academic and clinical expectations of the\ncapture the diverse applicant characteristics considered\nappropriate for students going into these professional pro-\ngrams, many admission committees have added standardized\ninterviews into their admission selection tool kit. The reli-\nability of some of these admission tools remains unknown.\nThis study shows that the interview scale used for admission\ninto the MOT program at University of Manitoba is reliable\nand valid. The reliability of the MOT interview tool, PSI =\n0.87 and Cronbach's alpha = .88 is quite high. The high reli-\nability index of the MOT interview tool points to its high\ninternal consistency, and there is also the evidence that the\nscale adequately measures those candidates'characteristics it\nis intended to measure.\nPrevious studies have shown that students' GPA obtained\nat the undergraduate or prior entry degree/certificate pro-\ngrams is a very important and reliable measure of their abil-\nbeen used as a standard criteria for evaluating other measures\nof knowledge. In this study, the MOT interview scale scores\nhave a positive linear correlation with prior GPA, a criterion\nthat points to its validity; candidates with high GPA per-\nformed better than others, in knowledge domains and charac-\nteristics for admission into the professional program.\nApart from the reliability and validity of the interview tool,\nthis study found the interview tool to be invariant across can-\ndidates. In other words, there was no evidence of DIF for any\nof the person characteristics. The lack of DIF in the tool shows\nthat it is unbiased and as such provided all participants with a\nfair assessment of their abilities. The studies by Blackman and\nDIF in the context of gender neutrality in the admission pro-\ncess. In both studies, females were found to have a slightly\nTable 6. One-Way ANOVA for the GPA Person Factor.\nSource SS df MS F-statistic p\nNote. GPA = grade point average; SS = Sum of squares; MS = Mean\nsquares.\n8 SAGE Open\nhigher chance of success on various items. In this study, the\nmale interviewees had a higher interview mean score (Table\n1), but it is possible that the higher standard deviation and\nsmaller sample size for the male could have biased this differ-\nence. Also, those holding an arts degree scored lower on aver-\nage than science or other degrees. The DIF analysis using\nRasch provides an analysis for inter-rater reliability at an item-\nby-item level. Contrary to previous studies where significant\nvariation was reported for examiner factor (Roberts et al.,\ndifference in interviewers' (faculty, clinician, student) rating.\nThis lack of difference can be attributed to the stringent train-\ning provided to all the interviewers. The interviewers receive a\n1-day training and simulated scoring session, using an inter-\nview video. Individual scores are discussed in small groups,\nfollowed by a large group feedback session. It is very likely\nthat variation in interviewers'scores was reduced by the train-\ning and standardized implementation procedure.\nBesides the reliability and validity of the interview scale,\nthis study identified some measurement issues which would\nrequire some attention to further improve the interview tool.\nThere was an evidence of ceiling effect in the scale, which is\nreflected by many of the items being located on the left side\nof the scale. Ceiling effect is used to describe the level above\nwhich variations within the population traits of interest are\nno longer being captured, as the scale does not have ques-\ntions that estimate such traits. This can affect the ability of\nthe scale to differentiate and discriminate between individu-\nals with higher ability levels and others. On this scale, the\nperson mean location of 1.17 is much higher than the item\nmean location of 0.00, suggesting a possible ceiling effect.\nThis ceiling effect has not, to our knowledge, been addressed\nin many previous studies. It is therefore an important area for\nfurther research, to determine whether scale items used in\nadmissions provide adequate challenge and truly capture\nhigh-ability levels.\nOne of the considerations for the adequacy of a scale is\nthe targeting of the population traits\nThe alignment of the PSI with the Cronbach's alpha, sug-\ngests that the scale has a wide targeting of the participants'\nability level. However, the information on Figure 1 and other\nstatistics indicate that several of the items on the scale mea-\nsured knowledge below the average ability level of the partici-\npants. In other words, the result suggests that most of the\ncandidates had knowledge or ability levels, which were higher\nthan what some of the interview questions were measuring.\nHowever, this might be a good characteristic for a screening\ntool such as this, to broadly target population traits that are\nconsidered relevant. However, in this scenario where most of\nthe candidates appeared to be knowledgeable and very well\nprepared, the items located on the left of the scale may not\ncontribute much to the measurement, as they are not able to\ndiscriminate between high- and low-performing candidates. It\ncould be argued that such interview items amount to a waste of\ntime for the interviewers and candidates.\nTypically, in an educational test similar but not necessar-\nily equivalent to this, it would be a waste of time to adminis-\nter a test to students, which was far too easy than their ability\nlevel. There are a number of possible explanations that may\nbe the case in this measure. One is that to qualify for the\ninterview, applicants would have scored a very high GPA\nbased on the last 60 credit hours of their pre-admission\ncourses. Also, some of the candidates may have participated\nin the interview in the past or learnt from their friends and\nwould have been better prepared for the interview. There was\nanecdotal evidence that some of the candidates preparing for\nadmission into the program had volunteered or worked in\nenvironments that prepared them for occupational therapy or\nmedical rehabilitation roles. The interviewees are usually a\npool of high-ability candidates. These facts will have some\nimplications for interpreting the result and power of fit to the\nRasch model.\nIt is also important to restate that one of the key objectives\nof developing interview scales for admission is to provide\nadditional screening tools that would help university pro-\ngrams and their admission committees to make the best deci-\nsions, while screening the large number of well-qualified\ncandidates into their program. Primarily, such interview tools\nincorporate measures targeting skills, competences, personal-\nity traits, and other capabilities, which the committee consid-\ners essential for successful completion of the program and\nfuture practice in the profession. Many of these criteria are\nbased on stakeholders'expectations and feedback. In Canada,\nmost professional university programs conduct regular stake-\nholder reviews of their programs to identify what future\nemployers, the clinical community, the professional associa-\ntions, regulatory body, the government, and the general pub-\nlic desire to see in graduates from their programs.\nConclusion\nThe result of this Rasch analysis indicates that the use of\ninterview tools in the student selection process by the MOT\nprogram is objective, especially in consideration of the asso-\nciation between interview scores and candidates' entry GPA.\nThis additional screening tool seems fair, and provides addi-\ntional means to ensure that candidates meet the program\nunique requirements.\nThe Rasch analysis reveals that participants' ability level\nwas above the item mean, which indicates that the current\nscale may not fully capture the full ability of the interview\ncandidates. The extent of discrimination of participants'abil-\nity therefore needs to be further investigated. However, as\nthe interview process provides an additional level of screen-\ning for admission candidates, the targeting of all levels of\nability may not be very crucial. However, this may not fully\ndiscriminate candidates' ability levels, due to the potential\nceiling effect. It may be necessary to further review the tool\nto identify and modify or drop those items that lie on the\nextreme left of the measure, which do not necessarily\ndifferentiate between applicants'ability levels. This will help\nthe admissions committee to more accurately differentiate\napplicants' ability and further select the most competitive\ncandidates. A contrary argument will be that those \"easy\"\nitems provide an introductory and more relaxed level into a\nthree-stage interview, helping candidates to overcome their\ninitial anxiety. This will be valid, if the items are sequentially\nordered to achieve this.\nIt is important that admission interview questions incor-\nporate important psychometric measures on knowledge, atti-\ntudes, personality traits, educational achievement, relevant\nexperiences and abilities, because professional and licensing\nbodies commonly consider these characteristics.\nLimitations\nRasch analysis is a unidimensional measure, so the use of data\nfrom a multidimensional tool, as in this study, may have impli-\ncations for the model fit interpretation. The findings of this\nstudy are based on the interview tool used by the MOT pro-\ngram, so the comparability with other graduate-level admis-\nsion interview tools may not be quite correct, as some of the\nprograms have different criteria and procedures. Finally,\nalthough the sample size of 258 is relatively large, the average\nnumber of participants in the three cohorts was 86. A much\nlarger total sample size would give a stronger analysis power.\n"
}