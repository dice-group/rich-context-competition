{
    "abstract": "Abstract\nThe ease and affordability of Amazon's Mechanical Turk make it ripe for longitudinal, or panel, study designs in social science\nresearch. But the discipline has not yet investigated how incentives in this \"online marketplace for work\" may influence\nunit non-response over time. This study tests classic economic theory against social exchange theory and finds that despite\nMechanical Turk's transactional nature, expectations of reciprocity and social contracts are important determinants of\nparticipating in a study's Part 2. Implications for future research are discussed.\n",
    "reduced_content": "Creative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons\nAttribution-NonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use,\nreproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open\nAccess pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nMethodological Innovations\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\nmio.sagepub.com\nA growing number of social psychological studies are reli-\nant on Amazon's Mechanical Turk (MTurk) worker pool to\ninvestigate human attitudes and behavior, returning tens of\nthousands of studies of Google Scholar search results. A\nwealth of cross-sectional scholarship investigating social\nscience concepts, like framing (Berinsky et al., 2012), cul-\ntivation (Stravrositu, 2014), and credibility (Appelman and\nSundar, 2016) have been tested using MTurk samples,\nmany of which argue their work would benefit from longi-\ntudinal designs. And while MTurk is an apt platform for\nconducting panel studies, it remains unclear how token\nresearch incentives operate in this commercial, transac-\ntional marketplace. This study employs a methodological\nexperiment to compare unit response rates across classic\ncost-benefit incentive strategies and social exchange incen-\ntive strategies and discusses how to best maximize partici-\npation in longitudinal designs.\nThe nature of MTurk samples\nLike all volunteer convenience samples, the use of MTurk\nworkers in social science has been greeted with mixed\nreviews. These participants may deviate from the general\npopulation in important ways, and studies comparing them to\nboth representative adult samples and student participant\npools have produced cautious results. Although Amazon\nlaboratories estimate that populations of only about 7300\nworkers are available at any given time (Stewart et al., 2015).\nAnd many of these are super workers, who disproportionally\nparticipate in social science research studies (Rand et al.,\n2010). These subjects are susceptible to completing numer-\nous surveys and experiments that share conceptually related\nlearning more about the social scientific process than many\nresearchers would like (Hauser and Schwarz, 2016).\nCompared to in-person samples, they also have been shown\nto pay less attention to experimental stimuli and are signifi-\ncantly less extraverted (Clifford et al., 2015; Goodman et al.,\nHowever, others have found MTurk workers to mirror\nstudent-sample limitations in terms of data quality (Sprouse,\n2011) while providing demographic diversity more compa-\nrable to the US population at large (Berinsky et al., 2012).\nMany recent studies recommend MTurk workers for scholar-\nship that seeks to understand relationships between traits,\nPlease participate in Part 2:\nMaximizing response rates in\nlongitudinal MTurk designs\nElizabeth Stoycheff\n Keywords\nResponse rate, longitudinal, panel, Mechanical Turk, surveys, experiments\nDepartment of Communication, Wayne State University, Detroit, MI,\nUSA\nCorresponding author:\nElizabeth Stoycheff, Department of Communication, Wayne State\nEmail: elizabeth.stoycheff@wayne.edu\nDiscussion Piece\n2 Methodological Innovations\nattitudes, and behaviors based on results that show virtually\nindistinguishable differences across various sample types\nGlick, 2013) and other indicators of high data quality, includ-\ning comparable test-retest reliability (Holden et al., 2013;\n2014). The ease and affordability of MTurk samples have\nmade them common practice for a variety of methodologies\nin social research, and studies that rely upon them are regu-\nlarly published in leading journals across the social sciences\n(for a review, see Chandler and Shapiro, 2016).\nMaximizing longitudinal responses\nMore than just a one-stop shop, the Mechanical Turk portal\nis also appropriately suited for online research that employs\nlongitudinal, or panel, designs (Chandler et al., 2014;\nChristenson and Glick, 2013). And needing to re-contact a\nparticipant after days, weeks, months, or even years intro-\nduces MTurk to the same recruitment challenges as other\nmodes of study administration. Although there are a myriad\nof factors that may contribute to unit response rate, including\nthe study topic, presentation of online material, design of\ninvitations, perceived credibility of the researcher, and use of\nincentive strategies are likely to occupy a central role in\nMTurk's transactional \"market place for work\" to re-attract\nsubjects to participate in subsequent survey or experimental\nsessions. Two competing approaches--classic economic\ntheory and social exchange theory--both provide insight\ninto how incentives may help maximize unit response.\nClassic economic theory contends that human behavior\nis largely predictable because individuals are rational in\ntheir decision-making (Homans, 1961). They weigh poten-\ntial economic benefits against potential economic costs,\nand if there is a high likelihood of receiving adequate ben-\nefits for minimal costs, social action is expected (Scott,\nments found that greater incentives increased response\nrates in all but two studies (Fox et al., 1988). And an even\nlarger meta-analysis spanning 80\nyears worth of web and\nmail surveys revealed a similar positive association\n(Auspurg and Schneck, 2014). MTurk, which operates on a\ncash payout system, provides added leverage for subjects\nwho view more perceived benefit from cash than other\nforms of compensation (Ryu et al., 2006). Thus, classic\neconomic theory would naturally predict that\nH1. MTurk workers who are offered a larger incentive are\nmore likely to participate in a study's Part 2 than MTurk\nworkers who are offered a smaller incentive.\nBut ethically, institutional review boards insist survey and\nexperimental subjects should not be \"paid\" for their time, in\norder to reduce the likelihood of coercion and undue\ninfluence, but they are rather offered a token incentive for\nparticipation (Wrench et al., 2013). On its surface, MTurk's\npreponderance of low-paying tasks suggests workers may\nhave motivations beyond merely maximizing their financial\nbenefits. An MTurker's average compensation was only\nranked fourth on reasons why workers report participating,\nbehind interest, enjoyment, and boredom (Buhrmester et al.,\nSocial exchange theory lends a more nuanced explana-\ntion for why subjects may choose to continue their partici-\npation across multiple study sessions. Rather than operating\non an economic cost-benefit calculation, social exchange\ntheory argues that individuals also view social outcomes,\nlike approval, trust, and reciprocity, as benefits to partici-\nparticipate in a longitudinal study establishes a social con-\ntract between the subject and the researcher, whereby both\nparties are expected to fulfill the expectations set forth in\nthe first interaction. Other modes of study administration\nhave used prepaid incentives to build a relationship of trust\n(e.g. Singer et al., 2000), but MTurk does not allow for pre-\npayment. Instead, the gesture of goodwill routinely\nemployed via this channel is the use of bonus payments.\nResearchers can grant bonuses to workers who perform\noutstanding or additional work. Bonuses for Part 2 of a\nstudy should signal to the participant that it's a continuation\nof an established social contract, as opposed to a new inter-\naction. As such, social exchange theory would predict that\nH2. MTurk workers who are offered a bonus incentive are\nmore likely to participate in a study's Part 2 than MTurk\nworkers who are offered a new task for Part 2.\nMethods\nTo determine the most effective incentive strategy, partici-\npants were recruited via MTurk in early 2016 for a two-part\nstudy. The initial human intelligence task (HIT) instructed\ninterested participants that they would receive US$0.50 com-\npensation to complete a 5-minute survey about their online\nbehaviors and would be re-contacted in 1week to complete a\nsecond 5-minute survey on the same subject for additional\ncompensation. The initial survey included a series of ques-\ntions that tapped participants' attitudes about current US\nevents, their likelihood to engage in a series of online behav-\niors the following week (e.g. post on social media, visit news\nwebsites, and bank online), their personality traits, and\ndemographic information. Participants were required to meet\nAmazon's qualification of residing in the United States, have\na task approval rating of greater than or equal to 95%, and the\nfinal sample was demographically similar to the US popula-\ntion at large; 55% of the sample was female, with an average\n=\n=\nStoycheff 3\nand educational level of some college, but no degree\npants also had an average 2015 household income of slightly\nor Caucasian (6% were Black, 5.4% were Asian-American\nor Pacific Islander, and 1.5% identified as other). A total of\n331 participants fully completed Part 1 of the study such that\nthey were eligible for future correspondence.\nA week later, participants who successfully completed\nPart 1 were randomly assigned to one of three experimental\nconditions and re-contacted advertising either: A new survey\nHIT for Part 2 for US$0.50 compensation (N\nsurvey HIT for Part 2 that offered US$1.00 compensation\n(N\n=\n111), or a continuation of the Part 1 survey HIT for a\nall conditions were given 24\nhours to complete the second\nsurvey and were sent a reminder email if they had not com-\npleted it within this time. The Part 2 survey terminated data\ncollection after an additional 48hours.\nResults\nticipants had successfully completed Part 2 of the survey,\namounting to an impressive 74.9% response rate, which is\ncomparable to retention rates of other carefully conducted\n219 participated after initial contact and 29 participated after\na second follow-up email. A series of chi-square analyses\nwas conducted on each pairing of conditions using the origi-\nnal N=331 sample to determine differences in response rate\nbetween the incentive strategies, both after the initial contact\nfor Part 2 participation and the follow-up reminder email\nissued 24hours later. Results show that there was no signifi-\ning that at this level of financial incentive, a 100% increase\nin payment did not entice greater participation, failing to\nsupport classic economic theory proposed in H1.\nHowever, individuals who initially offered a US$1.00\nbonus were significantly more likely to participate in Part 2\nsocial exchange approach proposed in H2. But the signifi-\ncance of this effect wore off for those respondents who\nneeded a second reminder email. Once those who needed a\nfollow-up were included in the total response rate, there was\nno significant difference in response rate between individu-\nals offered bonus compensation and those offered a new task\nrates for all payment conditions are summarized in Table 1.\nDiscussion\nDespite MTurk's reputation as a commercial enterprise,\ndesigned to facilitate economic or market transactions\nbetween parties, higher financial incentives alone did not\nsignificantly increase response rates. These results mirror\nprevious research that has explored how incentives influence\nother modes of survey response, like face-to-face, email, and\nnew and increasingly popular means of data collection.\nInstead, this research suggests that granting bonus pay-\nments--which represent gestures of goodwill between par-\nticipants and researchers--is more effective at increasing\nunit response, even when financial incentives are equal. This\nfinding is particularly interesting given that the completion\nof a new task, as opposed to a bonus, adds to each worker's\nsuccess rate, which enhances future payment opportunities.\nIf individuals were operating on a mere cost-benefit calcula-\ntion alone, it would behoove them to participate in a new task\nrather than a task that is paid as a bonus. However, workers\ndo face the very real possibility that requestors may withhold\ncompensation, even for attentive participation, so bonuses\nmay signal work security between the two parties, suggest-\ning, from a utilitarian perspective, that good work is likely to\nbe rewarded. The finding that the social exchange bonus\nincentives diminished after reminder emails may indicate\nthat participants perceive the extra cost a researcher incurred\nto remind them to participate, compelling them to recipro-\ncate, regardless of the compensation amount.\nAs is customary in longitudinal designs, this study noti-\nfied all participants at the onset that there would be subse-\nquent study waves, which may have contributed to an\nelevated expectation of reciprocity across all conditions.\nResearch that fails to inform subjects of future interactions\nmay experience greater attrition because an initial social\nTable 1. Payment response rates at time 1 and time 2.\nN completed Part 1 N completed Part 2 after 24hours N completed Part 2 after 72hours\nHIT: human intelligence task.\n4 Methodological Innovations\ncontract has not been established. This research also only\ninvestigated two waves of data collection, and future research\nis needed to determine whether bonus payments operate sim-\nilarly for Part 3 and beyond; however, one would expect an\neven greater level of trust and reciprocity after further inter-\nactions, suggesting bonus payments may be even more ben-\neficial for subsequent invitation requests. Self-report\nmeasures that capture workers'feelings of trust and reciproc-\nity toward MTurk requestors would further shed light on the\nincentives offered in this study are small, but comparable to\nthose of social scientific studies regularly recruited on\nMTurk's platform, and are reasonable given the modest,\n5-minute time commitment required of participants.\nAlthough higher incentives do not appear to increase unit\nresponse, and previous research has shown they do not sig-\nnificantly affect data quality (Buhrmester et al., 2011) or the\npopulation willing to participate (Stewart et al., 2015), the\nscientific community should be ethically mindful of not eco-\nnomically exploiting MTurk workers (Fort et al., 2011).\nRather, researchers should strive to provide compensation to\nonline workers, which is a fair token of the time required,\nwhile also considering non-financial incentives that build\nmutually beneficial relationships with workers via learning\nexperiences and social benefits in order to maximize high-\nquality responses and a positive worker experience.\nDeclaration of conflicting interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research, author-\nship, and/or publication of this article.\nReferences\nAppelman A and Sundar SS (2016) Measuring message credibility:\nConstruction and validation of an exclusive scale. Journalism\nAuspurg K and Schneck A (2014) What difference makes a differ-\nence? A meta-regression approach on the effectiveness condi-\ntions of incentives in self-administered surveys. In: Proceedings\nof the MAER-Net 2014 Athens colloquium (MAER-Net), Athens.\nAvailable at: http://metaanalysis2014.econ.uoa.gr/fileadmin/\nmetaanalysis2014.econ.uoa.gr/uploads/Schneck_Andreas.pdf\nBerinsky AL, Huber GA and Lenz GS (2012) Evaluating online\nlabor markets for experimental researcher: Amazon.com's\nBoulianne S (2013) Examining the gender effects of different incen-\nBrawley AM and Pury CLS (2016) Work experiences on MTurk:\nJob satisfaction, turnover, and information sharing. Computers\nBuhrmester M, Kwang T and Gosling SD (2011) Amazon's\nMechanical Turk: A new source of inexpensive, yet high-\nquality data? Perspectives on Psychological Science 6(1): 3\u00ad5.\nCasler K, Bickel L and Hackett E (2013) Separate but equal? A\ncomparison of participants and data gathered via Amazon's\nMTurk, social media, and face-to-face behavioral testing.\nChandler J and Shapiro D (2016) Conducting clinical research\nusing crowdsourced convenience samples. Annual Review of\nChandler J, Mueller P and Paolacci G (2014) Nonnaivete among\nAmazon mechanical Turk workers: Consequences and solu-\ntions for behavioral researchers. Behavior Research Methods\nChristenson DP and Glick DM (2013) Crowdsourcing panel\nstudies and real-time experiments in MTurk. The Political\nClifford S, Jewell RM and Waggoner PD (2015) Are samples\ndrawn from Mechanical Turk valid for research on political\nideology? Research & Politics. Epub ahead of print December.\nCohen EL and Lancaster AL (2014) Individual differences in in-\nperson and social media television coviewing: The role of\nemotional contagion, need to belong, and coviewing orien-\ntation. Cyberpsychology, Behavior, and Social Networking\nCropanzano R and Mitchell MS (2005) Social exchange theory: An\nFan W and Yan Z (2010) Factors affecting response rates of the web\nsurvey: A systematic review. Computers in Human Behavior\nFort K, Adda G and Cohen KB (2011) Amazon mechanical Turk:\nGold mine or coal mine? Computational Linguistics 37:\nFox RJ, Crask MR and Kim J (1988) Mail survey response rate:\nA meta-analysis of selected techniques for inducing response.\nGoodman JK, Cryder CE and Cheema A (2012) Data collection\nin a flat world: The strengths and weaknesses of mechanical\nTurk samples. Journal of Behavioral Decision Making 26(3):\nGrauenhorst T, Blohm M and Koch A (2016) Respondent incen-\ntives in a national face-to-face survey: Do they affect response\nGroves RM, Singer E and Corning A (2000) Leverage-saliency\ntheory of survey participation: Description and an illustration.\nHauser DJ and Schwarz N (2016) Attentive Turkers: MTurk partici-\npants perform better on online attention checks than do subject\nHolden CJ, Dennie T and Hicks AD (2013) Assessing the reliability\nof the M5-120 on Amazon's mechanical Turk. Computers in\nHomans G (1961) Social Behavior: Its Elementary Forms. London:\nRoutledge.\nHorton JJ and Chilton LB (2010). The labor economics of paid\ncrowdsourcing. In: Proceedings of the 11th ACM conference\non electronic commerce, Cambridge, MA, 7\u00ad11 June, pp.\nHuff C and Tingley D (2015) \"Who are these people?\" Evaluating\nthe demographic characteristics and political preferences of\nMTurk survey respondents. Research & Politics. Epub ahead\nStoycheff 5\nPaolacci G, Chandler G and Ipeirotis PG (2010) Running experi-\nments on Amazon mechanical Turk. Judgment and Decision\nPeer E, Vosgerau J and Acquisti A (2014) Reputation as a sufficient\ncondition for data quality. Behavior Research Methods 46(4):\nRand DG, Peysakhovich A, Kraft-Todd GT, et al. (2014) Social\nheuristics shape intuitive cooperation. Nature Communication\nRouse SV (2015) A reliability analysis of mechanical Turk data.\nRyu E, Couper MP and Marans RW (2006) Survey incentives: Cash vs.\nin-kind; face-to-face vs. mail; response rate vs. nonresponse error.\nScott J (2000) Rational choice theory. In: Browning G, Halcli A and\nWebster F (eds) Understanding Contemporary Society: Theories\nSinger E, Van Hoewyk J and Maher MP (2000) Experiments with\nincentives in telephone surveys. Public Opinion Quarterly\nSprouse J (2011) A validation of Amazon mechanical Turk for\nthe collection of acceptability judgments in linguistic theory.\nStewart N, Ungemach C, Harris AJ, et al. (2015) The average\nlaboratory samples a population of 7,300 Amazon mechani-\ncal Turk workers. Judgment and Decision Making 10(5):\nStravrositu CD (2014) Does TV viewing cultivate meritocratic\nbeliefs? Implications for life satisfaction. Mass Communication\nWrench JS, Thomas-Maddox C, Peck Richmond V, et al. (2013)\nQuantitative Research Methods (2nd edn). New York: Oxford\nUniversity Press.\nAuthor biography\nElizabeth Stoycheff is an Assistant Professor in the Department of\nCommunication at Wayne State University. Her research uses\nonline sampling techniques to investigate the effects of new media\non democratic development and opinion formation."
}