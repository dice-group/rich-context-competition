{
    "abstract": "Abstract\nA great deal has been written about the role of clinical decision support systems in medicine in recent years--an important\ncategory of which are expert systems. Expert systems would normally contain an explanation module--the subject of a\ngreat deal of research interest in the 1980s when the main problem-solving task for medical expert systems was diagnostics.\nHowever, expert systems nowadays are more likely to perform tasks other than diagnosis, yet the role of explanation in\nexpert systems has been largely ignored in the health care literature since this time. Furthermore, user requirements can\nvary considerably in the health care domain and may include physicians, medical researchers, administrators, and patients.\nSuch user groups would have differing levels of knowledge and goals,which would impact on the type of explanatory support\nprovided by the system.This article examines the potential benefits of explanation facilities for a range of clinical tasks and\nalso considers the ways in which explanation facilities may be delivered so as to be of benefit to these categories of health\ncare user for these tasks.\n",
    "reduced_content": "http://sgo.sagepub.com\nExplanation and Expert Systems\nIntroduction\nMuch had been written in the 1980s about the role of expla-\nnation facilities in medical diagnostic expert systems:\nKunz, Shortliffe, & Fallat, 1983) were both prototype medi-\ncal expert systems of interest because of their explanation.\nHowever, despite being widely recognized as a useful\nadjunct to expert systems, explanation facilities have been\nlargely ignored in the health care literature in recent years.\nThis is partly because explanation facilities were first used\nin diagnostic applications, such as the MYCIN expert sys-\ntem and its derivatives in the 1980s, but the clinical tasks\nserved by expert systems have changed considerably since\n2006). Currently, expert system tasks are more likely to be\nused in the determination of drug dosing and drug prescrib-\ning or in reminding clinicians to engage in preventive inter-\nventions through inoculations. Indeed, Hunt, Haynes, Hanna,\nand Smith (1998) published a systematic review of clinical\ntrials of clinical expert systems, which found clear evidence\nfor the effectiveness of such systems with 43 out of 65 trials\nshowing an improvement of clinician performance. These\ntrials involved a variety of expert system tasks that included\ndiagnostic aid, preventive care, and reminder systems.\nHowever, only 20% of the diagnostic aid trials in this study\nwere effective, as compared with74% of trials based on\npreventive care and reminder systems. A more recent study\nby Garg et al. (2005) shows that clinical expert systems\nstudies assessing this outcome. Of these trials, 21 were\nbased on reminder systems, with 16 of them (76%) deemed\nto be successful, and 10 of these trials were diagnostic based\nwith 4 (40%) successful. This improvement in the use of\ndiagnostic expert systems reflects an increasing trend in\nrecent years as will be shown later in this article.\nBenefits of Explanation Facilities in\nClinical Expert Systems\nVery little attention seems to have been given to the ways in\nwhich explanation facilities could support modern expert\nsystem medical tasks. This is surprising, for Walton (1996)\npresents evidence to suggest that advice from a computer\nwill be more convincing if supported with explanation\nfacilities. In their evaluation of CAPSULE, an expert system\ngiving advice to general practitioners about prescribing\n1London South Bank University, London, England\nCorresponding Author:\nKeith W. Darlington, Senior Lecturer in Artificial Intelligence and\nComputing, Knowledge Based Systems Centre, Department of Informatics,\nLondon South Bank University, 103 Borough Rd., London, SE1 OAA,\nEngland\nEmail: keithd@lsbu.ac.uk\nDesigning for Explanation in Health Care\nApplications of Expert Systems\nKeith W. Darlington1\n Keywords\nexpert systems, explanation, decision support, stakeholders\n2 SAGE Open\ndrugs, they say that \"finding the most effective way of pre-\nsenting the explanation is an important goal for future studies\nof computer support for prescribing drugs.\" Many other stud-\nies have demonstrated the importance of a system being able\nto explain its own reasoning. For example, in a study of physi-\ncian's expectations and demands for computer-based consul-\ntation systems, it was found that explanation was the single\nmost important requirement for advice giving systems in\nmedicine (Buchanan & Shortliffe, 1984). Moreover, accord-\ning to Berry, Gillie, and Banbury (1995), explanation is seen\nas a vital feature of expert systems--particularly in high-risk\ndomains, such as medicine, where users need to be convinced\nthat a system's recommendations are based on sound and\nappropriate reasoning. The inclusion of explanation facilities\ncan enhance the quality of the decision making. Gregor and\nBenbasat (1999) have shown that well-designed explanation\nfacilities could improve user acceptance and performance in\nterms of speed and accuracy of judgments. Furthermore,\nexplanation facilities can lead to greater adherence to the rec-\nommendations of the expert system (Darlington, 2008;\nGregor & Benbasat, 1999). In addition, according to Friedman\nand Wyatt (1997), expert systems raise complex and profes-\nsional issues, in that, to avoid exposure to liability, every\nexpert system must treat its users as a \"learned intermediary.\"\nConsequently, opaque programs such as neural networks are\nclinically doubtful as compared with the transparency offered\nby explanation facilities.\nThis article briefly examines the technical means by\nwhich explanation facilities can be implemented in expert\nsystems and the components of explanation. Explanation\nfacilities are then considered with regard to a range of\nproblem-solving tasks in the health care domain as well as\ndiscussing user requirements for explanation with regard to\nthe range of users in this domain. These may include physi-\ncians, medical researchers, administrators, and patients. The\nnext section looks at the reasons for the low usage of expla-\nnation facilities and suggests ways to increase usage.\nUsage of Explanation Facilities in\nExpert Systems\nExplanation facilities can be viewed as an optional feature\nfor expert systems without interfering with the operation of\nthe underlying system, and although potentially very useful,\nresearch indicates that usage of explanation facilities is fre-\nquently very low. For example, a study by Dhaliwal (1996)\nfound that only 25% to 30% of available explanations were\nrequested during consultations with an expert system,\nwhereas Everett (1994) did a study that found only about\nhalf of the participants chose to view explanations at all.\nSome ways in which the developer can improve the relative\ninattention given to explanation facilities are described in\nmore detail later in this article and include the access mech-\nanism (see section titled \"The Communication\"), the content\nof the explanation delivery (see section titled \"Explanatory\nContent\"), and the adaptation to the user by delivering the\nappropriate level of abstraction (see section titled \"User\nAdaptation\"; Berry et al., 1995). Moreover, according to\nexplanation types in rule-based expert systems are inade-\nquate and should be enhanced with other explanatory con-\ntent (see section titled \"Explanatory Content\"). These\nshortcomings can be improved with some effort from the\nsystem developer without necessarily restructuring the\nexpert system knowledge base--which could be very time-\nconsuming. However, Berry et al. (1995) provide another\nreason for the low frequency of explanation use, in that,\nsystem developers often fail to involve users in the specifi-\ncation and evaluation of explanation facilities in the early\nstages of project development, but explanation facilities are\nstrongly correlated with the structure and knowledge content\nin an expert system (Wick, 1992b). This means that the\nknowledge sources adopted for developing an expert system\nwill affect the quality of the explanation facilities provided\nby that system. As Wyatt (1997) says, \"All too often in the\npast the knowledge used for decision support expert systems\nhas been acquired by a knowledge engineer from a single\nexpert--or even by browsing out of date narrative text-\nbooks, with all their defects\" (p. 167). Understanding the\ndifficulties of implementing explanation facilities in expert\nsystems requires knowledge of the way that both compo-\nnents work together as described in the next sections.\nExplanation in Clinical\nExpert Systems\nCharacteristics of Clinical Expert systems\nThe sections that follow will briefly examine some of the\nways that knowledge is represented in expert systems fol-\nlowed by a description of the way in which explanation\nfacilities would normally integrate with components of an\nexpert system, before considering the types of explanations\nsuitable in the health care domain.\nRepresenting Knowledge. Several knowledge representa-\ntional models have been used to construct clinical expert sys-\ntems, including symbolic methods, such as simple decision\ntrees, statistical/probabilistic methods, and rule-based and\nframe (descriptive logic) based expert systems (Peleg & Tu,\nare also quite common and include certainty factors (Short-\nliffe, 1981) and belief networks, based on probability theory,\nsuch as Bayesian networks (Korver & Lucas, 1993; Lacave\n& Diez, 2004). Machine learning methods using rule induc-\ntion, case-based reasoning, genetic algorithms, or even a\ncombination of these techniques can also be used to develop\nthe knowledge bases used by medical expert systems (Coeira,\nniques can be used in the medical domain by, for example,\nusing a set of clinical cases that act as examples from which\nDarlington 3\na machine learning system can then produce a systematic\ndescription of those clinical features that uniquely character-\nize the clinical conditions. This knowledge can be expressed\nin the form of rules to represent the knowledge in an expert\nsystem (Coeira, 2003). Neural networks provide another\nmachine learning technique suited to expert system develop-\nment but are not well suited to provide explanations because\nof their \"black-box\" opaque nature of operation (Cunning-\nRule-based expert systems and frame (descriptive logic)\nbased are better-suited representations for explanation as\ntheir inferences can provide transparency because of the\nexplicit way in which the knowledge is represented.\nA Typical Rule-Based Expert System Architecture. Fig-\nure 1 describes an archetypal expert system component\nstructure. An end user would communicate with the system\nvia a user interface and an explanation facility that would\ninteract with an expert system inference engine. The infer-\nence engine would use medical domain knowledge stored in\nthe knowledge base and control the consultation by deter-\nmining the questions that are to be asked to achieve its goals\nand derive conclusions or specify actions to be taken--such\nas proposed therapies and so on. The explanation component\nwould combine with the inference engine to provide expla-\nnations that could follow a consultation and provide postad-\nvice explanations--called feedback. Explanations before\nadvice--that is, during the question input phase--could also\nbe generated, called feedforward explanations. The latter\nprovides the user with a means to find out why a question is\nbeing asked during a consultation (i.e., during the data input\nstage). Feedforward explanations would frequently take the\nform of a description of technical terms (terminological) to\nenable the user to answer the question(s) in a meaningful\nway. Feedforward explanations are general in that they are\nnot dependent on any particular output case. By contrast,\nfeedback explanations are case specific in that they will\nnormally present a trace of the rules that were invoked dur-\ning a consultation and display intermediate inferences to\narrive at a particular conclusion. Feedback explanations pro-\nvide the user with a record of problem-solving action during\na consultation to that the user can see how a conclusion was\nreached when the data have been completely input. Figure 1\nalso shows how the explanation facility is dependent on the\nexpert system knowledge base.\nExplanation Properties\nThere are three main components of an explanation that are\nto be considered by the designer. They are the communica-\ntion, the adaptation to the user, and the content of an expla-\nThe Communication. As Figure 1 shows, the explanation\nfacility is a component of a rule-based expert system inter-\nface. The designer will therefore need to consider issues\nsuch as the language dialogue and style of presentation--\nthat is, graphical, textual, audio, or some combination as\nwell as the access provision mechanism of the explanatory\ncomponent. Three types of provision mechanisms are pos-\nsible: embedded, automatic, and user invoked. Embedded\nexplanations would be permanently visible on the display\nbut can result in confusion and consume valuable screen\nspace. However, automatic explanations are automatically\ninvoked as determined by the system (Gregor & Benbasat,\n1999). They could be invoked as a result of some action\ntaken by the user during a consultation. User-invoked\nexplanations are the most commonly used in practice for\nthey appear when they are explicitly requested by the user\nand could be implemented using hypertext links or other\ninterface commands (Mao & Benbasat, 2000). Payne, Bet-\ntman, and Johnson (1993) have conducted research in the\nusage of user-invoked explanations and from their studies\nhave proposed the cognitive effort perspective theory. This\nsuggests that expert system users will only invoke explana-\ntions voluntarily if the perceived benefits in accessing them\nare outweighed by the amount of mental effort in doing so.\nThis theory suggests good reasons for the relative inatten-\ntion to explanation facilities in the absence of any specific\ntrigger for their use. Consequently, careful thought has to\nbe given to the design of the access mechanism (Gregor,\n1996a). The main triggers for user-invoked explanations\nidentified as follows:\n1. A desire for long-term learning--normally trig-\ngered by novice users. In the health care domain,\nthis category might include patients.\n2. A strong disagreement with the advice given by the\nsystem--this type of trigger is normally invoked\nby experts whose views may conflict with that of\nthe system advice.\nClinical\nExpert\nClinical\nUser User Interface\nInference\nMechanism\nExplanaon\nFacility\nKnowledge\nacquision\nfacility\nKnowledge\nbase\nFigure 1. Relationship between an expert system and\nexplanation component\n4 SAGE Open\n3. The complexity of the task itself--normally trig-\ngered by novice users who will often need explan-\natory support with tasks involving perhaps more\ncomplicated procedures or the use of more com-\nplex technical jargon.\nUser Adaptation. An explanation designer has to consider\nthe recipient of an explanation, in terms of his or her knowl-\nedge and expectations. Unfortunately, default expert system\nexplanations do not take into account the variability of\nknowledge between different users. User modeling refers to\nthe process of managing these differences. One possible\nsolution to this problem of user modeling has been advo-\ncated by (Cawsey, 1993). The technique involves a dialog\nplanning method for the generation of interactive explana-\ntions. The method consists of not only planning the text to be\npresented to the user but also planning the dialog with the\nuser, based on the retaining of a model of the user's knowl-\nedge, which would be updated during the explanatory inter-\naction between the system and the user. One prominent\nproject incorporating a user model was OPADE (Carolis\net al., 1996). This was a European Community Project\u00ad\nfunded expert system for generating beneficiary-centered\nexplanations about drug prescriptions that takes into account\nthe user requirements. The main objective of OPADE was to\nimprove the quality of drug treatment by supporting the phy-\nsician in their prescription process and by increasing compli-\nance with the clinical guidelines (Berry et al., 1995). OPADE\nsupports two types of users: those who directly interact with\nthe system such as general practitioners and nurses, and\nthose who receive a report of results--that is, the patients.\nThe explanations that are generated are dynamic (unlike\nstatic canned text explanations) in that a \"user model\" is\nmaintained containing the characteristics of the user. A \"text\nplanner\" component plans the discourse during a consulta-\ntion. The text planner will build a tree containing the dis-\ncourse plan, which will depend on the objectives that are to\nbe met by the user model. The explanation is then delivered\nin natural language by taking the tree generated by the text\nplanner as input and transforming it using text phrases into\nthe appropriate format.\nExplanatory Content. Early (first generation) expert sys-\ntem explanation facilities were characterized by design\nincorporating one or more of the following types of explana-\ntion (Chandrasekaran, Tanner, & Josephson, 1989).\nRule-trace explanations. As its name suggests, the rule-\ntrace explanation links problem-solving knowledge with an\nexplanation of a trace of rules that were invoked during a\nconsultation. Most of the early expert systems were honed\nfor diagnostic support and were predominantly rule-based\nexpert systems (Darlington, 2000). The explanation facilities\nprovided in these systems, such as MYCIN (Shortliffe,\n1981), would have been predominantly problem solving\nknowledge via a rule trace. This is, essentially, a record of\nthe system's run-time rule invocation history during a\nconsultation--frequently syntactically doctored to present\nthe explanation in natural language form (Binsted, Cawsey,\n& Jones, 1995). The rule-trace facility would enable users to\nfind out Why a system is asking a question or How, following\na consultation, the system arrived at its conclusions by dis-\nplaying the trace of rules invoked. This is what sets explana-\ntion facilities in expert systems apart from \"help\" facilities\nfound in conventional software systems: They can provide a\nrule trace of the problem-solving mechanism of the inference\nengine during a consultation so that case-specific explana-\ntions can be delivered, whereas help facilities would nor-\nmally be provided by the developer preparing text in advance\nto explain the different outcomes. This is known as canned\ntext and is often used with rule-trace or other explanation\nfacilities to enhance or supplement explanations. However,\nthere are problems associated with canned text for the sys-\ntem builder has to anticipate all the possible user questions in\nadvance to invoke the appropriate response, which can result\non a lengthy development overhead.\nJustification explanations. The rule-trace explanation can\nreconstruct a trace from what problem-solving knowledge is\ncontained in the expert system knowledge base. A rule trace\ncan be executed without access to any rules that justify the\nexistence of this knowledge. If the builder has not included\nthe knowledge to justify the knowledge in the rule base, then\nthe system will not be able to justify the existence of the\nknowledge. The importance of this justification knowledge\nwas recognized by Clancey (1983) when attempting to\nextend the MYCIN system to support the training of junior\nphysicians. He found that MYCIN failed to do this because\nit did not contain justification knowledge as the rules that\nmodel the domain did not capture all the forms of knowledge\nused by experts in their reasoning. Expert physicians would,\nof course, use rules of thumb themselves in solving prob-\nlems, but they would also--as a result of their training and\nexperience--possess a deep theoretical understanding of\ntheir subject domain called \"deep knowledge.\" They may,\nfor example, use \"rules of thumb\" or heuristic knowledge\nwhen performing a diagnosis but would need to draw on\ntheir deep knowledge if an explanation that justifies such\nknowledge is required. In the same way, justification knowl-\nedge would have to be explicitly captured by the system\ndesigner if it was required for explanation. Justification\nknowledge may be captured by using deep knowledge mod-\nels. Empirical research has consistently shown that user\nacceptance of expert systems increases for nonexpert users\nwhen this justification knowledge is present and that justifi-\ncation is the most effective type of explanation to bring about\npositive changes in user attitudes toward the advice-giving\nStrategic explanations. Strategic explanations provide\nknowledge about how to approach a problem by choosing an\nordering for finding subgoals to minimize effort in the search\nfor a solution. For example, the rule of thumb that alcoholics\nDarlington 5\nare likely to have an unusual etiology can lead the expert to\nfocus on less common causes of infection first--thereby\npruning the search space to find a solution. In rule-based\nexpert systems, the strategic knowledge is frequently implic-\nitly incorporated in the problem-solving rules. This is accept-\nable if the designer wants the system to provide a\nproblem-solving role only. However, Clancey (1983) real-\nized that this knowledge needed to be explicitly represented\nin the MYCIN system so that it could become transparent to\nstudents training to use the system--rather like the justifica-\ntion knowledge discussed earlier. A follow-up system called\nNEOMYCIN was developed by Clancey (Clancey, 1983;\nClancey & Letsinger, 1981): This was a consultation system\nwhose medical knowledge base contained the strategic\nknowledge explicitly available for training purposes. Sys-\ntems containing explicit strategic knowledge could be better\nable to answer questions such as \"Why not pursue this line of\nreasoning instead of that\" as the solution planning would be\nexplicitly available.\nTerminological explanations. Terminological explanations\nprovide knowledge of concepts and relationships of a domain\nthat domain experts use to communicate with each other. The\ninclusion of terminological explanations is sometimes neces-\nsary because in order for one to understand a domain, one\nmust understand the terms used to describe the domain.\nTerminological explanations are a category of explanations\nthat are frequently used with feedforward explanations and\nwould frequently be implemented using canned text. They\nare more likely to be used by novice or nonclinical users\nsuch as patients, rather than the more knowledgeable users.\nTerminological explanations provide generic rather than\ncase-specific knowledge (Mao & Benbasat, 2000; Swartout\nSome approaches to second generation explanation. One of\nthe problems with explanation in first generation expert sys-\ntems incorporating the above explanation types was the\nunnatural and inflexible dialogues that often resulted during\nconsultations--often as a consequence of the restrictions of\nthe interrogatives Why and How described earlier. One\napproach to resolving this problem was advocated by Wick\nand Slagle (1989), who developed a system called Journalis-\ntic Explanation Facility (JOE). JOE delivers explanation\nbased on a journalistic analogy for news writing reported\nevents. It does this by extending the Why and How inter-\nrogatives to include the interrogatives Who, What, Where,\nand When providing scope for extending explanation queries\nto answer in past, present, and future text. Despite being lim-\nited by the absence of explicit domain knowledge, JOE is an\nexample of a prototype that can make the most of the inbuilt\nexplanation facilities and lead to low construction overheads.\nHowever, Wick (1992a) recognizes shortcomings in JOE in\nits inability to present the \"big picture\" in its explanations.\nThus, different research directions were advocated, which\nfocused more on understandability of explanations--taking\ninto account such issues as abstraction into different levels,\nlinguistic competence, and summarization (Swartout &\ntechnique called Reconstructive Explanation. This technique\nuses one knowledge base for the expert system and another\nfor the explanation component. The rationale for this is that\nan expert's line of reasoning is not necessarily the same as\nthe explanation provided, yet the basis for default explana-\ntions has been the rule trace of the line of reasoning. The\nReconstructive Explanation system can improve on under-\nstandability but at a cost of higher construction overheads.\nThe techniques described above can be used to tailor\nexplanation facilities to suit different categories of users, or\nstakeholders, without necessarily altering the reasoning of\nthe underlying expert system.\nThe Stakeholders\nClassification of Health Care Domain\nUser-Groups\nAccording to Leroy and Chen (2007), modern medical\nDecision Support Systems (DSS) are developed for four\ndifferent groups of decision makers--two of whom have\na medical background (clinicians and medical research-\ners), the other two (administrators and patients) may not.\nClinicians could include physicians (including junior\nphysicians) and nurses to use their knowledge and expert\nsystems to make decisions on behalf of others by, for\nexample, diagnosing diseases or evaluating drug interac-\ntions from a treatment or adopting a nursing strategy to\nalleviate pain. General research has shown that expert\nphysicians do make use of explanation facilities, but their\nrequirements are very different to that of other users.\nExperts tend to use feedback rule-trace explanations and\nare more likely--than nonexperts--to use explanations\nfor resolving anomalies, such as disagreements with sys-\ntem advice, exploring alternative diagnoses, and verifica-\ntion of assumptions (Arnold, Clark, Collier, Leech, &\nperts such as trainee physicians are more likely to use a\nrange of explanations types for short- and long-term\nlearning. For example, Arnold et al. (2006) have shown\nthat nonexperts tend to use both feedback and feedfor-\nward justification explanations as well as terminological\nfeedforward explanations.\nAdministrators do not have direct clinical interaction with\npatients but are responsible for the management of health\ncare options and facilities. They may therefore use expert\nsystems to manage health care options and could benefit\nfrom explanation facilities by aiding clinicians, through, for\nexample, the managing of patient referrals by finding out\nwhether a patient is suitable for a referral. All of the explana-\ntion types discussed previously may benefit administrators\ndepending on the nature of the application task. Furthermore,\nadministrators may use expert system for patient care\n6 SAGE Open\nmanagement, that is, automatically scheduling follow-up\nappointments or treatments, or automatically generating\nreminders relative to preventive care (i.e., inoculations) or\ntracking adherence to research protocols. Explanation facili-\nties which enable time-dependent queries such as through\nusing the interrogative When, as described in the JOE proto-\ntype in the previous section may be useful in this context.\nPatients are the largest group of decision makers with the\nleast amount of medical training. However, chronic patients\noften become expert patients because in many cases of\nchronic illness, patients are likely to acquire skills to help\nthem manage their illness better (Bury, 2003) because of the\nseverity of their discomfort, and they may have a much\ngreater desire to understand and access to web-based expert\nsystems could help. However, Berland, Elliot, Morales, and\nAlgazy (2001) have shown that the general public have dif-\nficulty with wordy medical jargon. Patients would therefore\nclearly benefit from terminological explanations in some\nhealth care systems.\nExpert System MedicalTasks\nAmenable to Explanation\nMedical Expert SystemTaskTypes\nExpert systems are now in routine clinical use in several task\nareas, including those described below (Coeira, 2003;\nGenerating Alerts and Reminders. Alerts inform clini-\ncians of changes in a person's condition, perhaps by attach-\ning an expert system to a monitor. Alerts could also be used\nto scan lab test results and send reminders or warnings. For\nexample, a patient could be attached to an electrocardiogram\n(ECG) or pulse oximeter, and the alert expert system could\nwarn of changes in a patient's condition. In less acute cir-\ncumstances, it might scan laboratory test results, or drug or\ntest orders, and then send alerts or warnings--either via\nimmediate on-screen feedback or through a messaging sys-\ntem like email. Reminder systems serve a slightly different\npurpose in that they are used to notify clinicians of important\ntasks that need to be done before an event occurs. An exam-\nple of a reminder system could be the generation of a list of\nimmunizations that is required by each patient on the daily\nschedule in an outpatient clinic (Randolph, Guyatt, Calvin,\nDoig, & Richardson, 1998). According to Bindels, De\nClercq, Winkens, and Hasman (2000), reminder systems are\nfar more successful than diagnostic expert systems, for the\nso-called Greek Oracle approach is not very popular among\nphysicians because it reduces their role. Reminder systems\nare successful because they leave the physician in control\nand only provide feedback when the physician does not obey\nthe rules. Explanation facilities could be important for these\ntasks for as Hanson (2006) says, the clinician wants access to\nthe knowledge rules associated with the condition and the\ninterventions proposed by the expert systems. The clinician\nmay want to evaluate the evidence that underpins the expert\nsystems rules, suggesting that justification and problem-\nsolving explanation types may be appropriate for clinicians\nusing alert systems. Moreover, as the purpose of alert and\nreminder systems is to provide interventions to clinicians\n(junior and senior), explanations may only be appropriate for\nthese user categories.\nTherapy Critiquing and Planning. These are expert system\ntasks that may look for inconsistencies and omissions in an\nexisting treatment plan but would not necessarily assist in\nthe generation of the plan. Some examples of their uses could\nbe applied to physician order entry systems. For example, on\nentering an order for a blood transfusion, a clinician may\nreceive a message stating that the patient's hemoglobin level\nis above the transfusion threshold, and the clinician must jus-\ntify the order by stating an indication, such as active bleeding\n(Randolph et al., 1998). However, planning systems have\nmore knowledge about the structure of treatment protocols\nand can be used to formulate a treatment based on a data on\npatient's specific condition from the Electronic Medical\nRecord (EMR) and accepted treatment guidelines. Therapy\ncritiquing explanations could benefit clinicians or adminis-\ntrators in that they could allow the user to enter a proposed\nsolution to a problem and then explain flaws in the solution.\nStrategic explanations might be appropriate for administra-\ntors, enabling them to make resource or allocation decisions.\nClinicians could also benefit from strategic explanations so\nthat \"why not\" scenarios may be explored (Martincic, 2003).\nClinicians (junior and senior) may benefit from access to\nrule trace and justification knowledge to understand the\nrationale underlying the possible flaws in the solution.\nDrug Advisory Systems. Drug advisory systems such as\nCAPSULE (Walton, 1996) are used to assist clinicians with\nthe prescription and medications and the selection of the\nmost cost-effective treatments. There is much evidence to\nshow that computer-based clinical decision support systems\nhave been shown to improve physician performance in rela-\ntion to drug treatment and adherence to protocols (Hunt\net al., 1998). Furthermore, some expert systems can improve\nphysician performance for drug dosing and preventive care\n(Hunt et al., 1998) without reducing drug expenditure\n(Vested, Nielsen, & Olesen, 1997). Explanation facilities can\nserve an important role in drug prescribing and dosing. An\nexample of an expert system incorporating explanation for\ndrug dispensing is OPADE (Carolis et al., 1996), which was\ndescribed earlier. The system generates beneficiary centered\nexplanations about drug prescriptions that take into account\nthe user requirements (Berry et al., 1995). Patients could\nbenefit from this type of task with the provision of termino-\nlogical explanations--enabling them to understand medical\nterms and jargon. More generally, justification explanations\ncould be appropriate for other categories of users for\nthis type of medical task, so that, for example, users can see\nDarlington 7\ntheoretical reasons for why a treatment has side effects\n(Berry et al., 1995) and/or be aware of interactions with other\ndrugs. Strategic explanations could also be useful to these\nusers so that they could understand why alternative drugs have\nbeen ruled out for consideration with a particular patient.\nDiagnostic Systems. Early medical expert systems were\nmainly used for diagnosis and many of these have fallen by\nexpert systems have proved to be least required where they\nwere thought to be most required: in diagnosis applications.\nDelaney, Fitzmaurice, Riaz, and Hobbs (1999) believe that\ncomputerized diagnostic systems have not yet been devel-\noped to the stage where they can significantly aid diagnostic\naccuracy and also claimed that there was no groundswell of\ninterest in diagnostic systems because physicians do not per-\nceive that they have a deficiency of expertise within their\nown area of speciality. However, a number of focused diag-\nnostic systems are still in use, and new uses are emerging, for\nexample, ECG interpretation and laboratory test interpreta-\ntions (Warner, Sorenson, & Bouhaddou, 1997). In particular,\nwhen a patient's case is complex or rare, or the person mak-\ning the diagnosis is simply inexperienced, an expert system\ncan help in the formulation of likely diagnoses based on\npatient data presented to it, and the system's understanding\nof illness, stored in its knowledge base. For example, diag-\nnostic assistance may be useful with complex data, such as\nthose provided by an ECG analysis, where most clinicians\ncan make straightforward diagnoses but may miss rare pre-\nsentations of common illnesses like myocardial infarction or\nmay struggle with formulating diagnoses, which typically\nrequire specialized expertise. Explanation facilities could\nplay an important part here for, as noted earlier, experts tend\nto use explanation facilities when there is disagreement with\nthe advice given by the system (Gregor & Benbasat, 1999).\nComputer-aided decision making could also speed diagno-\nsis, especially for difficult cases, thus providing the physi-\ncian with time for other matters--such as interacting with\npatients. A number of web-based diagnostic systems, such as\nISABEL (www.isabelhealthcare.com)--a dynamic diagnos-\ntic checklist system--are also gaining in popularity. The\nISABEL diagnostic aid has been shown to be of potential use\nin reminding junior doctors of key diagnoses in the\nemergency department (Ramnarayan et al., 2007). The\neffects of its widespread use on decision making and diag-\nnostic error can be clarified by evaluating its impact on\nroutine clinical decision making. Another study by Graber\nand Ashlie (2008) shows that the ISABEL clinical deci-\nsion support system quickly suggested the correct diagno-\nsis in almost all complex cases. However, ISABEL\nprovides user-invoked canned text explanations at various\nlevels. For example, when ISABEL ranks likely diagnoses\nto symptoms, it can provide textual explanations with\nhyperlinks to lower level explanations if required. In the\ncase of diagnostic systems, rule trace, justification, and\nstrategic explanations could be appropriate to clinicians\n(junior and senior), although patients who use web-based\ndiagnostic systems could benefit again from the provision\nof terminological explanations possibly delivered through\ncanned text or other means activated by hyperlinks.\nPatients may find this type of explanatory support particu-\nlarly important during question answering phase where\nthey may not fully understand the meaning of a question\nor the relationship that the question has on the overall\nconsultation.\nSummary\nTable 1 provides a summary based on suggestions for\nexplanation facilities for the tasks described in the previous\nsections. These are only suggestions because in practice,\nmuch would depend on the characteristics of a particular\ndomain.\nConclusions\nThis article recognizes the changing role of health care\nexpert systems tasks over the past 30 years. However, one of\ntheir main features--explanation facilities--has been largely\nignored in health care systems--despite the potential bene-\nfits that can be derived from their inclusion. This article has\nshown that there is much potential for the use of explanation\nfacilities for problem-solving tasks other than diagnostics--\nthe only problem-solving task that was used in the early\nmedical expert systems.\nTable 1. Summary Explanation Types,Tasks, and Users\nES task/user category Clinicians Junior clinicians Administrators Patients\nGenerating alerts and\nreminders\nRule trace, justification Rule trace, justification Not applicable Not applicable\nTherapy critiquing and\nplanning\nRule trace, justification, strategy Rule trace, justification,\nstrategy\nStrategy Not applicable\nDrug advisory systems Justification, strategy justification, strategy Justification, strategy Terminological\nDiagnostic systems Rule trace, justification, strategy Rule trace, justification,\nstrategy\nNot applicable Terminological\nNote: ES = expert system.\n8 SAGE Open\nAnother trend emerging, especially with the growth\nof web-based medical expert systems, is the range of\nstakeholders--which can vary from clinicians to patients.\nThis article has described a range of techniques that can be\nused to tailor and enhance explanation facilities to suit dif-\nferent stakeholders without necessarily altering the underly-\ning system. However, the developer will determine the way\nin which the knowledge is represented and this will affect the\nway in which these techniques could be implemented in\nterms of the construction overheads--how difficult and\ntime-consuming it is to build the explanations.\nThe implications of this research is then that builders of\nexpert systems should no longer consider explanation facili-\nties an unnecessary adjunct but should give careful consider-\nation to the way that they might help support users of the\nsystem. In doing so, builders of health care expert systems\nshould canvass the views of the stakeholders to gauge what\nexplanation content, type of interaction, and access mecha-\nnism may be suitable. The low usage of explanation facilities\ndiscussed earlier in the article could be improved substan-\ntially, for this article has shown that users are more likely to\nadhere to expert system recommendations when quality\nexplanation facilities are available as well as improve perfor-\nmance and result in more positive user perceptions about the\nsystem.\nHowever, further empirical research is necessary to inves-\ntigate the potential benefits and changes in performance aris-\ning from using explanation facilities in different medical task\nsettings as well as expand on the general results exploring\nthe likely benefits and performance when applied to different\nstakeholders in the health care domain.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with\nrespect to the research, authorship, and/or publication of this\narticle.\nFunding\nThe author(s) received no financial support for the research,\nauthorship, and/or publication of this article.\nReferences\nAikins, J. S., Kunz, J. C., Shortliffe, E. H., & Fallat, R. J. (1983).\nPUFF: An expert system for interpretation of pulmonary func-\nArnold, V., Clark, N., Collier, P. A., Leech, S. A., & Sutton, S. G.\n(2006). The differential use and effect of knowledge-based sys-\nBerland, G. K., Elliot, M., Morales, L., & Algazy, J. (2001). Health\ninformation on the Internet: Accessibility, quality and readabil-\nity in English and Spanish. Journal of the American Medical\nBerry, D., Gillie, D. T., & Banbury, S. (1995).What do patients want\nto know: An empirical approach to explanation generation and\nBindels, R., De Clercq, P. A., Winkens, R. A. G., & Hasman, A.\n(2000). A test ordering system with automated reminders for\nprimary care based on practice guidelines. International Jour-\nBinsted, K., Cawsey, A., & Jones, R. B. (1995). Generating person-\nalized information using the medical record. In Artificial intel-\nligence in medicine: Proceedings of AIME 95 (pp. 29-41). New\nYork, NY: Springer Verlag.\nBuchanan, B. G., Shortliffe, E. H. (1984). Rule-based expert sys-\ntems: The MYCIN experiments of the Stanford heuristic pro-\ngramming project. Reading, MA: Addison-Wesley.\nBury, M. (2003). Perspectives on the expert patient. London, UK:\nRoyal Pharmaceutical Society of Great Britain.\nCarolis, B. D., Rosis, F. D., Grasso, F., Rossiello, A., Berry, D.,\n& Gillie, T. (1996). Generating recipient-centered explanations\nabout drug prescription. Artificial Intelligence in Medicine, 8,\nCawsey, A. (1993). User modeling in interactive explanations.\nJournal of User Modeling and User Adapted Interaction, 3,\nChandrasekaran, B., Tanner, M. C., & Josephson, J. R. (1989,\nSpring). Explaining control strategies in problem solving. IEEE\nClancey, W. J. (1983). Epistemology of a rule-based expert system:\nClancey, W. J., & Letsinger, R. (1981). NEOMYCIN: Reconfigur-\ning a rule-based expert system for application to teaching. In\nW. J. Clancey & E. H. Shortliffe (Eds.), Readings in medical\nartificial intelligence: The first decade (pp. 361-381). Reading,\nMA: Addison-Wesley.\nCoeira, E. (2003). Guide to health informatics. London, England:\nHodder Arnold.\nCunningham, P., Doyle, D., & Loughrey, J. (2003). An evalua-\ntion of the usefulness of case-based explanation. Proceedings\nof the 5th International Conference on Case-Based Reasoning\nDarlington, K. (2000). The essence of expert systems. London,\nEngland: Prentice Hall.\nDarlington, K. (2008). Using explanation facilities in healthcare\nexpert systems. HEALTHINF 2008 Conference, Funchal,\nMadeira, Portugal.\nDelaney, B. C., Fitzmaurice, D. A., Riaz, D., & Hobbs, F. D. (1999).\nCan computerised decision support systems deliver improved\nDhaliwal, J. S. (1996). An experimental investigation of the use of\nexplanations provided by knowledge-based systems (Unpublished\ndoctoral dissertation). University of British Columbia, Canada.\nEverett, A. M. (1994). An empirical investigation of the effect of\nvariations in expert system explanation presentation on users'\nacquisition of expertise and perceptions of the system (Unpub-\nlished doctoral dissertation). University of Nebraska, Lincoln.\nFriedman, C., & Wyatt, J. (1997). Evaluation methods in medical\ninformatics. New York, NY: Springer.\nGarg, A. X., Adhikari, N. K., McDonald, H., Rosas-Arellano, M. P.,\nDevereaux, P. J., Beyene, J., . . . Haynes, R. B. (2005). Effects\nDarlington 9\nof computerized clinical decision support systems on practitio-\nner performance and patient outcomes: A systematic review.\nGraber, M. L., & Ashlie, M. (2008). Performance of a web-based\nclinical diagnosis support system for internists. Journal of Gen-\nGregor, S., & Benbasat, I. (1999). Explanations from intelligent\nsystems: Theoretical foundations and implications for practice.\nGregor, S. D. (1996). Explanations from knowledge-based systems\nfor human learning and problem solving (Unpublished doctoral\ndissertation). University of Queensland, Brisbane, Australia.\nHanson, C. W. (2006). Healthcare informatics. New York, NY:\nMcGraw-Hill.\nHunt, D., Haynes, R., Hanna, S., & Smith, K. (1998). Effects of\ncomputer-based clinical decision support systems on physician\nperformance and patient outcomes: A systematic review. Jour-\nKorver, M., & Lucas, P. (1993). Converting a rule-based system\nLacave, C., & Diez, F. J. (2004). A review of explanation methods\nfor heuristic expert systems. Knowledge Engineering Review,\nLeroy, G., & Chen, H. (2007). Introduction to the special issue on\ndecision support in medicine. Decision Support Systems, 43,\nMao, J., & Benbasat, I. (2000). The use of explanations in knowl-\nedge-based systems: Cognitive perspectives and a process-trac-\ning analysis. Journal of Management Information Systems, 17,\nMartincic, C. J. (2003). QUE: An expert system explanation facility\nthat answers \"Why Not\" types of questions. Journal of Comput-\nPayne, J. W., Bettman, J. R., & Johnson, E. J. (1993). The adaptive\ndecision maker. Cambridge, UK: Cambridge University Press.\nPeleg, M., & Tu, S. (2006). Decision support knowledge represen-\ntation and management in medicine. In Reinhold Haux & Casi-\nmir Kulikowski (Eds.), IMIA Yearbook of Medical Informatics\n(pp. 72-80). Stuttgart, Germany: Schattauer.\nRamnarayan, P., Cronje, N., Brown, R., Negus, R., Coode, B., Moss,\nP., . . . Britto, J. (2007). Validation of a diagnostic reminder sys-\ntem in emergency medicine: A multi-centre study. Emergency\nRandolph, A., Guyatt, G., Calvin, J., Doig, G., & Richardson, W.\n(1998). Understanding articles describing clinical prediction\nShortliffe, E. H. (1981). Computer based medical consultations:\nMYCIN. New York, NY: Elsevier.\nSwartout, W. R., & Moore, J. (1993). Explanation in second gen-\neration expert systems. In J. David, J. Krivine, & R. Simmons\n(Eds.), Second generation expert systems (pp. 543-585). Berlin,\nGermany: Springer.\nSwartout, W. R., & Smoliar, S. W. (1987). On making expert sys-\nTaylor, P. (2006). From patient data to medical knowledge:\nThe principles and practice of health informatics. London,\nEngland: Blackwell.\nVan Bemmel, J., & Musen, M. (1997). Handbook of medical infor-\nmatics. Houten, Netherlands: Springer.\nVested, P., Nielsen, J. N., & Olesen, F. (1997). Does a computerized\nprice comparison module reduce prescribing costs in general\nWalton, R. (1996). An evaluation of CAPSULE, a computer system\ngiving advice to general practitioners about prescribing drugs.\nWarner, H., Sorenson, D., & Bouhaddou, O. (1997). Knowledge\nengineering in health informatics. New York, NY: Springer.\nWick, M. (1992a). Expert system explanation in retrospect: A case\nstudy in the evolution of expert system explanation. Journal of\nWick, M. (1992b). Explanation as a primary task in problem solv-\nWick, M. R., & Slagle, J. R. (1989). An explanation facility for\nWyatt, J. R. (1997). Evaluation of clinical information systems. In\nJ. H. van Bemmel & M. A. Musen (Eds.), Handbook of medical\nYe, L. R., & Johnson, P. E. (1995). The impact of explanation facili-\nties on user acceptance of expert systems advice. MIS Quar-\nBio\nKeith W. Darlington is a senior lecturer in Computing and\nArtificial Intelligence at London South Bank University. He spe-\ncialises in Expert Systems and has published a book on the subject\ncalled The Essence of Expert Systems."
}