{
    "abstract": "Abstract\nWith more than 3 million participants per year, the Advanced Placement (AP) program is one of the most popular programs\nin the United States for exposing high-achieving high school students to advanced academic content. Sponsored by the\nCollege Board, the AP program provides a framework in which high school teachers can teach introductory college-level\ncourses to high school students. These students then take one of 34 standardized tests at the end of the year, and students\nwho score well on their course's AP test can receive college credit from their university in which they later enroll. Despite\nthe popularity of the AP program, remarkably little independent research has been conducted on the academic benefits of\nAP. In this article, I summarize the state of knowledge about the academic benefits of AP. Previous research and descriptive\ndata indicate that AP students outperform non-AP students on a variety of academic measures, but many other aspects of the\nprogram are poorly understood, partially due to variability across AP subjects. These aspects include the causal impact of AP,\nwhich components of the program are most effective in boosting academic achievement, and how students engage with the\nAP program. I also conclude by making suggestions for researchers to use new methodologies to investigate new scientific\nand policy questions and new student populations to improve the educational scholars' and practitioners' understanding of\nthe AP program.\n",
    "reduced_content": "journals.sagepub.com/home/sgo\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nThe Advanced Placement (AP) program is an academic pro-\ngram in which high school students can learn college-level\nmaterial from their high school teachers and then take a stan-\ndardized exam at the end of the school year. Students who\nperform well on their course's AP test may be granted credit\nby their university and/or be exempted from taking introduc-\ntory courses in college. Started by the College Board in the\nlate 1950s, today the AP program is one of the most common\nways for American high schools to provide advanced aca-\ndemic content to high-achieving high school students. In\nrecent years, more than 2 million high American school stu-\ndents have taken more than 3 million AP tests annually\nDespite the widespread popularity of the AP program,\nthere is remarkably little independent research on its aca-\ndemic benefits. (The College Board has numerous publica-\ntions in this area.) The purpose of this article is to summarize\nthe empirical research on the academic benefits of the AP\nprogram and to create a list of several scientific and policy\nquestions that remain unanswered about the AP program.\nMy overall goal with this article is to take stock of the AP\nprogram by looking back--through an examination of prior\nresearch--and looking forward--through new research\nquestions--to the future of AP research. I will accomplish\nthis by discussing previous research and data and discuss-\ning the results. In this way, I hope to give readers a strong\nunderstanding of the empirical research on the functioning\nand benefits of the AP program and to suggest avenues of\nresearch to improve understanding of AP.\nLooking Back . . . at Prior AP Research\nCollege Board Research\nFor decades, the College Board has had an impressive\nresearch program investigating the AP program. Some of the\nCollege Board's studies are psychometric in nature, including\nresearch related to the validity of AP test scores (e.g., Gray,\nHildebrant, & Strauss, 2006; Melican, Debebe, & Morgan,\n1Utah Valley University, Orem, USA\nCorresponding Author:\nRussell T. Warne, Department of Behavioral Science, Utah Valley\nEmail: rwarne@uvu.edu\nResearch on the Academic Benefits\nof the Advanced Placement Program:\nTaking Stock and Looking Forward\nRussell T. Warne1\n Keywords\neducation, social sciences, academics, achievement, curriculum, educational research, literature review, Advanced Placement\n2 SAGE Open\nLukhele, Thissen, & Wainer, 1994), and test item bias (e.g.,\nStricker & Emmerich, 1999). This research uniformly shows\nthat AP tests are developed in accordance with the highest\nprofessional standards of test development and interpretation.\nThis research also shows that recent developments in test\nconstruction are often quickly incorporated into AP tests\nwhen they are revised by College Board staff (Brennan, 2010;\nA line of research related to the College Board's psycho-\nmetric work is the noteworthy number of non-experimental\ngroup comparison studies of the academic performance of\nnon-AP students and AP students. College Board studies\nhave shown that AP students score higher on standardized\ntest scores than non-AP students (Ewing, Camara, & Millsap,\n2013). AP students also attend college at higher rates\n(Chajewski, Mattern, & Shaw, 2011; Wyatt & Mattern,\nare less likely to drop out of college (Mattern et al., 2009;\nWyatt & Mattern, 2011), and graduate from college at higher\nrates than non-AP students (Mattern, Marini, & Shaw, 2013).\nAP students also hold more favorable attitudes toward the\nsame academic material that their AP course covered\n(Patterson, 2009) and are more likely to major in a field\nrelated to their AP course than non-AP students are (Keng &\nAlthough the College Board's body of research concern-\ning the AP program is impressive, there is a potential for the\nconflict of interest in College Board scientists or consultants\nstudying the benefits of a College Board program. From an\noutsider's perspective, it seems convenient that much of the\nCollege Board's research on the AP program indicates that\nthe program is beneficial for high school students. Therefore,\nindependent research on the College Board's program is vital\nNon\u00adCollege Board Research\nIt has only been in recent years that the AP program has\nattracted the interest of non\u00adCollege Board researchers.\nBefore the 21st century, most peer-reviewed articles on the\nAPprogram were merely descriptive (e.g., Barnett & Durden,\nunder the assumptions that the AP program is beneficial and\nthat success in the program was prima facie a good thing for\nstudents (Tai, 2008). Yet, no independent researchers tested\nthis assumption until recently.\nLichten (2000) was the first scientific author to question\nthe efficacy of the AP program. He had several criticisms,\nincluding the claim that less able and/or less prepared group\nof students were participating in AP than previous years, as\nshown by falling AP exam passing rates. Lichten also had\nconcerns about the potential misalignment between the col-\nlege introductory curriculum and the AP curriculum and\nexams. In the years since Lichten's work, other education\nscholars have questioned the premise that AP is a beneficial\nprogram for students, including Tai (2008), Farkas and\nrevisiting the issue a decade later.\nThe first non\u00adCollege Board study that examined the aca-\ndemic impact of AP program participation was Geiser and\nSantelices's (2004) study of the relationship between stu-\ndents' high school records and their performance at college.\nThe authors found that neither AP course participation nor\nAP exam scores had any predictive power for students' col-\nlege grade point average (GPA) or for college persistence\nafter controlling for demographic variables, SAT scores,\nhigh school GPA, and other variables. Even though Geiser\nand Santelices's (2004) study was a technical report for the\nUniversity of California system--and not published in a\npeer-reviewed scientific journal--it has accumulated more\nthan 200 citations (according to Google Scholar) and even\nprompted the College Board to issue an official response\n(Camara & Michaelides, 2005) and conduct a similar study\non the same research questions (Shaw et al., 2013).\nhas since prompted an avalanche of independent research on\nthe AP program in which other researchers have investigated\nthe claims of the College Board and the widespread beliefs\nabout the academic benefits of the AP program. These\nresearchers have produced results that question the belief\nthat AP is beneficial for students, high schools, and\nuniversities.\nAcademic Benefits From AP\nPerhaps the most vexing unanswered questions about the AP\nprogram are the most basic questions of all: Does AP provide\nacademic benefits? And if so, what are these benefits? How\nlarge are they? Early non\u00adCollege Board efforts to answer\nthis question use basic quasi-experimental methodologies\ncomparing AP students with non-AP students. These efforts\nto compare non-AP students with AP students have produced\nfindings indicating that AP students outperform their peers in\na variety of measures of academic achievement, including\ncollege entrance exam scores (Ackerman, Kanfer, &\nFlowers, 2008), measures of writing ability in college\n(Hansen et al., 2006), college graduation rates (Ackerman\n2010), and rates of obtaining advanced degrees (Bleske-\nfindings support similar results in College Board studies.\nYet, these studies tell educators little about the actual\neffectiveness of the AP program because these authors\ncontrolled for few or no confounding variables. Participation\nand success in the AP program are the results of non-random\nprocesses. Therefore,AP students and non-AP students differ\nfrom one another in many ways. AP students tend to be from\nhigher income families, are more likely to be White and\nattend suburban schools, and have better academic prepara-\ntion for high school than non-AP students (Cisneros,\nHolloway-Libell, Gomez, Corley, & Powers, 2014;\n2008). Therefore, making simple comparisons between AP\nstudents and non-AP students without controlling for con-\nfounding variables is problematic. Klopfenstein and Thomas\n(2010) summarized this issue best by saying,\nWhile there is evidence of a correlation between AP experience\nand college success (because AP students tend to be capable and\nhighly motivated), there is no evidence from methodologically\nrigorous studies that AP experience causes students to be\nsuccessful in college. . . If the relationship between AP\nexperience and college success is not causal, then policies that\nbroaden AP-taking will not improve rates of college success for\nnontraditional AP students. (p. 170)\nIn response to these concerns, more researchers are con-\ntrolling for confounding variables to ascertain the causal\nimpact of theAP program. Most authors have found that con-\ntrolling for covariates reduces the apparent impact of the AP\nprogram on academic achievement (e.g., Klopfenstein &\nthe impact of AP participation vanish completely (e.g.,\nThe tendency for the AP program to appear to be less\neffective after controlling for covariates is particularly\napparent in one of the largest non\u00adCollege Board studies\never to investigate the AP program (Warne et al., 2015). In\nthis study, the researchers used marginal mean weighting\nthrough stratification (MMW-S) to control for 71 con-\nfounding variables in a sample of 45,558 students, which\nwas the entire population of the Utah public schools' grad-\nsample into four groups: (a) students who never took an AP\nEnglish course, (b) students enrolled in AP English who\nnever took an AP exam, (c) AP English students who took\nan AP English exam but did not pass because they obtained\na score of 1 or 2, and (d) AP English students who passed\nan AP English exam because they obtained a score of 3\nthrough 5. The researchers labeled these groups \"non-AP\nstudents,\" \"exam non-participants,\" \"exam non-passers,\"\nand \"exam passers,\" respectively. The academic achieve-\nment of these groups was measured with ACT scores.\nTable 1 shows that, before controlling for any covariates,\nthe ANOVA effect sizes measuring the impact of participa-\ntion of AP on ACT scores ranged from 2 = 6.21% to 2 =\nthe effect sizes were much smaller, ranging from 2 =\nThis example shows rather dramatically the statistical\nbias of most estimates of the academic benefits of the AP\nprogram and the importance of controlling for covariates.\nResearchers who fail to control for confounding variables\nmay be grossly overestimating the impact of the AP pro-\ngram on academic achievement. Yet, even after controlling\nfor so many variables, Warne et al. (2015) recognized that\nthere were still likely covariates that they failed to control,\nand they stated, \". . . the results in this article should be\nconsidered upper limits of the effectiveness of the AP pro-\nTable 1. Descriptive Statistics and Effect Sizes for the AP Program, Before and After Controlling for Covariates.\nStudent group\nACT composite\nM (SD)\nACT math\nM (SD)\nACT English\nM (SD)\nACT reading\nM (SD)\nACT science\nM (SD)\nANOVA results (no covariates controlled)\nMarginal M weighting through stratification results (71 covariates controlled)\nSource. Data taken from Warne, Larsen, Anderson, and Odasso (2015).\nNote. AP English analysis on data from the high school graduating class of 2010 in Utah. AP = Advanced Placement.\n4 SAGE Open\nTo sum up, researchers working independently of the\nCollege Board have also generally found that AP participa-\ntion is associated with higher academic achievement. Yet,\nthese results are more nuanced and less uniformly positive\nthan the College Board's research. Researchers who have\ncontrolled for confounding variables generally find weaker\neffect sizes than researchers who do not make any attempt to\ncontrol for pre-existing group differences among AP and\nnon-AP students. Therefore, the AP program might still pro-\nvide academic benefits for students, but the magnitude of\nthese benefits is not clear.\nEconomic Costs and Benefits of AP\nThe academic benefits of AP courses cannot be considered in\nisolation, though; it is important to consider the economic\ncosts and benefits of the program, as some educational pol-\nicy scholars have done. The costs and benefits of AP may be\nunevenly distributed among stakeholders, including stu-\ndents, their families, school districts, and taxpayers.\nOne widespread belief is that earning credits via AP will\nreduce a student's time to earn a college degree and therefore\nsave his or her money in college tuition (Klopfenstein, 2010).\nIn addition, government education agencies--such as school\ndistricts, public universities, and state governments--pay for\nthe AP program in various ways believing that these invest-\nments will reduce the expense of educating former AP stu-\ndents in college. Many agencies provide incentives for AP\nparticipation, such as exam fee subsidies for low-income stu-\ndents, financial bonuses for teachers who have large number\nof students succeed, and merit college scholarships for stu-\ndents who achieve at high levels in AP courses (e.g., Dendy,\n2016). However, there are few studies showing the cost-\neffectiveness of these programs--and what few studies exist\ndo not show that these incentives result in lower education\nspending for governments. For example, analyses in Florida\nand Texas have shown that generous state incentives to\nencourage AP participation are not cost-effective for taxpay-\nSikes, Underhill, & West, 2009). It is possible that more\nmodest spending on the AP program, such as the most com-\nmon state incentive of providing AP exam fee subsidies to\nstudents, are also not cost-effective. Klopfenstein and\nexam subsidies go to students who fail the AP exam,\" and it\nseems difficult to argue that paying money for students to\ntake anAP exam for which they will likely receive no college\ncredit will somehow save a state money later in the student's\neducational career.\nOther researchers have investigated whether students who\nparticipate in the AP program save money by graduating\nfrom college more quickly--a widespread belief that the\nCollege Board itself propagates (e.g., Mattern et al., 2013).\nThe research verifying this claim is mixed, with College\nBoard researchers finding that AP participation increases\n4-year college graduation rates (Mattern et al., 2013) and\nnon\u00adCollege Board researchers finding that concurrent\nenrollment was more effective than AP at reducing gradua-\ntion time for students (Klopfenstein, 2010). On the contrary,\nAckerman et al. (2013) found that time to graduation was\nreduced only for AP exam passers. However, non-partici-\npants in the AP program had higher graduation rates than AP\nstudents who took and failed their AP exams (Ackerman\nAlthough financial benefits of the AP program have been\nstudied, students may receive non-financial benefits from AP\nparticipation. The College Board (2014c) argues that the\nchallenge of AP is itself a benefit because it builds college-\nlevel study skills and provides opportunities for academic\nrigor. Other researchers have mentioned that students receive\nbenefits from merely enrolling in AP courses because their\nschools weight their GPA (Geiser & Santelices, 2004;\nNagaishi, Slade, Hermesmeyer, & Peck, 2014) or because\nstudents believe that having an AP course on their transcript\nincreases their chances of acceptance to college (Geiser &\nSantelices, 2004). However, there are also non-financial\ncosts that schools accrue from providing AP courses: These\ninclude diverting resources away from other academic pro-\ngrams (such as remediation to improve basic skills for stu-\ndents), larger non-AP classes, and diverting better teachers\naway from non-AP students (many of whom would receive\nacademic benefits from having high-quality teachers). These\nnon-financial opportunity costs of theAP program are largely\nunexplored (Klopfenstein & Thomas, 2010).\nLooking Forward . . . to Future AP\nResearch\nWith this background and understanding of priorAPresearch,\nit is now my goal to look forward to the future ofAP research.\nI have identified five ways that educational scholars can\nadvance the body of literature on the AP program: (a) meth-\nodologically, (b) with an awareness of AP course variability,\n(c) through examining new student populations, (d) with new\npurposes for research, and (e) with new scientific and policy\nquestions.\nLooking Forward Methodologically\nControlling for covariates. The example in Table 1 shows the\nimportance of controlling for confounding variables to\nreduce the magnitude of statistical bias in effect sizes. Recent\nmethodological advances have made it more feasible than\never to control for confounding variables and reduce statisti-\ncal bias. One major methodological advancement is propen-\nsity score analysis (Guo & Fraser, 2010), which allows\nresearchers to control for a greater number of covariates than\ntraditional methods, such as matching (e.g., Chamberlain,\nANCOVA (e.g., Mattern, Shaw, & Kobrin, 2011). Research-\ners who use propensity score analysis use covariate variable\nvalues to calculate a probability that a student will belong to\na subgroup within the sample (e.g., non-AP students, or AP\nexam passers). Instead of matching subjects on exact values\nof all covariate values (as in traditional methods), the\nresearcher only matches on the propensity scores (Fan &\nHornik, 2005). Researchers studying the AP program have\nbegun to embrace propensity score modeling, both within the\nCollege Board (McKillip & Rawls, 2013) and among inde-\npendent researchers (Jackson, 2010; Long, Conger, & Iat-\nThe results of studies using propensity score analysis are\npromising, and these studies bring scholars to a more realis-\ntic understanding of the actual benefits of the AP program.\nYet, there is still progress to be made in the use of propen-\nsity score analysis in AP program research. One fundamental\nchallenge is that creating accurate causal models requires\nknowledge of how students get assigned to a particular AP\ngroup (i.e., non-AP students, AP exam non-participants, AP\nexam passers, and AP exam passers). Steiner, Cook, Shadish,\nand Clark (2010) showed that controlling for non-random\nassignment to groups requires controlling for covariates that\neither have a causal impact on group assignment or are\nstrongly correlated with group assignment processes.\nControlling for the wrong combination of covariates fre-\nquently leads to failure to accurately measure the causal\nimpact of the intervention. In these cases, the statistical\nmodel tends to overestimate the effectiveness of a program,\nthough the overestimates are not as large as in studies in\nwhich researchers do not control for any covariates.\nWith the AP program, though, the assignment process is\ncomplex, as shown in Figure 1. Rather than a simple one-\ntime selection process, there are three stages of non-random\nassignment that determine the level of a student's participa-\ntion in the AP program. First, school personnel must choose\nwhether to offer an AP course. Second, if the child's school\noffers a particular AP course, then the child must choose to\nenroll in the course. Finally, if a student is enrolled in an AP\ncourse, he or she must choose whether to participate in the\nAP exam.\nThese three stages of non-random assignment in the AP\nprogram are not well understood. Table 2 shows a summary\nof the literature examining the correlates of the assignment\nprocess at each stage. There are two aspects of Table 2 that\nare striking. First, they are almost surely incomplete; only a\nlimited number of covariates have been discovered to relate\nto each assignment stage. Second, no covariates appear in all\nthree lists. In fact, there is no reason to believe that the same\nvariables are relevant in each stage of the assignment pro-\ncess. For example, school variables--such as the percentage\nof students from low-income families--seem very important\nin the first stage of non-random assignment (e.g., Barnard-\nBrak, McGaha-Garnett, & Burley, 2011). Yet, other research\nshows that when AP courses are offered in high-poverty\nschools, low-income students seem to enroll eagerly in AP\nThe point of this discussion is to emphasize that the meth-\nodological strides made controlling covariates inAP research\nare inadequate until researchers understand the assignment\nprocess at each of the three stages in Figure 1. This requires\nmore research on how each of these processes works and\nwhat measurable covariates are related to each decision\npoint. Understanding each assignment stage is a requirement\nto untangling the impact of the AP program from that of con-\nfounding variables. Although without a true experiment in\nwhich students are randomly assigned to AP courses, it may\nbe impossible to completely isolate the academic impact of\nAP courses in non-experimental research studies; it is none-\ntheless imperative to understand the AP enrollment process\nto create statistical models that reduce the amount of statisti-\ncal bias as much as possible. A study with less statistical bias\nis better than a study with a large amount of bias--or no\nstudy at all.\nClustering effects.In addition to ignoring the influence of\nconfounding variables (and therefore producing positively\nbiased estimates of the impact of the AP program), much of\nFigure 1. The three stages of non-random assignment to the AP program.\nNote. AP = Advanced Placement.\n6 SAGE Open\nthe research on the AP program also produces distorted\nestimates of the accuracy of parameter estimates, as quanti-\nfied by standard errors. Most AP research is based on tradi-\ntional statistics, which are based on the independence\nassumption that the sample data are drawn randomly from\na defined population. Yet, in many educational data sets\nmembers of the sample belong to pre-existing groups (e.g.,\nclassrooms, high schools), which is a violation of the inde-\npendence assumption.\nFor many years methodologists and educational research-\ners have known about the challenges in dealing with clus-\ntered data. The prime problem is that using traditional\nstatistics with clustered data underestimate the standard\nerrors of parameter estimates, which inflate Type I error\nable (e.g., McCoach & Adelson, 2010) for standard errors\nwhen researchers use traditional statistics with clustered\ndata. However, these corrections do not handle an additional\nproblem of traditional statistics, which is that they do not\npermit the accurate estimates of cluster-level variable rela-\ntionships, such as the impact of cluster variables (e.g., school\nquality) on individual outcomes (e.g., AP test scores; Hox,\nmodeling (HLM)--also called multilevel modeling--has\nbeen developed to handle clustered data (Warne et al., 2012).\nAPdata are almost always clustered in some way. Students\nare clustered in classrooms, which are in turn clustered into\nschools, districts, and states. As a result, school-level vari-\nables, district and state policies, and student variables can all\nexert an effect or combine to form interactions. But the use\nof traditional statistics in most studies prevents the proper\ninvestigation of these types of variable relationships. Some\nresearchers have used HLM to study the impact of the AP\nthough AP participation increased in most public schools in\nschools with a higher percentage of middle-class students.\nKlugman used this information (about school-level vari-\nables) to draw conclusions about an \"Advanced Placement\narms race\" (p. 115) among public schools, driven mostly by\nthe demands of groups of constituents (e.g., middle-class\nfamilies) more than the demands or characteristics of indi-\nvidual students. The result was an increase in inequality of\nAP offerings across schools. Following the logic in Figure 1,\nthis cluster-level behavior puts pressure on school personnel\nto offer AP courses.\nLike the efforts to incorporate propensity score modeling\ninto AP research, the incorporation of HLM into AP research\nis incomplete. One issue is that most research is with\nTable 2. Variables Associated With Each Stage of the Three-Stage Non-Random Assignment Process to an AP Course.\nSchool's choice to offer an\nAP course\nStudent's choice to enroll in an\nAP course\nStudent's choice to take\nAP exam\nState policies (Jeong, 2009; Klopfenstein &\nStudent race/ethnicity (Cisneros,\nHolloway-Libell, Gomez, Corley, &\n\u00b7\n\u00b7 White and Asian American students\nmore likely to enroll in AP courses.\n\u00b7\n\u00b7 Hispanic and Black students less likely\nto enroll in AP courses.\nStudent gender (varies by test; Campbell,\nNegative correlations:\n\u00b7\n\u00b7 School poverty level (Barnard-Brak,\n\u00b7\n\u00b7\n\u00b7 Percentage of minority students\nNegative correlation:\n\u00b7\n\u00b7 Living in a rural area (Barbour &\nPositive correlations:\n\u00b7\n\u00b7 Presence of high-achieving students\n\u00b7\n\u00b7 School enrollment size (Barnard-Brak\n\u00b7\n\u00b7 Number of teachers (Iatarola et al., 2011)\n\u00b7\n\u00b7 Higher percentage of students in\n\u00b7\n\u00b7 Perceived demand by administrators\nfrom parents and/or students\nPositive correlations:\n\u00b7\n\u00b7 School's use of weighted GPA\n\u00b7\n\u00b7 Economic ability to pay exam fee\nPositive correlations:\n\u00b7\n\u00b7 Economic ability to pay exam fee\nNote. AP = Advanced Placement; GPA = grade point average.\ntwo-level models, usually with students clustered into\nschools--thus preventing the investigation of the impact of\nclassroom variables (e.g., teacher characteristics), district\nvariables, and state policies on dependent variables.\nIncorporating all these clusters would require a five-level\nmodel, which is an extraordinarily complex HLM model.\nThere are a few isolated examples of four-level HLM models\nin the social science research literature, but I am unable to\nfind an example of a five-level model in any peer-reviewed\nIn addition, many clusters are not mutually exclusive,\nwhich introduces more complexity to HLM models. A tradi-\ntional HLM model has clusters nested within clusters, and a\nstudent belonging to a particular classroom also automati-\ncally belongs to a particular school, district, state, and so\nforth. But clustering becomes more complex later in stu-\ndents' educational careers as AP students go on to college.\nWhen students graduate from high school and attend college,\nthey leave one set of clusters (i.e., classrooms, schools, etc.)\nand move into new clusters (i.e., college majors, universities,\netc.), with the newer clusters no longer being mutually exclu-\nsive with old clusters. Modeling these non-exclusive clusters\nrequires a cross-classified model (see Goldstein, 1994; Luo\n& Kwok, 2012). To date, only one study on the AP program\nhas used a cross-classified model to take into account non-\nexclusive clusters of AP students (Patterson et al., 2011),\nwhich showed that AP examinees had higher college GPAs\nin classes related to their AP course than students who never\ntook an AP test in that subject.\nIn sum, the potential for HLM to improve the quality of\nAP research--through improved estimates of standard errors\nand the investigation of cross-level interactions--has been\nlargely unrealized. I urge researchers to incorporate HLM\nand cross-classified models into their investigations of the\nAP program. However, given the possibilities of cross-clas-\nsified/HLM models with up to five levels of clustering, it is\nunlikely that any one study could incorporate all the ways\nthat AP data could be clustered. In this way, the discussion of\nthe problem of inaccurate standard errors is not unlike the\nprevious section's discussion of positively biased estimates\nof the effectiveness of AP. And--again--even though incor-\nporating every aspect of data clustering into a statistical\nmodel may be impossible, if using HLM models to investi-\ngate the AP program became commonplace among research-\ners, then educational scholars would have more accurate\nestimates of standard errors and a better ability to investigate\ncross-level variable relationships. These two benefits of\nHLM models are often not realized in current AP research\nstudies.\nCluster-level randomized control trials (RCT). Although statisti-\ncally controlling for covariates and managing clustered data\nwould improve the quality of AP research, neither strategy\nwould fully establish cause-and-effect relationships between\nAP participation and academic outcomes. Rather, the \"gold\nstandard\" of educational research is the RCT, which involves\nrandomly assigning research subjects to a treatment and a\ncontrol group. This randomization equalizes the groups, and\nany differences in the dependent variable would be due\nsolely to the impact of the intervention (Gall, Gall, & Borg,\nprogram could greatly benefit from the use of RCTs.\nConsidering the non-random assignment process shown\nin Figure 1, it is clear that forcing AP students to take (or not\ntake) the AP test would be infeasible, and perhaps unethical.\nHowever, the first two stages of AP participation--offering\nan AP course and enrolling in an AP course--could be part of\na cluster-level RCT. For example, in a sample of participat-\ning schools, some campuses could be randomly assigned to\nreceive funds and training to offer a new AP course, while\nremaining schools would continue to offer their regular cur-\nriculum. Any later mean differences in student outcomes\nbetween schools would be due to the differences in curricu-\nlum. Although this sort of design would not answer all ques-\ntions about the AP program's effectiveness, it would help\nschool personnel and scholars understand the likely impact\nof offering an AP course and whether adding new AP courses\nto the curriculum would be an effective policy.\nLikewise, an RCT could be designed in which some\ngroups of students are forced to enroll in an AP or a non-AP\ncourse. This could happen, for example, if randomly selected\nschools convert all their English courses into AP English\ncourses. This design would provide stronger information\nabout the impact of enrolling in an AP course than is avail-\nable now from published studies (e.g., Warne et al., 2015).\nUnfortunately, the cluster-level RCT has never been imple-\nmented in any research on the AP program--a fact that I\nwould like to see changed.\nLooking Forward With an Eye on Course\nVariability\nThe College Board offers 34 AP exams in a wide variety of\ntopics. What all these exams have in common is an instruc-\ntion format where, (a) high school students learn college\nmaterial from high school teachers and (b) an opportunity for\nstudents to earn college credit by performing well on the AP\nexam (usually a score of 3, 4, or 5). But nearly everything else\nabout AP courses are heterogeneous. There are immense dif-\nferences among these courses in incentives, availability, exam\nand course content, examinee demographics, and more.\nSkills needed to succeed on anAP test. One of the most striking\nexamples of the variability across AP exams comes from a\nCollege Board study where the authors found the correlation\nbetween PSAT scores in students' sophomore year and the\nAP exam scores in their junior or senior years. The correla-\ntions range from a low of r = .033 for the AP Spanish Lan-\nguage and Culture test to a high of r = .695 for theAP English\nLanguage and Composition, with a median correlation of\n8 SAGE Open\ncorrelations is shown in Figure 2. Because the PSAT is an\nexcellent measure of academic reasoning skills (Camara &\nMillsap, 1998), it is apparent that the degree of academic\nskill needed to succeed in AP exams varies across courses\nAlthough the PSAT data from the College Board (Ewing\net al., 2006) is interesting, it is important to note that the PSAT\nmeasures global academic aptitude via two subtests: a verbal\nsubtest and a mathematics subtest. The College Board has\ncreated a tool called AP Potential (at http://www.college-\nboard.com/counselors/app/expectancy.html) that uses PSAT\nscores to predict a student's probability of passing a particular\nAP exam. Beyond general academic skills in math and lan-\nguage arts, nothing is known about more specific academic\nskills or knowledge (e.g., writing skills, mastery of trigonom-\netry, previous knowledge of American history) for particular\ntests. Therefore, school personnel have little guidance in\ndeciding whether a particular AP course is appropriate for\ntheir school or whether they should require pre-requisites for\nstudents who hope to enter an AP course. Research on the\nskills needed to succeed in AP courses is vital so that the pro-\ngram can operate more efficiently and so that students can\nmaximize their chances for success when taking AP exams.\nAP exam and class participation. Participation is another area\nwhere there is high variability in the AP program. This vari-\nability is important because it means that researchers need to\ninvestigate each AP subject separately. After all, students do\nnot enroll in the entire AP program. Rather, they enroll in\nindividual courses and decide on a test-by-test basis whether\nto take an AP exam.\nIn 2014, the least popular AP test was the AP Japanese\nLanguage and Culture exam, with 1,942 examinees. The most\npopular test was the AP English Language and Composition\nthe United States in 2014; just 3 tests (AP English Language\nand Composition, AP U.S. History, and AP Literature and\nComposition) accounted for one third of all tests (for a total of\n1.18 million tests). Six tests accounted for over half of all AP\ntion to the numbers of students participating in AP exams,\npassing rates (as defined by a score of 3 or higher) show high\nvariability. In 2014, the lowest passing rate was for the AP\nEnvironmental Science test (47.2%), and the highest passing\nrate was for theAP Chinese Language & Culture test (93.4%).\nThe weighted mean passing rate was 58.7%, with a median\nThis information about participation numbers and passing\nrates is based on College Board data, which only includes AP\nexaminees. But the College Board does not collect data on\nstudents enrolled in AP courses who do not take AP exams.\nTherefore, no one really knows exactly the percentage of AP\nstudents who actually take AP exams. Estimates from previ-\nous studies vary somewhat. In Geiser and Santelices's (2004)\nsample of students in the University of California system,\napproximately 55% to 60% of AP students enrolled in AP\nclasses take their course's AP exam. On the contrary, Sadler\nand Tai (2007b) cited a National Research Council report in\nwhich it was estimated that 60% to 70% of AP students take\ntheir course's AP exam. The College Board's (2001, p. 3)\nown estimate is that approximately 66% of students in AP\ncourses take the corresponding AP exam.\nFigure 2. Histogram showing the distribution of correlation coefficients between students' PSAT scores and AP exam scores.\nSource. Data taken from Ewing, Camara, and Millsap (2006, p. 8).\nNote. AP = Advanced Placement.\nIt has recently become clear, though, that these estima-\ntions of overall participation rates mask the variability in\nparticipation rates across AP examinations. The first clue to\nthis variability is found in a study from Sadler, Sonnert,\nHazari, and Tai (2014) in which the participation rates for\nvariety of courses, though, require tracking each student\nenrolled in an AP course to identify whether that particular\nstudent took an AP exam. These data are usually only avail-\nable from state education agencies. Only the West Virginia\nDepartment of Education (n.d.) publishes information on\nthe number of students enrolled publicly in AP courses,\nwhich can then be used in combination with College Board\ndata to estimate participation rates in AP exams. Private raw\ndata from the Warne et al. (2015) study, though, can be used\nto calculate this information also. The results for both states\nare displayed in Table 3.\nTable 3 shows that natural sciences courses seem to have\nlower AP exam participation rates, while humanities and\nsocial science courses (especially U.S. History and English)\ntend to have higher exam participation rates. Comparing\nthe AP participation rates in Table 3 with the participation\nrates from Sadler et al. (2014) shows that the natural sci-\nence test participation rates all hover in roughly the mid-\n50% range. On the contrary, calculus participation rates\ntend to be higher than for participation rates in science\ncourses. Yet, the differences between the two states are also\nenticing, such as West Virginia's extremely high AP Human\nGeography participation rate. This may indicate some vari-\nability across state lines. Indeed, West Virginia mandates\nthat every public high school offer AP courses, which alone\nmay indicate a greater support for the program and there-\nfore higher participation rates.2\nTable 3 also makes it clear that because students self-\nselect whether they will take a particular AP test, the exam-\ninee population differ across tests. Therefore, it is unclear\nwhether the variability in mean AP scores across tests is a\nresult of differences in difficulty of AP exams or differences\nin the student examinee populations. A clue to this issue is\nfound in the relationship between AP exam participation\nrates and AP exam mean scores in the 2014 national College\nBoard data. My analysis indicates that the rank correlation\nbetween these two variables is  = -.65.At the state level, the\ncorrelations were r = -.22 in Utah and r = -.24 in West\nVirginia, indicating that the most popular tests are also the\ntests that students score lowest on. This supports Lichten's\ndemically elite as it has grown in popularity.3 This informa-\ntion about the variability in participation rates and passing\nrates across AP tests shows that likely the decision process of\nwhether to take a test probably varies from one test to another.\nAs the data in Table 3 indicate, lumping data from all AP\ncourses together hides how each individual course functions\nTable 3. Participation Rates for Various AP Exams in Utah and West Virginia.\nAP test\n(%)\n(%)\nEconomics 71.8 (Macro- or microeconomics) Not available\nEnvironmental science Not available 57.2\nEuropean history 57.9 Not available\nPolitical science 55.8 (U.S. or Comparative) 75.9 (U.S. only)\nSpanish language and culture Not available 52.1\nSource. Utah data taken from the raw data in the Warne, Larsen, Anderson, and Odasso (2015) study. West Virginia data publicly available at West\nVirginia Department of Education (n.d.).\nNote. To be included in the table, an AP subject had to have at least 50 students statewide enrolled in an AP course every year for five consecutive years.\nUtah data were only provided for core AP subjects (i.e., no arts or language subjects). West Virginia data are publicly available for all AP subjects. AP =\nAdvanced Placement.\nand oversimplifies the nature of AP. In a way it may be inac-\ncurate to talk about \"the AP program\" at all. Thinking about\n\"the AP program\" as a collection of advanced curricular\noptions would better reflect how AP functions and how stu-\ndents participate in AP courses and tests. Researchers who\nproduce overall \"average\" results of AP participation and do\nnot consider courses may produce results that do not apply to\nany course (or any student) well.\nLooking Forward With an Eye on Subpopulations\nResearchers have become increasingly interested in issues of\nequity and fairness in relationship to the AP program. This is\nseen, for example, in the College Board's (2014c) state\nreports on equity of access and the recent efforts of indepen-\ndent scholars investigating how subgroups of students inter-\nact with the program, including rural students (Barbour &\nJordan, Hannum, & Farmer, 2014), English language learn-\ners (Kanno & Kangas, 2014), diverse students (Cisneros\nfemale students (Ackerman et al., 2013; Campbell, Brown,\n2010). Although these efforts are preliminary, they still\nreveal clues about the special challenges these subpopula-\ntions experience. For example, Kanno and Kangas (2014)\nshowed that school personnel often steer language learners\naway from AP courses, except for the AP language course\nthat corresponds to their native language. It is possible that\nthe self-selection process in Figure 1 may differ for various\nstudent populations. Therefore, researchers who attempt to\nunderstand how the AP program functions may need to\nmodel the process separately for each student population,\njust as they likely will need to understand the process sepa-\nrately for each AP class.\nAlthough research on the subpopulations mentioned above\nshould continue, I encourage researchers to investigate other\ndata indicate that most AP examinees are in their junior or\nsenior years of high school. Yet, there is a subset of AP exam-\ninees who are younger than the typical examinee. Two tests\n(AP European History andAPWorld History) have a majority\nof examinees in the 10th grade, and one AP test (AP Human\nGeography) is taken by more students in the ninth grade than\nany other grade. This raises a series of questions about how\nthese tests function. For example, is a 14-year-old's high\nschool coursework truly equivalent of an introductory college\ncourse? Because these tests have some of the lower passing\nrates, does failing an AP test in the early high school years\nhave consequences in students' junior or senior years?\nHowever, the College Board (2014a) states that the grade\nlevel with the highest passing rate is for students who are not\nyet in the ninth grade. Virtually nothing is known about stu-\ndents who take AP exams before their high school years.\nRural students are another subpopulation about whom\nresearchers know little. Previous research has shown that the\nmassive growth nationwide in AP offerings at high school\nhas largely bypassed rural schools (Klopfenstein, 2004).\nWhat little is known about rural students and the AP program\nis enticing, though. For example, two separate studies on\nrural students enrolled in online AP courses showed that\nthese students have attrition rates that are higher than class-\nroom-based face-to-face AP courses (Barbour & Mulcahy,\nThere has also been little research on gender differences\nacross AP tests--either in participation rates or in average\ntest scores. The few studies concerning this issue have been\nfocused on female students' participation in science and\nmathematics AP tests (Ackerman et al., 2013; Campbell\nof the relationship between gender and AP examination rates\nand test passing rates. For example, data from the 2014 test-\ning year show that females took 54.5% of AP tests, and of the\n34 AP tests administered that year, the number of female\nexaminees exceeded the number of male examinees for 22\nAP tests (College Board, 2014b). However, the passing rate\nfor females was higher for only 11 AP tests. In addition, the\ngender ratio among examinees is much more discrepant\namong tests than the gender ratio among AP exam passers.\nAmong examinees, the male-to-female ratio ranges from\nScience A). Yet, the gender male-to-female gender ratio\namong students who pass AP tests has a much narrower\n(for AP Environmental Science). It is not clear why some AP\ntests have a very marked gender imbalance among examin-\nees while also having near-parity between males and females\namong students who pass.\nLooking Forward With New Uses for AP Data\nThe College Board designs AP tests to be used by colleges\ndeciding whether to give students college credit for high\nschool work. The efforts of scientists and researchers affili-\nated with the College Board supporting this use for AP test\nscores are remarkable and supported by empirical data (Gray\n1990), though some outsiders and universities question\nwhether a score of 3 should be considered \"passing\" for\nsome AP tests (e.g., Lichten, 2010). Increasingly AP data are\nbeing used for other purposes, such as making college admis-\nsions decisions or calculating high school GPAs--uses that\nthe College Board neither advocates nor prohibits. However,\nmodern standards of professional test use require that test\nusers validate test scores for new uses (American Educational\nResearch Association, American Psychological Association,\n& National Council on Measurement in Education, 2014).\nFew--if any--of these other uses of AP scores have been\nsubject to validation research.\nOne way that AP data are being used is in the weighting of\nhigh school GPAs. In most American high schools, adminis-\ntrators give to students taking advanced courses--such as\nhonors or AP classes--on their GPA (Lang, 2007). This\nincentivizes students to challenge themselves academically\nbecause otherwise some students could avoid taking chal-\nlenging classes to preserve a high GPA (Klopfenstein &\nLively, 2016). Most schools give this bonus for merely\nenrolling in an advanced course, though in some areas (e.g.,\nArkansas), weighted course credit is only given if the student\nalso takes the AP test (see Education Commission of the\nStates, 2015). This could create a strong incentive for stu-\ndents to participate in the AP program because high school\nGPAs are often used as a determining factor in college admis-\nsions, scholarship decisions, and academic honors (Warne\net al., 2014). The research on using AP enrollment and test\nscore data for the purpose of calculating a high school GPA\nis preliminary and contradictory (e.g., compare Warne et al.,\nresearch is needed before personnel can confidently use AP\ncourse and test data for the purpose of adjusting students'\nhigh school GPAs.\nMore research exists supporting using AP data in the col-\nlege admission process. But, as Tai (2008) explained,\nThe AP program was not originally intended for use in the\ncollege admissions process. . . . Decades later, however, the\ncircumstances and the program both have changed. It would be\nna\u00efve to expect college admissions officers to overlook or ignore\nthe tendency of high-achieving students to seek out these courses\nin high school. (p. 40)\nIndeed, advanced coursework is highly important in the col-\nlege admissions process. In one recent study, college admis-\nsions counselors stated that the two most important aspects\nof a student's application were--grades in college prepara-\ntion courses and the degree of rigor of an applicant's high\nschool curriculum--surpassing in importance even college\nadmission test scores and overall GPA (Clinedinst, Hurley, &\nHawkins, 2012). What is not clear is which pieces of AP data\nare relevant and how college personnel should use them in\nthe admissions decision process. Researchers investigating\nthis question have used the average AP test score, highest AP\ntest score, lowest AP test score, the number of college credits\nearned through AP, the number of passing AP scores, and\nnominal variables indicating that a student had taken AP\ncourses or tests or had passed an AP test (Ackerman et al.,\nthat some variables are better predictors of college success\nthan others; the mean AP score seems to have a strong cor-\nrelation with college performance measures (Ackerman\nAP courses that a student takes seem unrelated to college\nperformance, indicating that college admissions committees\nshould give preference to students with AP exam scores over\nstudents who merely enrolled in a course (Ackerman et al.,\nThere are still empirical and practical questions about\nusing AP data for college admissions decisions. It is possi-\nble that some AP test scores are more predictive of college\nsuccess than others, though few researchers have investi-\ngated this possibility (see Ackerman et al., 2013, for an\nexception). Even if scholars reach a consensus about which\nAP variable can help college admissions personnel make\nbetter decisions, it is not clear how to practically implement\nthis information because students may take differing combi-\nnations of AP courses and tests. Finding a consistent and fair\nsystem for taking AP work into account in admissions will\nrequire further work.\nOther uses of AP data--such as how to incorporate AP\ndata into student, teacher, and school accountability sys-\ntems--remain almost completely uninvestigated. Some\nresearch indicates that merely offering AP courses to students\ndoes not turn a poor school into an excellent school (Lichten,\non an AP test seems to be correlated with student-level vari-\nables like PSAT score and family income (Ewing et al., 2006;\nWyatt & Mattern, 2011), it is not clear how to isolate the\nschool-level influence and demographics from the student's\nefforts. Research into value-added accountability measures\n(e.g., Winters & Cowen, 2013) may be relevant.\nLooking Forward With New Scientific and Policy\nQuestions\nThroughout this article, I have suggested possible avenues of\nresearch on the way that the AP program functions. In this\nsection of the article, I provide a list of further research ques-\ntions. Most of these research questions are based on prior\nresearch, but some are based on the data reported in this arti-\ncle. I have divided these questions into two categories:\ndescriptive questions and causal questions.\nDescriptive questions.Although studies of descriptive ques-\ntions do not provide definite information about the benefits\nof the AP program, they can lay the foundation necessary to\nask questions about causes. These questions include the\nfollowing.\n\u00b7\n\u00b7 The College Board recommends that students receive\ncollege credit for earning a score of 3, 4, or 5 on an AP\ntest. But there is no evidence that scores are equiva-\nlent across tests. Are some tests easier than others? Do\nuniversities'AP policies on AP credit reflect the actual\nrelative difficulty of AP tests and the work needed to\npass them?\n\u00b7\n\u00b7 As AP grows in popularity, passing rates for most\ntests have dropped (Tai, 2008; Warne & Anderson,\n2015), possibly indicating that the AP program is\nreaching a saturation point and that most new popu-\nlations of AP students are not well prepared for AP\nexams (Lichten, 2010). Large numbers of unquali-\nfied students enrolling in AP are an inefficient use of\ntime and school resources. What variables (including\nnon-cognitive variables) should be considered when\npermitting a student to enroll in an AP course or take\nan AP test?\n\u00b7\n\u00b7 Attrition rates seem to be higher in online AP courses\nthan in face-to-face AP courses (Barbour & Mulcahy,\nis an online delivery system effective for an AP\ncourse? Does the AP curriculum or the student popu-\nlation creates special barriers to successful online\nlearning, or can online AP courses be as successful as\nface-to-face courses? What conditions and teacher\ntraining are necessary for a successful online AP\nexperience?\n\u00b7\n\u00b7 There are already inequalities in access to AP pro-\ngrams to students, with students in suburban and mid-\ndle- and upper-socioeconomic classes generally\nhaving the most access to AP course offerings. How\nshould college admissions personnel use the informa-\ntion on student participation in the AP program with-\nout exacerbating inequalities in AP offerings\n\u00b7\n\u00b7 Given evidence that English language learners some-\ntimes take the AP test that corresponds to their native\nlanguage (College Board, 2014b; Kanno & Kangas,\ntaking these AP language tests receive any academic\nbenefits? Should state education agencies or universi-\nties incentivize this behavior by granting credit or\nsubsidizing exam fees?\n\u00b7\n\u00b7 Who are the students who take AP exams before the\nninth grade? Are they similar to students who take the\nSAT and ACT in their early adolescence (see\nWarne et al., 2016)? Why do they take AP exams\nbefore enrolling in high school? How do they gain\naccess to AP courses and tests outside of a high school\nsetting?\n\u00b7\n\u00b7 Why is the female-to-male ratio of examinees so\nimbalanced for some AP exams, while the same tests\nhave a more equal sex ratio among students who earn\na score of 3 or higher? What variables predict males'\nand females' decision to self-select in sex stereotyped\nAP courses and to choose to take AP exams?\nCausal questions.Armed with the information derived from\ndescriptive questions, researchers are better able to investi-\ngate causal questions to learn information about the magni-\ntude of the benefits of the AP program. These questions\ninclude the following.\n\u00b7\n\u00b7 Some researchers have found that students do not\nreceive academic benefits from an AP course unless\nthey take the AP test (Ackerman et al., 2013; Geiser &\net al., 2015). But, do students in AP courses who do\nnot take the AP test gain some benefits from their AP\ncourses? How does this compare to what students\nlearn in non-AP courses? Are there perhaps improve-\nments in study skills and other non-achievement vari-\nables that may make enrolling in an AP course\nvaluable to students?\n\u00b7\n\u00b7 Ackerman et al. (2013) investigated whether certain\ncombinations of AP tests (called \"portfolios\") are bet-\nter predictors of college success than others. Are some\nportfolios more helpful for preparing students for col-\nlege than other portfolios? Do complementary portfo-\nlios (e.g., a mix of science and humanities courses)\nincrease students' chances of college success, or do\nstudents receive better college preparation with port-\nfolios of AP courses/tests that are more focused on\ntheir college major?\n\u00b7\n\u00b7 Jeong (2009) found that state policies that provide AP\nexam fee subsidies to low-income students encour-\naged more of these students to take AP exams. The\nimpact of other state policies, though, is unclear. How\ndo government policies--such as mandating that state\nuniversities accept AP credit, or that public high\nschools offer AP courses, or the use of AP data for\naccountability--impact student behavior?\n\u00b7\n\u00b7 In states with generous AP fee subsidies, the AP pro-\ngram is not cost-effective for taxpayers (Dendy et al.,\nmore common, modest financial incentives (e.g., only\npaying AP exam fees for low-income students or for\nstudents who have a high probability of passing the\nAP exam) cost-effective? Educational economists\nshould also perform a comprehensive cost-benefit\nanalysis to determine whether the economic and non-\neconomic benefits of AP outweigh the financial and\nopportunity costs of the program.\n\u00b7\n\u00b7 The policy of offering a large number of AP courses to\nall students who wish to take them is highly popular,\nbut this \"AP for all\" policy has critics (Blagaich,\nalternative policies (such a structured sequence of\ncourses, or a merit-based financial aid) could serve as\nmodels for AP policies that would improve the effi-\nciency and the effectiveness of the AP program?\nConcluding Thoughts\nSince the 1950s,AP has grown from an obscure program serv-\ning a small number of elite high schools to the most common\nway that high school students receive an advanced curriculum\neducational research on the AP program is preliminary and\nmostly undeveloped. In this article, I took stock of the past\nresearch on the AP program by presenting information on the\nhigh-quality psychometrics behind AP tests, the more positive\nacademic outcomes for AP students (compared with non-AP\nstudents), and recent progress that researchers have made\ntoward estimating the causal impact of the AP program.\nIn this article, I also looked forward to future research by\nshowing readers the many potential avenues of research that\nare available to educational scholars trying to understand the\nAP program better. There are five ways that I have suggested\nthat researchers look forward in their research: (a) methodolog-\nically, (b) with an eye on course variability, (c) by examining\nnew student populations, (d) studying the AP program for new\npurposes, and (e) with new research questions. It is my hope\nthat more researchers study the AP program so that students,\nteachers, parents, school personnel, and university personnel\ncan make more informed decisions about AP classes and tests.\nNevertheless, it is important to recognize that using this\narticle as a blueprint for future AP research will not fix all the\nscientific, policy, and educational problems associated with\nthe AP program. I am particularly cognizant, for example, of\nthe tension between reducing the inequality of access to AP\ncourses and the suggestions I have made about pre-requisites\nfor AP courses or steering resources into non-AP remediation\nprograms. And, methodologically, there is no guarantee that\nincorporating propensity score modeling, HLM, cluster-level\nRCTs, or other sophisticated statistical models will cure the\nproblems that have plagued AP research. One anonymous\nreviewer of the article suggested that these methodologies\ncould merely replace old problems in AP research with new\nproblems. She or he may be right. Despite these shortcomings,\nI believe that moving researchers to recognize variability\namong AP courses, investigating new populations and new\nquestions, and using new methodologies will at least advance\nthe knowledge about the AP program beyond its current state.\nThere is so much about this highly popular academic program\nthat researchers, parents, students, and school personnel do not\nunderstand. If researchers implement some of these sugges-\ntions, then perhaps America's national ignorance about AP's\neffectiveness can be reduced. I believe that it is certainly worth\ntrying, and I look forward to the new research that educational\nscholars produce in the next few decades.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) disclosed receipt of the following financial support\nfor the research, authorship, and/or publication of this article: The\narticle processing charge for this article was paid by the College of\nHumanities and Social Sciences, Utah Valley University.\nNotes\n1. Tracking longitudinal growth over time would require an addi-\ntional level in a hierarchical linear modeling (HLM) model,\nwhich would increase the complexity of the statistical model.\n2. It is also important to note the difference in calculating these\nparticipation rates. The Utah data linked every student's course\nenrollment data with their Advanced Placement (AP) test data.\nThe West Virginia data are less detailed and consist of two\nseparate pieces of data: the number of students enrolled in\neach AP course and the number of students who took each AP\nexam. Because it is possible that some students take an AP\nexam without enrolling in the corresponding AP test (Sadler\ntion probably overestimates participation rates.\n3. The stronger correlation at the nationwide level could be due\nto the larger number of tests in the nationwide data or the con-\nversion of the national data to ranks to calculate a Spearman's\n value. This latter change makes the data for both variables\nto have a uniform distribution; variables with similarly shaped\ndistributions tend to have stronger correlations (Thompson,\nReferences\nAckerman, P. L., Kanfer, R., & Calderwood, C. (2013). High\nschool Advanced Placement and student performance in col-\nlege: STEM majors, non-STEM majors, and gender differ-\nAmerican Educational Research Association, American\nPsychological Association, & National Council on\nMeasurement in Education. (2014). Standards for educa-\ntional and psychological testing. Washington, DC: American\nEducational Research Association.\nBarbour, M., & Mulcahy, D. (2006). An inquiry into retention and\nachievement differences in campus based and web based AP\nBarnard-Brak, L., McGaha-Garnett, V., & Burley, H. (2011).\nAdvanced Placement course enrollment and school-\nBarnett, L. B., & Durden, W. G. (1993). Education patterns of aca-\nBlagaich, P. (1999). Advanced Placement courses are not for every-\nBleske-Rechek, A., Lubinski, D., & Benbow, C. P. (2004). Meeting\nthe educational needs of special populations: Advanced\nPlacement's role in developing exceptional human capital.\nBrennan, R. L. (2010). Evidence-centered assessment design and\nthe Advanced Placement program: A psychometrician's per-\nBurney, V. H. (2010). High achievement on Advanced Placement\nexams: The relationship of school-level contextual fac-\nCamara, W. J., & Michaelides, M. (2005). AP\u00ae use in admis-\nsions: A response to Geiser and Santelices. Retrieved from\nhttp://research.collegeboard.org/sites/default/files/publica-\npdf\nCamara, W. J., & Millsap, R. (1998). Using the PSAT/NMSQT and\ncourse grades in predicting success in the Advanced Placement\nprogram. New York, NY: College Board.\nCampbell,D.M.,Brown,R.,&Perry,E.(2009).TrendsinAdvanced\nPlacement science and mathematics test-taking among female\nstudents in California: A latent variable approach. Electronic\nChajewski, M., Mattern, K. D., & Shaw, E. J. (2011). Examining\nthe role of Advanced Placement exam participation in 4-year\ncollege enrollment. Educational Measurement: Issues and\nChamberlain, P. C., Pugh, R. C., & Schellhammer, J. (1978). Does\nAdvanced Placement continue throughout the undergraduate\nCisneros, J., Holloway-Libell, J., Gomez, L. M., Corley, K. M.,\n& Powers, J. M. (2014). The Advanced Placement opportu-\nnity gap in Arizona: Access, participation, and success. AASA\nClinedinst, M. E., Hurley, S. F., & Hawkins, D. A. (2012). State of\ncollege admission 2012. Arlington, VA: NACAC.\nCollege Board. (2001). Access to excellence: A report of the com-\nmission on the future of the Advanced Placement Program\u00ae.\nRetrieved from http://research.collegeboard.org/sites/default/\nment.pdf\nCollege Board. (2014a). AP score distributions for specific student\ngrade-level groups. Retrieved from http://media.collegeboard.\nGradeLevel.pdf\nCollege Board. (2014b). National report. Retrieved from http://\nmedia.collegeboard.com/digitalServices/pdf/research/2014/\nNational_Summary.xlsx\nCollege Board. (2014c). The 10th annual AP report to the nation.\nRetrieved from http://media.collegeboard.com/digitalServices/\npdf/ap/rtn/10th-annual/10th-annual-ap-report-to-the-nation-\nsingle-page.pdf\nCollege Board. (2015). AP program size and increments (by year).\nRetrieved from https://secure-media.collegeboard.org/digi-\nCone, J. K. (1992). Untracking Advanced Placement English:\nCreating opportunity is not enough. Phi Delta Kappan, 73,\nde la Varre, C., Irvin, M. J., Jordan, A. W., Hannum, W. H., &\nFarmer, T. W. (2014). Reasons for student dropout in an online\nDendy, E., Graf, S., Hartley, S., & Smith, N. (2006). Acceleration\nprograms provide benefits but the costs are relatively expen-\nsive (OPPAGA Report No. 06-24). Tallahassee: Florida Office\nof Program Policy Analysis & Government Accountability.\nDougherty, C., & Mellor, L. T. (2010). Preparing students for\nAdvanced Placement: It's a pre-K issue. In P. M. Sadler, G.\nSonnert, R. H. Tai, & K. Klopfenstein (Eds.), AP: A critical\nexamination of the Advanced Placement program (pp. 219-\n232). Cambridge, MA: Harvard Education Press.\nEducation Commission of the States. (2015). Advanced Placement\npolicies: All state profiles. Retrieved from http://ecs.force.\ncom/mbdata/mbprofgroupall?Rep=APA\nEwing, M., Camara, W. J., & Millsap, R. E. (2006). The relation-\nship between PSAT/NMSQT\u00ae scores and AP\u00ae examination\ngrades: A follow-up study. New York, NY: College Board.\nEwing, M., Huff, K., & Kaliski, P. (2010). Validating AP exam\nscores. In P. M. Sadler, G. Sonnert, R. H. Tai, & K. Klopfenstein\n(Eds.), AP: A critical examination of the Advanced Placement\nprogram (pp. 85-105). Cambridge, MA: Harvard Education\nPress.\nFan, X., & Nowell, D. L. (2011). Using propensity score match-\ning in educational research. Gifted Child Quarterly, 55, 74-79.\nFarkas, S., & Duffett, A. (2009). Growing pains in the Advanced\nPlacement program: Do tough trade-offs lie ahead?\nWashington, DC: Thomas B. Fordham Institute.\nFlowers, L. A. (2008). Racial differences in the impact of participat-\ning in Advanced Placement programs on educational and labor\nGall, M. D., Gall, J. P., & Borg, W. R. (2007). Educational research:\nAn introduction (8th ed.). Boston, MA: Pearson.\nGallagher, S. A. (2009). Myth 19: Is Advanced Placement an ade-\nquate program for gifted students? Gifted Child Quarterly, 53,\nGeiser, S., & Santelices, V. (2004). The role of Advanced Placement\nand honors courses in college admissions (Research &\nOccasional Paper Series: CSHE.4.04). Berkeley, CA: Center\nfor Studies in Higher Education.\nGoldstein, H. (1994). Multilevel cross-classified models.\nGray, P. T., Jr., Hildebrant, B. S., & Strauss, T. R. (2006). Advanced\nPlacement human geography: The first five years. Journal of\nGriffith, A. L. (2010). Persistence of women and minorities in\nSTEM field majors: Is it the school that matters? Economics\nGuo, S., & Fraser, M. W. (2010). Propensity score analysis:\nStatistical methods and applications. Thousand Oaks, CA:\nSage.\nHallett, R. E., & Venegas, K. M. (2011). Is increased access\nenough? Advanced Placement courses, quality, and success in\nlow-income urban schools. Journal for the Education of the\nHansen, K., Reeve, S., Gonzalez, J., Sudweeks, R. R., Hatch, G.\nL., Esplin, P., & Bradshaw, W. S. (2006). Are Advanced\nPlacement English and first-year college composition equiva-\nlent? A comparison of outcomes in the writing of three groups\nof sophomore college students. Research in the Teaching of\nHox, J. (2002). Multilevel analysis: Techniques and applications.\nMahwah, NJ: Lawrence Erlbaum.\nIatarola, P., Conger, D., & Long, M. C. (2011). Determinants of high\nschools'advancedcourseofferings.EducationalEvaluationand\nJackson, C. K. (2010). A little now for a lot later: A look at a Texas\nAdvanced Placement Incentive Program. Journal of Human\nJeong, D. W. (2009). Student participation and performance\non Advanced Placement exams: Do state-sponsored incen-\ntives make a difference? Educational Evaluation and Policy\nKanno, Y., & Kangas, S. E. N. (2014). \"I'm not going to be,\nlike, for the AP\": English language learners' limited access\nto advanced college-preparatory courses in high school.\nKeng, L., & Dodd, B. G. (2008). A comparison of college perfor-\nmances of AP and non-AP student groups in 10 subject areas.\nNew York, NY: College Board.\nKlopfenstein, K. (2004). The Advanced Placement expansion of\nthe 1990s: How did traditionally underserved students fare?\nKlopfenstein, K. (2010). Does the Advanced Placement program\nsave taxpayers money? The effect of AP participation on time\nto college graduation. In P. M. Sadler, G. Sonnert, R. H. Tai,\n& K. Klopfenstein (Eds.), AP: A critical evaluation of the\nHarvard Education Press.\nKlopfenstein, K., & Lively, K. (2016). Do grade weights promote\nmore advanced course-taking? Education Finance and Policy,\nKlopfenstein, K., & Thomas, M. K. (2009). The link between\nAdvanced Placement experience and early college success.\nKlopfenstein, K., & Thomas, M. K. (2010). Advanced Placement\nparticipation: Evaluating the policies of states and colleges. In\nP. M. Sadler, G. Sonnert, R. H. Tai, & K. Klopfenstein (Eds.),\nAP: A critical examination of the Advanced Placement Program\nKlugman, J. (2013). The Advanced Placement arms race and the\nreproduction of educational inequality. Teachers College\nKramer, D. A., II. (2016). Examining the impact of state level merit-\naid policies on Advanced Placement participation. Journal of\nLacy, T. (2010). Examining AP: Access, rigor, and revenue in the\nhistory of the Advanced Placement Program. In P. M. Sadler,\nG. Sonnert, R. H. Tai, & K. Klopfenstein (Eds.), AP: A critical\nexamination of the Advanced Placement Program (pp. 17-48).\nCambridge, MA: Harvard Education Press.\nLang, D. M. (2007). Class rank, GPA, and valedictorians: How high\nschools rank students. American Secondary Education, 35(2),\nLichten, W. (2000). Whither Advanced Placement? Education\nLichten, W. (2010). Whither Advanced Placement--now? In P. M.\nSadler, G. Sonnert, R. H. Tai, & K. Klopfenstein (Eds.), AP:\nA critical examination of the Advanced Placement program\nLong, M. C., Conger, D., & Iatarola, P. (2012). Effects of high\nschool course-taking on secondary and postsecondary suc-\nLukhele, R., Thissen, D., & Wainer, H. (1994). On the relative\nvalue of multiple-choice, constructed response, and examinee-\nselected items on two achievement tests. Journal of Educational\nLuo, W., & Kwok, O.-m. (2012). The consequences of ignoring\nindividuals' mobility in multilevel growth models: A Monte\nCarlo Study. Journal of Educational and Behavioral Statistics,\nMattern, K. D., Marini, J. P., & Shaw, E. J. (2013). Are AP\u00ae stu-\ndents more likely to graduate from college on time? New York,\nNY: College Board.\nMattern, K. D., Shaw, E. J., & Ewing, M. (2011). Advanced\nPlacement exam participation: Is AP exam participation and\nperformance related to choice of college major? New York,\nNY: College Board.\nMattern, K. D., Shaw, E. J., & Kobrin, J. L. (2011). An alterna-\ntive presentation of incremental validity: Discrepant SAT\nand HSGPA performance. Educational and Psychological\nMattern, K. D., Shaw, E. J., & Xiong, X. (2009). The relationship\nbetween AP\u00ae exam performance and college outcomes. New\nYork, NY: College Board.\nMcCoach, D. B. (2010). Hierarchical linear modeling. In G. R.\nHancock & R. O. Mueller (Eds.), The reviewer's guide to\nquantitative methods in the social sciences (pp. 123-140). New\nYork, NY: Routledge.\nMcCoach, D. B., & Adelson, J. L. (2010). Dealing with dependence\n(part I): Understanding the effects of clustered data. Gifted\nMcKillip, M. E. M., & Rawls, A. (2013). A closer examination of\nthe academic benefits of AP. Journal of Educational Research,\nMeckna, S. H. (1999). Teaching Advanced Placement European\nhistory in a multi-ethnic urban setting. The History Teacher,\nMelican, C., Debebe, F., & Morgan, R. (1997). Comparing AP and\ncollege student learning of economics. Journal of Economic\nMo, L., Yang, F., Hu, X., Calaway, F., & Nickey, J. (2011). ACT\ntest performance by Advanced Placement students in Memphis\nMoore, G. W., & Slate, J. R. (2008). Who's taking the Advanced\nPlacement courses and how are they doing: A statewide two-year\nMorgan, R., & Klaric, J. (2007). AP students in college: An analysis\nof five-year academic careers. New York, NY: College Board.\nMurphy, D., & Dodd, B. (2009). A comparison of college perfor-\nmance of matched AP and non-AP student groups. New York,\nNY: College Board.\nOlszewski-Kubilius, P., & Lee, S.-Y. (2011). Gender and other\ngroup differences in performance on off-level tests: Changes\nPatterson, B. F. (2009). Advanced Placement statistics students'\neducation choices after high school. New York, NY: College\nBoard.\nPatterson, B. F., Packman, S., & Kobrin, J. L. (2011). Advanced\nPlacement exam-taking and performance: Relationships with\nfirst-year subject area college grades. New York, NY: College\nBoard.\nPushkin, D. B. (1995). The AP exam and the introductory college\nSadler, P. M. (2010). Key findings. In P. M. Sadler, G. Sonnert, R.\nH. Tai, & K. Klopfenstein (Eds.), AP: A critical examination of\nMA: Harvard Education Press.\nSadler, P. M., & Sonnert, G. (2010). High school Advanced\nPlacement and success in college coursework in the sciences. In\nP. M. Sadler, G. Sonnert, R. H. Tai, & K. Klopfenstein (Eds.),\nAP: A critical examination of the Advanced Placement Program\nSadler, P. M., Sonnert, G., Hazari, Z., & Tai, R. (2014). The role of\nadvanced high school coursework in increasing STEM career\nSadler, P. M., & Tai, R. H. (2007a). Accounting for advanced high\nschool coursework in college admission decisions. College and\nSadler, P. M., & Tai, R. H. (2007b). Weighting for recognition:\nAccounting for Advanced Placement and honors courses when\ncalculating high school grade point average. NASSP Bulletin,\nSchneider, J. (2009). Privilege, equity, and the Advanced Placement\nprogram: Tug of war. Journal of Curriculum Studies, 41, 813-\nSchumacker, R. E. (2014). Graduation completion amongst IB\nand AP students in postsecondary education. Multiple Linear\nShaw, E. J., Marini, J. P., & Mattern, K. D. (2013). Exploring the\nutility of Advanced Placement participation and performance\nin college admission decisions. Educational and Psychological\nSikes, E., Underhill, B., & West, M. (2009). Modifying Advanced\nPlacement program incentive funding could produce signifi-\ncant cost savings (Report No. 09\u00ad12). Tallahassee, FL: Office\nof Program Policy Analysis & Government Accountability.\nSkidmore, S. T., & Thompson, B. (2012). Propagation of misinforma-\ntion about frequencies of RFTs/RCTs in education. Educational\nSteiner, P. M., Cook, T. D., Shadish, W. R., & Clark, M. H. (2010).\nThe importance of covariate selection in controlling for selec-\ntion bias in observational studies. Psychological Methods, 15,\nStricker, L. J., & Emmerich, W. (1999). Possible determinants of\ndifferential item functioning: Familiarity, interest, and emo-\ntional reaction. Journal of Educational Measurement, 36, 347-\nTai, R. H. (2008). Posing tougher questions about the Advanced\nTai, R. H., Liu, C. Q., Almarode, J. T., & Fan, X. (2010). Advanced\nPlacement course enrollment and long-range educational out-\ncomes. In P. M. Sadler, G. Sonnert, R. H. Tai, & K. Klopfenstein\n(Eds.), AP: A critical examination of the Advanced Placement\nPress.\nThompson, B. (2006). Foundations of behavioral statistics: An\ninsight-based approach. New York, NY: Guilford Press.\nWarne, R. T. (2014). Using above-level testing to track growth\nin academic achievement in gifted students. Gifted Child\nWarne, R. T. (2016). Testing Spearman's hypothesis with\nAdvanced Placement examination data. Intelligence, 57, 87-\nWarne, R. T., & Anderson, B. (2015). The Advanced Placement\nprogram's impact on academic achievement. New Educational\nWarne, R. T., Doty, K. J., Malbica, A. M., Angeles, V. R., Innes,\nS., Hall, J., & Masterson-Nixon, K. (2016). Above-level\ntest item functioning across examinee age groups. Journal\nWarne, R. T., Larsen, R., Anderson, B., & Odasso, A. J. (2015).\nThe impact of participation in the Advanced Placement pro-\ngram on students' college admissions test scores. The Journal\nWarne, R. T., Li, Y., McKyer, E. L. J., Condie, R., Diep, C.\nS., & Murano, P. S. (2012). Managing clustered data\nusing hierarchical linear modeling. Journal of Nutrition\nWarne, R. T., Nagaishi, C., Slade, M. K., Hermesmeyer, P., &\nPeck, E. K. (2014). Comparing weighted and unweighted grade\npoint averages in predicting college success of diverse and\nWest Virginia Department of Education. (n.d.). Courses taught in\nWest Virginia. Retrieved from http://wveis.k12.wv.us/nclb/\nWimmers, E., & Morgan, R. (1990). Comparing the performance of\nhigh school and college students on the Advanced Placement\nFrench language examination. The French Review, 63, 423-\nWinters, M. A., & Cowen, J. M. (2013). Who would stay, who\nwould be dismissed? An empirical consideration of value-\nadded teacher retention policies. Educational Researcher, 42,\nWyatt, J. N., & Mattern, K. D. (2011). Low-SES students and col-\nlege outcomes: The role of AP\u00ae fee reductions. New York,\nNY: College Board.\nYanovitzky, I., Zanutto, E., & Hornik, R. (2005). Estimating causal\neffects of public health education campaigns using propensity\nscore methodology. Evaluation and Program Planning, 28,\nAuthor Biography\nRussell T. Warne is an assistant professor of psychology in the\nDepartment of Behavioral Science at Utah Valley University. His\nresearch interests focus on advanced academics, human intelli-\ngence, and quantitative methods. He is the author of Statistics for\nthe Social Sciences: A General Linear Model Framework, which\nwill be published by Cambridge University Press in 2017."
}