{
    "abstract": "Abstract\nIs Big Data science a whole new way of doing research? And what difference does data quantity make to knowledge\nproduction strategies and their outputs? I argue that the novelty of Big Data science does not lie in the sheer quantity of\ndata involved, but rather in (1) the prominence and status acquired by data as commodity and recognised output, both\nwithin and outside of the scientific community and (2) the methods, infrastructures, technologies, skills and knowledge\ndeveloped to handle data. These developments generate the impression that data-intensive research is a new mode of\ndoing science, with its own epistemology and norms. To assess this claim, one needs to consider the ways in which data\nare actually disseminated and used to generate knowledge. Accordingly, this article reviews the development of sophis-\nticated ways to disseminate, integrate and re-use data acquired on model organisms over the last three decades of work\nin experimental biology. I focus on online databases as prominent infrastructures set up to organise and interpret such\ndata and examine the wealth and diversity of expertise, resources and conceptual scaffolding that such databases draw\nupon. This illuminates some of the conditions under which Big Data needs to be curated to support processes of\ndiscovery across biological subfields, which in turn highlights the difficulties caused by the lack of adequate curation for\nthe vast majority of data in the life sciences. In closing, I reflect on the difference that data quantity is making to\ncontemporary biology, the methodological and epistemic challenges of identifying and analysing data given these devel-\nopments, and the opportunities and worries associated with Big Data discourse and methods.\n",
    "reduced_content": "Original Research Article\nWhat difference does quantity make?\nOn the epistemology of Big Data\nin biology\nS Leonelli\n Keywords\nBig Data epistemology, data-intensive science, biology, databases, data infrastructures, data curation, model organisms\nIntroduction\nBig Data has become a central aspect of contemporary\nscience and policy, due to a variety of reasons that\ninclude both techno-scientific factors and the political\nand economic roles played by this terminology. The\nidea that Big Data is ushering in a whole new way of\nthinking, particularly within the sciences, is rampant \u00ad\nas exemplified by the emergence of dedicated funding,\npolicies and publication venues (such as this journal).\nThis is at once fascinating and perplexing to scholars\ninterested in the history, philosophy and social studies\nof science. On the one hand, there seems to be some-\nthing interesting and novel happening as a consequence\nof Big Data techniques and communication strategies,\nwhich is, however, hard to capture with traditional\nnotions, such as `induction' and `data-driven' science\n(partly because, as philosophers of science have long\nshown, there is no such thing as direct inference from\ndata, and data interpretation typically involves the use\nof modelling techniques and various other kinds of con-\nceptual and material scaffolding).1 On the other hand,\nmany sciences have a long history of dealing with large\nquantities of data, whose size and scale vastly outstrip\navailable strategies and technologies for data collection,\ndissemination and analysis (Gitelman, 2013). This is\nUniversity of Exeter, UK\nCorresponding author:\nS Leonelli, University of Exeter, Byrne House, St Germans Road, Exeter\nEmail: S.Leonelli@exeter.ac.uk\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://\nwww.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (http://www.uk.sagepub.com/aboutus/open-\naccess.htm).\nparticularly evident in the life sciences, where data-\ngathering practices in subfields, such as natural history\nand taxonomy, have been at the heart of inquiry since\nthe early modern era, and have generated problems\n\u00a8 ller-Wille and\nSo what is actually new here? How does Big Data\nscience differ from other forms of inquiry, what can and\ncannot be learnt from Big Data, and what difference\ndoes quantity make? In this article, I discuss some of\nthe central characteristics typically associated with Big\nData, as conveniently summarised within the recent\nbook Big Data by Mayer-Scho\n\u00a8 nberger and Cukier\n(2013), and I scrutinise their plausibility in the case of\nbiological research. I then argue that the novelty of Big\nData science does not lie in the sheer quantity of data\ninvolved, though this certainly makes a difference to\nresearch methods and results. Rather, the novelty of\nBig Data science lies in (1) the prominence and status\nacquired by data as scientific commodity and recog-\nnised output both within and beyond the sciences and\n(2) the methods, infrastructures, technologies and skills\ndeveloped to handle (format, disseminate, retrieve,\nmodel and interpret) data. These developments gener-\nate the impression that data-intensive research is a\nwhole new mode of doing science, with its own epis-\ntemology and norms. I here defend the idea that in\norder to understand and critically evaluate this\nclaim, one needs to analyse the ways in which data\nare actually disseminated and used to generate know-\nledge, which I refer to as `data journeys'; and I consider\nthe extent to which the current handling of Big Data\nfosters and validates its use as evidence towards new\ndiscoveries.2\nAccordingly, the bulk of this article reviews the\ndevelopment of sophisticated ways to disseminate, inte-\ngrate and re-use data acquired on model organisms,\nsuch as the small plant Arabidopsis thaliana, the nema-\ntode Caenorhabditis elegans and the fruit-fly Drosophila\nmelanogaster (including data on their ecology, metab-\nolism, morphology and relations to other species) over\nthe last three decades of work in experimental biology.\nI focus on online databases as a key example of infra-\nstructures set up to organise and interpret such data;\nand on the wealth and diversity of expertise, resources\nand conceptual scaffolding that such databases draw\nupon in order to function well. This analysis of data\njourneys through model organism databases illumin-\nates some of the conditions under which the evidential\nvalue of data posted online can be assessed and inter-\npreted by researchers wishing to use those data to foster\ndiscovery. At the same time, model organism biology\nhas been one of the best funded scientific areas over the\nlast three decades, and the curation of data produced\ntherein has benefited from much more attention and\ndedicated investments than data generated in the rest\nof the life sciences and biomedicine. Considering the\nchallenges encountered in disseminating this type of\ndata thus also highlights the potential problems\ninvolved in assembling data that have not received\ncomparable levels of care (i.e. the vast majority of bio-\nlogical data).\nIn my conclusions, I use these findings to inform a\ncritique of the supposed revolutionary power of Big\nData science. In its stead, I propose a less sensational,\nbut arguably more realistic, reflection on the difference\nthat data quantity is making to contemporary bio-\nlogical research, which stresses both continuities with\nand dissimilarities from previous attempts to handle\nlarge datasets. I also suggest that the natural sciences\nmay well be the area that is least affected by Big Data,\nwhose emergence is much more likely to affect the pol-\nitical and economic realms \u00ad though not necessarily for\nthe better.\nThe novelty of Big Data\nI will start by considering three ideas that, according to\nMayer-Scho\n\u00a8 nberger and Cukier (2013) among others,\nconstitute core innovations brought in by the advent of\nBig Data in all realms of human activity, including sci-\nence. The first idea is what I shall label comprehensive-\nness. This is the claim that the accumulation of large\ndatasets enables scientists to ground their analysis on\nseveral different aspects of the same phenomenon,\ndocumented by different people at different times.\nAccording to Mayer-Scho\n\u00a8 nberger and Cukier, data\ncan become so big as to encompass all the available\ndata on a phenomenon of interest. As a consequence,\nBig Data can provide a comprehensive perspective on\nthe characteristics of that phenomenon, without need-\ning to focus on specific details.\nThe second idea is that of messiness. Big Data, it is\nargued, pushes researchers to embrace the complex and\nmultifaceted nature of the real world, rather than pur-\nsuing exactitude and accuracy in measurement obtained\nunder controlled conditions. Indeed, it is impossible to\nassemble Big Data in ways that are guaranteed to be\naccurate and homogeneous. Rather, we should resign\nourselves to the fact that `Big Data is messy, varies in\nquality, and is distributed across countless servers\naround the world' (Mayer-Scho\n\u00a8 nberger and Cukier,\nexactitude: `With Big Data, we'll often be satisfied with\na sense of general direction rather than knowing a phe-\nnomenon down to the inch, the penny, the atom'\n(Mayer-Scho\nThe idea of messiness relates closely to the third\nkey innovation brought about by Big Data, which\nMayer-Scho\n\u00a8 nberger and Cukier call the `triumph of\n2 Big Data & Society\ncorrelations'. Correlations, defined as the statistical\nrelationship between two data values, are notoriously\nuseful as heuristic devices within the sciences. Spotting\nthe fact that when one of the data values changes the\nother is likely to change too is the starting point for\nmany discoveries. However, scientists have typically\nmistrusted correlations as a source of reliable know-\nledge in and of themselves, chiefly because they may\nbe spurious \u00ad either because they result from serendip-\nity rather than specific mechanisms or because they are\ndue to external factors. Big Data can override those\nworries. Mayer-Scho\ngive the example of Amazon.com, whose astonishing\nexpansion over the last few years is at least partly due\nto their clever use of statistical correlations among the\nmyriad of data provided by their consumer base in\norder to spot users' preferences and successfully suggest\nnew items for consumption. In cases such as this, cor-\nrelations do indeed provide powerful knowledge that\nwas not available before. Hence, Big Data encourages\na growing respect for correlation, which comes to be\nappreciated as not only a more informative and plau-\nsible form of knowledge than the more definite but also\na more elusive, causal explanation. In the words of\nMayer-Scho\ntions may not tell us precisely why something is hap-\npening, but they alert us that it is happening. And in\nmany situations this is good enough'.\nThese three ideas have two important corollaries,\nwhich shall constitute the main target of my analysis\nin this article. The first corollary is that Big Data makes\nreliance on small sampling, and even debates over sam-\npling, unnecessary. This again seems to make sense\nprima facie: if we have all the data about a given phe-\nnomenon, what is the point of pondering which types of\ndata might best document it? Rather, one can now skip\nthat step and focus instead on assembling and analysing\nas much data as possible about the phenomenon of\ninterest, so as to generate reliable knowledge about it:\n`Big Data gives us an especially clear view of the granu-\nlar; subcategories and submarkets that samples can't\nassess' (Mayer-Scho\nThe second corollary is that Big Data is viewed,\nthrough its mere existence, as countering the risk of\nbias in data collection and interpretation. This is\nbecause having access to large datasets makes it more\nlikely that bias and error will be automatically elimi-\nnated from the system, for instance via what sociolo-\ngists and philosophers call `triangulation': the tendency\nof reliable data to cluster together, so that the more\ndata one has, the easier it becomes to cross-check\nthem with each other and eliminate the data that look\nOver the next few sections, I show how an empirical\nstudy of how Big Data biology operates puts both of\nthese corollaries into question, which in turn comprom-\nises the plausibility of the three claims that Mayer-\nScho\n\u00a8 nberger and Cukier make about the power of\nBig Data \u00ad at least when they are applied to the\nrealm of scientific inquiry. Let me immediately state\nthat I do not intend this analysis to deny the wide-\nspread attraction that these three ideas are generating\nin many spheres of contemporary society (most obvi-\nously, big government) and which is undoubtedly mir-\nrored in the ways in which biological research has been\nre-organised since at least the early 2000s (which is\nwhen technologies for the high-throughput production\nof genomic data, such as sequencing machines, started\nto become widely used). Rather, I wish to shed some\nclarity on the gulf that separates the hyperbolic claims\nmade about the novelty of Big Data science from the\nchallenges, problems and achievements characterising\ndata-handling practices in the everyday working life\nof biologists \u00ad and particularly the ways in which new\ncomputational and communication technologies such\nas online databases are being developed so as to trans-\nform these ideas into reality.\nBig Data journeys in biology\nFor scientists to be able to analyse Big Data, those data\nhave to be collected and assembled in ways that make it\nsuitable to consider them as a single body of informa-\ntion (O'Malley and Soyer, 2012). This is a particularly\ndifficult task in the case of biological data, given the\nhighly fragmented and pluralist history of the field.\nFor a start, there are myriads of epistemic communities\nwithin the life sciences, each of which uses a different\ncombination of methods, locations, materials, back-\nground knowledge and interest to produce data.\nFurthermore, there are vast differences in the types of\ndata that can be produced and the phenomena that can\nbe targeted. And last but not least, the organisms and\necosystems on which data are being produced are both\nhighly variable and highly unstable, given their con-\nstant exposure to both developmental and evolutionary\nchange. Given this situation, a crucial question within\nBig Data science concerns how one can bring such dif-\nferent data types, coming from a variety of sources,\nunder the same umbrella.\nTo address this question, my research over the last\neight years has focused on documenting and analysing\nthe ways in which biological data \u00ad and particularly\n`omics' data, the quintessential form of `Big Data' in\nthe life sciences \u00ad travel across research contexts, and\nthe significant conceptual and material scaffolding used\nby researchers to achieve this. For the purposes of this\narticle, I shall now focus on one case of Big Data hand-\nling in biology, which is arguably among the most\nsophisticated and successful attempts made to integrate\nLeonelli 3\nvast quantities of data of different types within this\nfield for the purposes of advancing future knowledge\nproduction. This is the development of model organ-\nbases were built with the immediate goal of storing\nand disseminating genomic data in a formalised\nmanner, and the long-term vision of (1) incorporating\nand integrating any data available on the biology of\nthe organism in question within a single resource,\nincluding data on physiology, metabolism and even\nmorphology; (2) allowing and promoting cooperation\nwith other community databases so that the available\ndatasets would eventually be comparable across spe-\ncies; and (3) gathering information about laboratories\nworking on each organism and the associated experi-\nmental protocols, materials and instruments, thus pro-\nviding a platform for community building. Particularly\nuseful and rich examples include FlyBase, dedicated to\nD. melanogaster; WormBase, focused on C. elegans;\nand The Arabidopsis Information Resource, gathering\ndata on A. thaliana. At the turn of the 21st century,\nthese were arguably among most sophisticated com-\nmunity databases within biology. They have played\na particularly significant role in the development\nof online data infrastructures in this area and continue\nto serve as reference points for the construction\nof other databases to this day (Leonelli and\nAnkeny, 2012). They therefore represent a good\ninstance of infrastructure explicitly set up to support\nand promote Big Data research in experimental\nbiology.\nIn order to analyse how these databases enable data\njourneys, I will distinguish between three stages of data\ntravel, and briefly describe the extent to which database\ncurators are involved in their realisation.\nStage 1: De-contextualisation\nOne of the main tasks of database curators is to de-\ncontextualise the data that are included in their\nresources, so that they can travel outside of their ori-\nginal production context and become available for inte-\ngration with other datasets (thus forming a Big Data\ncollection). The process of de-contextualisation\ninvolves making sure that data are formatted in ways\nthat make them compatible with datasets coming from\nother sources, so that they are easy to analyse by\nresearchers who see them for the first time. Given the\nabove-mentioned fragmentation and diversity of data\nproduction processes to be found within biology, there\ntends to be no agreement on formatting standards for\neven the most common of data types (such as metabo-\nlomics data, for instance; Leonelli et al., 2013). As a\nresult, database curators often need to assess how to\ndeal with specific datasets on a one-to-one basis.\nDespite constant advances, it is still impossible to auto-\nmate the de-contextualisation of most types of bio-\nlogical data.\nFormatting data to ensure that they can all be ana-\nlysed as a unique body of evidence is thus exceedingly\nlabour-intensive, and requires the development of\ndatabases with long-term funding and enough person-\nnel to make sure that data submission and formatting\nis carried out adequately. Setting up such resources is\nan expensive business. Indeed, debate keeps raging\namong funding agencies about who is responsible for\nmaintaining these infrastructures. Many model organ-\nism databases have struggled to attract enough fund-\ning to support their de-contextualisation activities.\nHence, they have resorted to include only data that\nhad been already published in a scientific journal \u00ad\nthus vastly restricting the amount of data hosted by\nthe database \u00ad or that were donated by data producers\nin a format compatible to the ones supported by the\ndatabase (Bastow and Leonelli, 2010). Despite the\nincreasing pressure to disseminate data in the public\ndomain, as recently recommended by the Royal\nSociety (2012) and several funding bodies in the UK\n(Levin et al., in preparation), the latter category com-\nprises a very small number of researchers. Again, this\nis largely due to the labour-intensive nature of de-con-\ntextualisation processes. Researchers who wish to\nsubmit their data to a database need to make sure\nthat the format that they use, and the metadata that\nthey provide, fit existing standards \u00ad which in turn\nmeans acquiring updated knowledge on what the\nstandards are and how they can be implemented, if\nat all; and taking time out of experiments and grant-\nwriting. There are presently very few incentives for\nresearchers to sacrifice research time in this way,\nas data donation is not acknowledged as a contribu-\ntion to scientific research (Ankeny and Leonelli,\nin press).\nStage 2: Re-contextualisation\nOnce data have been de-contextualised and added to a\ndatabase, the next stage of their journey is to be re-\ncontextualised \u00ad in other words, to be adopted by a\nnew research context, in which they can be integrated\nwith other data and possibly contribute to spotting new\ncorrelations. Within biology, re-contextualisation can\nonly happen if database users have access not only to\nthe data themselves but also to the information about\ntheir provenance \u00ad typically including the specific strain\nof organisms on which they were collected, the instru-\nments and procedures used for data collection, and the\ncomposition of the research team who originated them\nin the first place. This sort of information, typically\nreferred to as `metadata' (Edwards et al., 2011;\n4 Big Data & Society\nLeonelli, 2010), is indispensable to researchers wishing\nto evaluate the reliability and quality of data. Even\nmore importantly, it makes the interpretation of the\nscientific significance of the data possible, thus enabling\nresearchers to extract meaning from their scrutiny of\ndatabases.\nGiven the challenges already linked to the de-con-\ntextualisation of data, it will come as no surprise that\nre-contextualising them is proving even harder in bio-\nlogical practice. The selection and annotation of meta-\ndata is more labour-intensive than the formatting of\ndata themselves, and involves the establishment of sev-\neral types of standards, each of which is managed by its\nown network of funding and institutions. For a start, it\npresupposes reliable reference to material specimens of\nthe model organisms in question. In other words, it is\nimportant to standardise the materials on which data\nare produced as much as possible, so that researchers\nworking on those data in different locations can order\nthose materials and reasonably assume that they are\nindeed the same materials as those from which data\nwere originally extracted. Within model organism biol-\nogy, the standardisation, coordination and dissemin-\nation of specimens is in the hands of appositely built\nstock centres, which collect as many strains of organ-\nisms as possible, pair them up with datasets stored in\ndatabases, and make them available for order to\nresearchers interested in the data. In the best cases,\nthis happens through the mediation of databases them-\nselves; for instance, The Arabidopsis Research\nDatabase has long incorporated the option to order\nmaterials associated with data stored therein at the\nsame time as one is viewing the data (Rosenthal and\nAshburner, 2002). However, such a well-organised\ncoordination between databases and stock centres is\nrare, particularly in cases where the specimens to be\ncollected and ordered are not easily transportable\nitems, such as seeds and worms, but organisms that\nare difficult and expensive to keep and disseminate,\nsuch as viruses and mice. Most organisms used for\nexperimental research do not even have a centralised\nstock centre collecting exemplars for further dissemin-\nation. As a result, the data generated from these organ-\nisms are hard to incorporate into databases, as\nproviding them with adequate metadata proves impos-\nAnother serious challenge to the development of\nmetadata consists of capturing experimental protocols\nand procedures, which in biology are notoriously idio-\nsyncratic and difficult to capture through any kind of\ntextual description (let alone standard categories). The\ndifficulties are exemplified by the recent emergence of a\nJournal of Visualized Experiments, whose editors claim\nthat actually showing a video of how a specific experi-\nment is performed is the only way to credibly\ncommunicate information about research methods\nand protocols. Indeed, despite the attempted implemen-\ntation of standard descriptions such as the Minimal\nInformation about Biological and Biomedical\nInvestigation, standards in this area are very under-\ndeveloped and rarely used by biologists (Leonelli,\n2012a). This makes the job of curators even more dif-\nficult, as they are then left with the task of selecting\nwhich metadata to insert in their database, and which\nformat to use in order to provide such information.\nAdditionally, curators are often asked to provide a pre-\nliminary assessment of the quality of data, which can\nact as a guideline for researchers interested in large\ndatasets. Curators achieve this through so-called `evi-\ndence codes' and `confidence rankings' which, however,\ntend to be based on controversial assumptions (for\ninstance, the idea that data obtained through physical\ninteraction with organisms are more trustworthy than\nsimulation results) which may not fit all scenarios in\nwhich data may be adopted.\nStage 3: Re-use\nThe final stage of data journeys that I wish to examine\nis that of re-use. One of the central themes in Big\nData research is the opportunity to re-use the same\ndatasets to uncover a large number of different correl-\nations. After having been de-contextualised and re-\ncontextualised, data are therefore supposed to fulfil\ntheir epistemic role by leading to a variety of new\ndiscoveries. From my observations above, it will\nalready be clear that very few of the data produced\nwithin experimental biology make it to this stage of\ntheir journeys, due to the lack of standardisation in\ntheir format and production techniques, as well as the\nabsence of stable reference materials to which data can\nbe meaningfully associated for re-contextualisation.\nData that cannot be de-contextualised and re-\ncontextualised are not generally included into model\norganism databases, and thus do not become part of a\nbody of Big Data from which biologically significant\ninferences can be made. Remarkably, the data that are\nmost successfully assembled into big collections are\ngenomic data, such as genome sequences and micro-\narrays, which are produced through highly standar-\ndised technologies and are therefore easier to format\nfor travel. This is bad news for biological research\nfocused on understanding higher-level processes, such\nas organismal development, behaviour and susceptibil-\nity to environmental factors: data that document these\naspects are typically the least standardised in both\ntheir format and the materials and instruments\nthrough which they are produced, which makes their\nintegration into large collections into a serious\nchallenge.\nLeonelli 5\nThis signals a problem with the idea that Big Data\ninvolves unproblematic access to all data about a given\nphenomenon \u00ad or even to at least some data about\nseveral aspects of a phenomenon, such as multiple\ndata sources concerning different levels of organisation\nof an organism. When considering the stage of data\nre-use, however, an even more significant challenge\nemerges: that of data classification. Whenever data\nand metadata are added to a database, curators need\nto tag them with keywords that will make them retriev-\nable to biologists interested in related phenomena. This\nis an extremely hard task, given that curators want to\nleave the interpretation of the potential evidential value\nof data as open as possible to database users. Ideally,\ncurators should label data according to the interests\nand terminology used by their prospective users, so\nthat a biologist is able to search for any data connected\nto her phenomenon of interest (e.g. `metabolism') and\nfind what the evidence that she is looking for is. What\nmakes such a labelling process into a complex and con-\ntentious endeavour is the recognition that this classifi-\ncation partly determines the ways in which data may be\nused in the future \u00ad which, paradoxically, is exactly\nwhat databases are not supposed to do. In other pub-\nlications, I have described at length the functioning of\nthe most popular system currently used to classify data\nin model organism databases, the so-called `bio-ontol-\nogies' (Leonelli, 2012b). Bio-ontologies are standard\nvocabularies intended to be intelligible and usable\nacross all the model organism communities, sub-disci-\nplines and cultural locations to which data should\ntravel in order to be re-used. Given the above-men-\ntioned fragmentation of biology into myriads of epi-\nstemic communities with their own terminologies,\ninterests and beliefs, this is a tall order. Consequently,\ndespite the widespread recognition that model organ-\nism databases are among the best sources of Big Data\nwithin biology, many biologists are suspicious of them,\nprincipally as a result of their mistrust of the categories\nunder which data are classified and distributed. This\nputs into question not only the idea that databases\ncan successfully collect Big Data on all aspects of\ngiven organisms but also the idea that they succeed in\nmaking such data retrievable to researchers in ways\nthat foster their re-use towards making new discoveries.\nWhat does it take to assemble Big Data?\nImplications for Big Data claims\nThe above analysis, however brief, clearly points to the\nhuge amount of manual labour involved in developing\ndatabases for the purpose of assembling Big Data and\nmaking it possible to integrate and analyse them; and to\nthe many unresolved challenges and failures plaguing\nthat process.\nI have shown how curators have a strong influence\non all three stages of data journeys via model organism\ndatabases. They are tasked with selecting, formatting\nand classifying data so as to mediate among the mul-\ntiple standards and needs of the disparate epistemic\ncommunities involved in biological research. They\nalso play a key role in devising and adding metadata,\nincluding information about experimental protocols\nand relevant materials, without which it would be\nimpossible for database users to gauge the reliability\nand significance of the data therein. All these activities\nrequire large amounts of funding for manual curation,\nwhich is mostly unavailable even in areas as successful\nas model organism biology. They also require the sup-\nport and co-operation of the broader biological com-\nmunity, which is however also rare due to the pressures\nand credit systems to which experimental biologists are\nsubjected. Activities such as data donation and partici-\npation in data curation are not currently rewarded\nwithin the academic system. Therefore, many scientists\nwho run large laboratories and are responsible for their\nscientific success perceive these activities as an inexcus-\nable waste of time, despite being aware of their scien-\ntific importance in fostering Big Data science.\nWe thus are confronted with a situation in which\n(1) there is still a large gap between the opportunities\noffered by cutting-edge technologies for data dissemin-\nation and the realities of biological data production and\nre-use; (2) adequate funding to support and develop\nonline databases is lacking, which greatly limits cur-\nators' ability to make data travel; and (3) data donation\nand incorporation into databases is very limited, which\nmeans that only a very small part of the data produced\nwithin biology actually get to be assembled into Big\nData collections. Hence, Big Data collections in biol-\nogy could be viewed as very small indeed, compared to\nthe quantity and variety of data actually produced\nwithin this area of research. Even more problematic-\nally, such data collections tend to be extremely partial\nin the data that they include and make visible. Despite\ncurators' best efforts, model organism databases mostly\ndisplay the outputs of rich, English-speaking labs\nwithin visible and highly reputed research traditions,\nwhich deal with `tractable' data formats. The incorpo-\nration of data produced by poor or unfashionable labs,\nwhether in developed or developing countries, is very\nlow \u00ad also because scientists working in those condi-\ntions have an even lesser chance than scientists working\nin prestigious locations to be able to contribute to the\ndevelopment of databases in the first place (the digital\ndivide is alive and well in Big Data science, though\ntaking on a new form).\nA possible moral to be drawn from this situation is\nthat what counts as data in the first place should be\ndefined by the nature of their journeys. According to\n6 Big Data & Society\nthis view, data are whatever can be fitted into highly\nvisible databases; and results that are hard to dissem-\ninate in this way do not count as data at all, since they\nare not widely accessible. I regard this view as empiric-\nally unwarranted, as it is clear from my research that\nthere are many more results produced within the life\nsciences which biologists are happy to call and use as\ndata; and that what biologists consider to be data does\ndepend on their availability for scrutiny (it has to be\npossible to circulate them to at least some peers who\ncan assess their usefulness as evidence), but not neces-\nsarily on the extent to which they are publicly available\n\u00ad in other words, data disseminated through paper or\nby email can have as much weight as data disseminated\nthrough online databases. Despite these obvious prob-\nlems, however, the increasing prominence of databases\nas supposedly comprehensive sources of information\nmay well lead some scientists to use them as bench-\nmarks for what counts as data in a specific area of\ninvestigation. This tendency is reinforced by wider pol-\nitical and economic forces, such as governments, cor-\nporations and funding bodies, for whom the prospect\nof assembling centralised repositories for all available\nevidence on any given topics constitutes a powerful\nHow do these findings compare to the claims made\nby Mayer-Scho\n\u00a8 nberger and Cukier? For a start, I think\nthat they cause problems to both of the corollaries to\ntheir views that I listed above. Consider first the ques-\ntion of sampling. Rather than disappearing as a scien-\ntific concern, looking at the ways in which data travel in\nbiology highlights the ever-growing significance of sam-\npling methods. Big Data that is made available through\ndatabases for future analysis turns out to represent\nhighly selected phenomena, materials and contribu-\ntions, to the exclusion of the majority of biological\nwork. What is worse, this selection is not the result of\nscientific choices, which can therefore be taken into\naccount when analysing the data. Rather, it is the ser-\nendipitous result of social, political, economic and tech-\nnical factors, which determines which data get to travel\nin ways that are non-transparent and hard to recon-\nstruct by biologists at the receiving end. A full account\nof factors involved here far transcends the scope of this\narticle.5 Still, even my brief analysis of data journeys\nillustrates how they depend on issues as diverse as\nnational data donation policies (including privacy\nlaws, in the case of biomedical data); the good-will\nand resources of specific data producers, as well as\nthe ethos and visibility of the scientific traditions and\nenvironments in which they work (for instance, biolo-\ngists working for private industries may not be allowed\nto publicly disclose their data); and the availability of\nwell-curated databases, which in turn depends on the\nvisibility and value placed upon them (and the data\ntypes therein) by government or relevant public/private\nfunders. Assuming that Big Data does away with the\nneed to consider sampling is highly problematic in such\na situation. Unless the scientific system finds a way to\nimprove the inclusivity of biological databases, they\nwill continue to incorporate partial datasets that never-\ntheless play a significant role in shaping future research,\nthus encouraging an inherently conservative and irra-\ntional system.\nThis partiality also speaks to the issue of bias in\nresearch, which Mayer-Scho\n\u00a8 nberger and Cukier insist\ncan potentially be superseded in the case of Big Data\nscience. The ways in which Big Data is assembled for\nfurther analysis clearly introduce numerous biases\nrelated to methods for data collection, storage, dissem-\nination and visualisation. This feature is recognised by\nMayer-Scho\n\u00a8 nberger and Cukier, who indeed point to\nthe fact that the scale of such data collection takes focus\naway from the singularity of data points: the ways in\nwhich datasets are arranged, selected, visualised and\nanalysed become crucial to which trends and patterns\nemerge. However, they assume that the diversity and\nvariability of data thus collected will be enough to\ncounter the bias incorporated in each of these sources.\nIn other words, Big Data is self-correcting by virtue of\nits very unevenness, which makes it probable that\nincorrect or inaccurate data are rooted out of the\nsystem because of their incongruence with other data\nsources. I think that my arguments about the inherent\nimbalances in the types and sources of data assembled\nwithin big biology casts some doubt as to whether such\ndata collections, no matter how large, are diverse\nenough to counter bias in their sources. If all data\nsources share more or less the same biases (for instance,\nthey all rely on microarrays produced with the same\nmachines), there is also the chance that bias will be\namplified, rather than reduced, through such Big Data.\nThese considerations do not make Mayer-\nScho\n\u00a8 nberger and Cukier's claims about the power of\nBig Data completely implausible, but they certainly\ndent the idea that Big Data is revolutionising biological\nresearch. The availability of large datasets does of\ncourse make a difference, as advertised for instance in\nthe Fourth Paradigm volume issued by Microsoft to\npromote the power of data-intensive strategies (Hey\net al., 2009). And yet, as I stressed above, having a\nlot of data is not the same as having all of them; and\ncultivating such an illusion of completeness is a very\nrisky and potentially misleading strategy within biology\n\u00ad as most researchers whom I have interviewed over the\nlast few years pointed out to me. The idea that the\nadvent of Big Data lessens the value of accurate mea-\nsurements also does not seem to fit these findings. Most\nsciences work at a level of sophistication in which one\nsmall error can have very serious consequences (the\nLeonelli 7\nblatant example being engineering). The constant\nworry about the accuracy and reliability of data is\nreflected in the care employed by database curators in\nenabling database users to assess such properties; and\nin the importance given by users themselves to evaluat-\ning the quality of data found on the internet. Indeed,\ndatabases are often valued because they provide means\nto triangulate findings coming from different sources,\nso as to improve the accuracy of measurement and\ndetermine which data are most reliable. Although\nthey may often fail to do so, as I just discussed, the\nvery fact that this is a valued feature of databases\nmakes the claim that `messiness' triumphs over\naccuracy look rather shaky. Finally, considering data\njourneys prompts second thoughts about the supposed\nprimacy of correlations over causal explanations. Big\nData certainly does enable scientists to spot patterns\nand trends in new ways, which in turn constitutes an\nenormous boost to research. At the same time, biolo-\ngists are rarely happy with such correlations, and\ninstead use them as heuristics that shape the direction\nof research without necessarily constituting a discovery\nin itself. Being able to predict how an organism or eco-\nsystem may behave is of huge importance, particularly\nwithin fields such as biomedicine or environmental sci-\nence; and yet, within experimental biology, the ability\nto explain why certain behaviour obtains is still very\nhighly valued \u00ad arguably over and above the ability\nto relate two traits to each other.6\nConclusion: An alternative approach\nto Big Data science\nIn closing my discussion, I not only want to consider\nits specificity with respect to other parts of Big Data\nscience but also the general lessons that may be drawn\nfrom such a case study. Biology, and particularly the\nstudy of model organisms, represents a field where\ndata have been produced long before the advent of\ncomputing and many data types are still generated\nin ways that are not digital, but rather rely on physical\nand localised interactions between one or more inves-\ntigators and a given organic sample. Accordingly, bio-\nlogical data on model organisms are heterogeneous\nboth in their content and in their format; are curated\nand re-purposed to address the needs of highly dispar-\nate and fragmented epistemic communities; and pre-\nsent curators with specific challenges to do with the\nwish to faithfully capture and represent complex,\ndiverse and evolving organismal structures and behav-\niours. Readers with experience in other forms of Big\nData may well be dealing with cases where both data\nand their prospective users are much more homoge-\nneous, which means that their travel is less contested\nand tends to be curated and institutionalised in com-\npletely different ways. I view the fact that my study\nbears no obvious similarities to other areas of Big\nData use as a strength of my approach, which\nindeed constitutes an invitation to disaggregate the\nnotion of Big Data science as a homogeneous whole\nand instead pay attention to its specific manifestations\nacross different contexts. At the same time, I maintain\nthat a close examination of specialised areas can still\nyield general lessons, at the very least by drawing\nattention to aspects that need to be critically scruti-\nnised in all instances of Big Data handling. These\ninclude, for instance, the extent to which data are \u00ad\nand need to be \u00ad curated before being assembled into\ncommon repositories; the decisions and investments\ninvolved in selecting data for travel, and their impli-\ncations for which data get to be circulated in the first\nplace; and the representativeness of data assembled\nunder the heading of `Big Data' with respect to\nother (and/or pre-existing) data collection activities\nwithin the same field.\nAt the most general level, my analysis can be used\nto argue that characterisations of Big Data science as\ncomprehensive and intrinsically unbiased can be mis-\nleading rather than helpful in shaping scientific as\nwell as public perceptions of the features, opportu-\nnities and dangers associated with data-intensive\nresearch. If one admits the plausibility of this pos-\nition, then how can one better understand current\ndevelopments? I here want to defend the idea that\nBig Data science has specific epistemological and\nmethodological characteristics, and yet that it does\nnot constitute a new epistemology for biology. Its\nstrength lies in the combination of concerns that\nhave long featured in biological research with oppor-\ntunities opened up by novel communication technol-\nogies, as well as the political and economic climate in\nwhich scientific research is currently embedded. Big\nData brings new salience to aspects of scientific prac-\ntice which have always been vital to successful empir-\nical research, and yet have often been overlooked by\npolicy-makers, funders, publishers, philosophers of\nscience and even scientists themselves, who in the\npast have tended to evaluate what counts as `good\nscience' in terms of its products (e.g. new claims\nabout phenomena or technologies for intervention\nin the world) rather than in terms of the processes\nthrough which such results are eventually achieved.\nThese aspects include the processes involved in valu-\ning data as a key scientific resource; situating data in\na context within which they can be interpreted reli-\nably; and structuring scientific institutions and credit\nmechanisms so that data dissemination is supported\nand regulated in ways that are conducive to the\nadvancement of both science and society.\n8 Big Data & Society\nMore specifically, I want to argue that the novelty of\nBig Data science can be located in two key shifts char-\nacterising scientific practices over the last two decades.\nFirst is the new prominence attributed to data as com-\nmodities with high scientific, economic, political and\nsocial value (Leonelli, 2013). This has resulted in the\n"
}