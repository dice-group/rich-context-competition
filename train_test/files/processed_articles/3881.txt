{
    "abstract": "Abstract\nAlthough categorization can take place at different levels of abstraction, classic studies on semantic\nlabeling identified the basic level, for example, dog, as entry point for categorization. Ultrarapid\ncategorization tasks have contradicted these findings, indicating that participants are faster at\ndetecting superordinate-level information, for example, animal, in a complex visual image. We\nargue that both seemingly contradictive findings can be reconciled within the framework of parallel\ndistributed processing and its successor Leabra (Local, Error-driven and Associative, Biologically\nRealistic Algorithm). The current study aimed at verifying this prediction in an ultrarapid\ncategorization task with a dynamically changing presentation time (PT) for each briefly\npresented object, followed by a perceptual mask. Furthermore, we manipulated two defining\ntask variables: level of categorization (basic vs. superordinate categorization) and object\npresentation mode (object-in-isolation vs. object-in-context). In contradiction with previous\nultrarapid categorization research, focusing on reaction time, we used accuracy as our main\ndependent variable. Results indicated a consistent superordinate processing advantage,\ncoinciding with an overall improvement in performance with longer PT and a significantly more\naccurate detection of objects in isolation, compared with objects in context, at lower stimulus PT.\nThis contextual disadvantage disappeared when PT increased, indicating that figure-ground\nseparation with recurrent processing is vital for meaningful contextual processing to occur.\n",
    "reduced_content": "Article\nThe Time-Course of\nUltrarapid Categorization:\nThe Influence of Scene\nCongruency and\nTop-Down Processing\nSteven Vanmarcke, Filip Calders and Johan Wagemans\nBrain and Cognition, University of Leuven, Belgium\n Keywords\nultrarapid categorization, rapid gist perception, individual differences, scene congruency, object\nperception, psychophysical research, recurrent processing\nCorresponding author:\nSteven Vanmarcke, Brain and Cognition, University of Leuven (KU Leuven), BE-3000 Leuven, Belgium.\nEmail: steven.vanmarcke@kuleuven.be\ni-Perception\nipe.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sage-\npub.com/en-us/nam/open-access-at-sage).\nIntroduction\nTo understand our visual surroundings, we need to be able to categorize the complex visual\ninput as efficiently as possible. A semantic category can be defined as a group of two or more\nobjects with different attributes, properties, or qualities, which are treated similarly with\nregard to their meaning. Within the hierarchical organization of semantic information,\ncategorization can take place at different levels of abstraction (Rosch, Mervis, Gray,\nJohnson, & Boyes-Braem, 1976). The same object or scene can be categorized at a more\ngeneral, superordinate level and at a less general, basic level of abstraction. Rosch et al.\n(1976) defined the basic object level as the level of categorization at which the categories\ncan mirror the structure of attributes perceived in the world by sharing a common shape.\nBasic Versus Superordinate Advantage\nIn this classic study by Rosch et al. (1976), the basic level was identified as entry point for\nvisual information and categorization (e.g., ``dog'' rather than ``animal'' or ``golden\nretriever'') in a free naming task. If participants were allowed to use this level, they\nresponded faster compared with when they used the superordinate-level categorization\n(Mervis & Rosch, 1981). This led to the belief that we can only label an object at a\nsuperordinate level, for example, animal, if we already know it at a basic level, for\nexample, dog (Jolicoeur, Gluck, & Kosslyn, 1984). This idea was based on the theory that\nobject processing occurs in a hierarchical system where the entry point or gateway was\ndefined at the basic object level. If true, a basic-level categorization advantage should\noccur in every experimental design. Importantly, all experiments claiming this initial basic-\nlevel advantage used conscious (verbal) semantic labeling tasks without any time constraints\n(Mace\n\u00b4 , Joubert, Nespoulous, & Fabre-Thorpe, 2009; Vanmarcke & Wagemans, 2015). More\nrecent evidence suggests, however, that learning the superordinate level does not necessarily\nimitation techniques to show that very young children, when they did not label but\ninteracted with objects to imitate a story, clearly understood and manipulated the world\non a superordinate level. Even infants between 9 and 14 months old seemed to grasp\nsuperordinate concepts like animals and vehicles. This convinced the authors that the\nbuilding of a conceptual system starts at a superordinate level and that children only learn\nto differentiate between objects on a basic level in a later stage. For example, children first\nlearn that animals need to drink water, whereas inanimate objects do not. Later on, they\nlearn to link the conceptual knowledge of ``barking'' to dogs. This does not mean however\nthat children or infants do not see the difference between the objects at a basic level. When\ninfants imitated a story and had to choose between different animals, they preferred the same\nanimal as in the example shown by the experimenter. Nonetheless, children seemed to build\ntheir semantic knowledge based on a more abstract level (Mandler & McDonough, 2000). So,\neven before they started to use language, infants categorized on the superordinate level\nwithout clustering different basic levels together. Such a learning process seems to\nundermine the idea that the superordinate level is purely an abstraction of the basic level.\nInterestingly, ultrarapid go/no-go categorization tasks (Mace\nMarlot, 1996; Wu, Crouzet, Thorpe, & Fabre-Thorpe, 2015) showed that the basic object\nlevel is not necessarily the semantic information activated fastest during visual processing. In\nsuch ultrarapid go/no-go categorization, a naturalistic image is briefly (20 ms) presented and\nparticipants are asked to indicate whether a predefined basic (e.g., dog) or superordinate\n(e.g., animal) object class is present in the display. Research indicated that people were able to\ndo this nearly perfectly (Thorpe et al., 1996) and that participants were consistently faster\nat detecting an object at the superordinate level, in comparison with detecting an object at the\nbasic level (Mace\n\u00a8 nig, & Fahle, 2014). Similar observations\n(e.g., Kadar & Ben-Shahar, 2012; Rousselet, Joubert, & Fabre-Thorpe, 2005) were made for\nscene gist categorization in which participants have to judge the broad semantic category of\nthe presented scene picture (e.g., ``forest'' or ``desert''): People were faster at distinguishing\nnatural or manmade (superordinate scene level) than sea or mountain (basic scene level).\nThese findings were further supported by the observation that participants, when choosing\nbetween a target and a distracter image by making a saccade toward the target, needed more\ninformation when making an eye movement toward a basic object level (Wu et al., 2015).\nThis would suggest that the visual system rapidly accesses coarse level more abstract\nrepresentations of an object or scene first, before activating more fine-grained\nrepresentations corresponding to a smaller category. This consistent behavioral effect in\nultrarapid categorization tasks is denoted as the superordinate advantage and seems\nrobust for increased presentation time (PT) of the stimuli (Poncet & Fabre-Thorpe, 2014).\nFurthermore, recent findings suggest that this perceptual categorization of rapidly presented\ninformation is also influenced by the chosen experimental trial context (Mack & Palmeri,\n2015; Palmeri & Mack, 2015). Specifically, the superordinate advantage disappeared when a\nrandomized target category design was used, in which superordinate- or basic-level\ncategorization always changed after a few consecutive trials (e.g., maximum of four\nrepetitions). This would suggest that the dynamics of object categorization is flexible and\nrequires a blocked trial design, focusing specifically on either superordinate- or basic-level\ncategories over a long series of trials, in order to observe a superordinate processing\nadvantage in ultrarapid categorization. The current study aimed to replicate and extend\nthese previous findings on semantic categorization by systematically manipulating the\ntime-dependent task properties of ultrarapid categorization, within a blocked experimental\ntrial design, and by providing an overarching theoretical framework which includes time\ncourse and task dependency.\nTheoretical Framework\nMcClelland and Rogers (2003) were the first to apply the parallel distributed processing\n(PDP) framework to hierarchical semantic categorization. In this framework, semantic\nprocessing uses the propagation of activation among simple neuron-like processing units,\nforming a bottom-up processing network. Initially, the connection weights between the\nprocessing units within this neural network remain uninformative. During the learning\nprocess, the weights change slowly, gradually reducing errors, and becoming informative\nabout how the activation of units in one level of representation determines the activation\nat another level of representation (McClelland & Rogers, 2003; Rogers & McClelland, 2004).\nLater, Leabra (Local, Error-driven and Associative, Biologically Realistic Algorithm), the\nsuccessor of PDP, incorporated more biologically realistic mechanisms (Aisa, Mingus, &\nO'Reilly, 2008). This led to the development of LVis (Leabra Vision; Wyatte,\nHerd, Mingus, & O'Reilly, 2012). This computational model of visual processing is\ncapable of identifying stand-alone objects and labeling them at the basic object level\n(O'Reilly, Wyatte, Herd, Mingus, & Jilk, 2013). Other neural network simulations with\nLeabra demonstrated the critical role inhibition plays in lexical selection (Snyder et al.,\n2010) . In contradiction to strictly bottom-up models of object recognition, the LVis\nmodel uses recurrent processing as a key feature to correctly identify partially occluded\nobjects (Wyatte et al., 2012). More precisely, the neural interconnectivity within- and\nbetween-brain regions helps to create a stable and clear representation of an ill-visible\nVanmarcke et al. 3\nobject by allowing high-level visual brain areas to shape the neural activation and predictions\nof low-level visual areas through inhibitory and excitatory recurrent connections (O'Reilly\net al., 2013). Recurrent processing also plays an important role in figure-ground segmentation\n(Lamme, Zipser, & Spekreijse, 2002), border ownership, and subjective surface perception\n(Kogo & Wagemans, 2013a, 2013b). The neural activation from the low-level visual areas\nneeds to be linked to one distinct lexical category. In ultrarapid categorization tasks, the label\nof the object is predefined, allowing top-down biasing of the relevant visual features. This\nfacilitates the predefined categorization decisions, resulting in a top-down bias favoring\nsuperordinate categorization (Bar, 2004; De Cesarei, Peverato, Mastria, & Codispoti,\n2015) and translating into the observed superordinate advantage in the ultrarapid\ncategorization literature (e.g., Rogers & Patterson, 2007; Vanmarcke & Wagemans, 2015).\nHowever, the PDP theory (Rogers & McClelland, 2004; Wyatte et al., 2012) also proposed\nthat, after initial activation, similarity-based generalization implies strong generalization\nwithin basic categories but weak generalization between them. In this way, similarity-\nbased generalization promotes rapid and active learning of basic-level names. For instance,\nthe name ``dog'' tends to generalize strongly to items with similar representations, such as\nother dogs, but not to items with more distal representations, such as other kinds of animals.\nBecause superordinate category learning benefits more slowly from this similarity-based\ngeneralization in comparison to basic-level information, this superordinate advantage can\nturn into a basic-level advantage over time when the task requires an active, conscious\nlabeling of different semantic categories. This dynamic network characteristic is in\nagreement with the finding that people are faster to confidently name and verify category\nmembership verbally at the basic level when no time constraints are in place (Rosch et al.,\n1976). The same reasoning applies with regard to the influence of the experimental trial\ncontext on flexibly categorizing semantic information at different levels of abstraction\nTime-Dependent Task Properties of Ultrarapid Categorization\nIn the current study, we use the theory summarized earlier to formulate specific experimental\npredictions for behavioral processing in masked ultrarapid basic- and superordinate-level\ncategorization in a blocked experimental trial design. In contradiction with previous\nultrarapid categorization research, focusing on differences in participant reaction times, the\ncurrent study used accuracy as its main dependent variable. More precisely, we estimated the\nbest-fitting sigmoid function (Weibull distribution) per participant by using maximum\nlikelihood parameter estimation (psychometric performance curve) for categorization\nperformance by dynamically changing stimulus PT (ranging from 16 to 80 ms). After\nstimulus presentations, a perceptual mask was shown in order to explicitly control for the\ninfluence of the top-down biasing of the relevant visual features during categorization\n(Fahrenfort, Scholte, & Lamme, 2007). This was done in an ultrarapid categorization task\nin which we varied two defining task variables: (a) level of categorization (basic vs.\nsuperordinate categorization) and (b) object presentation mode (object-in-isolation vs.\nobject-in-context).\nHypothesis 1: Basic versus superordinate advantage in ultrarapid categorization.\nLonger PT of the ultrarapid categorization task without perceptual masking did not reverse\nthe superordinate advantage in a blocked experimental trial context (Poncet & Fabre-\nThorpe, 2014). However, previous psychophysical studies showed that masking the\nstimulus presentation allowed to dissociate between the bottom-up and top-down (recurrent)\ncomponents of the neural response (for review, see Breitmeyer & Ogmen, 2006; Enns & Di\nLollo, 2000). Previous research on ultrarapid categorization suggested that significant top-\ndown modulation should be possible after a stimulus presentation of 40 to 60 ms (Roland,\n2010; Serre, Oliva, & Poggio, 2007). To investigate the possible influence of both bottom-up\nand top-down categorization processes on the behavioral performance in a masked and\npredefined ultrarapid categorization task, we dynamically manipulated the stimulus PT\nwithin a range of 16 to 80 ms. Based on the PDP theory and Leabra models (O'Reilly\net al., 2013), and in accordance with previous research on the time course of object\ncategorization with a blocked trial design (Mack & Palmeri, 2015), we predicted\nparticipants to show a consistent superordinate accuracy advantage across all PT\n(Hypothesis 1). This hypothesis followed from the unambiguously predefined search goal\nin ultrarapid categorization, which would allow the top-down biasing of the relevant visual\nfeatures for coarse (superordinate vs. basic) scene categorization (Bar, 2004; De Cesarei et al.,\nHypothesis 2: Influence of object presentation mode on ultrarapid object detection.\nObjects can either be presented in isolation or can be embedded within a meaningful scene\ncontext. Previous research indicated that such contextual information can facilitate object\nidentification compared with incongruent background information (Re\nprecisely, when objects are embedded in a familiar context, for example, a plane in the sky,\nobject recognition is both faster and more accurate than when objects are presented in an\nincongruent context in which they are less likely to appear, for example, a bed in a forest\n(Fize, Cauchoix, & Fabre-Thorpe, 2011; Joubert, Fize, Rousselet, & Fabre-Thorpe, 2008).\nSimilar findings were observed in an ultrarapid categorization paradigm without perceptual\nmasking (Crouzet, Joubert, Thorpe, & Fabre-Thorpe, 2012). This direct influence of context\non object recognition might be related to the lifelong experience of the visual system with our\nvisual surrounding world and its efficiency at extracting visual regularities (Davenport, 2007;\nRe\n\u00b4 my et al., 2013). Furthermore, electroencephalography research showed that masking\nstimuli interrupts figure-ground segmentation by interrupting recurrent (top-down)\nprocessing (Fahrenfort et al., 2007). In the current study, we were therefore interested\nwhether object-congruent background information could speed up categorization before\nfigure-ground segmentation was completed. More concretely, we were interested in how\nthe time course of masked ultrarapid object categorization would influence the\ndiscrimination accuracy of objects, either presented in isolation or embedded within a\nmeaningful everyday scene. Based on the Leabra theory (O'Reilly et al., 2013), we\npredicted that participant performance would only be influenced by the object-congruent\ncontext when longer PT in perceptually masked rapid categorization allowed top-down\nprocessing to affect response speed (Hypothesis 2). This hypothesis was based on the\nexpected impact of meaningful contextual information on object detection, resulting from\nthe inherent influence of relevant global scene statistics diagnostic for object categorization\nMaterials and Methods\nParticipants\n(SD \u00bc 3.57; [min, max] age \u00bc [17, 43]; interquartile range [IQR] \u00bc 1). All participants were\nVanmarcke et al. 5\nfirst-year psychology students at the University of Leuven (KU Leuven). They received\ncourse credits for participation. Participants who did not follow the task instructions or\ndid not complete the task as requested were deleted from the data set before onset of the\nactual analysis. The final participant set therefore contained exactly 136 participants (20 men\nIQR \u00bc 1). The study was conducted in line with the ethical principles regarding research\nwith human participants as specified in The Code of Ethics of the World Medical\nAssociation (Declaration of Helsinki). The study was approved by the Ethical Committee\nof the Faculty of Psychology and Educational Sciences (EC FPPW) of the University of\nLeuven (KU Leuven), and the participants provided written informed consent before starting\nthe experiment.\nComputer Task\nThis section provides an overview of the ultrarapid categorization task completed by all\nparticipants. Participants were asked to take a comfortable position before the computer\nscreen (at about 57 cm of the computer display) and placed both hands on the keyboard\ntype: DellP2211H). The experiment was conducted using the open-source software library\nPsychoPy, which is written in Python (Peirce, 2008).\nDesign. The ultrarapid categorization task (Figure 1) took about 30 minutes, and all\ninstructions were projected on the computer screen. Every trial started with a fixation\ncross (300 ms). Then the stimulus was presented for a variable duration (see later). After\nthe stimulus presentation, a perceptual mask was shown (350 ms). The mask was computed\nby dividing each image into pixel-squares (2 by 2 pixels per square) and then randomly\nscrambling these stimulus elements (per image). Such a scrambled version was created for\neach stimulus in the object-in-context condition, as exemplified in Figure 2(b). To avoid\nFigure 1. A graphical overview of the trial design of the ultrarapid categorization task.\na strong influence of the gray image background in the object-in-isolation condition masks,\nwe first imposed a diagonal black-white watermark (50% transparency) before scrambling\nthe images. This is exemplified in Figure 3(a). Importantly, rigorous pilot testing on an\nindependent sample of nai\u00a8ve participants, several weeks before the main experiment,\nindicated that these masks, in both conditions, made it impossible (chance-level\nperformance) to correctly categorize the stimuli when the PT was 16 ms (or less) and\nstimulus presentation was masked (chance-level performance). Furthermore, in the main\nexperiment, the stimulus PT in the ultrarapid categorization task depended on subject\nperformance. Performance was calculated every 10 stimuli and PT decreased (or increased)\nwith performance above (or below) 75% with 16 ms. The experiment started with two\nalternating PT: one initialized at 16 ms, a second at 80 ms. Subjects who did not reach\n75% performance at a PT of 80 ms during the entire task were assumed to be inattentive\nor not understanding the task. Their results were discarded before the actual data analysis\nbegan. In general, information on the following stimulus PT was collected: 16, 32, 48, 64, and\n80 ms. Subjects had a 1000-ms response window and when the target was not presented, the\nFigure 2. A general overview of the type of images used within the ultrarapid categorization task for both\nthe (a) object-in-isolation and the (b) object-in-context condition. The complete picture set is made available\nonline on http://www.gestaltrevision.be/en/resources/supplementary-material/76-resources/supplementary-\nVanmarcke et al. 7\nsubject had to wait until the trial ended. After the response, a new trial started about 200 ms\nlater. In the practice trials, visual feedback was given. The word ``correct'' flashed in green\nwhen a correct answer was given. When the answer was incorrect, the word ``wrong'' flashed\nin red. Practice trials ended after six correct answers, and PT during these trials was always\n80 ms. Analysis indicated that all subjects (except those discarded due to inattentiveness) had\na high performance in the practice trials, indicating that the task was well understood. The\npractice trials were followed by two test sessions of 10 minutes, one test session on basic and\none on superordinate object level. Between the test sessions, a break of 1 minute was given.\nAfter the task, a short debriefing followed and participants were randomly divided into\ndifferent conditions.\nStimuli. Every participant was randomly assigned to one of two conditions (Figure 2). In the\nobject-in-isolation condition, stand-alone objects with a gray background were used, while in\nthe object-in-context condition, the object was embedded in a scene. For the object-in-\ndataset (O'Reilly et al., 2013). The object could be everywhere in the image but was always in\nthe foreground and fully visible. In both the superordinate and the basic-level test session of\nthe object-in-isolation condition, the stimuli were randomly chosen from this set of 480 gray-\nscaled pictures: 50% of the selected stimuli were used as targets, 50% were used as\nnontargets. Every stimulus was shown only once or twice. In line with previous research\n(e.g., Mace\ncategorization were used in each stimulus category: at superordinate- and basic-level object\nFigure 3. (a) To avoid a strong influence of the gray image background in the object-in-isolation condition\nmasks, we first imposed a diagonal black-white watermark (50% transparency) before scrambling the images.\n(b) The first panel exemplifies one of the stimuli (e.g., a chair) used in the object-in-context condition, and the\nsecond panel shows the mask as it was made in the main experiment. The third and last panels show how the\nperceptual mask was constructed in the control experiment, using a black-white watermark (50%\ntransparency), similar to the perceptual mask in the object-in-isolation condition of the main experiment.\ncategorization. To make this more explicit: (a) in the furniture (vehicle) category, vehicle\n(furniture) stimuli were used as nontargets and (b) in the bed (chair or plane or dog) category,\nchair, plane, and dog (bed), stimuli were used as nontargets. In each of the different image\ncategories, a wide variety of possible scenes were selected. For the object-in-context\ncondition, a total set of 480 gray-scaled images (320 \u00c2 214 pixels) were used for this task.\nThese scenes were selected (by unanimous consensus between several lab members including\nthe first two authors) on the Internet and taken with a 1NIKKOR camera. The same\nprinciples as in the objects-in-isolation condition were used to create the sets of target and\nnontarget stimuli. To avoid low-level confounds eliciting behavioral differences between\nstimulus categories and conditions (Wichmann, Braun, & Gegenfurtner, 2006), each of the\nselected images was set to the same global luminance and root mean square contrast\n(corresponding to a luminance distribution, within the gray-scale spectrum, with a mean of\n[110] and a standard deviation of [25.00]) by computing the average luminance and root mean\nsquare contrast across all images. The mean luminance of the images on the screen was 10 to\nTask instructions. Every participant was randomly assigned to either the object-in-context or\nthe object-in-isolation condition. In each of these conditions, participants were asked to\ncomplete one test session on superordinate level and one on basic object level\ncategorization (in a random order). Both types of test sessions started with an instruction\nquestion: (a) Is there a piece of furniture in the photo? (press spacebar), (b) . . . a vehicle . . . ,\n(c) . . . bed . . . , (d) . . . chair . . . (e), . . . plane . . . , or (f) . . . car . . . . Sessions 1 and 2 are at the\nsuperordinate level, while Sessions 3, 4, 5, and 6 are at the basic ordinate level. While\nSession 1 was always performed together with 3 or 4, Session 2 was always performed\ntogether with 5 or 6. An exact overview of the number of participants (in the final sample)\nin each of the possible combination of test session is provided in Appendix A. Similar to\nprevious findings in ultrarapid categorization (e.g., Mace\ndifferences in performance were observed for the different detection tasks (e.g., car, plane,. . .),\nfor neither the object-in-isolation or the object-in-context condition, at the same basic or\nsuperordinate level of categorization.\nMask control experiment. It might be argued that the perceptual masks used in either the object-\nin-isolation (Figure 2(a)) or the object-in-context condition (Figure 2(b)) could have a\ndifferential influence on participant performance given that they visually differed\nsubstantially between both conditions (Figure 2(a) compared with Figure 2(b)). To test\nthis possible confounding variable, we conducted a control experiment on an independent\nmax] age \u00bc [18, 31]; IQR \u00bc 3). These participants were asked to perform the exact same go/\nno-go categorization task as in the main experiment. The only difference with the original set-\nup was the mask in the object-in-context condition. This control mask was a scrambled\nversion of the presented image for which, similar to the object-in-isolation condition mask\nof the main experiment, we imposed a diagonal black-white watermark (50% transparency)\nbefore scrambling the images (Figure 3(b)). Furthermore, similar to the main experiment, in\nthe mask control experiment, participants were asked to take a comfortable position before\nthe computer screen (at about 57 cm of the computer display) and placed both hands on the\nkeyboard (spacebar) in front of the computer monitor (resolution: 1920 \u00c2 1080; refresh rate:\n60 Hz; type: DellP2211H). Furthermore, in this control experiment, but not in the main\nexperiment, the head position of the participants was stabilized by means of a head and\nchin rest during testing.\nVanmarcke et al. 9\nAnalysis\nFor every participant separately, the accuracy data on each test session of the ultrarapid\ncategorization task (consisting out of an average of 318 trials) were used to determine the\nbest-fitting sigmoid function (Weibull distribution) using maximum likelihood estimation for\nparameter estimation (Wichmann & Hill, 2001a). This psychometric fitting was done using\nthe Psignifit Toolbox in MATLAB R2009a (Wichmann & Hill, 2001b), with accuracy as the\ndependent variable (DV) and PT as the independent variable. This resulted in two separate\npsychometric fits per participant: one for the object-in-context and one for the object-in-\nisolation condition (Figure 4). The main parameters of these sigmoid functions, alpha (a) and\nbe\n` ta (b), provided an overall estimation of the time-dependent categorization performance in\neach test session per participant. We then used both a and b as the DVs in a mixed analysis of\nvariance (ANOVA) with presentation mode (object-in-context vs. object-in-isolation\ncondition) as a between-subjects factor and level of categorization (basic vs. superordinate)\nas a within-subjects factor. Participants were regarded as a random factor. Furthermore, we\nalso used a and b separately as DVs in a general linear mixed modeling (GLMM) approach\n(McCullagh, 1984). Furthermore, to further pinpoint possible differences in categorization\nbetween participants, also the threshold and slope values at specific points of the individual\npsychometric fits (60, 75, and 90%) were taken into account. Deviance values were calculated\nfor the regression models based on a maximum likelihood estimation (Aitkin, 1999) of all\nDVs on the tasks. By evaluating the drop in deviance together with the Akaike (Akaike,\n1973) and Bayesian Information Criterion (Schwarz, 1978) values (for overview, see\nAppendix C), the final model was selected. After model selection, the individual predictive\nvalue of each selected parameter was tested using Welch's t test with Satterthwaite\napproximation for the denominator degrees of freedom (McArdle, 1987) in the random\nintercepts regression analysis. Descriptive measures (e.g., age and gender) were tested as\nFigure 4. The figure provides an example of the best-fitting sigmoid functions for a single participant. In\n(a) the basic level object-in-isolation (light blue bar) and object-in-context (dark blue bar) conditions are\npresented, while in (b) the superordinate-level object-in-isolation (pink bar) and object-in-context (red bar)\nare provided. The main parameters of these sigmoid functions, alpha (a) and be\n`ta (b), provided an overall\nestimation of the time-dependent categorization performance in each test session per participant.\npossible covariates. The mixed ANOVA analysis provided very similar results as the GLMM\napproach. We therefore decided only to report the GLMM outcomes and to include all other\nresults in Appendix B. The outcomes of the GLMM were obtained by using the lme4 package\nmixed ANOVA was done using IBM SPSS (Version 22).\nFor the psychometric function parameter a, both presentation mode (object-in-context vs.\nobject-in-isolation condition) and level of categorization (basic vs. superordinate) were\nregarded as fixed effects in the final model. These observations were further refined when\ntaking the performance of participants at specific psychometric threshold (60%, 75%, and\n90%) values into account: level of categorization was a significant predictors of performance\non all specified threshold levels (60%, 75%, and 90%), but presentation mode only predicted\nperformance on the lower threshold levels (60% and 75%). Furthermore, for the\npsychometric function parameter b, only presentation mode (object-in-context vs. object-\nin-isolation condition) was withheld as a fixed effect in the final model. These observations\nwere further refined when taking the performance of participants at specific psychometric\nslope (60%, 75%, and 90%) values into account: presentation mode was a significant\npredictor of performance for slope at all points of the curve. The Presentation\nMode \u00c2 Level of Categorization interaction was not significant for any of the conducted\nanalysis. Descriptive variables such as test order, age, or gender were also not withheld as\nsignificant predictors of performance in the final models for a and b. The goodness-of-fit\nmeasures (for overview, see Appendices) for each of the parameter estimates (2) in the final\nGLMM model are provided in the results section. Data and an example of the analysis code\nare available online on http://www.gestaltrevision.be/en/resources/supplementary-material/\nFinally, we also analyzed the data of the control experiment using the same GLMM\nmodeling approach as in the main experiment. These findings replicated our original\nresults and indicated that the mask type was no confounding variable in explaining the\ncurrent results. We added the regression parameter estimates for the main parameters,\nalpha (a) and be\n` ta (b), of the individual sigmoid maximum likelihood fits in Appendix D.\nResults\nHypothesis 1: Basic versus superordinate advantage in ultrarapid categorization (Figure 5).\nWe found a significant main effect of the within-subjects variable level of categorization for\nprovides an estimate of the overall PT necessary for participants to correctly judge whether a\npredefined basic or superordinate object was presented or not. More precisely, the current\nresults indicated that participants were generally faster at detecting superordinate (more\nabstract) information (e.g., vehicle) than in observing more basic (more concrete) level\nrepresentations of its constituting subcategories (e.g., plane). In line with the absence of a\nmain effect of level of categorization for the psychometric (slope) parameter b\nSuch a finding indicated that PT and categorization performance differed between basic and\nsuperordinate processing when categorization required a rapid, predefined object detection.\nWhen graphically exploring this effect further (Figures 6(a) and 7(a)), it seemed to result from\nFigure 5. Overview of (a) average a estimates across participants and (b) average presentation time\nnecessary to achieve an overall accuracy of 60%, 75%, or 90% correct in rapidly detecting a basic-level object-in-\nisolation (light blue bar), a basic-level object-in-context (dark blue bar), a superordinate-level object-in-isolation\n(pink bar), or a superordinate-level object-in-context (red bar). The data are represented as the mean\nperformance across participants, with error bars depicting the standard error of the mean (SEM).\nFigure 6. Overview of the overall distribution of the (a) a and (b) b estimates of the participants. The different\nboxplots represent the four different conditions: basic-level object-in-isolation (light blue bar), basic-level object-in-\ncontext (dark blue bar), superordinate-level object-in-isolation (pink bar), and superordinate-level object-in-\ncontext (red bar).\na shift in threshold distribution. More precisely, the superordinate a distribution peaks earlier\nthan the basic-level distribution due to the presence of more participants with low a values\n(heavier tale). This consistent processing advantage for superordinate information was in line\nwith the PDP theory (O'Reilly et al., 2013) and our predictions (Hypothesis 1). See Table 1\nfor parameter estimates and 95% confidence intervals of the final models.\nHypothesis 2: Influence of object presentation mode on ultrarapid object detection (Figure 8).\nWe found a significant main effect of the between-subjects variable presentation mode for a\nnecessary for participants to correctly judge whether a predefined object, either in context or\nin isolation, was presented or not. More precisely, the current results indicated that\nparticipants were generally faster in detecting an object-in-isolation than in detecting an\nFigure 7. Overview of the overall distribution of the (a) a and (b) b estimates of the participants. The\ndifferent side-by-side histograms represent the four different conditions: basic-level object-in-isolation (light\nblue bar), basic-level object-in-context (dark blue bar), superordinate-level object-in-isolation (pink bar), and\nsuperordinate-level object-in-context (red bar). While the ordinate depicts the (a) a or (b) b estimates, the\nfrequency values are provided on the abscissa.\nTable 1. Overview of the Regression Parameter Estimates for the Main Parameters, Alpha (a) and Be\n`ta (b),\nof the Individual Sigmoid Maximum Likelihood Fits.\nParameter Estimate (SE) P(2) 95% CI\nAlpha (a)\nBe\n`ta (b)\nNote. These provided an overall estimation of the time-dependent categorization performance in each test session per\nparticipant and were used separately as DV in a General Linear Modeling (GLMM) approach (McCullagh, 1984).\nobject embedded within a meaningful context (Hypothesis 2). Simultaneously, we also found\nwhich participants were found to have steeper, PT-dependent, slope values in the context\ncondition compared with the object-in-isolation condition. This difference in steepness was\nthis in terms of the specific threshold values, we found that participants only performed better\nin the object-in-isolation condition, compared with the context condition, at lower PT (60%\nbecame larger, this difference in performance disappeared (90% threshold: t(138) \u00bc .25;\nP(2) \u00bc .80). When graphically exploring this effect further (Figures 6 and 7), we observed\na general distribution shift for both threshold (a) and slope (b) values. More precisely, the\nobjects-in-isolation distributions peak earlier than the objects-in-context distributions. See\nTable 1 for parameter estimates and 95% confidence intervals of the final models.\nFigure 8. Overview of (a) average b estimates across participants and (b) average steepness of the\npsychometric curve at 60%, 75%, or 90% correct categorization performance when rapidly detecting a basic-\nlevel object-in-isolation (light blue bar), a basic-level object-in-context (dark blue bar), a superordinate-level\nobject-in-isolation (pink bar), or a superordinate-level object-in-context (red bar). The data are represented\nas the mean performance across participants, with error bars depicting the standard error of the mean (SEM).\nMask control experiment\nOur analysis of the mask control experiment (see Appendix D for parameter estimates and\n95% confidence intervals for the main parameters, a and b, of the individual sigmoid\nmaximum likelihood fits) provided a clear replication of our experimental findings. More\nprecisely, we found significant main effects of level of categorization (t(29) \u00bc \u00c02.14;\npresentation mode (t(29) \u00bc \u00c02.13; P(2) \u00bc .04) for b. This indicated that the difference in\nmask type, used in either the object-in-isolation or the object-in-context condition, was no\nconfounding variable in explaining the current results.\nDiscussion\nHypothesis 1: Basic versus superordinate advantage in ultrarapid categorization.\nWe observed a consistent advantage across participants to rapidly identify superordinate-level,\ncompared with basic-level, object information correctly (Hypothesis 1). This was in line with\nprevious studies on ultrarapid categorization without perceptual masking (e.g., Poncet &\nformulated based on the Leabra model (O'Reilly et al., 2013). This model stated that a\npredefined search task would allow top-down biasing of the relevant visual features even\nwhen stimulus PT lasted long enough to allow recurrent processing to influence the initial\nbottom-up sweep of information in the visual cortex (Bar, 2004; De Cesarei et al., 2015).\nMore specifically, this idea follows from the PDP prediction that the categorization\nmechanism uses a general-to-specific process of conceptual differentiation, allowing\nunambiguous and well-learned object recognition with prolonged stimulus PT to occur in a\ndominantly bottom-up manner (Liu, Agam, Madsen, & Kreiman, 2009; VanRullen & Koch,\n2003). The observed basic-level advantage in verbal semantic labeling tasks without any time\nconstraints (Mervis & Rosch, 1981; Rosch et al., 1976) only follows when an active, conscious\nlabeling of different semantic categories becomes necessary to resolve the given task (Rogers &\nPatterson, 2007). Simultaneously, Fabre-Thorpe and coworkers followed a similar reasoning\nclaiming that the visual processing stage of object categorization has the property of the\nobserved superordinate-level advantage, while active semantic processing leads to a basic-\nthat the requirement for lexical access was critical: the behavioral superordinate- versus basic-\nlevel categorization advantage was determined by the extent to which the semantic domain\ncanceled out the superordinate-level advantage in the visual domain. This prediction was\nsupported by evidence indicating that ultrarapid categorization was color-blind (Delorme,\nRichard, & Fabre-Thorpe, 2010), robust to contrast reductions (Mace\n\u00b4 , Delorme, Richard, &\nFabre-Thorpe, 2010) and relied on very coarse object representations (Thorpe, Gegenfurtner,\nFabre-Thorpe, & Bu\n\u00a8 lthoff, 2001). Furthermore, different studies indicated that rapid\ncategorization can be performed in the near absence of attention (e.g., Li, VanRullen, Koch,\n\u00b4 , & Fabre-Thorpe, 2003). The detection of objects in\nultrarapid categorization was therefore regarded as a preattentive and automatic process\n(VanRullen, Reddy, & Koch, 2004). Recurrent processing even was found to incur small\ncosts in raw overall performance in relatively simple categorization tasks (O'Reilly, 2001). It\nwas argued that these costs could provide a processing benefit in more complex, conscious\nrecognition problems involving generalization or occlusion across nonvisual semantic\ndimensions. This argumentation would fit the predictions of the PDP theory within the more\ngeneral cognitive framework of the reverse hierarchy theory of visual processing (Ahissar &\nHochstein, 2004; Hochstein & Ahissar, 2002). Ultrarapid categorization can be successfully\ncompleted by the rapid and implicit bottom-up processing of visual information without\ngaining any processing advantage when activating explicit, attention-focused top-down or\nreverse hierarchical pathways to effectively inform low-level representations in the visual cortex.\nHypothesis 2: Influence of object presentation mode on ultrarapid object detection.\nWe predicted that participant performance would only be influenced by the congruent context\nwhen and if longer PT in perceptually masked rapid categorization allowed recurrent processing\nto affect response speed (Hypothesis 2). Results indicated that participants were generally faster\nin detecting an object-in-isolation correctly than in detecting an object embedded within a\nmeaningful context, when stimulus PT remained short. When stimulus PT and categorization\naccuracy increased, differential performance between both conditions decreased rapidly and it\ndisappeared completely with almost perfect categorization performance. These outcomes argue\nagainst a contextual processing advantage for participants who are instructed to rapidly detect a\nsalient object in a masked ultrarapid categorization task. This would suggest that embedding the\nsalient object within a meaningful surrounding initially increases stimulus ambiguity and\ncomplexity and therefore increases the overall task difficulty (O'Reilly, Wyatte, Herd,\nMingus, & Jilk, 2013). This contradicted with previous findings, indicating the existence of a\nreaction times advantage when rapidly detecting objects-in-context, compared with objects-in-\nisolation, in an ultrarapid categorization task (e.g., Crouzet et al., 2012; Sun, Simon-Dack,\nGordon, & Teder, 2011). The absence of such a contextual facilitation effect could be linked\nto the absence of perceptual masking in previous categorization designs (Fahrenfort et al., 2007).\nThis prediction follows from the idea that masking derives its effectiveness from disrupting\nrecurrent processing, while leaving feedforward signals intact (Lamme & Roelfsema, 2000).\nThese recurrent connections have been suggested to play an integral role in a range of visual\nprocesses (Hochstein & Ahissar, 2002; Kastner & Ungerleider, 2000; Spratling & Johnson,\n2004), such as figure-ground segmentation (Lamme et al., 2002). The latter process thereby\nseems especially important within the current time-dependent and masked ultrarapid\ncategorization design. Due to the lack of contextual distractor elements or items in the\nisolated object condition, object identification is more accurate than the detection of objects\nembedded in a congruent background. We therefore predict that scene context will only\nfacilitate response speed in ultrarapid categorization, when stimulus PT becomes larger than\nthe time needed for participants to rapidly categorize the presented objects or scenes nearly\n\u00b4 my et al., 2013). This contextual processing advantage might be\nbased on excitatory recurrent processing, predominantly selecting the most likely object category\nwithin the contextual surrounding (Bar & Ullman, 1996; Fabre-Thorpe, 2011). More precisely, it\nwas shown that humans can implicitly learn the temporal covariance of semantic categories of\nnatural scenes (Brady & Oliva, 2008) and the global features of these scenes could be used to\nmodulate the saliency of different contextual regions to guide visual search to pertinent scene\nlocations (Torralba, Oliva, Castelhano, & Henderson, 2006). This would further underline the\nflexible dynamics of object categorization, depending jointly on the level of abstraction, time for\nperceptual encoding, and category context (Mack & Palmeri, 2015).\nFuture Research\nBecause task performance depends on the intersection between task demand and object\ninformation, performance cannot be described in absolute terms (Schyns, 1998). Concretely,\nin our study, this means that performance depends on how much information is available to\nperform the task in each target image and in each nontarget image, on the similarities among\nimages of each group, and also on the information overlap between target and nontarget\nimages. So unless task-related information can be quantified for every image, it remains\ndifficult to directly compare absolute performance between image categories and between\ntasks. For similar (and additional) reasons, differences in stimulus PT can also not directly,\nor indirectly, reflect the timing of the corresponding (underlying) brain processes (VanRullen,\n2011). As a result, the estimation of processing speed for different image categories or in\ndifferent tasks, and its generalization across different image sets, has to be done with\ncaution. The more conservative conclusion, when two psychometric functions are found to\ndiffer, is that the two processes cannot be equated, and thus rely (at least in part) on distinct\nneuronal mechanisms. Future research on the influence of scene congruency and top-down\nprocessing during (ultrarapid) categorization should further focus on quantifying the low-level\nimage properties (e.g., orientation, complexity, and shape) of the selected stimulus set (Joubert,\nRousselet, Fabre-Thorpe, & Fize, 2009; Wichmann et al., 2006) and benchmarking it based on\nthe available information in the specific images (VanRullen, 2011). This is necessary because\nthe diversity of the image set by itself is no guarantee to avoid possible systematic differences\nbetween various image classes, and it has been shown that these differences can allow\nparticipants to discriminate between natural image categories almost perfectly (Brand &\nJohnson, 2014). For instance, it might be that different distributions of attention facilitate\nthe extraction of different types of information within a scene (Brand & Johnson, 2014; Chong\n& Treisman, 2005). When attention is focused locally (e.g., on more low-level physical\nproperties), features are bound together resulting in the identification of an object. When\nattention is distributed more globally (e.g., on more high-level physical properties), the\nsemantic meaning of a scene is extracted based on its global layout. Finally, future research\nshould also focus more on using electrophysiological, rather than psychophysical, methods to\npinpoint the precise latency of the brain processes involved during categorization.\nAuthor's Note\nAuthors Steven Vanmarcke and Filip Calders contributed equally to this work.\n"
}