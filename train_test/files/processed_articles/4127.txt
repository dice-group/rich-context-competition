{
    "abstract": "Abstract\nAchieving data and information dissemination without harming anyone is a central task of any entity in charge of collecting data.\nIn this article, the authors examine the literature on data and statistical confidentiality. Rather than comparing the theoretical\nproperties of specific methods, they emphasize the main themes that emerge from the ongoing discussion among scientists\nregarding how best to achieve the appropriate balance between data protection,data utility,and data dissemination.They cover\nthe literature on de-identification and reidentification methods with emphasis on health care data.The authors also discuss the\nbenefits and limitations for the most common access methods.Although there is abundant theoretical and empirical research,\ntheir review reveals lack of consensus on fundamental questions for empirical practice: How to assess disclosure risk, how to\nchoose among disclosure methods, how to assess reidentification risk, and how to measure utility loss.\n",
    "reduced_content": "http://sgo.sagepub.com\nIntroduction\nThe U.S. Government is devoting unprecedented attention to\nthe health care sector. Among the different initiatives is the\npolicy of increasing the openness of information by providing\nthe public with better access to federal data sets. Achieving\ndata and information dissemination without harming anyone\nis a central task of any entity in charge of collecting data.\nThe balance lies in protecting the privacy of those in the data\nwhile minimizing data utility loss (Kinney, Karr, & Gonzalez,\n2009). Although the need for such balance is true of every\ndata set, it becomes more critical when the information col-\nlected is about personal health status and personal health\ncare received.\nThe benefits of data dissemination are potentially enor-\nmous. Rigorous research providing information about the\nquality, efficiency, effectiveness, and safety of the health\ncare received by members of society can inform and guide\ndecisions in all public policy debates. At the same time, data\nprivacy and confidentiality violations could be potentially\ndisastrous for individuals or groups (Rothstein, 2010). Such\na breach, in turn, could cause irreparable damage to the cred-\nibility of the data collector and disseminator.\nIn this article, we examine the literature on data and\nstatistical confidentiality. Rather than comparing the theo-\nretical properties of specific methods, we emphasize the main\nthemes that emerge from the ongoing discussion among\nscientists regarding how best to achieve the appropriate bal-\nance between data protection and data dissemination. With\nthat objective, we examine the literature published in aca-\ndemic journals and books and proceedings from specialized\nconferences. The fields in which much of the discussion is\nconcentrated include statistics, computer science, data pri-\nvacy and security, electrical engineering, bioinformatics, and\nhealth services.\nThis article provides a summary of key concepts and issues\nin the literature. It is designed to be a point of entry for policy\nmakers, researchers, and practitioners in the health care sector\nwho are new to the literature and for those considering mak-\ning data sets publicly available. The article discusses only the\nliterature on statistical disclosure methods. It does not discuss\ncomputational disclosure control (i.e., computer programs that\nmaintain anonymity by automatically generalizing, substitut-\ning, and removing information when users access the data),\nmethods for tabular data, attribute disclosure (i.e., disclosure\nof sensitive information other than direct identifiers), or\n1IMPAQ International LLC, Columbia, MD, USA\n2Optimal Solutions Group LLC, College Park, MD, USA\n3NORC at the University of Chicago, Bethesda, MD, USA\nCorresponding Author:\nSergio Prada, IMPAQ International LLC, 10420 Little Patuxent Parkway,\nEmail: sprada@impaqint.com\nAvoiding Disclosure of Individually\nIdentifiable Health Information:\nA Literature Review\nSergio I. Prada1, Claudia Gonz\u00e1lez-Mart\u00ednez2, Joshua Borton3,\nJohannes Fernandes-Huessy3, Craig Holden3, Elizabeth Hair3,\nand Tim Mulcahy3\n Keywords\npublic use files, disclosure avoidance, reidentification, de-identification, data utility\n2 SAGE Open\ninferential disclosure (i.e., information that can be inferred\nabout a record in a data set with better accuracy). There is\nsignificant literature on each of these topics, which are\nbeyond the scope of this article.\nOur article is divided into six sections, of which this\n\"Introduction\" is the first. The second section presents\n\"The Policy and Academic Context\" surrounding the dis-\ncussion. The third section discusses the state of the art in\n\"De-Identification Methods,\" while the fourth emphasizes\nthe state of the art in \"Reidentification Methods.\" The fifth\nsection presents the conclusions from the literature on the\ndifferent ways in which users may \"Access\" public data,\nstressing the trade-offs between (a) confidentiality and util-\nity and (b) confidentiality and ease of access. The last sec-\ntion presents the \"Conclusion.\"\nThe Policy and Academic Context\nHistoric Perspective\nConcerns about privacy and confidentiality in governmental\nefforts to collect and disseminate information are not new.\nAs a review by Anderson and Seltzer (2009) suggests, \"the\nroots of the modern concept of federal statistical confiden-\ntiality can be traced directly back to the late nineteenth\ncentury\" (p. 8). Notwithstanding this history, the literature\non statistical disclosure methods is fairly recent by mod-\nern standards (Dalenius, 1977, is considered the seminal\npaper). In 1975, the U.S. Federal Committee on Statistical\nMethodology (FCSM) was organized by the Office of\nManagement and Budget (OMB) to investigate issues of\ndata quality affecting federal statistics. As part of this effort,\nthe Subcommittee on Disclosure Limitation Methodology,\ncreated within the FCSM, published its 1994 Statistical\nPolicy Working Paper 22 (SPWP22). This paper, which\nwas revised in 2005 by the Confidentiality and Data Access\nCommittee (CDAC, 2005), sets good practice guidelines and\nrecommendations for all agencies regarding confidentiality\nprotection.\nDefining Confidentiality and Disclosure\nAdefinition of confidentiality is given in SPWP22.According\nto the document, the definition endorsed by the President's\nCommission on Federal Statistics states that \"[Confidential\nshould mean that the dissemination] of data in a manner that\nwould allow public identification of the respondent or would\nin any way be harmful to him is prohibited, and that the data\nare immune from legal process.\" This definition originates\nfrom the book Private Lives and Public Policies by Duncan,\nJabine, and de Wolf (1993). Similarly, and according to the\nsame source, \"confidentiality differs from privacy\" because\n\"it applies to business as well as individuals. Privacy is an\nindividual right whereas confidentiality often applies to data\non organizations and firms.\"\nSeveral different definitions of disclosure risk have been\nrelates to inappropriate attribution of information to a data\nsubject, whether an individual or an organization.\" The same\nauthors distinguish three types of disclosure: (a) when a data\nsubject is identified from a released file (identity disclosure),\n(b) when sensitive information about a data subject is revealed\nthrough the released file (attribute disclosure), or (c) when the\nreleased data make it possible to determine the value of some\ncharacteristic of an individual more accurately than otherwise\nwould have been possible (inferential disclosure).\nNeed for Protection, Need for Information\nThe biggest policy tension underlying the debate in the literature\nis the need to balance two inherently competing goals: need\nfor information and need for protection. Federal agencies are\nrequired by law to protect the confidentiality of individual\ninformation. For instance, Title 13 of the U.S. Code prevents\nthe census from releasing data in which any particular indi-\nvidual or establishment can be identified. Other legislation\naimed at preventing such disclosures includes the Health\nInsurance Portability and Accountability Act (HIPAA) and the\nConfidential Information Protection and Statistical Efficiency\nAct (CIPSEA).At the same time, the government is committed\nto providing data to the research community for the advance-\nment of knowledge (e.g., Open Government Directive).\nOn one hand, access to microdata (i.e., data collected on\nindividualsorhouseholds)generates\"moreandbetterresearch,\nhigher transparency, better assessment of data quality, bet-\nter assessment of data gaps, and replicability\" (Lane, 2007).\nAdvocates of greater access to health care data justify their\nposition based on the public's need for information about\n\"quality, efficiency, effectiveness, and safety of the health\ncare they receive and pay for\" (Rosenbaum, 2010). On the\nother hand, scholars highlight the need to \"protect patient\nprivacy, the confidential nature of the patient/professional\nrelationship, and health information security\" (Rosenbaum,\n2010). Data breaches also represent legal and financial liabili-\nties for data custodians and erode public trust in their ability to\nhandle data (Couper, Singer, Conrad, & Groves, 2008; Couper,\nThe disclosure literature is divided into two competing\nresearch paradigms: (a) that it is possible to protect and release\ndata and (b) that privacy and confidentiality cannot be achieved\nin an environment in which personal information is gathered at an\nincreasing rate by multiple people with multiple interests. The lit-\nerature on disclosure limitation techniques and their achieve-\nments is extensive. In 1998, for example, the Journal of Official\nStatistics devoted an entire issue to this question. In addition,\nevery 2 years the UNESCO sponsors an international conference\n(i.e., Privacy in Statistical Databases) that gathers worldwide\nexperts from different disciplines to discuss current issues\nin the field. Proceedings are published by Springer in the\nseries Lecture Notes in Computer Science (Domingo-Ferrer,\nPrada et al. 3\nMore recently, the book by Duncan, Elliot, and Salazar-\nGonz\u00e1lez (2011) provides a comprehensive understanding of\nthe principles and practice of statistical confidentiality. Other\nrecent technical reviews are those by B. Chen, Kifer, LeFevre,\nand Machanavajjhala (2009) and Fung, Wang, Chen, and Yu\n(2010) on the latest developments in the field of Privacy-\npreserving data publishing, and by Fayyoumi and Oommen\n(2010) on statistical disclosure control and microaggregation\ntechniques for secure statistical databases.\nThe belief behind the statistical literature is that it is, indeed,\npossible to minimize the risk of disclosure and, therefore,\nto release data to the public. This belief is, however, not\nshared by other scientists. For instance, computer scientists\nNarayanan and Shmatikov (2010) criticize such an approach\nby suggesting that the underlying assumption behind\nde-identification techniques is that personally identifiable\ninformation is a fixed set of attributes such as names and\ncontact information. This, according to the same research-\ners, \"creates the fallacious distinction between `identifying'\nand `non-identifying' attributes.\" In particular, these authors\nclarify that such a distinction might make sense in the context\nof one attack but is increasingly meaningless as the amount\nand variety of publicly available information about individu-\nals grow exponentially (Narayanan & Shmatikov, 2010).\nIn a similar vein, computer scientists Cynthia Dwork and\nMoni Naor show that the type of privacy defined by Dalenius\nin 1977 (\"access to a statistical database should not enable\none to learn anything about an individual that could not\nbe learned without access\") cannot be achieved in general.\nThe obstacle, according to these authors, \"is in the auxiliary\ninformation.\" The main result in their paper is that \"in any\n`reasonable' setting there is a piece of information that is in\nitself innocent, yet in conjunction with even a modified\n(noisy) version of the data yields a privacy breach\" (Dwork\ndefines differential privacy and shows that this type of pri-\nvacy can be implemented and formally proven.\nAs pointed out by one of our reviewers, while both\napproaches (favored by statisticians or by computer scien-\ntists) balance utility and disclosure risk, differential privacy\nis limited in practice. It can only be used in settings where\naccess to the data is remote and controlled. This removes\ntraditional public use files (PUFs) or any microdata delivery\nfrom its scope. Consistent with our objective of providing a\nuseful literature review for practitioners interested in making\ndata sets publicly available, we concentrate our literature\nreview on statistical disclosure methods.\nPrivacy Compromises\nPrivacy compromises in published data have been documented\ncited paper, Narayanan and Shmatikov (2008) show the feasi-\nbility of large-scale reidentification using movie-viewing his-\ntories. These authors suggest that statistical de-identification\ntechniques provide only a weak form of privacy (Narayanan\n& Shmatikov, 2010) and that privacy protection has to be\nbuilt and reasoned about on a case-by-case basis (Narayanan\nLast, other scholars suggest that while attempts to reiden-\ntify individuals are partly mitigated through legal barriers,\nsuch as data user agreements or confidentiality agreements\nthat explicitly ban users from doing it, \"such policies provide\nno formal privacy protection guarantees\" (Loukides, Denny,\nConsequences of Misapplication\nof Disclosure Avoidance Procedures\nAlthough Winkler (2007), using artificial data, warned of\n\"the severe analytic distortions of many widely used mask-\ning methods that have been in use for a number of years,\"\nnot until recently have researchers documented problems\nwith data released in PUFs.\nIn particular, Alexander, Davern, and Stevenson (2010)\ndiscover and document errors in public use microdata sam-\nCurrent Population Survey, due to the misapplication of dis-\nclosure avoidance procedures. The particular procedure that\ncaused the problem is not cited in the paper, nor disclosed in\nThese authors show that for women and men ages 65 and\nolder, age- and sex-specific population estimates generated\nfrom the PUMS files differ by as much as 15% from counts in\npublished data tables. Additional analysis of labor force par-\nticipation and marriage rates in the same publication shows\nthat PUMS samples are not representative of the population\nat individual ages for those ages 65 and older, and that PUMS\nfiles substantially (a) underestimate labor force participation\nfor those near retirement age and (b) overestimate it for those\nat older ages.\nFinally, these authors emphasize that the problem could affect\nawholerangeof stakeholders: researchers,social serviceagencies\nthat rely on the data for policy research, and survey researchers\nwho use PUMS data to generate population estimates.\nDe-identification Methods\nThe first step to prevent disclosure is to remove all direct iden-\ntifying variables, such as name, phone number, and address.\nThis step is intuitive, and one could naively think the data set\nis then safe from disclosure because no individual is explicitly\nidentifiable. However, as cited earlier (Agrawal & Srikant,\nprotect all individuals from data disclosure or reidentifica-\ntion. A combination of just a few indirect identifying vari-\nables (such as birth date, gender, and zip code) can be used\n4 SAGE Open\nto identify a large portion of individuals on any data set.\nThese variables could then be matched to publicly available\ndata to identify records. Disclosure risk represents the risk of\nindirect identifiers (IVs) being used to match records to an\nexternal data source that contains direct identifiers. The chal-\nlenge of any de-identification technique is to limit reidentifi-\ncation via the use of indirect identifying variables.\nThis section describes the methods used to de-identify a\ndata set. First, we define disclosure and disclosure risk and\ndiscuss the goals of disclosure treatment. We then describe\nthe methods for de-identification of microdata and the use\nof nonsynthetic and synthetic treatments. Finally, we dis-\ncuss the issues related to de-identification of longitudinal\nmicrodata.\nDisclosure and Disclosure Risk Defined\nDisclosure is the communication, either directly or by infer-\nence, of information about a member of a data set that could\nnot be known without viewing the data set. We call this\ninformation \"sensitive information\" going forward. It is the\nobligation of the owner of the data set or data provider to\nprevent disclosure of sensitive information about members\nof the data set.\nAn intruder is someone who wants to discover sensitive\ninformation about any person (or other entity) from the data\nset. In short, disclosure is the process of an intruder discover-\ning sensitive information about a target they did not know\nprior to intrusion.\nDisclosure risk is a measure of the probability of disclo-\nsure for either an individual record or a data set as a whole.\nSkinner (2009) gives two useful definitions: (a) Disclosure\nrisk is concerned with the possibility that the intruder will be\nable to determine a correct link between a microdata record\nand a known unit and (b) disclosure risk might be defined as\nthe probability of disclosure with respect to specified sources\nof uncertainty, such as whether the disclosed information is\naccurately attributed to a target.\nThese definitions show that, for any data set, the calcula-\ntion of disclosure risk relies heavily on assumptions about the\nintruder's knowledge. The more knowledge an intruder has\nabout a potential target in a data set, the higher the probability\nthat the intruder will be able to correctly identify the target\nand disclose information.\nWhat Are the Goals of DisclosureTreatment?\nDalenius (1977) states that \"access to a statistical database\nshould not enable one to learn anything about an individual\nthat could not be learned without access.\" While this is a\nnoble goal, Dwork (2006) states that this level of privacy,\nzero disclosure risk, cannot be achieved for microdata or\neven published macrodata. Given that disclosure risk cannot\nbe zero for analytically useful data sets, the goal should be\nto make the risk as low as possible. For instance, Skinner\n(2009) says that \"confidentiality of the answers provided by\na respondent might be said to be protected if the disclosure\nrisk for this respondent and the respondent's answers is suf-\nficiently low.\" Previous research (Dalenius, 1988; Fienberg\n& McIntyre, 2005) argues that data should be released if the\nprobability of identifying an individual or entity in the data\nfile is appropriately small. The emerging consensus of the\nfield is that if the disclosure risk is small then data should be\nreleased; however, current research has not been specific\nabout a defined risk measure, assumptions about the intruder,\nor what constitutes \"small.\"\nWinkler (1997) takes a different stance, stating, \"in pro-\nducing confidential public-use data files, statistical agencies\nshould first assure that the files are analytically valid.\" That is\nnot to say that data confidentiality is unimportant; it just calls\nout that producing a data set with low disclosure risk but little\nanalytic utility is akin to not producing a data set at all because\nits release has no analytic benefit.\nTherefore, the goal of the data producer should be to pro-\nduce an analytically useful data file, with acceptably small\ndisclosure risk for the individuals in the file. Currently, the\ndefinition of what is \"acceptably small\" is up to the provider,\nbased on the provider's obligation to the participants in the\ndata set. If risk cannot be made adequately small while pre-\nserving data utility, the provider should consider alternative\nmethods of publishing or limiting access to the data. A com-\nbination of de-identification, access control, and data use\nagreements may be a more appropriate solution (Abowd &\nThe following sections are discussed in the context of\ntreating a microdata file, but many of the same techniques\ncan also be applied to tabular data (Skinner, 2009).\nNonsyntheticTreatments of Microdata\nAs opposed to synthetic disclosure treatments, where all\nrecords in a data set are treated, nonsynthetic methods treat\nonly a fraction of the records in the data set. This is usually\ndone deterministically to reduce the disclosure risk of a\nsmall group of (or single) records. Nonsynthetic disclosure\ntreatments consist of three primary tools: global recoding,\nsuppression, and perturbation. Nonsynthetic disclosure treat-\nments are designed to specifically treat records with high\ndisclosure risk to produce a data set that has a low risk of\ndisclosure. However, the deterministic nature of the treat-\nment can introduce selection bias that can degrade analytic\nGlobal Recoding and\nLocal Suppression (GRLS)\nGlobal recoding is a process of reducing the number of\nvalues a single variable can have in a data set. For example,\nan individual's birth date may exist in a data set and could\nbe used as an indirect identifying variable. However, the\nPrada et al. 5\nvariable could be recoded to birth year and be less useful as\nan IV; it could be further recoded to 5-year intervals to make\nit even less identifying. Decisions on the appropriate level of\nrecoding are based on the trade-off made between the com-\npeting needs for data confidentiality and preservation of data\nutility.\nLocal suppression is the process of removing, or sup-\npressing, data from a data set. This can be done for single\nvariables within a record or an entire record. Level of sup-\npression is again determined by the de-identification strategy\nbeing used and the competing needs for data confidentiality\nversus data utility.\nSweeney (2002) defines k-anonymity as follows: \"A\nrelease provides k-anonymity protection if the information\nfor each person contained in the release cannot be distin-\nguished from k-1 individuals whose information also appears\nin the release.\" The concept of k-anonymity is the same as\n\"cell size,\" which was already in use by professional statisti-\ncians interested in limiting disclosure of information in pub-\nlic data sets. Willenborg and de Waal (1996) provide a\nhistorical perspective on this broadly used concept. Sweeney\nachieves k-anonymity in a data set through the use of global\nrecoding. Indirect identifying variables are recoded until\neach combination of recoded variables has at least k number\nof records associated with it. At this point, no individual in\nthe data set can be identified with certainty because no indi-\nvidual has a unique IV profile.\nThere are algorithms whose purpose is to make the pro-\ncess of recoding as efficient as possible by minimizing the\namount of information loss while reducing disclosure risk.\nEl Emam et al. (2009) discuss some of these information loss\nmetrics in detail. Note that they are only useful in making\ndecisions regarding recoding and suppression; they do not\ngive the user/analyst any measure of data utility.\nThe concept of k-anonymity drives several real world\nsystems, including Datafly, k-Similar (Sweeney, 2002);\nSamarati, Incognito, and Optimal Lattice Anonymization\n(OLA; El Emam et al, 2009); and \u00b5-argus (Hundepool et al.,\n2008). Most of these packages use local suppression in addi-\ntion to global recoding to create a k-anonymous data set.\nPerturbation\nPerturbation, another process that can be used to reduce\ndisclosure risk, alters the values of variables on the data set.\nThis could be performed to make reidentification more dif-\nficult on (a) variables expected to be known to the intruder\n(IVs) or (b) particularly sensitive information not known to\nthe intruder (sensitive variables or SVs). Nonsynthetic per-\nturbation treats only a portion of the records on the data set.\nSynthetic perturbation treats all records in the data set.\nNonsynthetic perturbation can be random or selective.\nSelective perturbation deterministically selects records for\ntreatment to reduce disclosure risk. Also called blank and\nimpute, this method selects values from single records,\nremoves them from the record, then imputes a new value.\nThis means that certain values may be targeted more fre-\nquently, creating a bias that is difficult for the analyst to\nquantify when interpreting the data (Skinner, 2009).\nData swapping was one of the first perturbation methods,\nproposed by Dalenius and Reiss as early as 1982. It was\nproposed as a method to transform a data set by exchanging\nvalues of SVs in such a way as to preserve their confidenti-\nality while maintaining data utility. Records were selected\nfor a \"data swap\" of a single SV if the swap resulted in a\ndecrease in disclosure risk and preservation of marginal\ncounts associated with that SV. This method was shown to\nreduce disclosure risk while protecting data utility for con-\ntingency tables and log-linear models; however, data utility\nwas found not to be preserved for other types of analysis\nSubstitution is similar to data swapping in that data from\none record are replacing data on another record. As proposed\nby Singh (2009), substitution is the process of replacing some\nor all IVs in a record with the IVs from another record. It is\ndifferent from data swapping in that the data only move in\none direction. The pairing of records for this substitution pro-\ncess relies on techniques common in survey sampling for the\nimputation of missing values. Substitution reduces disclo-\nsure risk by creating uncertainty about the association between\nthe IVs and SVs for a given record. Records can be selected\nfor substitution deterministically to reduce risk, although in\nSingh, they are selected randomly.\nGenMASSC: Global Recoding + Random\nPerturbation + Random Suppression +\nCalibration\nthe combination of multiple elements from synthetic and\nnonsynthetic frameworks to de-identify data while simulta-\nneously controlling disclosure risk and information loss. The\nfirst treatment step is global recoding, the amount of which\nis driven by reducing disclosure risk within constraints that\npreserve data utility. The second step is random perturbation\nby substitution. Records are randomly selected to have their\nindirect identifying variables replaced with variables from a\ndifferent record on the data set with similar properties. The\nthird operation is the random suppression of entire records\nfrom the data set. The rates of selection for substitution and\nsuppression can be functions of the disclosure risk of the\nrecord, so that records at higher risk may be chosen for treat-\nment at a higher rate. After all treatment is complete, the\ndata set is calibrated so that predetermined analytic values\nare representative of the data set prior to treatment. The sto-\nchastic nature of this treatment limits the amount of bias in\nthe treated data set and allows the data provider to monitor\nand control the amount of bias and variance in the treated\ndata for a given level of disclosure risk.\n6 SAGE Open\nSyntheticTreatments of Microdata\nSynthetic treatments treat all records in the data set to create a\nnew, \"synthetic\" data set that is representative of the original\ndata file. This is usually done by treating all the indirect iden-\ntifying variables (such as birth date, gender, and zip code) for\neach record in the data set. The indirect identifying variables\nmay be changed by a variety of methods, including perturba-\ntion, multiple imputation, and other model-based techniques\n(Skinner, 2009). Such methods are tuned to preserve data\nutility, at least for anticipated analyses, and data confidenti-\nality is assumed to be improved because none of the records\nin the microdata represents an actual individual.\nIn stochastic perturbation, indirect identifying variables\nare modified by a random mechanism. Continuous variables\nare altered by adding random noise. The noise may be added\nso that the mean and variance of the variables within certain\ndomains will be preserved. However, correlations outside the\nspecifications (i.e., variables selected for synthetic treatment)\nat the time of the noise addition will not be preserved (Skinner,\n2009). For instance, consider the case when a data producer\napplied synthetic treatment aimed at preserving the relation-\nship between income and gender but decided to leave race\noutside the specification. If a user were to analyze the rela-\ntionship between race and income, the data might be distorted\nbecause the treatment was not trying to preserve that particu-\nlar relationship (i.e., race and income). In general, it may not\nbe practical, or possible, for all relationships to be preserved\nduring treatment. The data producer may have to make some\ntough decisions about which relationships are most important\nand, thus, to be preserved during treatment.\nFuller (1993) discusses methods that can be used to\nimprove the analytic utility regarding these \"unspecified\"\nvariables. These methods require the data provider to let the\nanalyst know the standard deviation of the noise that was\nadded to the indirect identifying variables. The analyst must\nthen add procedures that use this information to the analysis\nfor the output to gain the same inferences as the untreated\ndata. A potential difficulty with this approach is that some\nreidentification experts suggest that knowing information\nabout how the noise was applied to the variables can allow\nan intruder to reverse \"engineer\" the data file and poten-\ntially identify individuals. Details on how noise can be\nreverse engineered are available in Domingo-Ferrer, Sebe,\nOther forms of synthetic perturbation include data swap-\nping or substitution for all records in the data set. This is an\nextension of the previously mentioned nonsynthetic perturba-\ntion methods, where selection for treatment is expanded from\ndeterministic or random selection processes to 100% selec-\ntion as part of a synthetic treatment.\nCategorical variables can be reclassified using a modeling\nmechanism such as the postrandomization method (PRAM;\nGouweleeuw, Kooiman, Willenborg, & De Wolf, 1998). These\nmethods use other variables in the data set to find probabilities\nfor levels of the categorical indirect identifying variable that\nis to be treated. The model is then scored for all records and\nthe initial value of the variable replaced with the new variable.\nDepending on the structure of the model used, the treated data\nset can be analyzed as is, or may require additional informa-\ntion from the data provider to the researcher to perform valid\nanalysis. As with noise addition, the application of this extra\ninformation requires increased computations by the analyst\nto produce valid results.\nMultiple imputation uses a model to create synthetic\nrecords based on a known distribution of indirect identifying\nvariables for the data set. More records are created through\nthese processes than are intended for release. The population\nof synthetic records is then sampled multiple times to esti-\nmate the analytic properties of the data set, with one of the\nsamples released as the treated data set. The analyst can treat\nthis data set as a survey sample with known variances and\nuse standard survey sample techniques of analysis.Adetailed\nreview of this process can be seen in Rubin (1993). Abowd,\nStinson, and Benedetto (2006) present an implementation\nof this technique using linked data from the census, Social\nSecurity Administration, Internal Revenue Service, and\nCongressional Budget Office. Multiple imputation removes\nthe need for the provider to pass information regarding per-\nturbation to the analyst, which also frees the analyst from\nextra calculations required when analyzing data.\nSynthetic data approaches can also be applied to a subset of\nagency could be interested in replacing income when it exceeds\na certain threshold but is willing to release all other values\n(Reiter, 2009). The result is a partially synthetic data set.\nAs mentioned, the advantage of synthetic data is that they\nare designed to preserve data utility. The data confidentiality\nof the process is assumed to be implied because no \"real\"\nrecords are released. Domingo-Ferrer & Torra (2003) shows\nthat this is not necessarily the case, however, and that there\nare reidentification techniques capable of disclosing infor-\nmation about individuals in a synthetic data set that has been\nde-identified using synthetic treatments. Other limitations of\nsynthetic microdata are (a) the expertise and effort required\nto build a model and (b) that the quality of the treated data\nand its analysis is a direct result of the quality of the model\nDe-identification of Longitudinal Microdata\nThe de-identification of longitudinal data has not been explored\nby many researchers, as it has been thought an unobtainable\ngoal. Several researchers note that preserving the data confi-\ndentiality and data utility of a public use longitudinal data set\nmay be inherently incompatible goals (Abowd & Woodcock,\nand colleagues have successfully used multiple imputation\nsynthetic de-identification techniques to treat longitudinal\nPrada et al. 7\nimportant to note that these techniques summarize the longi-\ntudinal data prior to treatment and data are still de-identified\nby individual, not by longitudinal event or record. In addi-\ntion, data present on the longitudinal file that were not sum-\nmarized prior to treatment are not available to the analyst, as\nthere is no direct publication of the longitudinal data.\nAssessing Data Utility AfterTreatment\nData anonymity and analytic utility are in constant tension,\nwith increases in one resulting in a decrease in the other.\nHowever, assessment of data utility is a vital step following\nde-identification treatment and should go in concert with dis-\nclosure risk analysis. Researchers have developed a variety of\nmethods by which to automate the analysis of data utility\nusing specially designed software. Together with the assess-\nment of risk, it has also been shown that certain methods of\nde-identification are less effective than others at maintaining\ndata utility and protecting personal privacy (Winkler, 2007).\nKennickell and Lane (2007) give a good overview of the role\nof data utility in the context of de-identification treatment and\nseveral methods that appear throughout the literature.\nOne such method of comparison is between simulated\nresearch results on treated and untreated data sets. In their\nanalysis of disclosure risk and analytic utility, Brickell and\nShmatikov (2008) show it is necessary to render a data min-\ning utility near useless to researchers when using generaliza-\ntion and suppression of quasi-identifiers to de-identify a data\nset. These researchers compared their results with what\nwas called \"trivial sanitization\" of the data set, which simply\nomits either all quasi-identifiers or all sensitive attributes to\nprovide maximum privacy.\nRastogi, Suciu, and Hong (2007) depict a framework\nfor describing privacy and utility of a de-identified data set.\nPrivacy is defined as a comparison of an attacker's probabil-\nity of an ordered list of elements against the probability based\non experimental observation. To illustrate data privacy and\nutility, these researchers use census data to evaluate a selec-\ntion of queries with up to three attributes and estimates of the\nerror bound on counting queries. They describe a simple ano-\nnymization algorithm that uses random insertions and dele-\ntions of varying series of data in, or from, the database.\nOther methods compare estimates taken from treated and\nuntreated data sets. Winkler (2007); Raghunathan, Reiter, and\ncorrelation and regression coefficients to assess the analytic\nutility of data sets treated to reduce disclosure risk. These\nmethods assume the data provider can anticipate many of\nthe correlations that will be useful to the user and measure the\nimpact of treatment on these relationships prior to release.\nSingh (2009) assesses data utility of a treated data set by\ncomparing the means of several variables of interest across\nmultiple replications of treatment. By using multiple treated\ndata sets, a relative (to the mean of the untreated data) root\nmean squared error (RRMSE) can be computed to describe\nhow much the value in a treated data set can be expected to\nvary from the value in the untreated data. This measure of\ndata utility is on the same scale for all variables of interest\nbecause it represents the error of the treated values relative to\nthe untreated values. The maximum RRMSE represents the\nerror of the least reliable variable, or relation of interest, and\ncan be used as a simple measure of data utility. This method\nalso assumes that the data provider has a good idea of the\nrelationships that will be important to the user to measure\ndata utility in the proper context.\nReidentification Methods\nWith the collection and provision of data comes the risk of\nidentifying individuals within data sets and the associated\nharms that can run the gamut from inconsequential to cata-\nstrophic. Several methods have been developed to assess the\nrisk of this reidentification and test data sets for the ability\nto identify specific people. The literature reveals four areas\nby which reidentification practice occurs: linking records\nacross multiple data sets, linking data across multiple data\nsets, matching patterns within multiple data sets, and, most\nrecently, identifying individuals in the public space from\nThe methods presented in the following sections all seek\nto identify individuals when one or more data availability\nscenarios are present (Domingo-Ferrer & Torra, 2003):\n\u00b7 Where there are common variables and a common\nterminology in multiple data sets and these are lev-\neraged to effect reidentification of individuals;\n\u00b7 Where there may be common variables but differ-\ning terminology between data sets;\n\u00b7 Where there are no variables in common between\ncomparable data sets but an existing and common\nterminology exists; and\n\u00b7 Where, in the final and most challenging scenario,\nvariables and terminology are different between\ndata sets.\nIn each, the method used makes at least the assumption\nthat there are individuals in common within the associated\ndata sets.\nAssessing Risk\nGenerally, disclosure risk for a target increases as more is\nknown, in terms of quantity and precision of data. One of the\nmost common methods to measure disclosure risk is to count\nthe number of unique records within a data set with a limited\nset of individual record characteristics (El Emam et al., 2010).\nResearch has also focused on estimating the number of\nuniques within a population from a sample of data given dif-\nferent possible population distributions (Bethlehem, Keller, &\n8 SAGE Open\nThe challenge in developing different methodologies to\nassess risk is the need to accurately reflect risk. Too conserva-\ntive risk assessment needlessly sacrifices data utility in favor\nof individual anonymity; risk assessment that errs on the other\nside risks disclosure of sensitive information. This has been\ndescribed as the over- and under fitting of risk estimates and is\nthe main thrust of research in risk disclosure and development\nof the two-way interaction model (Skinner, 2007).\nSkinner and Shlomo (2008) further refine the two-way\ninteraction model for estimating disclosure risk measures\nthrough development and use of diagnostic criteria for model\nchoice with the goal of balancing over- and under fitting.\nThese researchers illustrate the ability to use Poisson log-\nlinear models in the assessment of risk in large and sparse\ncontingency tables spanned by key variables. Their approach\nhas been shown to be useful for file-level and record-level\nmeasures of risk.\nTruta, Fotouhi, and Barth-Jones (2004) introduce a gen-\neral framework for assessing disclosure risk by classifying\ndata set attributes based on either potential identification util-\nity or order with regard to domain of value. These values,\ntermed change factors, measure the magnitude of masking\napplied to data and the modification that has occurred to key\nattributes. Using simulated medical billing data with identifier\nattributes removed, the researchers are able to show minimum,\nmaximum, and weighted disclosure risk values for a number\nof different masking methods. They perform a series of exper-\niments whereby random noise is added to identifying attri-\nbutes (age, sex, ZIP code, and billed amount) and the effect\non disclosure risk measured. The method described by these\nresearchers allows for a measure to assess the amount of\ninformation loss as a result of the specific masking method\nused; it also presents a way to measure and set the level of\nmasking desired to achieve a preset level of risk.\nBenitez and Malin (2010) illustrate the wide gap between\nperceived threats of reidentification and actual results. The\npaper tests voter registration data as a route of potential\nreidentification of publicly released health records protected\nby the Safe Harbor policy. In particular, the authors sug-\ngest that allusion to the potential uses of voter lists in the lit-\nerature (Sweeney, 1997) rarely acknowledges the complexity\nof the data (i.e., access and quality) or the economic costs to\nan attacker.\nTheir risk analysis estimation in Benitez and Malin (2010)\nis probabilistic in nature, as it quantifies the likelihood of\nreidentification for each member of a group. The analysis con-\nsists of a three-step process: (a) determine the fields available\nto an attacker (i.e., year of birth, race, and gender in health\nrecords and date of birth, year of birth, race, gender, and\ncounty of residence in voter registration); (b) group census\ndata according to these fields to estimate population counts;\nand (c) add results obtained by applying risk estimation met-\nrics to the results, and normalize by total population.\nBenitez and Malin (2010) find that risk levels and costs\nvary widely across different states due to individual voter\nregistration policies, for example, with more permissive states\nhaving higher risks of disclosure.\nIndividual risk methodology, developed by Benedetti and\nFranconi (1998), involves the computation of individual risk\nfor each unit of analysis within a data set as the probabil-\nity of correct reidentification. The risk of reidentification\nis expressed through the concept of unique or rare combina-\ntions in the data, and the methodology uses sampling weights\nto account for the uncertainty of whether such unique combi-\nnations are common or rare in the population. All records\nwith individual risk above a fixed threshold are defined as\nbeing at risk, implying that disclosure control methods must\nbe used to protect these records.\nElliot (2000) defines an additional measure of disclo-\nsure risk that measures correct matches between actual and\nmasked data sets. Termed Data Intrusion Simulation (DIS),\nthe researcher describes a method that uses the target data set\nto estimate matches given a unique match. The method for-\ngoes the use of an entire population and instead uses a sam-\nple. The method contains five steps:\n1. Take a sample microdata file with sampling fraction.\n2. Remove a small random number of records to\nmake a new file.\n3. Copy back a random number of the records.\n4. Match a simulated fragment of the identifica-\ntion file with the target microdata file. Generate\nthe probability of a correct match given a unique\nmatch for the fragment.\n5. Iterate until the estimate stabilizes.\nThe effect is to generate a risk of disclosure for a given\ndata set without assuming that a given unique record is a\npopulation unique. In addition, this method retains the useful-\nness of matching against actual data without being a nonge-\nneralizable, ad hoc approach using the entire data set on which\nto attempt a match.\nSources of Data for Reidentification\nThere are many entities, not covered by HIPAA, that collect\nand disseminate identifying information to clients and other\nusers. This information is collected from a variety of sources\nand, if used in combination with information from health data\nsets, may potentially contribute to the identification of indi-\nviduals. To our knowledge, there have not been demonstrated\nreidentification attacks using these sources. Such sources\nof data and information include social networking websites,\ntransactional data, voter registration records, state agencies,\nand web crawlers, among others.\n\u00b7 Social networking sites collect a plethora of iden-\ntifying information including data on the habits\nand behaviors of consumers, for instance, websites\nsuch as patientslikeme.com, healthboards.com, and\nPrada et al. 9\nWebMD.com. There are many examples of this\ninformation already being used in a commercial\nmanner--for example, in targeted online advertise-\nment. However, it is worth noting that this is not\nevidence that the data are used for reidentification\npurposes.\n\u00b7 Transaction data, such as collected by credit card\ncompanies or credit-reporting companies (e.g.,\nEquifax, TransUnion, and Experian), hold enor-\nmous amounts of sensitive financial transactional\ndata that could potentially be recombined with pub-\nlicly released data to reidentify consumers and sold\nfor behavior prediction and targeted marketing.\n\u00b7 Public information, such as voter registration, court\ncases, and many other government transactions, is\npublicly available and aggregated by private com-\npanies. Examples include Intelius, NextMark, and\nInfogroup (previously InfoUSA).\n\u00b7 Health care data are also becoming increasingly\navailable. For instance, states such as Vermont and\nTexas have de-identified administrative data on hos-\npital discharges available either free (Vermont) or\nfor a fee (Texas). Similarly, the Pennsylvania Health\nCare Cost Containment Council provides identified\ntabular data on its website for free at the provider\nlevel (e.g., hospital, medical doctor) and microdata\nfor a fee.\n\u00b7 Internet search engines, such as Pipl.com, are also\nproliferating. Unlike most data aggregators, these\nsites crawl the web looking for other websites and\ndata miners with personal data. Interested users only\nneed to provide the search engine with the most they\nknow about the person they are looking for (e.g., first\nname, last name, city, state). In response, the search\nengine displays links to information available on the\nweb for persons with matching characteristics.\nIn what follows, we present selected methods that have\nbeen developed either as modifications of methods originally\nintended for other purposes or specifically for data reiden-\ntification. Reidentification methods expose weaknesses in\nmasking methodologies and other efforts to protect individ-\nual and group privacy. We do not present an exhaustive list of\nthe many available reidentification techniques, which vary\nwidely in their complexity. Rather, we provide a broad over-\nview of the major operating themes they represent.\nRecord Linkage\nInitially developed as a method to synchronize files in cases\nwhere one may contain incorrect or inaccurate data, record\nlinkage seeks to use two or more lists to classify pairs and\nform definite matches between each to string together records\nfrom different data sets (Malin, Sweeney, & Newton, 2003).\nFor record linkage to proceed, a number of assumptions must\nbe made about data within the sets in question. One such\nassumption is that there are common variables between the\nfiles. Matching data sets against commercially and publicly\navailable data is one method by which reidentification can\noccur (Winkler, 2004a). Increasingly sophisticated reidentifi-\ncation methods combined with greater availability of public\ninformation has resulted in increased risk of data disclosure\nWinkler (2004b) describes record linkage methods as\nusing metrics to scale the ranges of variables while partially\naccounting for dependencies between them. Scheuren and\nWinkler (1997) have illustrated how economic variables can\nsubstantially increase the accuracy by which linkages in\nadministrative lists can be made. Correlations between these\nvariables allow researchers to create predictors that permit\nrecords from one of the files to be closer to smaller subsets\nof other records in the other file. The probability of identify-\ning individuals increases as the subset of predicted records\ndecreases.\nThe most commonly used example in the literature is voter\nregistration records, but many other data sets can be used.\nLoukides et al. (2010) illustrate how genetic research data can\nbe used to reidentify patients within a health data set, even\nafter suppression methods including the application of gener-\nalization and the HIPAA Privacy Rule. The authors achieve\nthis by linking diagnosis codes (International Classification\nof Diseases\u00adNinth Revision [ICD-9]) derived from electronic\nmedical records with released research data in the form of\nDNA sequences.\nBacher, Brand, and Bender (2002) illustrate the potential\nto reidentify persons within data sets using a feature of com-\nmonly used statistical software. Specifically, these research-\ners use cluster analysis with SPSS to match survey data\nagainst register data in a German context. The approach cho-\nsen by the researchers transforms and weighs variables and\nobtains a reidentification risk of approximately 10%.\nData Aggregation\nWhile record and data linkage require direct relationships\nbetween features associated with the data sets, aggregation-\nrelated approaches attempt to reidentify when there are no\ncommon attributes (Winkler, 2004b). The objective of data\naggregation with regard to reidentification is to create an\nordering of the data using combinations of individual attri-\nbutes. To do this in data sets containing numerical data, sev-\neral assumptions are necessary, including the following:\n1. There are common individuals in the two data sets.\n2. The structures to the data contained within the data\nsets are similar.\nReidentification is then achieved by matching records\nthat have similar groups of attribute combinations; it occurs\nwhen public information is linked to data files, and names,\naddresses, or other information are at risk of being released\n(Domingo-Ferrer & Torra, 2003). When it is known that\nthe populations in the acquired data sets are overlapping, it\nbecomes possible to use variables from one of them to iden-\ntify a subset of records from the other (Reiter, 2003).\nProbabilistic Inference\nWith the use of Markov random fields and graph partitioning\nalgorithms, the ability to increase the chances of identifying\nindividuals through the linking of records and data in groups\nof files has been illustrated by McCallum and Wellner (2003).\nThere are a number of important differences between data\nlinkage and record linkage, particularly in that data linkage\nwas developed with the intent of reidentification. The aim of\ndata linkage is to make reidentification possible for data com-\npletely lacking seemingly identifiable information (Malin\net al., 2003). Furthermore, attributes of the associated data sets\nare not required to be the same, as the technique makes use of\ninferential relationships between file attributes, which is the\nprocess of attempting to reidentify when there are no com-\nmon attributes between data sources.\nNarayanan and Shmatikov (2008) develop a general class\nof algorithms to identify individuals within large, sparse data\nsets (i.e., data sets where only a fraction of the cells contain\nrelevant information). These algorithms take into account\nsome amount of auxiliary information provided on a target,\nand score the records within the data set according to how\nwell it matches the target. From here, a matching criterion is\napplied, and a single record or set of probable records is iden-\ntified as a match. Narayanan and Shmatikov also illustrate\nthe algorithms' resistance to de-identification data perturba-\ntion and methods of generalization and suppression. The\nalgorithms described were applied to the Netflix Prize set up\nby the movie rental company to improve their system. More\nthan 100 million customer movie ratings were made publicly\navailable. Despite the removal of identifying customer infor-\nmation, the researchers were able to illustrate that simply\nremoving identifying information is insufficient to produce\nanonymity.\nIn data linkage, characteristics of individual records of\nthe data set are combined to estimate the uniqueness within\na known population (Sweeney, 2000). Sweeney illustrated\nthat, based on gender, ZIP code, and full date of birth, 87%\nof the U.S. population can be uniquely identified. The addi-\ntion of extra information (i.e., race) adds more granularity\nand scarcity, thus increasing the likelihood that a record is\nunique. Sweeney has indicated that linkage is established\nthrough known attributes, and the probability of identify-\ning individuals increases with the addition of further data\nIt is important to note that these attacks were not on health\ndata. The question then becomes whether this kind of inference\ncan be applied to health data. We did not find any evidence of\nthis in the published literature.\nTrail Reidentification\nTrail reidentification expands on the concept of reidentifi-\ncation by seeking to identify people who visited named\nlocations in a network environment (Malin et al., 2003).\nTrail reidentification seeks to reconstruct a person through\nseparately collecting and subsequently relating de-identified\ndata on people who visited the location. The collected\nde-identifieddataconsistofveryfewdatafields.Recognizing\nuniquely occurring visit patterns across the de-identified and\nidentified data sets provides the basis for trail reidentifica-\ntion. These observations are made explicit by constructing a\nmatrix of shared de-identified data and a matrix of shared\nidentified data. The relationship to health information data\nsets exists in the ability to use this trail reidentification infor-\nmation, separately or in combination, to locate individuals\nand associated sensitive health information (Malin et al.,\n2003). Information gained by way of trail reidentification\nmay be leveraged with health data set information to further\nuncover health or chronic disease conditions.\nStandards for Acceptance of\na File as Safe in Health Care Data Sets\nThere are two elements to the HIPAA Safe Harbor method\nof de-identification: (a) 18 specific identifiers and (b) actual\nknowledge. The Safe Harbor method has two parts. Part I\ndictates the removal or coarsening of 18 direct, or almost\ndirect, identifiers that may be present in any data set. These\nidentifiers fall into four categories: names, dates, contact infor-\nmation, and record IDs. Part II of the Safe Harbor method\nrequires the covered entity to ensure that it possesses no\nactual knowledge of an individual being at risk of disclosure\nafter removal of the 18 identifiers.\nThe critical part of the aforementioned standard is its\nincorporation of a reasonable person standard. While not a\ndefined legal term of art, this language likely indicates a\nsignificant safeguard for those who de-identify data. From\nthe perspective of legal interpretation, language like \"to which\nthere is no reasonable basis to believe\" indicates that so long as\nthe covered entity was not negligent in the de-identification\nprocess, it is likely exempt from liability as long as it\nacts reasonably and does not believe that reidentification\ncould occur.\nIn 2002, researchers at the Carnegie Mellon's Data Privacy\nLab suggested the concept of \"The Minimal Risk Standard\" as\na way to operationalize Part II of the method for commercial\npurposes (Sweeney, 2010b). Two companies, Privacert Gold\nStandard and Quintiles, licensed the Data Privacy Lab's\nrisk-assessment technology to provide HIPAA certifications\nthe Minimal Risk Standard, the identifiability of proposed\ndata should be no more than the identifiability if the pro-\nposed data adhered to Safe Harbor Part I (Sweeney, 2010b).\nThis, in practice, became a question of measuring the risk of\nreidentification of a data set under Safe Harbor Part I. Work\nby Sweeney (2000) based on demographic uniqueness in the\nU.S. population showed that Safe Harbor Part I provides a\nrisk of reidentification of 0.04% when demographic infor-\nmation released is restricted to gender, year of birth, and\ncounty of residence.\nThe online appendix of El Emam (2011) includes a sum-\nmary of metrics that have been used for identity disclosure in\nactual de-identification projects, including approaches\nembedded in software such as \u00b5-argus.\nAccess\nThe vast amount of data now collected on human beings and\norganizations as a result of cyberinfrastructure advances has\ncreated significant opportunities for social scientists to study\nand understand human behavior. At the same time, technolo-\ngies have recently emerged, such as virtual private network\n(VPN), biometrics, and virtual computing, that permit\nmicrodata to be accessed in convenient ways while also\nprotecting data confidentiality (Lane, Heus, & Mulcahy,\n2008). The legal framework surrounding data access has\nevolved in recent years on a parallel course. For instance,\nlandmark legislation, the CIPSEA of 2002, establishes rigor-\nous confidentiality safeguards while setting provisions for\nthe statistical agencies to \"designate agents, by contract or\nby entering into a special agreement\" for the purpose of\nperforming \"exclusively statistical activities, subject to the\nlimitations and penalties\" within the boundaries stipulated in\nthe confidentiality safeguards. As Bradburn and Straf (2003)\nargue, such laws foster norms that facilitate access to mean-\ningful statistical records and protect respondent confidenti-\nality.\nAccess Modality Options\nGiven these recent changes in technology and legal guid-\nance, data producers have several data dissemination options\nfrom which to choose. These options vary considerably in\ndisclosure risk, analytical utility of the data, and ease of\naccess. And the different data access modalities may be used\nindependently or in combination, depending on one's dis-\nsemination objectives and intended audience.\nFor example, data producers may release microdata via\nPUFs that provide access to anonymized versions of data\nsets. PUFs are widely accessible through CD-ROMs or the\nInternet, and given their broad reach, statistical agencies use\ntechniques like variable suppression, top and bottom coding,\nnoise infusion, and geographic aggregation before releasing\nPUFs to protect the confidentiality of the respondents\n(Weinberg, Abowd, Steel, Zayatz, & Rowland, 2007).\nAlthough such techniques are understandably required, they\noften diminish the usefulness of the microdata (United\nNations, 2007) and, thus, are not the optimal dissemination\ntools in terms of analytic utility. Statistical agencies also\nrelease synthetic microdata. Importantly, however, all the\nbenefit of synthetic data depends on the validity of the models\nused to create them.\nSimilar to PUFs, online tabulation engines and statistical\ndata cubes provide another alternative to giving researchers\nfull access to raw microdata. At the request of the research-\ners, most often online, such tabulation engines generate\ncustomized summary tables and matrices after having gone\nthrough an automated disclosure review process. Online\ntabulation engines are easily accessible through the Internet\nand retain confidentiality through suppressed summary tables;\nhowever, they are arguably less useful than PUFs for ana-\nlytical purposes.\nRemote batch processing is another dissemination modal-\nity. Although researchers do not have full access to the data\nsets, they submit programs or codes remotely via the Internet\nand receive their output once it has been reviewed for disclo-\nsure control by the statistical agency. The execution is gener-\nally done offline; thus, the process is not interactive. While\nmost batch processing systems use filters to suppress certain\nqueries and results, in the same way as PUFs, the output\nobtained from this access modality is still potentially more\nuseful than that obtained through PUF data sets (Weinberg\net al., 2007). A review of such remote batch processing\narrangements shows, however, that while they are relatively\nsecure and can be effective for smaller requests, they can be\nslow when large computation is required (United Nations,\n2007). Also, the noninteractive aspect of this access modality\ncan be a hindering experience for researchers.\nThe general theme that emerges from the aforementioned\ndissemination modalities is that there are serious trade-offs\nthat need to be examined in terms of data access solutions,\nincluding data utility, confidentiality, security, and ease of\nuse. While PUFs, remote batch processing, and tabulation\nengines are easy to access and have incorporated security\nmeasures to protect confidentiality, those measures limit ana-\nlytic utility. There are, however, other options available to data\nproviders that allow researchers to increase the analytical\nutility of the data.\nLicensing is one such example. Under this method,\napproved researchers are granted a license via a contract\nto analyze restricted-use microdata (Weinberg et al., 2007),\nand access is provided through various means, such as\nCD-ROMs or remote access (United Nations, 2007). The\nU.S. National Center for Educational Statistics (NCES), for\nexample, uses this method for a large number of its confi-\ndential data sets; so does the U.S. Bureau of Labor Statistics\n(BLS) for access to its National Longitudinal Surveys of\nDue in part to recent congressional legislation and OMB\nguidance on data sharing, as well as increasing concerns that\nlicensing alone cannot adequately protect data confidential-\nity, since 2006 U.S. Government agencies have explored new\nways of disseminating information. For example, rather than\nsimply \"pushing out\" microdata through licensing contracts\nto researchers, the National Institute of Standards and\nTechnology, U.S. Department of Agriculture, the National\nScience Foundation (NSF), and other data producers are\ncurrently \"pulling in\" researchers via secure remote access\nnodes to sensitive data housed in the NORC Data Enclave.\nOne problem is that licensing allowing researchers direct\naccess to confidential microdata involves hundreds if not\nthousands of CDs with disclosive microdata being shipped\nacross the United States; hence, each access node (i.e., con-\ntracted researcher), arguably, is a potential confidentiality con-\ncern. Although licenses legally bind researchers to maintain\nconfidentiality, even a single breach can be damaging to the\nreputation and mission of the data producer. This concern is\nexacerbated by the fact that data in this model are delivered\nthrough mediums such as CD-ROMs that can easily change\nhands (challenging the chain of command), even without\nmalicious behavior on the part of researchers. Therefore, for\nvery sensitive microdata containing detailed personal identi-\nfiers, allowing this mode of access could be potentially risky,\nhowever trustworthy the individual researchers may be.\nBy sharp contrast, remote and physical data enclaves (also\nknown as Research Data Centers or RDCs) are secure dis-\nsemination mechanisms. Whereas remote access platforms\nprovide convenient access via an encrypted terminal session,\nRDCs only allow on-site access. To protect confidentiality,\nremote and physical data enclaves maintain stringent physical\nand computer security guidelines, preventing any results from\nbeing exported from the controlled environment without going\nthrough a formal disclosure review process.\nAn obvious advantage of remote and physical data enclaves\nis that researchers often have access to the most detailed ver-\nsion of the data, that is, raw microdata, devoid of suppression.\nAccess to such analytically useful data through RDCs, how-\never, comes at a price: They are very expensive to operate\nand are not convenient to all potential researchers (i.e., they\nrequire researchers to be physically present at the facility).\nFurthermore, the process for reviewing proposals or what\nresults may be publicly released out of an RDC is very cum-\nbersome and time-consuming (United Nations, 2007).\nRemote access data enclaves typically use secure technol-\nogies, such as virtual computing, to allow approved research-\ners to connect to a data server that hosts the actual microdata\nand work in a remote-desktop environment. While users\nwork in a familiar desktop environment, no output may leave\nthe secured environment without first undergoing stringent\nstatistical disclosure control. As pointed out by one of our\nreviewers, a determined intruder can find ways to overcome\nalmost any obstacle. For instance, she can easily print a data-\nbase to screen from the remote server and then capture the\ndata using optical character recognition (OCR) technology.\nThis is cumbersome and prone to error, but largely automatic\nand outside the control of the enclave administrator. Even if\nscreen capture technology is not available, an off-site user\ncan simply record the remote desktop with a camera. This\nis called the \"analogue hole\" in cryptography. Whether it is\npossible to apply stringent statistical disclosure to every sin-\ngle output, taking into account previously released outputs and\nfuture outputs is critical in this approach. This has not been\nproven in the literature.\nConclusion\nAchieving data and information dissemination without harm-\ning any individual or any group is a central task of any entity\nin charge of collecting data. The balance lies in protecting the\nprivacy of those in the data while minimizing data utility loss\n(Kinney et al., 2009). Although the need for such balance is\ntrue of every data set, it becomes more critical when the infor-\nmation collected is about personal health status and personal\nhealth care received.\nAlthough several scientific disciplines have different views\nabout the degree to which such balance can be achieved, they\nall agree that creating a file that is totally reidentification risk\nfree is an impossible task. The question is, then, how much\nrisk is tolerable. Statisticians offer several methodological\napproaches to balance disclosure treatment and utility, while\naccepting some level of risk. Some computer scientists are\nskeptical, and show low tolerance to risk, arguing for limit-\ning the release of data to the public. Critics of microdata\nreleases also point out that confidentiality agreements and\ndata use agreements provide no formal privacy protection\nguarantees.\nResearch showing analytic distortions of widely used\nmasking methods raises concerns about the misapplication\nof disclosure avoidance procedures. Besides embarrass-\nment to the agency, the problem could affect a whole range\nof stakeholders.\nScientists have developed a variety of definitions and\nframeworks to quantify disclosure risk and a variety of meth-\nods to limit disclosure risk. These methods range from the\nsimple suppression of a field or a subset of values in a field,\nto intricate perturbation methods such as data swapping and\nimputation via synthetic methods. Although some methods\nhave been shown to be better at masking specific fields in a\ndata set or to provide better protection while minimizing\nutility loss, the literature does not emphasize one method over\nanother. Similarly, although different software applications\nhave been designed and are available, there is no discussion\nin the literature about which one is best.\nOur literature review shows that scholars have devoted\nconsiderable attention to the development of methods to mask\nmicrodata in settings in which units are not followed over\ntime. However, the literature is sparse on longitudinal data.\nThe addition of time as a variable adds a level of complexity\nthat is still an open question in the field. The same conclusion\nholds for data utility metrics. Although several metrics have\nbeen proposed, the literature is vague regarding which one to\nuse in practice.\nSimilarly, scientists have developed several methods to\nassess whether disclosure techniques have achieved the desired\nprotection. These include (a) record linkage, in which unique\ncombinations of variables (e.g., gender, age, zip code) are\nused to match records in two or more data sets (e.g., a medical\nrecord and a voter registration list) and (b) probabilistic infer-\nence, in which sophisticated algorithms, taking into account\nsome amount of auxiliary information provided publicly on a\ntarget, are able to detect with high probability a record or set\nof records of any given individual (i.e., identify whether a\nperson with certain publicly known information is in a data\nset, and learn additional information about that person from\nthe data set).\nSuch methods operate under different assumptions and\nhave been validated empirically in very specific contexts.\nA common theme in the literature is the threat imposed by\nthe growing amount of auxiliary information available either\nfree or at very low prices, and the possibility that the mone-\ntary cost of an attack decreases with the availability of data\nand the growth in computer power. In addition, despite\nefforts to establish standards for acceptance of a data set as\nsafe for public release, methods for quantifying the risk of\nreidentification are scarce in the literature.\nAlthough there is abundant theoretical and empirical\nresearch, our review reveals lack of consensus on fundamental\nquestions for empirical practice: how to assess disclosure risk,\nhow to choose among disclosure methods, how to assess\nreidentification risk, and how to measure utility loss.As stated\nin Kinney et al. (2009), \"it is not known whether the choice of\nmeasures is a problem with theoretical or methodological\nstructure or merely disconnected special cases amenable only\nto empirical analysis\" (p. 132).\nAccess to microdata has also received attention in the lit-\nerature. Modalities vary in terms of disclosure risk, analytical\nutility, and ease of access. Several authors make interesting\ncases about the advantages and disadvantages of each method.\nBut more empirical research is needed.\n"
}