{
    "abstract": "Background Randomized clinical trials, particularly for comparative effectiveness research (CER), are frequently criticized for being overly restrictive or untimely for health-care decision making.",
    "reduced_content": "Do Bayesian adaptive trials offer advantages for\ncomparative effectiveness research? Protocol for\nthe RE-ADAPT study\n \n \nBackground Randomized clinical trials, particularly for comparative effectiveness\nresearch (CER), are frequently criticized for being overly restrictive or untimely for\nhealth-care decision making.\nPurpose Our prospectively designed REsearch in ADAptive methods for Pragmatic\nTrials (RE-ADAPT) study is a `proof of concept' to stimulate investment in Bayesian\nadaptive designs for future CER trials.\nMethods We will assess whether Bayesian adaptive designs offer potential efficien-\ncies in CER by simulating a re-execution of the Antihypertensive and Lipid Lowering\nTreatment to Prevent Heart Attack Trial (ALLHAT) study using actual data from\nResults We prospectively define seven alternate designs consisting of various com-\nbinations of arm dropping, adaptive randomization, and early stopping and describe\nhow these designs will be compared to the original ALLHAT design. We identify the\none particular design that would have been executed, which incorporates early stop-\nping and information-based adaptive randomization.\nLimitations While the simulation realistically emulates patient enrollment, interim\nanalyses, and adaptive changes to design, it cannot incorporate key features like the\ninvolvement of data monitoring committee in making decisions about adaptive\nchanges.\nConclusion This article describes our analytic approach for RE-ADAPT. The next\nstage of the project is to conduct the re-execution analyses using the seven prespeci-\nctj.sagepub.com\nIntroduction\nBayesian and adaptive trial designs have been used\nto support Food and Drug Administration (FDA)\napproval of drugs and medical devices and are\nproposed as an efficient way to achieve valid and\nreliable evidence from comparative effectiveness\nresearch (CER) [1\u00ad9] as defined by the Institute of\naBerry Consultants, Orlando, FL, USA, bUniversity of Central Florida College of Medicine, Orlando, FL, USA, cPCORI \u00ad\nPatient-Centered Outcomes Research Institute, Washington, DC, USA, dBerry Consultants, College Station, TX, USA, eDe-\npartment of Biostatistics, Evidera, Montreal, QC, Canada, fPharmaceutical Health Services Research Department, Univer-\nsity of Maryland School of Pharmacy, Baltimore, MD, USA, gDepartment of Population Health Sciences, University of\nWisconsin School of Medicine and Public Health, Madison, WI, USA, hDepartment of Medicine, University of Maryland\nSchool of Medicine, Baltimore, MD, USA, iDepartment of Biostatistics, The University of Texas School of Public Health,\nHouston, TX, USA\nAuthor for correspondence: Jason T Connor, Berry Consultants, Orlando, FL 32827, USA.\nEmail: jason@berryconsultants.com\nMedicine [10]. To our knowledge, there have been\nno Bayesian adaptive CER trials performed and just\none such trial plan published [11].\nWe initiated a project, `REsearch in ADAptive\nmethods for Pragmatic Trials' (RE-ADAPT), funded\nby the National Heart, Lung and Blood Institute\n(NHLBI), whose aim is a proof-of-concept that Baye-\nsian adaptive methods may have potential benefits\nfor CER trials. RE-ADAPT will re-execute the Antihy-\npertensive and Lipid Lowering Treatment to Prevent\nHeart Attack Trial (ALLHAT) [12] using patient-level\ndata to evaluate whether a Bayesian adaptive design\nmight have accomplished the original ALLHAT\nobjectives more efficiently in terms of the number\nof patients enrolled and the calendar time of the\ntrial.\nThe aim of the RE-ADAPT study is to emulate the\nactual process of designing a Bayesian adaptive\ndesign tailored to the original aims of the ALLHAT\ntrial (as opposed to performing a Bayesian reanalysis\nof ALLHAT). The aim of this article is to describe in\ndetail the design process of the simulation protocol.\nWe describe a systematic review of pre-ALLHAT lit-\nerature and derivation of priors to guide the designs,\nthe specific designs that will be considered, the\nadaptive mechanisms (e.g., adaptive randomization)\nthat are considered, the criteria on which adapta-\ntion decisions will be based, and factors on which\nthe designs will be compared (e.g., final allocation\nof patients, duration of trial, total sample size, and\nfinal conclusions).\nBy prospectively publishing our protocol before\nwe execute our new designs, we hope to address pos-\nsible concern of hindsight bias \u00ad simply choosing\nthe best design from a set of designs that would have\nled to a more efficient trial. To further reduce poten-\ntial/concern for bias, during the design process, the\ntwo primary designers (JTC and KRB) remained\nblinded to the ALLHAT dataset, did not read the\nclinical outcome articles, and relied only on the ori-\nginal ALLHAT protocol and clinical discussions with\ncardiovascular experts regarding what cardiologists\nand trial designers would have known prior to the\ndesign of ALLHAT. These discussions, as well as\ninput from the ALLHAT clinical trials center director\nand statistician (B.R.D.), were used to design our\nsimulation study as though it were occurring histori-\ncally at the time of ALLHAT.\nALLHAT was selected as a case study because it\nwas large, nationally prominent, evaluated active\ncomparators within community care settings, a pub-\nlic-use patient-level dataset [13] was available, was\ncostly (US$135 million) [14], and was sufficiently\nlengthy (8 years) that practice patterns and thus\nclinical questions (e.g., combination versus mono-\ntherapy becoming more standard) may have chan-\nged during the course of the trial [15].\nCo-sponsored by NHLBI and the Department of\ncan\u00adAmerican) with at least one cardiovascular dis-\nease risk factor besides hypertension [16] to compare\nthree newer antihypertensive medications to a\ndiuretic for reducing fatal coronary heart disease\n(CHD) and nonfatal myocardial infarction (MI)\n[3,4,12]. ALLHAT incorporated several adaptive\ndesign features including conditions for early study\ntermination and arm dropping [17]. ALLHAT moni-\ntored each comparison with its own monitoring\nguideline with alpha = 0.027 according to the Dun-\nnet procedure. Early success and early futility were\nboth based on stochastic curtailment. The number\nof looks depended on the information times and the\ncalendar times of the Data and Safety Monitoring\nBoard (DSMB) meetings. In the `Discussion' section\nof this article, we contrast the adaptive features\nemployed by the ALLHAT investigators with those\nwe have designed.\nThe RE-ADAPT simulation protocol\nThe RE-ADAPT protocol consists of developing a ser-\nies of potential redesigns of ALLHAT followed by\nsimulating them to measure efficiency and perfor-\nmance using de-identified ALLHAT patient data.\nThere are five steps to the process:\n1. Conducting a systematic literature review and deri-\nvation of priors based on literature existing when\nALLHAT was designed to provide the data upon\nwhich prior evidence distributions could be\nderived.\n2. Identifying possible adaptations that may improve\ntrial efficiency involves specifying possible\nadaptive features (e.g., adaptive randomization\nor arm dropping) that might be included in\nsuch a trial.\n3. Constructing candidate set of Bayesian adaptive\ndesigns for ALLHAT by selecting combinations of\npriors distributions and specific adaptive fea-\ntures from steps 1 and 2. This includes prespeci-\nfying designs, including timing and frequency\nof interim analyses when adaptations may occur,\nand thresholds (e.g., early stopping bounds).\n4. Selecting an optimal design for implementation\nreplications of each design created in step 3\nwere simulated under different scenarios (e.g.,\nno difference between treatments, different\neffect sizes). The design providing the best oper-\nating characteristics (e.g., smallest sample size,\nshortest trial duration, highest power, and most\npatients randomized to better therapy) over the\nwidest range of possible efficacy scenarios is\nchosen for implementation.\n5. Executing all designs using actual ALLHAT data to\nassess the performance of the chosen optimal\ndesign and all others considered, comparing\neach with the original ALLHAT design. Whereas,\nin reality, a single design must be chosen for\nimplementation, since this is a simulation exer-\ncise, we can/will execute the chosen design as\nwell as those we opted `not' to implement.\nDetails of these steps are explained in the follow-\ning sections. The first 4 items have been completed;\nwe describe the fifth prospectively.\nSystematic literature review and derivation\nof priors\nPriors are required for the Bayesian analysis. They\ncan be classified as non-informative (`vague')\nroughly corresponding to classical analysis in which\nonly new trial data inform the inference; or `infor-\nmative' (`historical') where the evidence distribution\nis formally incorporated with the new trial data.\nWe originally planned to create one set of designs\nwith non-informative priors and another using his-\ntorical priors for each of the four drugs under study.\nOur formal literature review, however, revealed that\nno such studies using the ALLHAT primary end\npoint were available for any of the three com-\nparators (angiotensin-converting enzyme (ACE)\ninhibitors, calcium-channel blockers (CCBs), and\nalpha-blockers). Therefore, the historical prior effec-\ntively matched the non-informative prior for the\nthree comparator drugs, and we chose to incorpo-\nrate only designs using non-informative priors for\nall four drugs.\nIdentify possible adaptations that may improve\ntrial efficiency\nThree types of adaptations are considered: adaptive\nrandomization, arm dropping, and early stopping.\nAdaptive randomization and arm dropping may\noccur in the accrual stage. All designs allow early\nstopping of the trial for futility or success, either of\nwhich may occur in the accrual or follow-up stages\n(criteria for early stopping are discussed in detail in\nthe section `Early Stopping of the Trial for Success or\nFutility'). Adaptive randomization and arm drop-\nping (during accrual) serve two key purposes: they\nincrease the probability that patients randomized\nlater in the trial receive a beneficial therapy; and\nthey can increase statistical power by prioritizing\ndata gathering for treatments where the research\nquestion remains more uncertain. Furthermore, by\nperforming multiple prospectively defined interim\nanalyses during accrual and follow-up phases, the\ntrial may stop early if primary goals are met or it\nbecomes evident that additional information is unli-\nkely to lead to a significant conclusion.\nThe following sections describe the different rules\nwith which adaptive randomization, arm dropping,\nand study termination may be incorporated into\ndesigns. These rules involve predetermined thresh-\nolds that govern when and which adaptations\nwould be made. These were determined based on\nsimulations testing a range of potential thresholds\nto find those values offering the most beneficial\ntrade-offs. The statistical and clinical benefits of\ndesigns based on the various potential thresholds\nwere discussed between the statistical designers and\nthe clinicians involved to replicate the actual trial\ndesign process. This included discussing the overall\noperating characteristics and also discussing many\nindividual trial simulations to illustrate how the trial\nwould proceed and the nature of possible realiza-\ntions and the adaptations that would result. This,\nlike an actual adaptive trial design, was an iterative\nprocess between the statistical team and clinical\nteam.\nAll thresholds/decision points were based upon\nsimulation and chosen before the lead statisticians\nacquired the ALLHAT data. Thresholds for early suc-\ncess stopping were chosen to conserve Type I error\nto less than 2.5% (one-sided). Thresholds for arm\ndropping were chosen to balance the probability of\ncorrectly dropping a poorly performing arm with\nincorrectly dropping an arm that was performing\npoorly early due to natural variability. Thresholds\nwere also chosen to optimize power, trial duration,\nand percentage of patients randomized to the best\ntherapy. The process of choosing thresholds is ana-\nlogous to choosing a cutoff threshold for a diagnos-\ntic test when weighing sensitivity and specificity \u00ad\nhigher, more aggressive values will lead to correctly\nstopping a trial early or dropping a poor arm sooner,\nbut will also lead to increased Type I errors or an\nincreased likelihood of erroneously dropping effica-\ncious arms.\nThese decisions are subjective, and different\ndesigners and clinicians may have chosen other\nvalues. This is similar to trial designers choosing\nmore or less aggressive stopping boundaries in a\ngroup sequential trial to match the clinical situation.\nFor instance, we simulate data from the five scenar-\nios discussed below (not ALLHAT data but plausible\nscenarios) and tuned the thresholds to behave well\nover this range of plausible `truths'. Once the values/\ndecisions points are set, the real ALLHAT data will be\nused to execute the trial. Another example is pro-\nvided below in the section describing adaptive arm\ndropping.\nBayesian adaptive methods for CER trials 809\nAdaptive randomization\nTrials with adaptive randomization begin with an\ninitial assignment probability for each study arm\nand are later updated at predetermined times based\non `real time' observed treatment effects. ALLHAT\nused fixed randomization with a greater proportion\nof patients allocated to the diuretic arm to increase\npower for each pair-wise comparison. We will com-\npare this original fixed randomization approach to\ntwo alternative approaches: randomize patients pro-\nportional to the probability that each comparator\narm offers the higher probability of being the best\narm (probability-weighting), and randomize patients\nproportional to both the observed treatment effects\nand the uncertainty in those treatment effects\n(information-weighting). In both cases, randomiza-\ntion to the diuretic arm remains fixed at one-third of\npatients since this is considered standard treatment\nto which others are compared. Due to the low inci-\ndence of the primary end point (fatal CHD + nonfa-\ntal MI), randomization probabilities will first be\nupdated after the 10,000th patient is enrolled and\nthen again every 3 months until the end of accrual.\nStarting adaptive randomization at 10,000 patients,\nlike other thresholds, was chosen based on compar-\ning the operating characteristics of a variety of alter-\nThe three randomization schemes explored are as\nfollows:\n1. Fixed randomization (reference case): Patients\nare randomized according to the original alloca-\ntion rules in ALLHAT throughout enrollment:\nthree comparator arms [12].\n2. Probability-weighted adaptive randomization:\n3 months thereafter, randomization probabil-\nities are updated to be proportional to the prob-\nability that each comparator offers the best\n(lowest) hazard ratio (HR) compared to diuretic.\nProbabilities are derived from posterior distribu-\ntions of HRs of each treatment at interim ana-\nlyses. Thus, if all comparators have similar\nposterior distributions, then randomization to\nnon-diuretic arms would occur with approxi-\nmate equal probability, and the more dramatic\nthe benefit to a particular arm, the higher the\nrandomization probability to that arm.\nThe result is that the comparator arms performing\nbetter will receive more patients and overall event\nrates will be lower than with fixed randomization\n[18\u00ad22]. However, statistical power for the compari-\nson of the best two arms in a multi-arm trial is\nincreased since the comparator arms of most inter-\nest receive larger numbers of patients [23].\n3. Information-weighted adaptive randomization:\nThis approach is similar to the probability-\nweighted approach, but further incorporates the\nprecision of the HRs in the derivation of revised\nrandomization ratios. Thus, in addition to favor-\ning arms with the lowest observed event rates,\nthis approach also prioritizes arms where preci-\nsion is lowest, and hence, the need for addi-\ntional data is highest. For example, if CCBs and\nalpha-blockers appear to be equally efficacious\nbut there is greater variability surrounding the\nestimate for alpha-blockers, more patients would\nbe randomized to that arm in the next cohort in\norder to refine its estimate. Adaptive randomiza-\ntion here will tend to be less aggressive than\nMany have criticized adaptive randomization\n[27,28] for its lack of power compared to fixed ran-\ndomization. However, these criticisms focus on the\ntwo-arm case and are not relevant to this four-arm\ntrial. We acknowledge that 1:1 randomization tends\nto optimize power in the two-arm case. However, as\nBerry [23] describes, adaptive randomization tends\nto increase study power for trials of three or more\narms. Furthermore, he suggests that we tend to do\ntwo-armed trials because it is hard and expensive to\ndo multi-armed balanced trials, but then having\nlimited most of our thinking to two-arm trials, we\ncriticize adaptive trials in the two-armed setting,\nwhich is clearly not where they shine brightest.\nAdaptive arm dropping\nAnother adaptation type that enhances treatment\nallocation is arm dropping (i.e., suspension of enroll-\nment). This can be viewed as an extension of adap-\ntive randomization in which one or more arms are\nassigned a zero probability of randomization. The\nfollowing four adaptive arm-dropping approaches\nare explored:\n1. No arm dropping (reference case) whereby\nenrollment is never stopped completely, but if\nthe adaptive allocation is allowed in the design,\nrandomization ratios can become very small,\neffectively zero, if one or more of the CCB, ACE\ninhibitor, and alpha-blocker arms is performing\npoorly compared to the others.\n2. Posterior probability-based adaptive arm drop-\nping extends designs with adaptive randomiza-\ntion by suspending enrollment into arms with\nlow randomization probabilities. This threshold\nis set at 0.05 with probability-weighted adaptive\nrandomization and 0.10 with information-\nweighted adaptive randomization. If randomiza-\ntion probabilities fall below these thresholds,\naccrual to the arm is suspended, and the remain-\ning arms will receive a proportional increase in\nrandomization. At the next interim analysis, the\nsuspended arm may resume accrual if randomi-\nzation probabilities increase above the thresh-\nolds. Patients in the suspended arms continue\ntreatment and follow-up as usual.\nArm dropping can be incorporated in the design\neven if randomization ratios are fixed. Instead of\nbasing the decision to discontinue enrollment on\nrandomization probabilities, adaptation is based on\nthe posterior probability of effectiveness \u00ad that is,\nthe probability that the HR of a comparator arm to\nthe diuretic is less than 1. If this probability drops\nbelow 0.2, enrollment into the arm is stopped with-\nout possibility of resuming and both treatment and\nfollow-up also stops.\nFor the 20% threshold, we looked at individual\nsimulations and considered the frequency with\nwhich beneficial arms were erroneously terminated\n(due to natural variability and usually occurring at\nearly interim analyses) versus the proportion of ter-\nminations that occurred to truly inferior arms. The\nhigher (more aggressive) this threshold, the greater\nthe likelihood of both good and bad arms being ter-\nminated. Therefore, this value was chosen to most\noften suspend poorly performing arms while rarely\nsuspending better arms that were just at a random\nlow. No formal utility rule was used in the decision\nprocess.\n3. Predictive probability-based arm dropping is\nemployed when arm dropping can occur, but is\nbased upon predictive probabilities of trial suc-\ncess. In this approach, no adaptive randomiza-\ntion is used, and control arm randomization is\nfixed at 1/3 with the remaining 2/3 being\ndivided equal between the remaining available\narms. This decision rule is based upon predictive\npower that incorporates data observed (at the\ntime of the interim analysis) and data likely to\nbe observed if each arm stays in the trial. An\narm is terminated if predictive power versus\ndiuretic is ever less than 10% and patients who\nwould have been randomized to that arm are\nredistributed to the remaining arms [29].\nEarly stopping of the trial for success or futility\nEarly stopping of the trial for success or futility, a\nfeature of all the designs, will be assessed at interim\nanalyses during the accrual and follow-up phases.\nUp to nine early stopping interim analyses are\n40,000th patients are enrolled (i.e., the latter being\nand 54 months after the end of accrual, as noted in\nTable 1. Final analysis occurs 60 months after final\npatient enrollment. Early stopping for futility may\noccur at any interim look, but early stopping for suc-\ncess begins at the 30,000th patient look. Even with\nand there is large variability in the observed treat-\nment effects. Thus, early success stopping is not\nallowed at the first interim analysis.\nThis serves to control Type I error rate in two ways.\nFirst, initiating adaptive randomization after 10,000\npatients are enrolled but not allowing early success\nstopping until the 30,000-patient analysis eliminates\nthe possibility of a Type I error at the early analyses.\nMeanwhile, all design variants that include adaptive\nrandomization will increase the number of patients\non arms performing best at that point in time. Thus,\nthese arms will be more rapidly `tested', and if we are\ntruly observing a random high, more patients will be\nrandomized to those arms and we will observe regres-\nsion to the mean more rapidly, thus decreasing the\nlikelihood of a Type I error at a subsequent analysis.\nIf the effect is real, additional patients to the most\neffective arm will increase power between the best\narm and the comparator.\nAt each early stopping analysis, the comparator\narm with the best (lowest) HR is compared to the\ndiuretic arm, and the posterior probability that the\nHR is below 1 is compared to the stopping bound-\naries for success (Sa\n) and failure (Fa\n) (Table 1). The\ntrial is stopped early for success if this probability\nexceeds Sa\n, and for futility if this probability is\nbelow Fa\n.\nTherefore, as soon as one comparator meets a\nstopping criterion, the trial stops. Two or all three\ncomparators could cross a threshold simultaneously,\nin which case it would be reported that two or all\ncomparators offer a significant improvement com-\npared to diuretic. Similarly, the best comparator\nmight cross a stopping boundary with the second\nbest close, but not quite, achieving statistical signifi-\ncance. The trial would nevertheless stop as the goal\nTable 1. Planned interim analyses for potential early stopping\nAnalysis Sa\nFa\nSa\n: success; Fa\n: failure.\nBayesian adaptive methods for CER trials 811\nis to most rapidly identify an alternative to diuretic\nthat offers improvements on the primary cardiac\noutcome.\nStopping boundaries are identical across all\ndesigns. Success criteria (Sa\n) have been calibrated to\nadjust for both multiple comparisons and frequent\ninterim analyses such that the overall one-sided\nType I error rate is less by 2.5%.\nConstruct candidate set of Bayesian adaptive\ndesigns for ALLHAT\nSeven different adaptive designs are created by com-\nbining the three randomization types with the\nthree-arm-dropping approaches, as noted in Table 2.\nThese include each of the three randomization\nschemes (none, probability-weighted, and informa-\ntion-weighted) paired with each of the three-arm-\ndropping schemes (none, posterior probability\nbased, and predictive probability based). The predic-\ntive probability arm-dropping scheme incorporates\nthe distribution of outcomes for future subjects,\nwhich is dependent upon their randomization\nassignments. Therefore, this strategy is more com-\nplicated if randomization assignments for future\nsubjects vary. Consequently, the predictive-probability-\nbased arm-dropping approach was used only in the\ncontext of fixed randomization leaving a total of\nseven designs.\nSelect optimal design for implementation\nAfter the seven candidate designs (each individually\noptimized via simulation) were identified, we com-\npared them to one another by simulating trials\nacross a variety of plausible effectiveness scenarios.\nThis involves testing each design via simulation to\nunderstand the potential performance measured in\nterms of expected (mean) sample size and duration\nof the trial, power, probability of stopping early for\nsuccess or futility, and proportion of patients rando-\nmized to the best therapy for five different efficacy\nscenarios:\n Null: no comparators arms better than control;\n Alternative: all equally better than control (HR =\n One Works: one better than control (HR = 0.837)\nand the other two equal to control;\n Better & Best: one best (HR = 0.837), one slightly\n Worse: all are equally worse than control (HR =\nThese scenarios are based on a range of plausible\neffectiveness scenarios including the null and alter-\nnative hypotheses from the original ALLHAT designs\nand other variants on these two scenarios. In this\nmanner, we seek to identify an optimal design: one\nwhich offers the best trade-off of highest power and\nmost likely to terminate a futile trial early, to identify\na successful treatment fastest, and to randomize the\nhighest proportion of patients to the best treatment,\nall while maintaining Type I error control.\nSimulating 1000 trials from each scenario for each\nof the seven designs produces operating characteris-\ntics for each scheme (Table 3). Note that power in\nthe null scenario is the one-sided Type I error rate,\nand controlled at less than 2.5% (Table 3). In addi-\ntion to studying average operating characteristics,\nunderstanding the range of possibilities for single\ntrials is important. Numerous individual simulated\ntrials were shown to the clinical team. Additionally,\nFigures 1 and 2 show distributions for study dura-\ntion and the proportion of patients within each\nsimulation randomized to the best treatment\n(according to the primary end point). Because each\ndesign variant uses the same stopping rules (the key\ndifferences are randomization assignment algo-\nrithms), trial durations do not change drastically\nacross designs.\nAfter considering operating characteristics of all\ncandidate designs, we chose Design 4 (probability-\nweighted adaptive randomization, and probability-\nbased adaptive arm dropping) as the design we\nwould have implemented. Its power is high (91%,\n79%, and 76%) for each scenario with an effective\nfor a fixed randomization trial when just one com-\nparator is superior to control and 76% versus 73%\nwhen one is clearly better and another is in between\n(one better, one best). In scenarios where one treat-\nment is preferable (One Works and Better & Best), a\ngreater proportion of patients are randomized to the\nbetter arm (34% and 30%) compared to trials with\nfixed randomization (including the original design)\nin which 21% of patients are randomized to the\nsuperior treatment. In the `Better & Best' scenario,\nthe inferior, middle, and best comparator,\nrespectively.\nTable 2. Planned adaptive trial designs\nDesign Allocation Arm dropping\n1 (a) Fixed (d) None\n2 (b) Probability wt (d) None\n3 (c) Information wt (d) None\n4 (b) Probability wt (e) Posterior probability\n5 (c) Information wt (e) Posterior probability\n6 (a) Fixed (f) Posterior probability\n7 (a) Fixed (g) Predictive probability\nThe effect of adding an arm-dropping component\nto adaptive randomization is small but important.\nWithout arm dropping, poor arms are eventually\ndropped as their probability of being the best arm\napproaches zero. With arm dropping, these prob-\nabilities are truncated to zero anytime they are less\nthan 5%, and then the randomization probabilities\nare redistributed, thus the arm-dropping component\nis a bit more aggressive in assigning patients to bet-\nter performing arms. For instance, when comparing\nDesign 2 with Design 4, 2% more patients are\nassigned to the best therapy in the One Works sce-\nnario producing an increase in power from 75% to\nTable 3. Operating characteristics for planned adaptive trial design variants\nMean\nsubjects\nPower Early\nsuccess\nEarly\nfutility\n% Randomized\nto best arm\nMean trial\nduration (months)\nDesign 1: early stopping only\nDesign 2: probability-weighted adaptive randomization; no arm dropping\nDesign 3: information-weighted adaptive randomization; no arm dropping\nDesign 4: probability-weighted adaptive randomization; 5% arm dropping\nDesign 5: information-weighted adaptive randomization; 10% arm dropping\nDesign 6: no adaptive randomization; arm dropping 20% vs control\nDesign 7: no adaptive randomization; predictive probability-based arm dropping\nBayesian adaptive methods for CER trials 813\nThis highlights the double benefit of adaptive ran-\ndomization: a higher proportion of patients rando-\nmized to the more effective therapy and by the end\nof the trial resources (patients) are being assigned to\nthe treatment that increases statistical power and\nrandomized away from treatments (the one equal to\ncontrol) for which we do not desire an increase in\nstatistical power. The Technical Appendix A includes\na full description of each design, including addi-\ntional operating characteristics for each design. An\nexample trial of Design 4 is also detailed there and\nillustrates how this design would be conducted.\nExecution of Bayesian adaptive designs\nThe first four steps have been completed and are\ndescribed in this article and in Technical Appendix\nA. The final step is to execute the chosen Bayesian\nadaptive design using the actual ALLHAT data.\nWe believe the best way to judge a trial (adaptive\nor fixed) is by simulating the trial's conduct over a\nbroad range of potential scenarios (efficacy scenar-\nios, accrual rates, patient populations, etc.) and\nstudying which trial offers the best operating charac-\nteristics over the broadest range of deviations from\nFigure 1. Distribution of trial duration (in years) for each design/scenario combination.\nthe primary trial assumptions. However, we under-\nstand that simulation, particularly to many clini-\ncians, is sometimes not the most convincing tool.\nTherefore, until a large-scale CER trial is performed\nusing the adaptive techniques described here, we\nwill implement each of our seven proposed designs\nusing the original ALLHAT data to illustrate how an\nactual Bayesian adaptive CER trial would likely pro-\nceed and how the inferences that result will compare\nto the traditional alternative (the actual ALLHAT\ntrial).\nIn addition to executing Design 4, we will also\nexecute the other six designs to understand how\neach would likely have performed had it been cho-\nsen. Actual ALLHAT data will be used to replicate\nthe trial conduct including enrollment, randomiza-\ntion, follow-up, and end-point ascertainment.\nShould one comparator arm perform particularly\nwell, the adaptive randomization process may call\nfor more patients than were contained in its original\nALLHAT arm, in which case we will resample sub-\njects in a bootstrap fashion.\nTo ensure proper timing for interim analyses, spe-\ncial attention will be given to ensure simulated time\nreflects actual time between enrollment and timing\nof end points. Thus, only end points that had\noccurred and were recorded and available for analy-\nsis may inform an interim adaptation. Accumulated\ndata at each prespecified interim looks will be ana-\nlyzed to test criteria for adaptation or early trial ter-\nmination. Resulting changes are incorporated and\napplied to the next patient cohort of enrollees. This\nvirtual process continues until an early stopping\ncriterion is met or the study reaches its administra-\ntive end defined by its maximum sample size and\nmaximum follow-up time.\nEvaluation criteria\nSimulated outcomes of each scheme will be com-\npared in terms of total sample size, trial duration,\npercentage of patients to each arm, and trial conclu-\nsions \u00ad how each trial's inferences compare to the\noriginal ALLHAT design's inferences. We will also\nlook at total numbers of events, both primary and\nsecondary, across all arms in the different designs\nand we will identify which components of the adap-\ntive trial are leading to benefits or drawbacks com-\npared with the original ALLHAT design.\nDiscussion\nThis article illustrates the redesign process we devel-\noped and will employ to simulate ALLHAT as a Baye-\nsian adaptive trial. ALLHAT was chosen strictly for\nconvenience as a proof of concept case study. The\noverall RE-ADAPT project includes several additional\ncomponents presently under consideration but not\naddressed here, including an economic analysis of\nefficiency differences (e.g., trial duration or size) we\nmay detect, and demonstrating how a combination\ntherapy arm could have been added (if desired) dur-\ning the course of the ALLHAT trial.\nIn this article, we describe the selected adaptive\nschemes we will use, stopping rules, and\nPercent Randomized to Best Treatment Design & Scenario\nProportion of Patients Randomized to Best Treatment\nDesignNo\nAlternative\nAlternative\nBetter & Best\nBetter & Best\nOne Works\nOne Works\nFigure 2. Distribution of proportion of patients randomized to the best therapy (according to primary end points) for each design/\nscenario combination. The null scenario (all doses equal) and scenario where all comparators are worse than control are not shown since\nthey are the same for all designs (100% and 33%, respectively).\nBayesian adaptive methods for CER trials 815\nrandomization probability updates that may\nincrease power by allocating more patients to better\nperforming study arms, possibly decrease trial dura-\ntion and sample size, and improve other aspects of\ntrial efficiency. We intentionally make public our\nprotocol prior to executing the reanalysis.\nTypically in designing adaptive trials, several candi-\ndate designs are developed and tested via simulation\nbased upon a range of possible outcome parameters\nand one design is selected for implementation. Select-\ning a single adaptive design in RE-ADAPT is not\nnecessary, however, since the actual ALLHAT data are\navailable and, thus, all candidate designs can easily\nbe executed, including a variant that corresponds clo-\nsely to the original design of the study. However, we\ndo identify one preferred design that our study group\nwould have implemented if an actual trial were being\nexecuted. This eliminates a multiplicity: executing all\nseven designs and simply comparing the best to the\noriginal ALLHAT design. Now, we will focus on one\nchosen design, prospectively identified, versus the\noriginal ALLHAT design.\nThe original ALLHAT trial was adaptive in that it\noffered early stopping at loosely defined times that\nwere to be based on Data Monitoring Committee\n(DMC) meeting times. The early stopping strategies\nhere are slightly more aggressive and not based on\nDMC meeting times. The major difference, however,\nis that adaptive randomization was not permitted\nunder the ALLHAT protocol but is a focus here. This\nmay offer increased power to the trial and offer\npatients, particularly those entering the trial at later\nstages, the opportunity to receive a better treatment.\nA final key difference is that our redesign focuses on\none-sided tests versus ALLHAT, which was a two-\nsided trial. We believe, particularly in a case where a\ncheaper, better understood control arm is studied\nversus newer, more expensive comparators, that a\nDMC is unlikely to allow patients to be continually\nexposed to a comparator arm that is performing\npoorly merely to show it is statistically significantly\nworse. In this sense, we believe a one-armed trial\nbetter reflects how a DMC would behave.\nThe benefit of adaptive randomization is most\nobvious when one arm is superior to others, in\nwhich case a larger proportion of patients get rando-\nmized to the best treatment. In our pretrial simula-\ntion exercise, on average, 34% of patients are\nrandomized to the best treatment arm when adap-\ntive randomization and arm dropping were allowed\n(Design 4) compared with only 21% with fixed\ndesign (Design 1) and original ALLHAT design.\nFurthermore, power also increased with adaptation\nbecause in the fixed design, patients continue to be\nrandomized to inferior treatments in the later stages\nof enrollment. In contrast, in Design 4, nearly all\npatients are randomized to diuretics and the best\ncomparator in the later stages of enrollment. This is\nclinically beneficial for patients and provides\nincreased statistical power for the primary question\nof interest.\nWhile our aim is to understand how Bayesian\nadaptive applications may perform in CER trials, we\nrealize that adaptive designs are situation specific,\ntailored to each unique clinical situation and\nresearch question. Therefore, we realize that find-\nings from RE-ADAPT will not generalize to all CER\nsituations that may be encountered.\nThe exercise described here will require some sim-\nplifications and assumptions. In reality, this deci-\nsion would be based on results from interim data\nanalyses and other factors that may be difficult to\ncapture quantitatively. For instance, the DMC may\nalso consider the safety profile of treatments, impor-\ntant secondary outcomes, or exogenous data that\nbecome available during the study. While one key\ntask of the DMC is ensuring proper implementation\nof the protocol \u00ad including resulting adaptations \u00ad it\nmay use its prerogative to make deviations to ensure\npatient safety (e.g., if adaptive randomization would\nincrease randomization probability to an arm that\nwas seeing an increase in a serious adverse event\nthat was not part of the adaptive algorithm). We\nbelieve, however, that a DMC's role in adaptive trials\nextends to ensuring that the protocol is followed\nunless there is strong reason to do otherwise. This\nmeans ensuring the implementation of all adaptive\ncomponents. A DMC should not view protocol-\ndefined adaptations as guidelines they may or may\nnot choose to implement. Overruling protocol-\ndefined adaptations leads to poorly understood\noperating characteristics and unknown Type I and\nType II error rates.\nThis article discusses the primary aims of the RE-\nADAPT study. Future articles will explore broader\napplications of Bayesian adaptive designs, for\ninstance, by simulating the addition of new arms into\nthe study or modeling other adaptations that were\nnot consistent with the original aims of ALLHAT.\nLimitations\nAlthough our simulations will rely on actual patient\ndata from ALLHAT and will emulate the original\nenrollment and follow-up process, the obvious main\nlimitation is that it is a simulation and not a `live'\nstudy. However, until a large-scale CER study is con-\nducted using Bayesian adaptive trial methodologies,\nwe hope this exercise will serve to illustrate their\npotential benefits and challenges. Most notably, we\nwill not simulate the decision process a DMC may\nuse in interpreting findings from interim analyses\nand approving changes in design, including early\nstopping. Our simulations will algorithmically apply\nchanges based on adaptation rules without consid-\neration of other contextual factors.\nAn important challenge with our study has been to\nomit the benefit of hindsight and knowledge gained\nfrom the results of ALLHAT in retrospectively formu-\nlating new designs for the trial. Although we went to\nsome lengths to maintain a `veil of ignorance', that\neffort was undoubtedly imperfect. In some instances,\nthis `veil' may have worked against our aim of mak-\ning this simulation as realistic as possible.\nOur goal is to re-execute these designs using the\noriginal ALLHAT data to make the designs' outcomes\ndirectly comparable with the original findings. In\nsome instances, however, adaptive randomization to\nbetter performing arms may call for a larger number\nof patients on an arm than was observed in ALLHAT.\nOur plan is to resample patients in these situations,\nand this can lead to an underestimation of the\nuncertainty of the results from these arms.\nFinally, the ability to incorporate prior information\nis a fundamental advantage of the Bayesian\napproach, particularly in CER where we might expect\nhigh-quality phase-3 data are available on the thera-\npies of interest. Unfortunately, data limitations\nprecluded our planned systematic review and meta-\nanalysis from providing historical priors on the pri-\nmary outcome for beta-blockers, ACE inhibitors, and\nCCBs. Therefore, the designs described here used\nonly non-informative priors. However, the sensitivity\nof CER trial designs to incorporation of historical\ninformation is the subject of a future article.\nConclusion\nFor CER to achieve its lofty aims, investment in\ncomparative trials is needed. Since randomized con-\ntrolled trials (RCTs) are expensive and time-consum-\ning and not always tailored to achieve CER\nobjectives, the RE-ADAPT project was initiated to\ntest the degree to which Bayesian adaptive trial\ndesigns may be useful in increasing CER trial effi-\nciency and utility.\nAlthough the FDA has issued guidance documents\n[5,6] and both the FDA and manufacturers have\ngained increasing experience and acceptance of\nBayesian and adaptive designs, they have not been\ntested in CER settings. We hope our effort will be\nviewed as a valid proof-of-concept of the potential\nfor such designs to be useful for CER and will stimu-\nlate investment in them for future CER trials.\nThis article describes our plans for a redesigned\nand re-executed ALLHAT. By publishing the details\nof our prespecified plans, we hope to engender\nreader confidence that the process can be consid-\nered a reasonable approximation of what we would\nhave done had we designed ALLHAT itself as a Baye-\nsian adaptive trial.\nFinally, we wish to emphasize that our goal is not\nto criticize the ALLHAT study (it also contained aims\nnot mentioned here, for example, a statin study\ncomponent that affected the design) or to claim it\nshould have been designed differently. Rather, we\nchose to re-execute the ALLHAT study using\nBayesian adaptive trial methods because it was a\nwell-designed and conducted trial that provides\nhigh-quality data in a CER setting.\n"
}