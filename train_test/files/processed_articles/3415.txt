{
    "abstract": "Abstract\nAs Amazon's Mechanical Turk (MTurk) has surged in popularity throughout political science, scholars have increasingly\nchallenged the external validity of inferences made drawing upon MTurk samples. At workshops and conferences\nexperimental and survey-based researchers hear questions about the demographic characteristics, political preferences,\noccupation, and geographic location of MTurk respondents. In this paper we answer these questions and present a\nnumber of novel results. By introducing a new benchmark comparison for MTurk surveys, the Cooperative Congressional\nElection Survey, we compare the joint distributions of age, gender, and race among MTurk respondents within the\nUnited States. In addition, we compare political, occupational, and geographical information about respondents from\nMTurk and CCES. Throughout the paper we show several ways that political scientists can use the strengths of MTurk\nto attract respondents with specific characteristics of interest to best answer their substantive research questions.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://www.\ncreativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further permission\nprovided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nIntroduction\nIn the last several yearsAmazon's Mechanical Turk (MTurk)\nhas surged in popularity in experimental and survey-based\nsocial science research (Berinsky et al., 2012; Chandler\nChandler, 2014). Researchers have used the results from\nMTurk surveys to answer a wide array of questions ranging\nfrom understanding the limitations of voters to exploring\ncognitive biases and the strengths of political arguments\nAs this type of work has grown in popularity, researchers\nhear an increasing number of important questions at work-\nshops and conferences about the external validity of the\ninferences made drawing upon MTurk samples. Questions\nsuch as: \"Are your respondents all young White males?\",\n\"Do any of them have jobs?\" and \"Where do these people\nlive?\" are rightfully voiced. In this paper we seek to answer\nsome of these questions by unpacking the survey-specific\nrespondent attributes of MTurk samples.\nBerinsky et al. (2012) take an important first step in\nexploring the validity of experiments performed using\nMTurk. They show that while respondents recruited via\nMTurk are often more representative of the US population\nthan in-person convenience samples, MTurk respondents\nare less representative than subjects in Internet-based pan-\nels or national probability samples. Berinsky et al. (2012)\nreach this conclusion by comparing MTurk to convenience\nsamples from prior work (Berinsky and Kinder, 2006; Kam\nassess numerous characteristics of MTurk respondents that\nare of interest to political scientists. These variables include\n\"Who are these people?\" Evaluating\nthe demographic characteristics\nand political preferences of MTurk\nsurvey respondents\nConnor Huff and Dustin Tingley\n Keywords\nSurvey Research, Mechanical Turk, Experimentation\nDepartment of Government, Harvard University, USA\nCorresponding author:\nConnor Huff, Department of Government, Harvard University, 1737\nEmail: cdezzanihuff@fas.harvard.edu\nResearch Article\n2 Research and Politics \nparty identification, race, education, age, marital status,\nreligion, as well as numerous other variables of interest.\nThe comparisons presented by Berinsky et al. (2012) pro-\nvide an excellent foundation for exploring the relationship\nbetween samples drawn from MTurk surveys and other\nsubject pools commonly used by political scientists.1\nIn this paper we present a number of results that contrib-\nute to a broader goal of understanding survey data collected\nfrom platforms such as MTurk. In doing so we provide a\nframework that will allow social science researchers, who\nfrequently use this platform, to better understand the charac-\nteristics of their respondent pools and the implications of\nthis for their research. This paper builds upon Berinsky et al.\n(2012) to make four contributions. First, in Section 2 we\npresent a new benchmark comparison for MTurk surveys:\nthe Cooperative Congressional Election Survey (CCES).2,3\nThe CCES is a nationally stratified sample survey adminis-\ntered yearly from late September to late October. The survey\nasks about general political attitudes, demographic factors,\nassessment of roll call voting choices, and political informa-\ntion.4 In this paper we present the results of a simultaneous\nthis design allows us to focus our comparisons on the simi-\nlarities and differences between CCES and MTurk samples\nat a common point in time.\nSecond, we provide a partial picture of the joint distribu-\ntions of a number of demographic characteristics of interest\nto social science researchers. Berinsky et al. (2012) take an\nimportant first step toward understanding the racial, gender,\nand age characteristics of MTurk samples by reporting the\npercentage of respondents in each of these categories.\nHowever, they do not explore the relationship between these\nkey variables of interest. By presenting the joint distribu-\ntions of several of these variables we are able to analyze the\nproperties of MTurk samples within the United States as\nthey cut across these different categories. For example, we\nshow that MTurk is excellent at attracting young Hispanic\nfemales and young Asian males and females. In contrast,\nMTurk has trouble recruiting most older racial categories\nand is particularly poor at attracting African Americans. We\nfocus on race, gender, and age as these are some of the most\nprominent attributes of respondents across which research-\ners might expect to observe heterogeneous treatment\neffects.7 This means that providing information about the\nnumber of respondents within each of these categories of\ninterest, and how this differs from other prominent survey\nplatforms, can assist researchers in both the design and\ninterpretation of their experimental results.\nThird, we compare the political characteristics of\nrespondents on MTurk and CCES. In Section 3 we show\nhow the age of respondents interacts with voting patterns,\npartisan preferences, news interest, and education.8 We\ndemonstrate that, on average, the estimated difference\nbetween CCES and MTurk markedly decreases when we\nsubset the data to younger individuals. In Section 4, we\ncompare the occupations of MTurk and CCES respond-\nents. We show that the percentage of respondents employed\nin a specific sector is similar across both platforms, with a\nmaximum difference of less than 7%. For example, the\npercentage of respondents employed as \"Professionals\" is\nin the range of approximately 12\u00ad16% across both sur-\nveys. These results show that MTurk and CCES have a\nsimilar proportion of respondents across industry. In\nSection 5, we present geographic information about\nrespondents. We show that the number of respondents liv-\ning in different geographic categories on the rural\u00adurban\ncontinuum is almost identical in MTurk and CCES. Both\nMTurk and CCES draw approximately 90% of their\nrespondents from urban areas. Using geographic data from\nthe surveys, we map the county-level distribution of\nrespondents across the country.\nFinally, we discuss how researchers can build \"pools\" of\nprior MTurk respondents, recontact these respondents\nusing the open-source R package MTurkR,9 and then use\nthese pools to over-sample and stratify to create samples\nthat have desired distributions of covariates. This is a useful\ntool for social science researchers because it allows them to\ndirectly stratify on key moderating variables.10 By drawing\non the strengths and weaknesses of MTurk samples and\ncutting-edge research tools such as MTurkR, researchers\ncan use similar sampling strategies to those of professional\npolling firms to directly address concerns about the exter-\nnal validity of their survey research.\nAge, gender, and race: exploring the\njoint distributions of key demographic\ncharacteristics\nIn this section we compare distributions of basic demo-\ngraphic variables in a CCES team survey11 and a survey\nconducted on MTurk at the same time during the fall of\nCCES had 1300. The questions in both surveys were asked\nin the exact same ways, though the CCES survey respond-\nents were also asked additional questions.12\nObtaining a survey sample with the desired racial, age,\nor gender characteristics is a difficult endeavor that has per-\nsistently challenged the external validity of research. For\nexample, scholars have frequently debated the quality of\ninferences when the results are drawn from college-age\nconvenience samples (Druckman and Kam, 2011; Peterson,\n2001). Some argue that research must be replicated with\nnon-student subjects before attempting to make generaliza-\ntions. Experimentalists push back and invite arguments\nabout why a particular covariate imbalance would moder-\nate a treatment effect. We argue in this paper that insofar as\nthis debate plays out with respect to MTurk, we should\nhave detailed information about what exact covariate\nimbalances actually exist.\nHuff and Tingley 3\nIn the survey research tradition there are a variety of\nmethods for achieving a \"nationally\" representative poll.\nFor example, the CCES creates a nationally representative\nsample of US adults using approximate sample weights\nfrom sample matching on registered and unregistered vot-\ners. This means that in order to generalize to the target pop-\nulation of US adults the CCES must weight respondents\nwith certain background characteristics more heavily than\nothers.13 Figure 1 shows the survey weights placed on indi-\nviduals in different age brackets. The results demonstrate\nhow the CCES up-weights younger individuals while\ndown-weighting older individuals.14 The cutpoint for age is\nfound by taking the mean of all the data (including CCES\nand MTurk). This method is used since we want to directly\ncompare individuals in the different age categories across\nMTurk and CCES. The results do not change when using\nother similar cutpoints. In the remainder of the paper we\nwill not use the CCES survey weights.15 Individuals could\nalways construct weights for MTurk samples. By ignoring\nweights we get to observe the underlying differences in the\nunweighted samples.\nIn Figure 2 we get a sense of the joint distributions of\nthree key variables: age, gender, and race. The mosaic plots\nshow, for each racial category, the proportion of respondents\nthat are male or female and young or old. For example, the\nfirst row of mosaic plots show for individuals of all races,\nthe proportion that are older females, older males, younger\nfemales, and younger males. If the width of a box under\nfemale is larger than for male, this means that there is a large\nproportion of females within that particular race. Similarly,\nif a box is taller for younger than for older individuals, this\nmeans that there is a larger proportion of younger than older\nindividuals of a particular race represented in the sample.\nFigure 2 demonstrates that the young individuals\nweighted most heavily by CCES are often the same catego-\nries that MTurk was best at attracting.16 We can see that\napproximately 75% of all respondents in CCES and MTurk\nwere White. Figure 2 also demonstrates differences in the\nCCES and MTurk samples with respect to African\nAmerican, Hispanic, and Asian respondents. For example,\nMTurk is able to attract between 2% and 5% more Hispanic\nand Asian respondents.17 In contrast, CCES is approxi-\nmately 6% better at recruiting African-American respond-\nents. We can take this analysis a step further by exploring\nthe joint distributions of age, gender, and race. For exam-\nple, we can see that in all racial categories MTurk attracts a\nlarge number of young respondents with this contrast at its\nstarkest among young Asian males.18\nResearchers could leverage the differential abilities of\nsurvey pools to attract respondents with demographic char-\nacteristics most suited to answering their theoretical ques-\ntion of interest.19 Just as scholars select the methodological\ntools most suited to addressing their question, the same\nlogic can be applied to choosing between survey pools.\nRecognizing the differential abilities of MTurk and CCES\nto recruit specific individuals of particular demographic\ncharacteristics is an important step. For example, Figure 2\ndemonstrates that MTurk is an excellent resource for\nexploring the opinions of Young Asian and Hispanic Males.\nHowever, CCES might be a better choice for exploring the\nopinion of Male African-Americans. As experimental and\nsurvey-based research continues to surge in popularity\npolitical scientists can and should take advantage of these\nstrengths and weaknesses of MTurk survey pools.\nParty ID, ideology, news interest,\nvoting, and education\nIn this section we explore the interaction between age and\nseveral variables commonly used in political science\nresearch. These include: (1) voter registration; (2) voter\nintentions; (3) ideology; (4) news interest; (5) party identi-\nfication; and (6) education. In doing so, we build upon the\nwork of Berinsky et al. (2012) by exploring the interaction\nof these variables with age. Using regression we demon-\nstrate that, on average, the estimated difference between\nCCES and MTurk decreases when we subset the data to\nyounger individuals. This means that when researchers are\nconsidering the dimensions along which they might expect\nto find heterogeneous treatment effects they should be cog-\nnizant of the ways in which older respondents differ across\nsurvey platforms. The regression estimates with standard\nerrors are presented in Figure 3.20\nFigure 3 depicts several differences across the two sur-\nvey platforms. First, voting registration and intention to\nturnout patterns among younger respondents are very simi-\nlar for both CCES and MTurk. In contrast, older respond-\nents in MTurk turnout and vote less than individuals of a\nWeight\nDensity\nFigure 1. Survey weights for different age cohorts in the CCES\ndata.\n4 Research and Politics \nsimilar age from the CCES. For party identification, which\nwas measured on a seven-point scale ranging from Strong\nDemocrat to Strong Republican, we again observe that\nyounger respondents are more similar for both CCES and\nMTurk. For older individuals the respondents in MTurk are\nconsistently more liberal than CCES. Somewhat similar\ntrends hold for ideology. The level of news interest, which\nvaries from most of the time to hardly at all, between\nrespondents in MTurk and CCES varies dramatically. Older\nindividuals in MTurk are less interested in the news than\nolder individuals from CCES. In contrast, younger MTurk\nrespondents are more interested in the news than younger\nindividuals from CCES. Finally, we can see that there are\nnot substantial differences in the levels of education between\nyounger and older MTurk and CCES respondents.\nWe can draw a number of conclusions from these\nresults. First, the similar registration and intention to vote\npatterns of CCES and MTurk respondents shows that\nFemale Male\nOlder\nYounger\nFemale Male\nOlder\nYounger\nFemale Male\nOlder\nYounger\nFemale Male\nOlder\nYounger\nFemale Male\nOlder\nYounger\nFemale Male\nOlder\nYounger\nFemale Male\nOlder\nYounger\nHispanic (34 Respondents)\nFemale Male\nOlder\nYounger\nMTurk\nFemale Male\nOlder\nYounger\nAsian (20 Respondents)\nCCES\nFemale Male\nOlder\nYounger\nFigure 2. Mosaic plots showing the gender and age composition for different racial categories in the CCES and MTurk modules.\nHuff and Tingley 5\nMTurk could be an excellent means for exploring how\nexperimental manipulations could influence voting ten-\ndencies. As we showed in the previous section, these\nmanipulations could be targeted at particular demographic\ngroups such as young Hispanic or Asian respondents.\nSecond, MTurk provides a useful means for attracting\nyoung respondents interested in the news. This means that\nMTurk could be used by political scientists to build upon\nprior research exploring the complex relationship between\nnews interest, political knowledge, and voter turnout\nresults presented in Figure 3 provide a means for political\nscientists to more fully understanding the external validity\nof MTurk surveys and also showing the strengths of MTurk\nfor exploring a number of substantive questions of interest\nto political science researchers.\nWhat do they do? The occupations of\nMTurk respondents\nOne of the most common questions we hear at workshops\nand conferences is about the occupational categories of\nMTurk respondents. Many scholars are rightfully con-\ncerned that MTurk respondents might all be unemployed\nor overwhelmingly draw from a small number of indus-\ntries. Depending on the particular research question, these\ndifferences could interact with our experimental manipula-\ntions in significant ways. Thus, the occupation of MTurk\nrespondents would be fundamentally different from that of\nother sectors of the population about which they are trying\nto make inferences. However, in this paper we show that\nthe percentage of MTurk respondents employed in specific\nindustries is strikingly similar to CCES.21 For example, we\ncan see that the percentage of individuals employed as\nProfessionals ranges from approximately 12% to 16% for\nCCES and MTurk. Indeed, in the 14 sector-specific occu-\npation categories we compare the maximum difference\nbetween MTurk and CCES is less than 6%. We can see this\ndifference in the \"Other Service\" sector of Table 1 where\n16.01% of individuals are employed in \"Other Service\" in\nsented in Table 1 should be reassuring to political scientists\nconcerned that the occupation of MTurk respondents is\nfundamentally different than other survey pools. Table 1\nEducation\nNews Interest\nVote Registration\nDifference\nVariable\nAge\nYoung\nOld\nFigure 3. Differences in means with 95% confidence intervals for the proportion of respondents registered to vote, proportion of\nrespondents that intend to vote in 2012, party identification, ideology, level of news interest, and education level in the CCES and\nMTurk modules. Positive values indicate that MTurk is greater than CCES. Dashed lines correspond with the confidence intervals\nfor older respondents and solid lines for younger.\n6 Research and Politics \ndemonstrates the occupational similarities between MTurk\nand CCES.\nWhere do respondents live? The\nurban\u00adrural continuum\nResearchers might also be concerned that MTurk respond-\nents are overwhelmingly drawn from either urban or rural\nareas. This, again, may or may not matter for estimating the\neffect of an experimental manipulation depending on the\nresearch question, but as with employment characteristics it\nis useful to know. In both the MTurk and CCES data we\nhave self-reported zip codes. We then link this data up with\nthe United States Department of Agriculture (USDA)\nrural\u00adurban continuum classification scheme to analyze the\ngeographic characteristics of survey respondents.23 These\nclassification codes range from metro areas coded 1\u00ad3 in\ndecreasing population size, to non-metro areas coded from\n4 to 9. In Table 2 we show that the number of respondents\nliving in different geographic categories on the rural-urban\ncontinuum is almost identical in MTurk and CCES.24 Both\nMTurk and CCES draw approximately 90% of their\nrespondents from urban areas with the remaining 10%\nspread across rural areas. For example, we can see that\nbetween 52% and 57% of respondents have a rural-urban\ncode of 1 which means they live in counties in metro areas\nof 1 million or more. In contrast, less than 2% of respond-\nents have a rural-urban code of 9 meaning that they live in\na location that is completely rural. The rural-urban com-\nparison of CCES and MTurk is presented in Table 2.\nTable 2 shows that MTurk and CCES respondents live in\nsimilar geographic locations on the rural-urban continuum.\nThis means that social science researchers should not be\nconcerned that MTurk respondents are overwhelmingly\ndrawn from either urban or rural areas in a way that might\nbias their results, compared to what they would get from a\nmajor professional polling firm. In Appendix B we present\na map showing the distribution of respondents at the county\nlevel in the MTurk sample across the United States (Figure\n5). Political scientists can explore the geographic distribu-\ntion of their respondents using this paper's replication files\ncross applied to their own studies. If, for example, an over-\nwhelming number of respondents are drawn from a particu-\nlar state or county we will be able to view this on the map.\nThe similarities between the occupation and rural-urban\nlocation of respondents from MTurk and CCES has impli-\ncations for experimental and survey-based research. For\nexample, political economists exploring preferences over\ntrade, immigration, and redistribution, for which occupa-\ntion and location are of critical importance, can consider\nusing MTurk and not be concerned that their respondents\nare overwhelmingly drawn from particular occupations or\ngeographic locations that look different from what profes-\nsional poll sampling would yield. These results provide a\nfirst response to questions frequently raised at workshops\nand conferences about whether the geographic and employ-\nment characteristics of MTurk respondents are fundamen-\ntally different from other survey pools.\nDeveloping survey pools\nResearchers can use MTurk to build \"pools\" of prior MTurk\nrespondents that they can then use in several different ways\nfor future surveys. This is done by first having a MTurk\nrespondent take a survey where the researcher records vari-\nables of interest, such as age, race, gender, and party, and\nthen match these characteristics to the unique identification\nnumber possessed by every MTurk respondent. Once this\npool is developed researchers can use the open-source R\npackage MTurkR to recontact their prior respondents.25\nMTurkR has the potential to revolutionize online experi-\nmental and survey research as political scientists can use\nthis package for over-sampling or stratifying on crucial\nvariables of interest such as party or gender.\nThere are two main techniques researchers can use to\nbuild pools. In the first technique, researchers can pool\nTable 1. The occupation of respondents by survey.\nOccupation CCES (%) MTurk (%)\nTable 2. The percentage of respondents in urban/rural areas\nby survey.\nUrban\u00adrural code CCES (%) MTurk (%)\nHuff and Tingley 7\nacross respondents from their prior MTurk surveys. Since\nresearchers commonly ask the same battery of questions\nabout the demographic characteristics of their respondents,\nthey can use these characteristics to then stratify on varia-\nbles of interest. For example, over time, we have collected a\nlarge pool of MTurkers that have taken our surveys and told\nus their gender, ideology, partisan affiliation, and zip code.\none- to seven-point ideology scale the average was 3.35,\n34% self-identified as Democrat, 22% as Republican, and\n26% as independent (the remaining identified with \"other\"\nparties), and the average age was 32.26 This technique of\npooling across multiple surveys is most useful for research-\ners that conduct a high volume of surveys on MTurk.27\nA recent strain of research exploring the characteristics\nof MTurk workers argues that what differentiates MTurkers\nis their status as permanent participants (Chandler et al.,\n2014). A potential concern with permanent participants is\nthat they have taken a number of similar studies which can\nthen subsequently affect the ways in which they both answer\nquestions and respond to treatment conditions.28 Moreover,\nthe use of high-volume survey takers has the potential\nto undermine some of the assumptions of experimental\nresearch methods.29 We view the ability to build pools of\nrespondents as a way to potentially address this concern.30\nSince researchers that build pools have data on the number\nof times an individual has taken their prior surveys, they\ncould build information into their pool about the types of\nrespondents that are \"high-volume\" takers and then test for\nheterogeneous treatment effects. The assumption here is that\nrespondents that take a high-volume of surveys are less\nlikely to be naive workers, and more likely to appear in prior\nsurveys with a higher frequency. Researchers can then\nincorporate this information about their respondents to test\nfor whether treatment differentially affects MTurk workers\nthat have taken a higher frequency of prior surveys.\nIn the second technique, researchers create a pool by first\ncreating a HIT that oversamples respondents and asks a\nsmall battery of questions upon which the researcher would\nlike to subsequently stratify. They then use this new pool to\nrecontact respondents with the desired attributes of interest.\nThis technique is useful to researchers that conduct an infre-\nquent number of surveys as they have likely not built up a\npool of adequate size to be able to pool across multiple sur-\nveys to directly recontact respondents. Moreover, this two-\nstage sampling procedure allows researchers to recruit\nrespondents over a relatively short timeframe. Gay et al.\n(2015) provide a concrete example of how this could be\ndone in practice in order to address concerns about not being\nable to obtain enough non-White respondents. Using a two-\nstage sampling procedure, they first recruited 1940 respond-\nents to take a demographic survey. From these 1940\nrespondents, they then recontacted a sample that included\nall of the Black, Hispanic, and Asian respondents from this\ninitial survey, as well as 200 randomly drawn White\nrespondents. This technique allowed Gay et al. (2015) to\nensure that they obtained a final sample with variation\nacross their theoretically motivated respondent characteris-\ntics of interest.\nThe ability to over-sample or stratify on variables of\ninterest can be a useful tool for social science researchers.\nFor example, this has been very helpful in our research on\nclimate change politics because we are particularly inter-\nested in individuals who deny climate change, which is rela-\ntively rare in the liberally oriented MTurk population.\nScholars can now ensure that the samples they draw from\nMTurk satisfy specific criteria of their choosing. Researchers\ncan use this tool to ensure that they obtain a sample with a\nspecified number of Democrats and Republicans. Or\nresearchers could stratify on other questions.31 Doing so\nallows the researcher to obtain larger sample sizes of other-\nwise hard to reach parts of the population that likely will\nrespond quite differently to experimental manipulations.\nThis then becomes very important for being able to estimate\nheterogeneous treatment effects which are interesting in\ntheir own right. Furthermore, this marks a step toward\naddressing external validity criticisms of research conducted\nusing MTurk samples since scholars can use similar sam-\npling strategies to those used by professional polling firms.\nFinally, researchers can create panel surveys by recontacting\nrespondents in much the same way.\nConclusion\nIn this paper we took a step toward answering the fre-\nquently voiced question of \"Who are these MTurk respond-\nents?\". In doing so we presented a number of results. First,\nwe compared the joint distributions of key demographic\ncharacteristics of interest to political scientists. In doing so\nwe analyzed they strengths and weaknesses of MTurk sam-\nples as they cut across these different categories. For exam-\nple, we showed that MTurk is relatively strong at attracting\nyoung Hispanic females and young Asian males and\nfemales. Second, we showed how the age of respondents\ninteracts with voting patterns, partisan preferences, news\ninterest, and education. We demonstrated that, on average,\nthe estimated difference between CCES and MTurk\ndecreased when we subset the data to younger individuals.\nFourth, we compared the occupations of respondents from\nMTurk and CCES. We showed that the percentage of\nrespondents employed in a specific sector were very simi-\nlar, with a maximum difference of less than 7%. Fifth, we\nshowed that the number of respondents living in different\ngeographic categories on the rural\u00adurban continuum is\nalmost identical in MTurk and CCES. Both MTurk and\nCCES draw approximately 90% of their respondents from\nurban areas. Finally, we discussed how experimental politi-\ncal scientists can build \"pools\" of prior MTurk respondents\nand recontact these respondents using the open-source\n8 Research and Politics \npackage MTurkR. Researchers can use these pools to over-\nsample and stratify to build samples that are balanced on\ntheoretically motivated variables of interest.\nThe results presented in this paper provide a number of\ncomparisons that could be useful for further understanding\nthe external validity of research relying on MTurk samples.\nThis is important for social science researchers when we\nhave strong theoretical reasons to suspect that our experi-\nmental manipulations will interact with characteristics of\nthe sample. We provided several examples of how experi-\nmental researches can leverage the strength and weaknesses\nof MTurk samples to their advantage. For example, we\nshow that MTurk is an excellent resource for attracting\nyoung individuals interested in the news, Hispanics and\nAsian respondents, as well as individuals from a number of\nindustries and geographic locations in ways that parallel\nother professionally supplied samples. The results demon-\nstrated in this paper show that there are strong reasons for\nresearchers to consider using MTurk to make inferences\nabout a number of broader populations of interest.\nThere are a number of takeaways from this paper that are\nuseful for both academics and non-academics alike. First,\nMTurk is a relatively inexpensive and easy to use survey\nplatform that allows researchers in both academia and the\nprivate sector to gain access to a large number of survey\nrespondents. This means that MTurk can serve as a \"democ-\nratizing\" force by allowing researchers to field surveys that\nmight otherwise be difficult given the high costs often asso-\nciated with professional survey firms. Second, respondents\non MTurk are not all that different from respondents on other\nsurvey platforms. These differences are even smaller as we\nfocus in on certain attributes of the worker pools such as\namong younger respondents. This means that researchers,\npolicymakers, and journalists reading work that utilizes the\nMTurk platform should not immediately dismiss the research\nas being fielded on a non-representative sample, but instead\nthink carefully about how the MTurk worker pool differs\nfrom other platforms and how we might theoretically expect\nthis to affect results. Third, the ability to build survey pools\nand recontact respondents with particular attributes is a use-\nful tool for anyone attempting to survey individuals with a\nspecific set of characteristics. This is useful for researchers in\nboth academia and the private sector as they attempt to gain\naccess to a particular set of respondents.32 As experimental\nand survey-based research continues to surge in popularity it\nis important that political scientists, journalists, and policy-\nmakers alike continue to ask and answer the important ques-\ntion of \"who are these people?\"\n"
}