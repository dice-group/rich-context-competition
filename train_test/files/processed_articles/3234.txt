{
    "abstract": "ABSTRACT. The literature applying information-theoretic ideas to economics has\nso far considered only Gaussian uncertainty. Ex post Gaussian uncertainty can be\njustified as optimal when the associated optimization problem is linear-quadratic,\nbut the literature has often assumed Gaussian uncertainty even where it cannot\nbe justified as optimal. This paper considers a simple two-period optimal saving\nproblem with a Shannon capacity constraint and non-quadratic utility. It derives\nan optimal ex post probability density for wealth in two leading cases (log and lin-\near utility) and lays out a general approach for handling other cases numerically. It\ndisplays and discusses numerical solutions for other utility functions, and consid-\ners the feasibility of extending this paper's approaches to general non-LQ dynamic\nprogramming problems. The introduction of the paper discusses approaches that\nhave been taken in the existing literature to applying Shannon capacity to eco-\nnomic modeling, making criticisms and suggesting promising directions for fur-\nther progress.\nIn a pair of earlier papers1 I have argued for modeling the observed inertial re-\naction of economic agents to external information of all kinds as arising from an\ninability to attend to all the information available, and for treating that inability as\narising from finite Shannon capacity. Shannon capacity is a measure of information\nflow rate that is inherently probabilistic. It uses the reduction in the entropy of a\nprobability distribution as the measure of information flow. The entropy of a dis-\ntribution is a global measure of the uncertainty implied by the distribution, relative\nto some base distribution. Because of this dependence on the base, the entropy of a\ndistribution is not uniquely defined, but if we consider the joint distribution of two\nrandom vectors or variables, the expected reduction in entropy of one of the two\nachieved by observing the other of the two, the mutual information implied by\nthe joint distribution, is uniquely defined, independent of any base. This measure\nThis research was supported in part by NSF grant SES-0350686. The paper was originally pre-\npared for and presented at the Deutsche Bundesbank Spring Conference, May 2005.\n1(Sims, 2003, 1998). The earlier paper contains an appendix arguing that Shannon capacity makes\nsense as a model of inattention. The later one gives explicit solutions for some simple economic\nmodels with a linear-quadratic structure.\nc 2005 by Christopher A. Sims. This document may be reproduced for educational and research\npurposes, so long as the copies contain this notice and are retained for personal use or distributed\nfree.\nof mutual information can be derived from a few reasonable axioms, but it is per-\nvasive less because of its axiomatic appeal than because has proved to be exactly\nthe concept appropriate for studying information flows in physical communica-\ntion channels. A Shannon \"channel\" is a set of possible inputs, a set of possible\noutputs, and a conditional distribution for outputs given inputs. From these ele-\nments, it is possible to calculate a tight upper bound for the mutual information\nbetween inputs and outputs, which is called the channel's capacity. It is the mea-\nsure of information flow we use in characterizing modems or internet connections\nin bits per second or bytes per second. Shannon showed that no matter what we\nmight wish to send through the channel, whether music, text, or spreadsheets, and\nno matter what the physical nature of the channel -- wires, optical cables, radio\ntransmission, or a messenger service -- it is possible to send information through\nthe channel at a rate arbitrarily close to capacity.\nEconomists, particularly macroeconomists, have recognized the need to account\nfor the inertia in observed economic behavior and have modeled it with a variety of\ndevices -- menu costs, adjustment costs, information delay, implementation delay,\netc. As my two earlier papers argued, these mechanisms can match the observed\npattern -- slow, smooth cross-variable responses, combined with less smooth id-\niosyncratic randomness -- only by postulating elaborate inertial schemes that are\nboth difficult to connect to observation or intuition and critically important in mak-\ning model behavior realistic. One appeal of the rational inattention idea (that is,\nof modeling agents as finite-capacity channels) is that it can in principle explain\nthe observed patterns of inertial and random behavior by a mechanism with many\nfewer free parameters. Another is that it fits well with intuition; most people every\nday encounter, or could very easily encounter, much more information that is in\nprinciple relevant to their economic behavior than they actually respond to. The\nnotion that this is because their are limits to \"attention\", and that such limits might\nbehave like finite Shannon capacity, is intuitively appealing.\nA number of recent papers in macroeconomics and finance have used information-\ntheoretic ideas (Ma\u00b4\nWhile these papers develop some valuable insights, it is worth noting that they\nhave made assumptions, to allow tractable modeling, that are hard to defend and\ncan lead to anomalous results. Some of these limitations are common to all or\nnearly all of the papers.\nII.1. Not allowing fully endogenous choice of the form of uncertainty. It is cen-\ntral to the idea of modeling individuals as capacity-constrained that the nature,\nnot just the quantity of their uncertainty about external signals (prices, income,\nwealth, asset yields, etc.) is subject to choice. The power of information theoretic\nideas arises from the fact that the available joint stochastic processes for channel\ninput and channel output are, to an arbitrarily good approximation, limited only\nby the capacity of the channel, not by its physical nature. In a model of an opti-\nmizing agent, the agents' objective function will therefore determine the stochastic\nprocess for the joint behavior of actions and external signals. The articles cited in\nthe previous paragraph, with the partial exception of Luo's, postulate directly a\nsimple parameteric form for this joint process, without deriving that form from\nthe model's objective function.\nTo be more specific, the papers all assume Gaussian prior uncertainty about a\nstate variable and Gaussian posterior (after information flow) uncertainty. Fur-\nthermore, in some cases they assume that the prior and posterior uncertainty is\nover a random vector and is i.i.d., either over the elements of the vector itself or\nover a set of factors that generate the distribution of the vector. (Luo and Mondria\ndo consider endogenous choice of posterior covariance structure.) It is true that\nGaussian posterior uncertainty can be shown to be optimal when the loss function\nis quadratic, but only Luo's paper considers cases of pure quadratic loss. Even if\nthe loss function is quadratic, it is not generally optimal for a capacity-constrained\nagent to have i.i.d. posterior uncertainty across the same variables or factors that\nwere a priori i.i.d. As we will see in some examples below, standard forms of utility\nfunctions in an economic model generate strongly non-Gaussian forms of optimal\nposterior uncertainty.\nII.2. Back-door information flows. Several of the papers develop market equilib-\nria, and to avoid complications assume that market prices are observed without\nerror. But in these equilibria market prices are information-carrying random vari-\nables. Assuming they can be observed without error amounts to assuming un-\nbounded information-processing capacity.2 Counter-intuitive results can emerge\nwhen we assume perfect observation of prices combined with capacity-constrained\nobservation of some other source of information.\nII.3. Distinguishing human information use, costly external information trans-\nmission and costly investigation. The models in Sims (2003) and those presented\nbelow in this paper are motivated by the idea that information that is freely avail-\nable to an individual may not be used, because of the individual's limited infor-\nmation processing capacity. That capacity is unitary, allocatable to control many\n2An infinitely long sequence of digits can carry an infinite amount of information. Such a se-\nquence, with a decimal point in front of it, is a real number. So if I can transmit an arbitrary real\nnumber without error in finite time, I have an infinite-capacity channel.\ndimensions of uncertainty the individual faces. The \"price\" of this information is\nthe shadow price of capacity in the individual's overall optimization problem.\nIndividuals or firms may also choose Shannon capacities of periodical subscrip-\ntions, telephone lines, internet connections, and other \"wiring\" that brings in in-\nformation from the outside world. For a financial firm with a large staff constantly\nactive in many markets, the wiring costs of information may indeed be more im-\nportant, or at least comparable to, the costs of mapping information, once it is\non the premises, into human action. However for most individuals, wiring costs\nare likely to be small relative to the costs associated with human information pro-\ncessing. In any case the costs of the two kind of information will generally be quite\ndistinct, on a per-bit basis. One can't replace the human decision-making that links\nprices, incomes and wealth to real actions with a fiber-optic cable.\nBoth wiring and internal human information processing are reasonably mea-\nsured in bits, with costs linear, or at least smooth, in bits. There is another kind\nof \"information\", however, whose cost is different, and probably usually not well\nmeasured in bits. In the stock market, an individual investor has a vast amount\nof information about individual stocks available at practically no or trivial cost, in\nnewspapers and on the internet. It is likely that he does not use all this information,\ndue to limited information processing capacity. But it is also possible to develop\ninformation through costly investigation -- interviewing experts in a firm's tech-\nnical area, conducting surveys of consumers to determine their reactions to the\ncompany's product, etc. The CEO of a drug company might contemplate approv-\ning a clinical trial to determine whether or not a new drug is an improvement on\nexisting treatments, approving a focus group investigation of which of two pack-\nages is most preferred by consumers, or stepping outside to see if it's raining. Each\nof these three actions would (if the answers had 50-50 probability in advance) yield\none bit of information. But it is no help to decision making to think of them as bits\nlimited by a capacity constraint.\nSeveral of the finance-oriented papers cited above consider at least some models\nin which uncertainty about an asset's yield is quantified as the standard deviation\nof its distribution, and information costs are quantified as bits, measured by re-\nduction in the log of the standard deviation. But this is only appropriate if the\ninformation is thought of as freely available, with only wiring costs or human ca-\npacity costs preventing it from being known with certainty. In asset markets this\nis almost never the case. Sophisticated, continuously trading investors have un-\ncertainty that is dominated by information that is not freely available, and less\nsophisticated investors, who do fail to use instantly all freely available informa-\ntion, do not have the option of reducing the log standard error of their uncertainty\nabout yields to arbitrarily low levels according to a linear cost schedule.\nWhile failure to make these distinctions does not necessarily make a model unin-\nteresting, it can make a model's interpretation difficult. Especially in highly liquid\nfinancial markets, It is probably important to recognize that wiring capacity and\nhuman information-processing capacity have different costs. It is certainly impor-\ntant to to distinguish information about asset returns that is freely available but\ncostly to act upon from information that can be obtained only through costly in-\nvestigation.\nFor some purposes linear-quadratic Gaussian (LQG) models may give reason-\nable approximations. Luo (2004) applies information-theoretic ideas to optimiza-\ntion problems with linearized first order conditions as are commonly used recently\nin macroeconomic modeling. The idea is that if uncertainty is fairly small, lin-\near approximations to the model's FOC's may be quite accurate, so that the LQG\nframework remains an adequate approximation even with capacity constraints.\nWhile this is an idea worth pursuing, because it yields insights and is tractable,\nthere is reason to worry about its range of applicability. If rational inattention is\nto explain much of observed inertia in behavior, people must be using a small\npart of their capacity to monitor economic variables. But in this case information-\nprocessing based uncertainty will be large, and this in itself will tend to undermine\nthe accuracy of the local LQG approximation. Also, there are many interesting is-\nsues, like the interaction of finite capacity with the degree of risk aversion that is\ninvestigated below, that cannot be studied in an LQG framework.\nIn this section, therefore, we show that moving beyond the LQG framework is\nfeasible. We consider several variations on a simple two-period saving problem.\nThe problem is so simple that the information flows we will be looking at are un-\nrealistically low. Nonetheless it is interesting to see that the model provides some\ninsights into behavior, is computationally manageable, and suggests that a more\ninteresting fully dynamic version might be feasible.\nThe problem is\nmax\nlog c \u00b7 (w - c) f (c, w) dw dc (1)\nsubject to\nf (w, c) dc = g(w) (3)\nlog f (c, w) \u00b7 f (c, w) dw dc\n-\n\nlog\n\nc\nf (c, w) dw \u00b7\n\nc\nf (c, w) dw dc\n-\n\nlog g(w) \u00b7 g(w) dw   . (4)\nThe expression (1) is a standard assertion that we are maximizing expected utility,\nwhere that is the sum of the expected utility of current consumption, log c, and that\nof next period's consumption, log(w - c). (We could include a gross interest rate\ngreater than one and a discount factor less than one without changing anything\nimportant.) What is unusual is that the \"choice variable\" with respect to which we\nmaximize is not current consumption c, but the joint pdf of c with wealth w. The\nconstraint (2) recognizes that f = 0 puts us at the boundary of feasible values for\nprobability densities. The constraint (3) tells us that the marginal distribution of\nwealth is fixed, so all that is available for choice is f (c, w)/g(w), the conditional pdf\nof c given w. The information constraint is (4). The last term in (4) is the entropy of\nthe marginal distribution of w, the next to last term is the entropy of the marginal\ndistribution of c, and their sum is what the entropy of c and w's joint distribution\nwould be if they were independent. The first term is minus the entropy of the\nactual joint distribution determined by f. The the three terms together form the\nmutual information between c and w. This is also the expected reduction in the\nentropy of the w distribution from observing c, and also the expected reduction in\nthe entropy of the c distribution from observing w.\nThe first order condition for the problem is\nlog c \u00b7 (w - c)\n=  1 + log f (c, w) - 1 - log\n\nc\nf (c, w) dw + \u00b5(w) + (c, w) . (5)\nHere \u00b5 is the Lagrange multiplier on (3) and (c, w) is a stand-in for the fact\nthat when f = 0, the FOC's do not have to hold. (Since at f = 0 we will have\nlog f = -, no finite value of (c, w) makes the FOC hold when log(c \u00b7 (w - c)) is\nfinite, but the non-convexity of the constraint set means that solutions on the f = 0\nboundary can nonetheless occur at such c, w values.) If we let q(w | c) denote the\nconditional pdf of w given c,  = 1/, and (w) = e-\u00b5(w), this expression can, at\npoints where f > 0, be rearranged as\nq(w | c) = (w)c(w - c) . (6)\nThe function  must, according to (6), make the integral of the right-hand side with\nrespect to w one, regardless of the value of c. One  that works (the only one?) is a \nproportional to w-2-1. With this choice of , if we rewrite in terms of v = w/c - 1,\nthe integral becomes\n\nc\n\nSince the terms in c cancel, the integral does not depend on c, and by choosing 0\nproperly we can make the integral one.\nThe form of the integrand in (7) is proportional to that of an F(2 + 2, 2) density.\nNormalized to have constant spread, this does approach normality as  = 1/ ap-\nproaches infinity, i.e. as the shadow price of information in utility units approaches\nzero. The peak is at (2 - 1/( - 1))c, the mean at (2 + 1/( - 1))c, for  > 1. The\ndistribution is more tightly concentrated around w = 2c the larger is  = 1/.\nIn other words, as the shadow price  on the information constraint declines, we\ncome closer and closer to the certainty solution.\nFigure 1 shows a contour map of the pdf of w | c for a case where  = .5. Note\nthat the conditional distribution of w | c in this case is centered roughly at 2c, as\nwe expect. The solid line on the figure shows the value of c that would be chosen\nunder certainty, w/2, and the dotted line shows c = w, which is the lower bound\non w for a given c.\nFigure 2 displays these conditional densities in a different way. Each line shows\nq(w | c) for a different value of c between zero and .5. These are all of course\ntruncated, scaled F densities. They have all been scaled to integrate to 1 over the\n(0, 1) interval, to make them comparable to numerically derived densities we will\nshow below.\nAs is true for the LQ case, we find here that the form of the distribution for\nw conditional on available information at decision time is invariant to g(w), the\nmarginal pdf for w before information flow, so long as the density has full support.\nThis is not to say that the conditional distribution itself is invariant to g. If g has\nhigh entropy, then with a given  it will not be possible to reduce entropy much,\n will be large, and the  parameter that determines how close c is to w/2 will be\nsmall. But there is a single parameter, , that controls all the possible variation in\nthe form of the distribution of w | c when f has full support.\nThe first-order conditions need not hold at points where the joint pdf f (c, w) is\nzero. Obviously if g(w) is zero over some range, f (c, w) must also be zero over the\ncorresponding range of w values. Even where g(w) has the whole non-negative\nreal line as its support, it is possible for p(c) to emerge as zero over some set of\nc values and indeed even possible for the marginal distribution of c to emerge as\ndiscrete. If the conditional distributions of c given w has, over any set of w's with\nnon-zero probability, discrete weight on points that do not have discrete weight\nin the marginal distribution of c, this would imply an infinite information flow.\np(w|c),  = 1  = 0.5, unbounded g(w)\nC\nW\nBut so long as any c values getting discrete weight in the conditional distributions\nalso have discrete weight in the marginal, the information flow is finite. Of course\nat discrete values of c that have non-zero probability, the first-order conditions of\nthe problem hold, so for these c's, as for any others with positive density value,\nwe expect q(w | c) to take on the form we derived above, over its support. It is\npossible, though, for q(w | c) to have less then full support. This obviously allows\ndeviation from the F distribution we derived above, but of a particular form, since\nit must have the shape of an F density over its support.\nWhile the distribution of w | c is easy to characterize here, it is not easy (for\nme, anyway) to characterize the distribution of c | w, even in the case where the\nmarginal on w is assumed to have the same scaled-F form as the post-observation\ndistribution for savings, or to find a form for the marginal of c that, together with\nthe known form for w | c, implies that the marginal for w is a scaled F. However\nthis is just a very simple example. Fully dynamic models are likely to generate\ndistributions complicated enough to require numerical methods for solution in\nany case.\np(w|c), log utility,  = 0.5, unbounded g\nWe can see an analytic solution for one other simple special case: where the util-\nity function is linear and the c < w constraint is maintained.3 If the utility function\nis undiscounted, so U(c, w) = c + w - c = w, the problem has the trivial solution\nc = 0, with no information at all used. The problem is a little more interesting if\nutility is discounted, so U = c + (w - c) with 0 <  < 1. For any U, the FOC's\ntake the same form as (5), but with U(c, w) replacing the log function on the left of\nthe equality. For this linear case, the analog of (6) is\nq(w | c) = v(w)e(w+(1-)c) (8)\nBy choosing v(w) proportional to e-w, we get this in a form that, when integrated\nfrom c to  w.r.t. w, gives a constant value. The implied form for the conditional\npdf of u = w - c given c is (1 - )e-(1-)u. Here we can note that as  increases\n(so information is flowing more freely) the solution converges toward c = w, which\nis the optimum without uncertainty. It is also interesting that c and w are implied\n3Without the c < w constraint, and with discounting, agents who can borrow at zero interest\nwill obviously push c to infinity.\nto be independent conditional on any rectangle inside the w > c region. These risk-\nneutral agents waste no capacity on matching c to w itself, except as it contributes\ntoward knowing where the w = c boundary is.\nIn the model with log utility, capacity-constrained agents have expected wealth,\ngiven their consumption, that exceeds the level corresponding to the deterministic\nsolution w = 2c. The higher are information costs (the lower is capacity), the longer\nis the tail on the w | c distribution and the larger the excess E[w | c] - 2c. There\nis an effect that seems to go in the opposite direction, of course: the mode of the\nw | c distribution falls further below the deterministic value as information costs\nincrease. However when data are aggregated across many individuals in different\ncircumstances, we would expect the expectation result to dominate. We thus see a\n\"precautionary savings due to information costs\" effect.\nBut notice that the model with linear utility produces the opposite result. These\nrisk-neutral agents who discount the future, while facing a gross rate of return of\n1, are constrained from consuming all their wealth in the first period only by their\nuncertainty about what what that total wealth is. Relaxing their capacity constraint\nproduces less saving.\nWith quadratic utility, the left-hand side of (5) is quadratic. Normalizing the\nutility function to U(c, w) = c - 1\n(w - c)2 leads to the analog of (6)\nas\nIf we drop the c < w constraint and also the c > 0 constraint, the right-hand-side\nof (9) as a function of w is proportional to a Gaussian pdf with variance 1/, with\nonly the mean of the distribution dependent on c. Hence we can make the right-\nhand-side's integral one by choosing v(w) to be constant. It is then easy to verify\nthat if the exogenously specified marginal pdf for w, g(w), is Gaussian, the joint\npdf for c and w is Gaussian. Observe that whatever g(w) we start with, so long\nas it allows a solution with f > 0 everywhere, the conditional distribution of w\nis Gaussian. Thus if this problem were part of a recursive scheme, all the joint\ndistributions of successive c's and w's after the first period would be Gaussian.\nHowever, this result depends crucially on there being no c < w or c > 0 re-\nstriction. With these restrictions, despite the form of (9), the dependence of limits\nof integration on c will require at least a non-constant v(w), and possibly some\nregions of f = 0.\nSo we can conclude from these examples:\n\u00b7 A hard budget constraint is not incompatible with a finite rate of informa-\ntion flow. The conditional distribution of c | w can be confined to the (0, w)\ninterval, even though observation of neither c nor w ever gives perfect in-\nformation about the other variable. An agent behaving this way would be\nmaking decisions that only imperfectly determine c, based on his imperfect\nknowledge of w. For example, writing checks or using credit cards and oc-\ncasionally finding that the account is overdrawn, or getting to the checkout\ncounter of the grocery store and realizing he will have to put a few things\nback, or buying $10 worth of gasoline without figuring out in advance how\nmany gallons that will be.\n\u00b7 Uncertainty arising from information processing can easily be quite non-\nnormal, even when exogenous shocks are small. In this example, there are\nno exogenous shocks. Normality is a good approximation only when the\ninformation constraint is not having a strong effect.\n\u00b7 A capacity constraint can have powerful implications for savings behavior.\nThis accords with the facts that most people only vaguely aware of their\nnet worth, are little-influenced in their current behavior (at least if under\n50) by the status of their retirement account, and can be induced to make\nlarge changes in savings behavior by minor \"informational\" changes, like\nchanges in default options on retirement plans.\nI have no recipe for exhaustively identifying cases like log utility, linear utility,\nand quadratic utility without borrowing constraints, in which an analytic solution\nfor q(w | c) is obtainable. Indeed I have the impression that such cases are very\nrare. So it is worthwhile to look at some examples of commonly used U(x, y)\nfunctions and see how hard it is to compute solutions.\nFor an f > 0 solution, the first-order conditions and the constraint that the mar-\nginal pdf for w be the given g(w) lead to the pair of equations\neU(c,w)v(w)dw = 1 , all c, (10)\nh(c)eU(c,w)v(w)dc = g(w) , all w , (11)\nwhich have to be solved for v and h, where h is the marginal pdf of c. This is\na recursive linear system. It looks like we could discretize it, solve the resulting\nsimple linear system from (10) for v, then use those results in (11) to create another\nsimple linear equation system to solve for h. This approach does not work.\nEach of these equations is what is known as a Fredholm integral equation of type\n1, which are notoriously ill-conditioned except in special cases. In other words, a\nv that makes the norm of the vector of discrepancies between right and left hand\nsides of (10) nearly zero, can differ from the true solution in v-space by a large\namount. Furthemore, it is not going to be uncommon for there to be regions of\nc, w space with f (c, w) = 0 in the solution. If one knew where these were, (10-\n11) could be used on the remaining c, w values. But in general we will not know\nwhere they are, and searching over all the possible combinations of such regions is\nprohibitively complicated.\nAn approach that I have found to work is simply to discretize f itself and maxi-\nmize the Lagrangian\nU(c, w) f (c, w) dc dw - H(W, C) - \u00b5(w) f (c, w) dc - g(w) . (12)\nwith  fixed at some positive number. Here H(W, C) is the mutual information\nbetween w and c in their joint distribution, the same object that appears on the left-\nhand side of the information constraint (4). This can work because points at which\nf = 0 simply drop out of both the information constraint and the expected utility.\nI have imposed f > 0 by maximizing over log f as the parameter vector. This\nmeans of course that the parameters corresponding to f (c, w) = 0 values are ill-\ndetermined, but gradient-based search methods (at least my own, csminwel.R,\nwhich is what I used) still perform well, converging nicely for the f (c, w) > 0\nvalues and leaving log f extremely negative at points where clearly f (c, w) = 0.\nThe discretized solutions below all are based on using an equi-spaced grid with\nmarginal pdf g for w is given in each case as g(w) = 2w. There are then 31 adding-\nup constraints, and 8 \u00d7 15 = 120 zero constraints on points where c > w, leaving\nunconstrained 15 \u00d7 31 matrix  of parameters, with the entry fij of the discretized\nf determined as gj exp i-1,j\n) for i > 1 and as gj/(1 + \u00af\n) for i = 1, and\nwith \u00af\n = i\nexp(ij\n).\nThough for some problems 345 parameters would be so many as to raise compu-\ntational difficulties, here there seems to be no trouble with them. Using interpreted\n(in the R language4) code and numerical derivatives, convergence is achieved in\nabout 450 iterations, with each iteration taking about 1.5 seconds (making the\nwhole computation take about 11 minutes) on a 3GHz Pentium 4 running Linux.\nThey could be made much faster using analytic derivatives, which would not be\nhard to program, and probably also by using compiled code. A more sophisticated\nalgorithm could also help. We know the solution should be smooth on its support\nand should have a density with respect to some product measure on c, w space.\nParameterizing the solution to reflect this knowledge should enable accurate ap-\nproximations with many fewer free parameters.\n4Available at http://lib.stat.cmu.edu/R/CRAN/\np(w|c), log utility,  = 0.5, triangle g\nFigure 3 shows the same kind of plot as Figure 2 -- q(w | c) densities with\nvarious values of c, log utility, and  = .5. The difference is that here, instead of\nshowing the theoretical densities for the case where the q s (and hence necessarily\nalso g) have unbounded support, we show the numerically computed densities for\nour case of a linearly increasing density with support [0, 1]. The lines on the two\nplots should not match exactly, because one shows a discretized approximation to\nthe solution for continuously distributed c and w, while the other shows an exact\nsolution for discretely distributed c and w. Nonetheless these conditional pdf's do\nmatch up, roughly. Note that in Figure 3 there are fewer lines shown, because the\ndiscreteness and the bounded support for g mean that c does not have full support\neven in [0, .5], as can be seen from Figure 4. In fact, with this degree of information\nconstraint, the solution sets c to .4 with probability .72.\nTo see the effects of varying degrees of risk aversion, consider calculations for\nthe same triangular g(w), but with utility given by the CRRA form\nU(c, w) =\npdf of c,  = 1,  = 0.5, triangle g(w)\nC\np(c)\nThe values of  in the two examples we consider have been adjusted so that the\ninformation flow (.88 bits and .85 bits), is about the same for the two cases ( = .5\nand  = 2, respectively) considered, so the differences in results are attributable\nto the differences in the risk aversion parameters. Of course if the model were\nformulated with an actual opportunity cost of information, in consumption goods\nunits, the solution with lower risk aversion would most likely imply a choice of\nlower information flow.\nShaded plots of the joint densities of c and w are shown in Figures 5 and 6. Both\nsolutions show discretization, with several values of c receiving zero probability.\nThe  = .5 solution puts probability .70 on the highest value of c, which is .37.\nThe  = 2 solution also puts probability .70 on its highest c value, but the value\nis slightly lower -- .34. The low risk-aversion solution shows a more discretized\ndistribution of c at lower values of w, and puts substantially more probability on\nc values close to the c = w line. This is as would be expected. The  = 2 utility\nfunction goes to minus infinity as w - c  0 or c  0. It therefore makes it\nworthwhile to be well informed about low wealth values, so that c is not forced\nC\nW\ntoo close to zero, and also to keep the probability of choosing c close to w (and\nhence w - c small) low.\nWith these non-log utility functions, we expect to see deviations from the F form\nof q(w | c). Plots analogous to Figures 2 and 3 for these two non-log utility exam-\nples are shown in Figures 5 and 8. The high-risk-aversion plot shows narrow finite\nsupport for q(w | c) at low values of c, and then very flat tails on the pdf's as we\nmove to higher c values. The lower-risk-aversion plot shows more of a right tail\nand less narrow support at the low c values, and thin tails at higher c values except\nfor the highest, which in both plots is the one with the most weight. These results\nmake sense, with the higher risk aversion solution trading higher probability of\nchoosing a modest value of c when wealth is in fact high for a lower probability of\na mismatch between c and w when wealth is low.\nThe fact that discreteness in the distribution of c and finite support, varying with\nc, in q(w | c) show up in both these standard cases, despite a continuous marginal\ndistribution for w, implies that any numerical approach to solution of rational inat-\ntention models must allow for these possibilities. They are not an artifact of the\np(c,w), triangle g(w)  = 2  = 1\nC\nW\ntriangular g(w). That g(w) was chosen to bring out the differences with degree\nof risk aversion, but the discreteness of the c distribution showed up with every g\nI considered, including standard Beta, F, and Gamma pdf's that peaked below .3\nand were small or zero at w = 1.\nWhile the discreteness will be a challenge for programmers, it may help the the-\nory rationalize observed patterns of behavior. Actual choices by individuals in\nresponse to external information often do seem to have a discrete character.\nV.1. On to a fully dynamic non-LQG model. Here is the Bellman equation for a\ndynamic programming problem with Shannon capacity as a constraint, the current\npdf g of w as the state variable, and f (c, w) as the control:\nV(g) = max\nf (\u00b7,\u00b7)\nU(c) f (c, w) dc dw +  V h(\u00b7; c, w) f (w | c) dw f (c, w) dc dw\nW\np(w|c)\nsubject to\nf (c, w) dc = g(w), all w (15)\nThe function h maps the current c, w pair into a conditional density for next pe-\nriod's w. In usual models, it is specified indirectly in the form of an equation like\nwt = (wt-1, ct-1, t) together with a specification that t has a certain pdf and\nis independent of wt-1 and ct-1. The constraint connecting f (c, w) to f (c | w) has\nbeen left implicit. This problem is in the form of a standard dynamic programming\nproblem, except that the state and control variables are both in principle infinite-\ndimensional. But, extending the approach taken above to a two-period problem, I\nbelieve computing solutions to such problems should be feasible. Economists are\nalready succeeding in calculating solutions to equilibrium models with infinite-\ndimensional state spaces.\np(w|c),  = 2,  = 1, triangle g\nW\np(w|c)\nNote the occurrence of f (w | c) in the argument of the value function on the\nright-hand side in (14). This reflects the fact that the agent must allow some \"noise\"\nto affect the choice of c in the current period, but can use the noisy observation that\nentered determination of c to update beliefs about next period's w.\nV.2. Rational inattention models of general equilibrium. The work cited earlier\nby van Niewerburgh and Veldkamp and by Mondria includes calculation of mar-\nket equilibrium with capacity constraints. As already noted in section II, though,\nthese models assume costless and perfect observation of market prices, which is\nboth contrary to the notion of finite-capacity agents and a source of anomalous re-\nsults. This modeling choice by these authors is not an easily corrected oversight.\nModeling a market equilibrium with agents who do not know exactly what prices\nare requires being explicit about aspects of market microstructure that are not stan-\ndard parts of the economic theory toolbox and about which we have few stylized\nfacts or modeling conventions to guide us.\nA model populated by capacity-constrained agents will not simply balance sup-\nply and demand via a price mechanism. Agents will not have perfect knowledge\nof prices, indeed may have only a very rough idea of what they are, as they take\ndecisions that affect economic exchange and production. Inventories, retailers,\nwholesalers, demand deposits, cash, and credit cards, are all devices that allow\nagents to make transactions in which quantities and price are known and cho-\nsen only approximately. Few of our models have explicit roles for retailers and\nwholesalers, our models of inventory behavior are only modestly successful, and\nmicrofounded models of money that connect to data are non-existent. The idea of\nShannon capacity may be of some help in modeling these phenomena, but they\nare inherently difficult, long-standing problems, so realistic general equilibrium\nmodels with capacity-constrained agents may not emerge for some time.\nV.3. Macro modeling. Since a standard approach to general equilibrium mod-\neling with rational inattention will not emerge soon, applying rational attention\nto the representative agent equilibrium models that now constitute mainstream\nmacro will also take some time. In the meantime, though, we can see even from\nsimple linear-quadratic examples that there are implications for current modeling\npractice. In the linear-quadratic framework, rational inattention behavior is a con-\nstrained special case of the behavior of an agent who observes state variables with\nerror. While there are some examples of such models in the macroeconomic litera-\nture (Lucas, 1973; Woodford, 2001), the rational inattention idea should encourage\nus to pay more attention to such models. The objection that there is no physical\ninterpretation for the observation error such models postulate is answered by the\nrational inattention framework, and the RI framework gives us some guidance as\nto reasonable properties for the observation error, even when we cannot derive it\nanalytically.\nV.4. Public and private information models. Recently, following the paper by\nMorris and Shin (2002), there have been a number of papers considering models\nThis work raises interesting questions and arrives at conflicting conclusions about\nthe value of \"transparency\", depending on assumptions about externalities that\nare difficult to calibrate against an actual economy. But for all the diversity in the\nconclusions from this literature, it is all formulated on the assumption that there\nare private information sources with exogenously given attributes and a public in-\nformation source whose stochastic character we can imagine controlling. From a\nrational inattention perspective, this is a strange setup. Capacity-limited agents\nwill act as if observing the state of the economy with error even if some public\nauthority announces it exactly. The amount of the error will depend both on the\nstochastic properties of the state itself and of any noise in public signals about it. If\nprivate signals carry a lot of information about privately important variables, in-\nformation they contain about an aggregate state may be ignored or reacted to very\nslowly and erraticly5. Before these abstract models are applied to policy debates\nabout transparency in monetary policy, they need to be tied closely enough to real\neconomies that we can judge which of their conflicting conclusions might be cor-\nrect, and this should include consideration of how their conclusions are affected\nby rational inattention.\nRational inattention may have far-reaching implications for macroeconomics\nand monetary policy generally, once its implications are fully worked out. In the\nmeantime, though, it may shed some light on transparency in monetary policy.\nFor a capacity-limited agent, it is necessary to take actions that respond to the true\nstate of the economy at a low information transmission rate. This means that reac-\ntions to the state are either delayed and smoothed, with added idiosyncratic error,\nor they are discretized and randomly timed. A central bank may provide the pub-\nlic with a heavily filtered view of its actions or judgments, on the assumption that\nthe public cannot take in the full detail and complexity of its actions and the think-\ning behind them. In the US, this takes the form of discretized, somewhat randomly\ntimed policy actions (changes in the Federal Funds rate) together with a brief para-\ngraph rationalizing the action. The paragraph occasionally changes the wording of\na phrase or two, and the market often reacts strongly to these changes. If enough\nmarket participants take this simple, discrete sequence of changes in wording as\na free, low-bit-rate summary of an important state variable, it is unsurprising that\nmarkets respond discretely to these changes in wording. Even market specialists,\nwho know the true state with high precision, will pay attention to the noise in the\nFed signal because of its effects on other agents.\nIf this is what is going on currently, what would be the effect of the Fed's issuing\na much more detailed inflation report, like those issued periodically by inflation-\ntargeting banks? A naive extrapolation might suggest that since the Fed now pro-\nvides very little information, which generates overreaction, overreaction would be\nmuch worse if the Fed proved much more information. But this is unlikely. Mar-\nket participants who need a low-bit-rate summary of the state (of the economy, or\nof the Fed's policy stance) would still look for it, but there would not be a unique\nlow-bit-rate summary in the Fed's own statements. Market participants who need\nto devote most of their attention elsewhere will still respond noisily and with de-\nlay to Fed statements, but the noise will come from other sources, probably many\nother sources, instead of mainly from the Fed's own efforts to provide a filtered\nckowiak and Wiederholt (2005) for a model with local and aggregate signals related to\npricing decisions and responded to with capacity constraints.\nsignal. If there are enough other semi-public filterers of monetary news (TV, news-\npapers, investment clubs, lunchtable conversation), the signal processing noise in\nthem may partially cancel out at the aggregate level. But even if not, the Fed would\nno longer be itself responsible for generating unnecessary market fluctuations.\nThere are other, in my view even stronger, arguments for transparency in mon-\netary policy. But a rational inattention perspective does help us understand why\nit can be that economies where central banks publish detailed inflation reports do\nnot seem to have as much of a problem with overreaction to those reports as the\nFed does with overreaction to its single paragraphs.\nAbstract and single-agent models incorporating rational inattention are already\nproviding us with some useful insights. There is still a long and interesting road\nahead, though, before we can build models incorporating these insights that can\nbe matched to observed data, either at the macro or micro level.\nREFERENCES\nANGELETOS, G.-M., AND A. PAVAN (2004): \"Transparency of Information and Co-\nordination in Economies with Investment Complementarities,\" American Eco-\nHELLWIG, C. (2004): \"Heterogeneous Information and the Benefits of Trans-\nparency,\" Discussion paper, UCLA.\nLUCAS, ROBERT E., J. (1973): \"Some International Evidence on Output-Inflation\nLUO, Y. (2004): \"Consumption Dynamics, Asset Pricing, and Welfare Effects under\nInformation Processing Constraints,\" Discussion paper, Princeton University.\nMA \u00b4\nCKOWIAK, B., AND M. WIEDERHOLT (2005): \"Optimal Sticky Prices under Ra-\ntional Inattention,\" Discussion paper, Humboldt University Berlin.\nMONDRIA, J. (2005): \"Financial Contagion and Attention Allocation,\" Discussion\npaper, Princeton University.\nMORRIS, S., AND H. S. SHIN (2002): \"The Social Value of Public Information,\"\nMOSCARINI, G. (2004): \"Limited Information Capacity as a Source of Inertia,\" Jour-\nPENG, L. (2005): \"Learning with Information Capacity Constraints,\" Journal of Fi-\nPENG, L., AND W. XIONG (2005): \"Investor Attention, Overconfidence and Cate-\ngory Learning,\" Discussion paper, Princeton University.\nSIMS, C. A. (1998): \"Stickiness,\" Carnegie-rochester Conference Series On Public Pol-\n(2003): \"Implications of Rational Inattention,\" Journal of Monetary Econom-\nVAN NIEUWERBURGH, S., AND L. VELDKAMP (2004a): \"Information Acquisition\nand Portfolio Under-Diversication,\" Discussion paper, Stern School of Business,\n(2004b): \"Information Immobility and the Home Bias Puzzle,\" Discussion\npaper, Stern School of Business, NYU.\nWOODFORD, M. (2001): \"Imperfect Common Knowledge and the Ef-\nfects of Monetary Policy,\" Discussion paper, Princeton University,\nhttp://www.princeton.edu/~woodford/.\nE-mail address: sims@princeton.edu\nThe following Discussion Papers have been published since 2004:\nSeries 1: Economic Studies\n1 2004 Foreign Bank Entry into Emerging Economies:\nAn Empirical Assessment of the Determinants\nand Risks Predicated on German FDI Data Torsten Wezel\n2 2004 Does Co-Financing by Multilateral Development\nBanks Increase \"Risky\" Direct Investment in\nEmerging Markets? \u00ad\nEvidence for German Banking FDI Torsten Wezel\n3 2004 Policy Instrument Choice and Non-Coordinated Giovanni Lombardo\nMonetary Policy in Interdependent Economies Alan Sutherland\nin an Asymmetric Currency Area Giovanni Lombardo\n5 2004 FDI versus cross-border financial services: Claudia M. Buch\nThe globalisation of German banks Alexander Lipponer\n6 2004 Clustering or competition? The foreign Claudia M. Buch\ninvestment behaviour of German banks Alexander Lipponer\n7 2004 PPP: a Disaggregated View Christoph Fischer\n8 2004 A rental-equivalence index for owner-occupied Claudia Kurz\n9 2004 The Inventory Cycle of the German Economy Thomas A. Knetsch\nUsing Data from the Ifo Business Survey Thomas A. Knetsch\nin Germany J\u00f6rg D\u00f6pke\nto Germany \u00ad a Structural Factor Approach Sandra Eickmeier\n13 2004 Consumption Smoothing Across States and Time: George M.\nInternational Insurance vs. Foreign Loans von Furstenberg\nin Japan and its Usefulness for\nInflation Forecasting and Policymaking Koichiro Kamada\nCurrency Union in Case of Member Countries\nof Different Sizes and Output Persistence Rainer Frey\nEvidence from privately-held firms Alexander Ljungqvist\nA comparative analysis of the explanatory power\nof accounting and patent information for the Fred Ramb\nmarket values of German firms Markus Reitzig\n18 2004 The Economic Impact of Venture Capital Astrid Romain, Bruno\nvan Pottelsberghe\n19 2004 The Determinants of Venture Capital: Astrid Romain, Bruno\nAdditional Evidence van Pottelsberghe\nspeed of adaption: Are innovators special? Ulf von Kalckreuth\nTheory and results for Germany and other Michael Scharnagl\nOECD countries Karl-Heinz T\u00f6dter\n22 2004 Asset Prices in Taylor Rules: Specification, Pierre L. Siklos\nEstimation, and Policy Implications for the Thomas Werner\nECB Martin T. Bohl\nCycles: The Experience of Countries in L\u00facio Vinhas\nthe Baltics and Central Eastern Europe de Souza\nMonetary Policy and the Dynamics of\nthe Term Structure of Interest Rates Ralf Fendel\n25 2004 How the Bundesbank really conducted Christina Gerberding\nmonetary policy: An analysis based on Andreas Worms\nreal-time data Franz Seitz\n26 2004 Real-time Data for Norway: T. Bernhardsen, \u00d8. Eitrheim,\nChallenges for Monetary Policy A.S. Jore, \u00d8. R\u00f8island\nForecast Consumer Spending in Real Time? Dean Croushore\n28 2004 The use of real time information in Maritta Paloviita\nPhillips curve relationships for the euro area David Mayes\n29 2004 The reliability of Canadian output Jean-Philippe Cayen\ngap estimates Simon van Norden\n30 2004 Forecast quality and simple instrument rules - Heinz Gl\u00fcck\na real-time data approach Stefan P. Schleicher\nforward-looking monetary policy: Thomas J. Jordan\nThe Swiss case Carlos Lenz\nMarcel R. Savioz\n32 2004 Estimating Equilibrium Real Interest Rates Todd E. Clark\nin Real Time Sharon Kozicki\nEvidence from panel data analysis Karsten Ruth\nDevelopment to Asymmetric Growth of\nManufacturing Industries: George M.\nCommon Claims vs. Evidence for Poland von Furstenberg\nstochastic general equilibrium model Jana Kremer\n36 2004 Inflation and core money growth in the Manfred J.M. Neumann\neuro area Claus Greiber\n37 2004 Taylor rules for the euro area: the issue Dieter Gerdesmeier\nof real-time data Barbara Roffia\nEmpirical evidence on creative accounting J\u00fcrgen von Hagen\nwith fiscal rules in the EU Guntram B. Wolff\nin different financial systems Marcel Tyrell\n40 2004 Expected budget deficits and interest rate swap Kirsten Heppke-Falk\nspreads - Evidence for France, Germany and Italy Felix H\u00fcfner\nbased on autoregressions with a\nMarkov-switching intercept Malte Kn\u00fcppel\n1 2005 Financial constraints and capacity adjustment\nin the United Kingdom \u00ad Evidence from a Ulf von Kalckreuth\nlarge panel of survey data Emma Murphy\nfactors in the euro area analyzed in a\nlarge-scale factor model Sandra Eickmeier\n3 2005 Financial intermediaries, markets, F. Fecht, K. Huang,\nand growth A. Martin\nin Europe: does it fit or does it fail? Peter Tillmann\n5 2005 Taxes and the financial structure Fred Ramb\nof German inward FDI A. J. Weichenrieder\n6 2005 International diversification at home Fang Cai\nand abroad Francis E. Warnock\n7 2005 Multinational enterprises, international trade,\nand productivity growth: Firm-level evidence Wolfgang Keller\nfrom the United States Steven R. Yeaple\n8 2005 Location choice and employment S. O. Becker,\ndecisions: a comparison of German K. Ekholm, R. J\u00e4ckle,\nand Swedish multinationals M.-A. Muendler\nevidence from German sectoral data Alexander Lipponer\nand the degree of backward linkages Kamal Saggi\nstock market comovement Marco Del Negro\n12 2005 The determinants of intra-firm trade: in search Peter Egger\nfor export-import magnification effects Michael Pfaffermayr\nabsorptive capacity: evidence from quantile Sourafel Girma\nregressions Holger G\u00f6rg\n14 2005 Learning on the quick and cheap: gains James R. Markusen\nfrom trade through imported expertise Thomas F. Rutherford\nevidence from German treasury auctions J\u00f6rg Rocholl\n16 2005 Consumption, wealth and business cycles: B. Hamburg,\nwhy is Germany different? M. Hoffmann, J. Keller\n17 2005 Tax incentives and the location of FDI: Thiess Buettner\nevidence from a panel of German multinationals Martin Ruf\nEuro/Dollar Exchange Rate Karsten Ruth\nDeutschland mit Hilfe von Filterverfahren Stefan Stamfort\nEuropean economies with the euro area? Sandra Eickmeier\nEvidence from a structural factor model J\u00f6rg Breitung\n21 2005 Asymptotic distribution of linear unbiased J.-R. Kurz-Kim\nestimators in the presence of heavy-tailed S.T. Rachev\nstochastic regressors and residuals G. Samorodnitsky\nWelfare Costs of Nominal Rigidities over\nthe Business Cycle Matthias Pastian\n23 2005 The cross-sectional dynamics of German J. D\u00f6pke, M. Funke\nbusiness cycles: a bird's eye view S. Holly, S. Weber\n24 2005 Forecasting German GDP using alternative Christian Schumacher\nfactor models based on large datasets\nsetting? \u00ad micro-evidence from German\nmetal-working industries \u00ad Harald Stahl\nuncertainty Wolfgang Lemke\nJ. Hilscher, J. Szilagyi\n28 2005 Recursive robust estimation and control Lars Peter Hansen\nwithout commitment Thomas J. Sargent\n29 2005 Asset pricing implications of Pareto optimality N. R. Kocherlakota\nwith private information Luigi Pistaferri\n30 2005 Ultra high frequency volatility estimation Y. A\u00eft-Sahalia,\nwith dependent microstructure noise P. A. Mykland, L. Zhang\npreisbasis \u00ad Konzept und Konsequenzen f\u00fcr die\naktuelle Wirtschaftsanalyse sowie die \u00f6kono-\nmetrische Modellierung Karl-Heinz T\u00f6dter\nin the central and east European EU member\nstates \u00ad consequences for the enlargement of Sabine Herrmann\nthe euro erea Axel Jochem\neconomy within the euro area Ernest Pytlarczyk\n34 2005 Rational inattention: a research agenda Christopher A. Sims\nSeries 2: Banking and Financial Studies\n1 2004 Forecasting Credit Portfolio Risk A. Hamerle,\nT. Liebig, H. Scheule\nAn Empirical Analysis of US Corporate Klaus D\u00fcllmann\nCredit Exposures Monika Trapp\n3 2004 Does capital regulation matter for bank Frank Heid\nbehaviour? Evidence for German savings Daniel Porath\nbanks St\u00e9phanie Stolz\n4 2004 German bank lending during F. Heid, T. Nestmann,\nemerging market crises: B. Weder di Mauro,\nA bank level analysis N. von Westernhagen\n5 2004 How will Basel II affect bank lending to T. Liebig, D. Porath,\nemerging markets? An analysis based on B. Weder di Mauro,\nGerman bank level data M. Wedow\nGerman savings banks and credit cooperatives Daniel Porath\nand bank efficiency in Germany Michael Koetter\n2 2005 The supervisor's portfolio: the market price\nAnalysis and models for risk aggregation Carsten Wehn\n3 2005 Do banks diversify loan portfolios? Andreas Kamp\nA tentative answer based on individual Andreas Pfingsten\nbank loan portfolios Daniel Porath\n4 2005 Banks, markets, and efficiency F. Fecht, A. Martin\n5 2005 The forecast ability of risk-neutral densities Ben Craig\nof foreign exchange Joachim Keller\nrequirements Frank Heid\nbusiness cycle: evidence for German St\u00e9phanie Stolz\nsavings and cooperative banks Michael Wedow\nindustrial countries: driven by fundamentals\nor different treatment? Thorsten Nestmann\n9 2005 Accounting for distress in bank mergers M. Koetter, J. Bos, F. Heid\nC. Kool, J. Kolari, D. Porath\n10 2005 The eurosystem money market auctions: Nikolaus Bartzsch\na banking perspective Ben Craig, Falko Fecht\nrisk Hans Peter Gr\u00fcner\nVisiting researcher at the Deutsche Bundesbank\nThe Deutsche Bundesbank in Frankfurt is looking for a visiting researcher. Visitors should\nprepare a research project during their stay at the Bundesbank. Candidates must hold a\nPh D and be engaged in the field of either macroeconomics and monetary economics,\nfinancial markets or international economics. Proposed research projects should be from\nthese fields. The visiting term will be from 3 to 6 months. Salary is commensurate with\nexperience.\nApplicants are requested to send a CV, copies of recent papers, letters of reference and a\nproposal for a research project to:\nDeutsche Bundesbank\nPersonalabteilung\nWilhelm-Epstein-Str. 14\nGERMANY",
    "reduced_content": "Rational inattention: a research agenda\nChristopher A.Sims\n(Princeton University)\nDiscussion Paper\nSeries 1: Economic Studies\nDiscussion Papers represent the authors' personal opinions and do not necessarily reflect the views of the\nDeutsche Bundesbank or its staff.\nEditorial Board: Heinz Herrmann\nThilo Liebig\nKarl-Heinz T\u00f6dter\nDeutsche Bundesbank, Wilhelm-Epstein-Strasse 14, 60431 Frankfurt am Main,\nPlease address all orders in writing to: Deutsche Bundesbank,\nReproduction permitted only if source is stated.\nNon-technical summary\nImplications of rational inattention\nThe majority of macro models currently in use assume that the market players\nhave rational expectations and can process information immediately and without\nrestrictions. These assumptions are justified, first, in that they make the models\neasier to solve. Second, the Lucas critique negates certain forms of limited rational\nexpectations. Rational expectations and the unrestricted ability to process\ninformation also imply, however, that prices and market players' behaviour adapt\nquickly and without error to new information. Yet this does not match what is\ngenerally observed in reality. At the same time, numerous empirical studies show\nthat market players' expectations are not rational in many cases.\nRecently, theories have been developed that model imperfect information and\ndeviations from rational expectations or cost-inducing information procurement\nand processing and study their implications. This includes, for example, learning\nis a contribution to this corpus of literature and adopts another way of modelling\nimperfect information of market players. It assumes that the market players have\nlimited capacities to process information if shocks hit the economy and the market\nplayers receive and analyse signals. In this work the author makes use of the\ncoding theory developed by engineering science. This is based on the idea that\ninformation initially flows through a \"channel\" before it reaches the market players.\nDepending on the degree of capacity to process information, it may then be\nfraught with errors which, in turn, influence the decisions of the market players.\nThe advantage of this approach over the more psychological approaches in some\nof the recent literature is that it develops predictions about deviations from perfect\noptimizing behavior from a minimal set of assumptions, without reliance on\npossibly controversial psychological detail. Its advantage over approaches that\nsimply postulate \"measurement error\" in people's observations is that the form of\nobservation errors is predicted by the theory, from the structure of people's\noptimization problems.\nNichttechnische Zusammenfassung\nImplikationen rationaler Unaufmerksamkeit\nDie Mehrzahl der gegenw\u00e4rtigen Makromodelle nimmt an, dass die Marktteilnehmer\nrationale Erwartungen haben und Informationen sofort und ohne Einschr\u00e4nkungen\nverarbeiten k\u00f6nnen. Diese Annahmen werden zum einen dadurch gerechtfertigt, dass\nso die Modelle einfacher zu l\u00f6sen sind. Zum anderen verneint die Lucas Kritik\nbestimmte Formen begrenzter rationaler Erwartungen. Rationale Erwartungen und die\nunbegrenzte F\u00e4higkeit, Informationen zu verarbeiten, implizieren aber auch, dass\nMarktteilnehmer und Preise schnell und ohne Fehler neue Informationen verarbeiten.\nDas trifft in der Realit\u00e4t aber im allgemeinen nicht zu. Gleichzeitig zeigen viele\nempirische Untersuchungen, dass die Erwartungen der Marktteilnehmer oft nicht\nrational sind.\nIn neuerer Zeit sind Theorien entwickelt worden, die unvollst\u00e4ndige Informationen und\nAbweichungen von rationalen Erwartungen modellieren oder auch die Beschaffung\nkostentr\u00e4chtiger Informationen und die die sich daraus ergebenden Implikationen\nstudieren. Dazu geh\u00f6rt z. B. die Literatur \u00fcber das Lernen (siehe Sargent, 1993 und\nEvans und Honkapohja, 2001), die Literatur zur robusten Steuerung (siehe Hansen und\nSargent, 2001, Giannoni, 1999) und die Literatur zum \u00f6konomischen Verhalten (siehe\nLaibson, 1997, Benabou und Tirole, 2001). Dieses Papier ist ein Beitrag zu dieser\nLiteratur; es beschreitet aber einen anderen Weg, um unvollst\u00e4ndige Informationen von\nMarktteilnehmer abzubilden. Es nimmt an, dass die Marktteilnehmer begrenzte\nF\u00e4higkeiten haben, Informationen zu verarbeiten, wenn Schocks die Volkswirtschaft\ntreffen und die Marktteilnehmer Signale empfangen und verarbeiten. In dieser Arbeit\nverwendet der Verfasser die Kodierungstheorie der Ingenieurswissenschaften. Sie\nbasiert auf der Idee, dass Informationen zun\u00e4chst durch einen Kanal flie\u00dfen, bevor sie\nbeim Marktteilnehmer ankommen. Je nachdem welche Kapazit\u00e4t zur Verf\u00fcgung steht,\num Informationen zu verarbeiten, kann sie mit Fehlern belastet sein, die dann die\nEntscheidungen der Marktteilnehmer beeinflussen.\nDer Vorteil dieses Ansatzes gegen\u00fcber einigen mehr psychologischen Ans\u00e4tzen\nbesteht darin, dass er Voraussagen \u00fcber die Abweichungen vom perfekten\noptimierenden Verhalten macht und dabei mit einer geringen Anzahl von Annahmen\nauskommt und nicht auf psychologische Details angewiesen ist, die m\u00f6glicherweise\numstritten sind. Der Vorteil gegen\u00fcber der einfachen Annahme von Messfehlern in den\nBeobachtungen besteht darin, dass die Form des Beobachtungsfehlers von der Theorie\nerkl\u00e4rt werden und von der Struktur des Optimierungsproblems der Menschen\nabgeleitet werden kann."
}