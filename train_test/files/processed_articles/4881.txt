{
    "abstract": "Abstract\nWe address some of the epistemological challenges highlighted by the Critical Data Studies literature by reference to\nsome of the key debates in the philosophy of science concerning computational modeling and simulation. We provide a\nbrief overview of these debates focusing particularly on what Paul Humphreys calls epistemic opacity. We argue that\ndebates in Critical Data Studies and philosophy of science have neglected the problem of error management and error\ndetection. This is an especially important feature of the epistemology of Big Data. In ``Error'' section we explain the main\ncharacteristics of error detection and correction along with the relationship between error and path complexity in\nsoftware. In this section we provide an overview of conventional statistical methods for error detection and review their\nlimitations when faced with the high degree of conditionality inherent to modern software systems.\n",
    "reduced_content": "Original Research Article\nCan we trust Big Data? Applying\nphilosophy of science to software\nJohn Symons and Ramo\n\u00b4n Alvarado\n Keywords\nBig Data, epistemology, software, complexity, error, Critical Data Studies\nIntroduction\nThe surveillance and manipulation of individuals and\npopulations through computing technologies for com-\nmercial or policy purposes raises a range of difficult\nphilosophical questions. While the most pressing chal-\nlenges have an obvious ethical and political component,\nwe need to understand what levels of control and\ninsight so-called Big Data allows before we can make\ninformed decisions concerning its moral status. Thus, in\nthe paper we argue for a careful assessment of the epi-\nstemic status of the computational methods that are\ncurrently in use. These technologies are deployed in\npursuit of particular pragmatic ends in the service of\ncorporate and political missions. The actions of corpor-\nations and political entities can be evaluated independ-\nently of the technology that they deploy. However, the\nextent to which users of Big Data can accomplish their\ngoals depends on the epistemic status of those technol-\nogies.1 In many contexts, moral and epistemic ques-\ntions are inextricably intertwined, and our goal here is\nto help lay the necessary groundwork for moral and\npolitical engagement with Big Data by understanding\nas clearly as possible how the appearance of Big Data\nhas changed the epistemic landscape over the past two\ndecades. What can Big Data technologies allow users to\nknow, what are the limits of these technologies, and in\nwhat sense is Big Data a genuinely new phenomenon?\nAnswering these questions is essential for guiding our\nmoral and political responses to Big Data.\nPopular literature on Big Data is often dismissive of\nphilosophy of science and epistemology. Popular\nauthors and journalists frequently suggest that the rise\nof Big Data has made reflection on topics like causation,\nevidence, belief revision, and other theoretical notions\nirrelevant. On this view, the turn towards Big Data is a\nturn away from concern with a range of traditional\nquestions in the philosophy of science.2 Big Data,\naccording to some, ``represents a move away from\nalways trying to understand the deeper reasons behind\nhow the world works to simply learning about an asso-\nciation among phenomena and using that to get things\nDepartment of Philosophy, Lawrence, KS, USA\nCorresponding author:\nJohn Symons, Department of Philosophy, University of Kansas, 1445\nEmail: johnsymons@ku.edu\nBig Data & Society\nReprints and permissions:\nsagepub.com/journalsPermissions.nav\nbds.sagepub.com\nCreative Commons NonCommercial-NoDerivs CC-BY-NC-ND: This article is distributed under the terms of the Creative Com-\nmons Attribution-NonCommercial-NoDerivs 3.0 License (http://www.creativecommons.org/licenses/by-nc-nd/3.0/) which permits\nnon-commercial use, reproduction and distribution of the work as published without adaptation or alteration, without further permission provided the\noriginal work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nThis atheoretical turn makes the false assumption that\nmore data means better inquiry. Worse than merely\nbeing a superficial view of knowledge and inquiry, the\natheoretical stance is blithely uncritical towards the cor-\nporations and governments that use technology to ``get\nthings done''.3 The assumptions governing the atheore-\ntical turn are false and, as we shall see, studying Big Data\nwithout taking contemporary philosophy of science into\naccount is unwise (Fricke\nand risks involved in the use of computational methods\nin public policy, commercial, and scientific contexts only\nbecome evident once we understand the ways in which\nthese methods are fallible. Thus, in the broader social\nand political context, a precondition for understanding\nthe potential abuses that can result from the deployment\nof Big Data techniques by powerful institutions is a care-\nful account of the epistemic limits of computational\nmethods. A clear sense for the nature of error in these\nsystems is essential before we can decide how much trust\nwe should grant them and what, if any, limits to their use\nwe should impose.4\nComing to understand error and trust in these con-\ntexts involves a range of philosophical and social-\nscientific questions. No single scholarly or scientific dis-\ncipline has the resources to respond to the questions\nand challenges posed by the rise of Big Data. Critical\nData Studies is the interdisciplinary field that has begun\nto consolidate around the task of engaging with these\nquestions. Critical Data Studies has, understandably,\nfocused on the important political and social dimen-\nsions of Big Data. However, this work urgently requires\nattention to the assumptions governing the use of soft-\nware in the manipulation of data and in the conduct of\ninquiry more generally.\nWe will argue that critical attention to the formal\nfeatures of software is important if we are to get a\nproper understanding of the relationship between Big\nData and reliable inquiry. We are friendly critics of\nexisting work in Critical Data Studies: Our contention\nis that the field has neglected highly relevant recent\nwork in philosophy of science. Critical Data Studies\nhas correctly recognized that the technology underlying\nBig Data has changed the epistemic landscape in\nimportant ways, but has been unclear with respect to\nwhat these changes have been (Kitchin, 2014). Many of\nthese changes have taken place with the advent of com-\nputational methodology in general, but more specific-\nally with the integration of computer simulations into\nthe toolkit of ordinary scientific practice. Thus, part of\nour purpose is to connect debates in philosophy of sci-\nence concerning the status of computational models,\nsimulations, and methods with the emerging field of\nCritical Data Studies. To this end, we explain the role\nof epistemic opacity in computational modeling and\nclose with an example of a basic epistemological\nchallenge associated with any software intensive prac-\ntice, the problem of determining error distribution.\nAnother feature of software intensive science (SIS)\nthat philosophers have highlighted in recent years\nis the effect that errors in code can have for the reliability\nof systems. Horner and Symons (2014a), for example,\nexplained the role of software error in scientific contexts.\nAlthough primarily epistemic in nature, such con-\nsiderations have direct implications for policy, law,\nand ethics.\nAs several authors have noted, the term `Big Data'\ndoes not refer strictly to size but rather to a range of\ncomputational methods used to group and analyze data\none cannot responsibly address the epistemic status of\n`Big Data' without understanding the implications of\nthe use of software for inquiry. We are not arguing\nthat philosophers of science have simply solved all the\nepistemic problems related to Big Data. In fact, given\nthe central role of software in Big Data projects, trad-\nitional accounts of epistemic reliability drawn from\nphilosophy of science are likely to prove inadequate\nfor reasons we explain below.\nFor some philosophers, the increasingly dominant\nrole of computational methods is not a matter of sig-\nnificant philosophical interest. On this view, there are\nno novel, philosophically relevant problems associated\nwith the increased use of computational methods in\ninquiry (Frigg and Reiss, 2009). Others, like Eric\ndefended the view that computational modeling and\nsimulation are associated with distinctive and novel\nstrategies for inquiry. Another recent line of inquiry\nthat has direct bearing on Big Data involves the prob-\nlem of tackling error in large software systems. The\neffect that increasing software dependency has wrought\nwith respect to the trustworthiness of scientific investi-\ngation carries over directly to Big Data. Big Data is\npart of a changed landscape of problems associated\nwith the use of computational methods in scientific\ninquiry. While the term `Big Data' rarely figures in\nthe work of philosophers of science, there is now a\nlarge literature that discusses the role of software in\nscience, particularly insofar as it relates to modeling\nand simulation (see for example Frigg and Reiss,\n2010). Symons and Horner have pointed, for example,\nto what they call the path complexity catastrophe in SIS\nHorner, forthcoming). In this paper, we will argue\nthat the path complexity catastrophe will have conse-\nquences for Big Data projects. We will explain why Big\nData, as a paradigmatic instance of SIS is especially\nvulnerable to intractably difficult problems associated\nwith error in large software systems.\n2 Big Data & Society\nIn ``Introduction'' section we introduce the many\nattempts to define Big Data and explain their limita-\ntions. This section has multiple aims. We begin by pro-\nviding an overview of Big Data as currently practiced.\nGiven the diverse uses of the term `Big Data', in this\nsection we stipulate a working definition that is precise\nenough for our purposes and that faithfully reflects the\nmain features of current usage. The second aim of\n``Introduction'' section is to show the unavoidable con-\nnection between the methods used in Big Data and the\nsoftware dependence mentioned above. We conclude\nthat Big Data is an example of what Horner and\nSymons call Software-Intensive Science. As such, Big\nData epitomizes the kind of inquiry to which philo-\nsophical debates concerning the role of computers in\nscience should apply.\nIn ``Big Data meets Critical Data Studies'' section,\nwe do several things. First we provide an overview of\nrecent criticisms of Big Data that originate from the\nCritical Data Studies literature. We provide reasons\nto think that although they may be important to the\noverall characterization of Big Data, the tools deployed\nby this interdisciplinary field of study are excessively\nanthropocentric and social in their orientation and\nare the product of debates in philosophy of science\nand social epistemology that have been largely super-\nseded by the developments in recent decades. Notably,\nsince they are generally related to science as a whole,\nthe insights that derive from socially and historically\ntively little new light on the use of software in scientific,\ncorporate, and policy settings.\nThe best way to address some of the epistemological\nworries highlighted by the Critical Data Studies litera-\nture is to attend to debates in the philosophy of science\nconcerning computational modeling and simulation.\nWe provide a brief overview of the principal debates\nin ``The epistemic status of Big Data'' section. In par-\nticular, we focus on issues that relate to what Paul\nHumphreys (2009) calls epistemic opacity. ``The epi-\nstemic status of Big Data'' section concludes by\nnoting that the existing debate in both Critical Data\nStudies and philosophy of science has neglected the\nissue of error management and error detection. This\nis an especially important feature of the epistemology\nof Big Data. In ``Error'' section we explain the main\ncharacteristics of error detection and correction along\nwith the relationship between error and path complex-\nity in software. In this section we provide an overview\nof conventional statistical methods for error detection\nand review their limitations when faced with the high\ndegree of conditionality inherent to software systems\nused in Big Data. And finally, in ``Example'' section\nwe offer an overview of the limitations exhibited by\nGoogle's Google Flu Trends (GFT). In particular, we\nfocus on the ambiguity concerning the sources of such\nlimitations. These limitations, we argue, exemplify the\ndeficiencies of an atheoretical approach but most\nimportantly they also clearly characterize the intrinsic\nepistemic challenges posed by large software systems\nconventional methods of error detection, correction,\nand general assessment.\nWhat is Big Data?\nThe term `Big Data' arose in the context of challenges\nfacing engineers dealing with large data sets and limited\ncomputational resources. For example, as noted by Gill\nterm ``Big Data'' in their discussion of challenges invol-\nving the limitations due to memory storage constraints\nand processing speed for data visualization at the NASA\nAmes Research Center. That paper focused on data sets\nthat exceeded only 100 Gbytes. Attempts to partition\nthose data yielded segments that were too large for any\nresearcher to work with given the tools and techniques of\nthe time. Specifically, desktop computers available to\nindividual NASA engineers in the mid-1990s faced\nmemory and processing constraints that limited their\ncapacity to make good use of the data at their disposal.\nCox and Ellsworth (1997) call this ``the problem of Big\nData''. Contemporary usage of the term `Big Data' dif-\nfers in significant ways from this original context. It is\ncommon today for everyday data storage applications to\nreliably exceed 100 Gbytes. While there are significant\ntechnical challenges involved in managing large\namounts of data, ``the problem of Big Data'' as charac-\nterized in the 1990s is not the pressing concern it once\nwas.\nMost, if not all, early definitions focused on resource\nconstraints and data set size. This is not the case today.\nIn fact, as Boyd and Crawford (2012) note, many data\nsets considered to be paradigmatic in the Big Data lit-\nerature today are smaller than those used to coin the\nterm. They cite, for example, the small size of the data\nsets involved in analyzing Twitter trends when com-\npared to low-tech research into often very large-scale\ndata sets generated by the US Census Bureau from the\nNineteenth Century. So, although `Big Data' connotes\nthe use of large data sets, size is not an essential feature\nof current usage of the term.5\nOther definitions (e.g. Chen et al., 2014) focus on the\nway the different elements of a data set relate and inter-\nact. In some cases this is described in terms of the\ndynamic interaction of the 3V's: velocity, variety, and\nvolume. Whether a set is deemed to be a Big Data set\nhas to do with the dynamical constraints of these three\nfactors. Volume is of course size, but variety and vel-\nocity are less easy to define. Variety, for example has to\ndo with the kind of data in the sets (i.e. pixels vs. nodes)\nSymons and Alvarado 3\nwhile velocity has to do with the physical and temporal\nresources required to economically process a set.\nWhether these three factors are sufficient to define Big\nData is a topic of ongoing discussion. Some cite the\nextra V's of veracity, value, and visualization as neces-\nsary components of a working definition. However,\nregardless of the number of V's one includes, all of\nthe definitions agree that analytical tools and methods\nare a core component of the definition of Big Data\nA working definition\nIn this paper we adopt what we think is the most faith-\nful definition of what Big Data means in contemporary\npractice. Here, we follow the analysis provided by Chen\net al. (2014) about the uses of the term in commercial\ncontexts. They review the range of definitions of Big\nData given by leading corporations in data manage-\nment (for example, International Data Corporation\n(IDC), IBM, and Microsoft) before settling on IDC's\n2011 definition. They preface their choice of definition\nby stating that ``Big Data is not a `thing' but instead a\ndynamic/activity that crosses many IT borders.'' They\ncite an IDC report from 2011 defining Big Data as\nfollows:\n``Big Data technologies describe a new generation of\ntechnologies and architectures, designed to economic-\nally extract value from very large volumes of a wide\nvariety of data, by enabling high-velocity capture, dis-\ncovery, and/or analysis.'' (Gantz and Reinsel, 2011 as\nThis definition serves to highlight the most important\nand distinctive characteristics of Big Data, namely its\nuse of statistical methods and computational tools of\nanalysis. It will be particularly important to consider\nthis definition in detail in ``The epistemic status of Big\nData'' section. It is in this section that the epistemic\nstatus of Big Data is discussed and in which the case\nis made that Big Data, insofar as it is an intrinsically\ncomputer-based method of analysis deployed in\ninquiry, is a SIS par excellence. Thus, this definition\nis particularly apt since it clearly captures the interplay\nbetween the epistemic, normative, and economic\ndimensions of Big Data. Most importantly, this defin-\nition will highlight the limitations concerning error\nassessment characterized in ``Error'' section.\nBig Data meets Critical Data Studies\nThis section presents and clarifies what we take to be\nsome of the most significant critical studies of Big\nData.6 Although we agree with many of the observations\nmade in the existing literature, we think that the crit-\nical scholarship to date has fallen short of addressing\nthe distinctive epistemic features of Big Data. In part,\nthis is because most criticisms are focused on the\nsocial level of analysis rather than on any distinctive\nfeatures of the technology of Big Data per se. That is\nto say, the focus has been on limitations due to\nhuman-centered interactions such as inescapable cog-\nnitive and social biases and the overall value-ladenness\nof human inquiry. The basic conceptual point made in\nthe field of Big Data studies is that data must be\ninterpreted and that interpretation is subject to\nhuman bias. We agree that the processes by which\ndata is selected and interpreted are important topics\nof study. However, they are not unique to Big Data.\nThus, in this section the development of Critical Data\nStudies will be connected to its focus on the distinctive\ncharacteristics of Big Data rather than on consider-\nations that could be addressed to human inquiry in\ngeneral. In this spirit, and for the purpose of this\npaper, we focus on the analysis of error, error distri-\nbution assessment, testing and reliability, as they\nrelate to the computational methods employed by\nBig Data.\nError is an epistemic concept and the treatment of\nepistemic questions arising from Big Data is in its early\nstage. In a recent article in this journal, for example,\nRob Kitchin (2014) argues that there are three main\ntypes of account concerning the epistemic implications\nof Big Data. He contends that these derive from differ-\ning general perspectives on the nature of science held by\nscholars investigating Big Data. The three perspectives\nhe identifies are the paradigmatic, the empirical, and\nthe data-driven. Big Data theorists who follow a para-\ndigmatic--or Kuhnian model--of scientific inquiry\nsuggest that science normally functions within settled\npatterns and only occasionally advances via radical\nshifts in methodology. Advocates of this view contend\nthat the advent of Big Data constitutes a paradigm shift\nof the sort described by Kuhn (Kitchin, 2014). That is,\nthat Big Data has indeed revolutionized not only the\nmethods by which we conduct science but also the goals\nof scientific inquiry per se. The second camp is that of\nthe empiricist.7 The motto of this camp is ``the death of\ntheory'' (see Anderson, 2008; Cukier and Mayer-\nthe advent of Big Data and its capacity to detect pat-\nterns as replacing theoretical analysis with unrestricted\nsampling. On this view, raw data and correlation pat-\nterns are sufficient for scientific development. In this\ncamp, terms such as causation, paradigmatic of scien-\ntific inquiry for centuries past, even in their conven-\ntional use in science, are regarded as being elusive\nand possibly even occult. The third camp, the data-\ndriven one, is a hybrid of sorts in that ``it seeks to\n4 Big Data & Society\ngenerate hypotheses and insights `born from the data'\nrather than `born from theory''' (Kelling et al., 2009 as\ndata-driven science is one whose epistemological strat-\negy is to use ``guided knowledge techniques to identify\npotential questions (hypotheses) worthy of further\nexamination and testing.'' (Kitchin, 2014) This last\ncamp recognizes a role for conventional scientific\nterms and methods beyond mere pattern recognition,\nbut its hypotheses are derived from the data itself and\nnot ``just'' from guiding theoretical principles.\nKitchin (2014) criticizes the first two camps, focusing\nprimarily on claims made by those advocating the end\nof theory.8 According to Kitchin, the so-called empiri-\ncists have four main claims concerning the scope, reach,\nand assumptions of Big Data:9\n1. full resolution (N \u00bc All),\n2. no a priori (theory/model/hypothesis) needed,\n3. agnostic data, and\n4. domain transcendence (the assumption that unre-\nstricted pattern recognition does away with scientific\nspecialization).\nGiven that many problems involving Big Data tech-\nniques are of a dynamic nature, in real time and invol-\nving changing demarcations and inputs, the N \u00bc All\nThat is to say, in a constantly dynamic landscape, like\nthe ones often involved in Big Data problems, one can\nnever be said to have all the data. However, for Kitchin\nthe problem lies elsewhere. He thinks that the problem\nhas rather to do with sampling bias that originates in\nthe technology deployed, the collection methods, and\nthe data ontology employed in the process. In other\nwords, the problems with one above have to do with\nsubjective limitations and biases of the agents conduct-\ning the inquiry. This argumentative strategy is not\nunique to Kitchin. It can be found in other widely\ncited authors in the Critical Data Studies literature\n(see for e.g. Boyd and Crawford, 2012) for whom the\nnature of the problems themselves (i.e. dynamic, real-\ntime problem solving) is not recognized as a constraint\non the quest for full resolution. Instead, they argue that\nconstraints are due to the subjectivity inherent in the\nchoice of discretization and the highly value-laden\nsocial aspects of inquiry that inevitably come into play.\nSimilarly, `empiricist' assumptions 2 and 3 are\nrejected by Kitchin on the grounds that whatever meth-\nods allow us to collect and analyze data are already\ntheory/model-laden to begin with. He explains that\n``data are created within a complex assemblage that\nactively shapes its constitution'' and that ultimately,\nidentifying patterns in data ``does not occur in a\nscientific vacuum'' and is ``discursively framed'' by\ntheories, practitioners, and legacy methodology alike\nAs mentioned above, other Critical Data studies'\nauthors provide similar criticisms of Big Data. Take\nBoyd and Crawford (2012), for example. In their article\n(2012) they address the ``death of theory'' camp, or\n`empiricists', by questioning their implicit claims to\nobjectivity. They attack these claims because, according\nto them, they are ``necessarily made by subjects and are\nbased on subjective observations and choices.'' (Boyd\nand Crawford, 2012) They also criticize assumptions 1\nand 2 by pointing that massive amounts of raw data are\nmeaningless unless a question is posed, an experiment\nstandardized and a sample curated (2012). All of which\nare subjective endeavors. This is an insight drawn from\nhistorically and socially oriented philosophy of science.\nKuhn's work (1962) has been especially influential here,\nalong with the critical work of philosophers like\nWhile Kuhn, Longino, and other mid-to-late 20th\ncentury philosophers have helped shape the contribu-\ntions of many in the Critical Data Studies community,\nthe project of understanding Big Data can benefit from\ntaking advantage of additional philosophical resources.\nAcknowledging that human bias influences inquiry is a\nreasonable, but relatively trivial philosophical observa-\ntion.11 Since it is applicable to all forms of inquiry at all\nlevels (Longino, 1990), the recognition of bias is not a\ncontribution that adds anything distinctive to the study\nof Big Data.12 This is particularly the case considering\nthe developments computer technologies deployed in\nthe aid of science have undergone precisely in the last\n70 years. Unfortunately, the influence of relativistic\nphilosophy of science has impeded the development\nof analyses of the epistemic questions that arise in the\ncontext of Big Data.\nSimilarly, the emerging field of Software Studies,\nwhich attempts to develop critical perspectives on the\ndevelopment and use of software, often relies on philo-\nsophical literature that although interesting in its own\nright, is orthogonal to the core questions that arise\nfrom the use of software. This is particularly problem-\natic since some in the field of Software Studies want to\nargue that the use of computational methods, in par-\nticular their capacity to deal with immense data sets in\nscience and policy-making, does in fact bring about\nBerry, 2011). Take the following example. In his book\nThe Philosophy of Software, David Berry (2011) defines\nsoftware studies as a research field that includes discip-\nlines as broad as platform studies, media archaeology,\nand media theory, all of which focus on the develop-\nment, use, and historicity of hardware, operating sys-\ntems, and even gaming devices (Berry, 2011). Berry\nargues that these technologies not only offer novel\nSymons and Alvarado 5\ninsight into the human experience, but that they are\nalso a novel part of it. However, the philosophical\nresources that he applies to these issues are restricted\nto authors like Kuhn and Heidegger. While these are\ndeeply significant figures in the history of philosophy,\nthey offer limited insight into the novel epistemic fea-\ntures of computational methods such as Big Data.\nConsider another prominent figure in software stu-\ndies: Louise Amoore. She addresses security risks in\nways that are relevant to the discussion of Big Data.\nShe argues that modern security risks' calculations can\nbe understood by analogy with financial derivatives\n(Amoore, 2011). She offers an analysis of the implica-\ntions of Big Data in risk assessment in the context of\nborder security policy (Amoore, 2011). On her view,\nrisk posed by individuals can be understood as a prod-\nuct of correlational patterns that derive from assorted\ndata sets that include origin and destination of travel,\nmeal choice, etc. Security risk, according to her, is con-\nstrued as an emergent phenomenon, not reducible and\nfrequently not directly related to the components from\nwhich it arises. Financial derivatives, she argues arise in\nthe same manner (Amoore, 2011). What she means here\nis that derivatives are not mere aggregations of fluctu-\nation in market stocks or patterns in debt, but are\ninstead a financial instrument in their own right.\nBecause of the fragmentation and manipulation of\nvalues derived from more conventional financial instru-\nments, derivatives manage to have novel financial prop-\nerties that are specific to them. According to her, the\nsame can be said about risk assessment of individuals\ncrossing borders that emerge from risk-based security\ncalculations in contemporary security practice. The risk\ntravelers pose, although derivative of certain specific\nchoices and information about an individual, is often\nan independent feature that is not found in any of these\nchoices and informational sets but as a product of an\nemergent whole. Although Amoore is indeed talking\nabout the inherent features of Big Data systems here,\nlike those involved in border-crossing security systems,\nwe find that she relies heavily on an anthropocentric\ntreatment of risk that focuses on policy and decision-\nmaking rather than on the distinctive features of those\nsystems.13 Big Data systems also involve risks that are\ndue not only to the effects of design or policy choices,\nbut also from the nature of the software systems them-\nselves. While Amoore correctly points to the emergent\nfeatures of large complex systems as important areas of\ninquiry, we think that the most important epistemic\nproblems facing them are due to the characteristic fea-\ntures of software systems themselves and not mere con-\ntingent limitations on the part of agents.\nInsofar as Critical Data Studies understands itself to\nbe addressing a distinctive area of research, scholars in\nthis field ought to recognize that Big Data, at its heart,\ninvolves the use of computational methods. The two\nprincipal areas of philosophical inquiry that have\nbeen missing from Critical Data Studies to date are\ncontemporary philosophy of science and philosophy\nof computer science. Connecting these debates to phil-\nosophy of computer science is beyond the scope of the\npresent paper.14 Instead, for the remainder of this\npaper, we will demonstrate the relevance of more\nrecent and growing literature on software, models,\nand simulations, in the philosophy of science to ques-\ntions of reliability and error in Big Data.\nThe epistemic status of Big Data\nThe most distinctive aspect of Big Data, as we argued\nabove, is the prominence of computational methods\nand in particular the central role played by software.\nWhat are the novel epistemic challenges brought about\nthe use of computational methods? Although there is a\nbroad debate in philosophical literature about the epi-\nstemic implications of the `introduction of computers'\n2010), it is important to recognize, following the work\nof Evelyn Keller (2003) that this introduction took\nplace gradually in a series of distinguishable stages\nfrom the end of the Second World War until relatively\nrecently. Evelyn Keller (2003) argues that just as the\nintroduction of computers was itself a gradual process\nthat posed distinct challenges in distinct disciplines for\ndifferent reasons, the epistemic challenges emerged in\ndifferent disciplines at different times and at different\nstages of technological innovation.\nFox-Keller identifies three main stages. The first\nbegins with the use of computers to overcome the prob-\nlem of mathematically intractable equations in the con-\ntext of research at Los Alamos in the years immediately\nfollowing the Second World War.16 This stage repre-\nsents an important deviation from conventional analyt-\nical tools of the sciences at the time because it directly\nchallenges the well-established use of differential equa-\ntions as the main tool in the physical sciences (Keller,\n2003). However, when computers were being used at\nthis stage the primary concern was still to `simulate'\nconventional differential equations and their probable\nsolutions using Monte Carlo methods (Metropolis and\nUlam, 1949). In this respect the Monte Carlo methods\nare directed towards the solution of equations and are\nremoved in one step from the phenomena described by\nthose equations. In other words, methods such as the\nMonte Carlo method were not deployed to simulate\nany system, but rather to provide a wide range of pos-\nsible solutions to differential equations later deployed\nin order to understand a given system. With time, stat-\nistical approaches to problem solving (like Monte\n6 Big Data & Society\nCarlo) offered a practical alternative to the differential\nThe second stage, according to Fox-Keller, has to do\nwith the use of dynamic models as representations of a\ntarget system, or ``approximate analogous systems''\n(Frigg and Reiss, 2009). That is to say, the use of com-\nputerized calculations was confined ``to follow the\ndynamics of systems of idealized particles'' (Keller,\n2003). In this stage, scientists were no longer merely\nsimulating possible solutions to differential equations\nbut rather working under an assumed isomorphism\nbetween the observed behavior of a phenomenon and\nthe dynamics expressed by the artificial system, or com-\nputer model, constructed to track its idealized develop-\nment. In other words, the aim was to simulate ``an\nidealized version of the physical system.'' (2003) Fox-\nKeller identifies two levels to the use of simulations in\nthis second stage: (1) substitution of the natural for the\nartificial system, and (2) replacement of the differential\nequations at the first level for discrete, ``computation-\nally manageable'', processes.18 This second stage\nalready posed a challenge to the conventional epistemic\nrelation between theory construction and modeling.\nThat is, while the mathematical formulations of the\ndifferential equations had strong and direct ties to the-\noretical principles to back them up, the discretized ver-\nsions were now merely approximations without a direct\nlink to the underlying theory (Winsberg, 2010).\nNevertheless, what these simulations attempted to\nrepresent were entire theories and some would say\nthat it is only in this second sense that the proper use\nof the term `simulation' in its current usage enters the\ncomputational terminology (Hugues, 1999, as cited by\nFinally, the third stage, according to Fox-Keller, is a\nreliance on the analysis and model-building of particu-\nlar and localized systems rather than generalized theor-\netical ones. Foregoing the wide scope of a full\ntheoretical framework, this approach focused on the\nmodeling of internally consistent mechanisms without\ngeneralizable principles or wide ranging laws at their\ncore. As Keller (2003) notes, this change has important\nimplications for scientific explanation (see also Symons,\nfrom the first two in that it ``is employed to model\nphenomena which lack a theoretical underpinning in\nBig Data falls somewhere between first and second\nstage of Fox-Keller's taxonomy. Big Data, we will\nargue, is a software intensive enterprise that is focused\non revealing patterns that can be used for commercial,\npolitical, or scientific purposes.22 Unlike the third stage\napplications of computational models that Fox-Keller\ndescribes, applications of Big Data are intended to\nreveal features of natural or social systems. Big Data\nprojects are generally not detached from specific prac-\ntical applications, nor do they involve testing or\ndemonstrating new theoretical frameworks.23 Big\nData is a relatively conservative and pragmatically\nmotivated application of computational techniques,\nespecially when compared with examples of the third\npart of Fox-Keller's taxonomy.\nWhat is meant by calling Big Data software intensive\nis relatively straightforward. Computer scientists call a\nsystem software intensive if ``software contributes\nessential influences to the design, construction, deploy-\nment, and evolution of the system as a whole.'' (IEEE,\n2000) Given this definition, by almost any standard, Big\nData, like much of contemporary science, is software\nOne aspect of the heavy reliance on software by sci-\nentific or commercial enterprises is to say that the kinds\nof insights available via computational methods would\nnot be available without the use of software. Embedded\nin many of the definitions of Big Data is the assumption\nthat even just given the vast amount of information\ninvolved, no equation worked by paper and pencil\ncould in practice be deployed to deal with it (Bryant\net al., 2008). In other words, Big Data deals with prob-\nlems where insights would be practically impossible\nwithout the help of computers.\nBig Data can also address problems involving com-\nplex systems where the relevant dynamics are not obvi-\nously accessible except through surveying vast amounts\nof data (see Symons and Boschetti, 2013). In addition\nto those problems which would simply require raw\ncomputing power beyond our innate capacities there\nare also analytically intractable problems that require\nsimulation by computer rather than admitting of ana-\nlytic solutions.25 Big Data is generally not deployed\nbecause the problems in question are analytically\nintractable. However, as we shall see below, computa-\ntional models of the kind that are central to Big Data\nare of great interest precisely because they promise new\nways to explore phenomena that are difficult to exam-\nine by other means (Barberousse and Vorms, 2014;\nBoschetti et al., 2012). As Symons and Boschetti\n(2013) note, computational models are currently allow-\ning research into topics where cognitive, ethical, polit-\nical, or practical barriers would otherwise loom large.\nWhether in nuclear weapons testing, climate science,\nstudies of the behavior of epidemics, or studies of the\ninternal dynamics of stars, to take just a handful of\ncases, computational models are often the only viable\ncations of Big Data science to epidemics, energy usage,\nsocial movements, and the like all have the property of\ngenerating results that are otherwise inaccessible (at\nleast within any practical timescales and resource con-\nstraints) without the use of software.\nSymons and Alvarado 7\nAnother way of thinking about the intrinsic reliance\nof Big Data on software is to focus not only on its\nmethods but also on the nature of its results. These\nresults mainly involve pattern discovery. We analyze a\nset of granular data points in order to detect relational\nstructures. Consider twitter trends. Millions of short\ntexts are mined to find concurrent terms or combin-\nations thereof. These are in turn correlated to other\nfactors related to the authors, i.e. gender, geographic\nlocation, etc. Patterns emerge. But the way we arrive at\nsuch patterns is through the statistical analysis of cor-\nrelated data points. Whether these results are conveyed\nvia visualizations or mathematical formulas they are\nthe result of very large numbers of computations. As\ndiscussed above, even just considering the number of\navailable data points, these methods are computational\nand they are so as a matter of practical necessity.\nConsider attempting to understand what is going on\ninside a star like our Sun. We can know facts about the\ncenter of the Sun. We have indirect means of learning\nabout chemical composition through spectral analysis\nand the like, but other than that, the only ways to draw\ninferences about the processes taking place under the\nsurface of the sun are those made available to us via\ncomputational models. This applies almost by defin-\nition (Gantz and Reinsel, 2011) to phenomena con-\nsidered paradigmatic in the Big Data literature. This\nis because many of the insights brought about with\nBig Data techniques would otherwise be unavailable,\nor simply neglected by other analytical methods. Thus\nBig Data science is unavoidably software dependent.\nIn addition to being an intrinsically computational\nmethod, the value of Big Data derives from the patterns\nit extracts and the correlations revealed thereby.\nHowever, this means two things. First, tied to the\nnotion of pattern recognition and correlating millions\nof bits of data comes the need to visualize them. Such\npatterns and their insights would be of no use if they\nwere presented to us solely via a spreadsheet and a\nmathematical function for example. As we discussed\nabove the term Big Data was coined because of the\nchallenging constraints of memory and processing\npower but more particularly as they relate to visualiza-\ntion.26 Beyond the challenge of static visualization,\nmany problems in Big Data involve real time inputs\nand processing and as such we can say that Big Data\ndoes not just create a static representations but rather\ncreates artifacts that are more akin to scientific simula-\ntions. This is the case for example in the case of\nNumerical Weather Prediction systems which not\nonly process past data to predict future occurrences\nbut also compare the model's output to real time sen-\nsors tracking the weather (Bauer et al., 2015). It is in\nthis sense that the model ceases to be merely an\nexplanatory representation and becomes a simulation\n(Weisberg, 2013) whose key insights derive from the\ndynamic nature of the visualization (Bollier and\nEpistemic opacity\nAmong the most challenging philosophical problems\nfacing Big Data as a SIS is assessing its role in the\nprocess of creating, gathering, and uncovering new\ninsights and knowledge. The scientific status of Big\nData is a topic of ongoing debate. Lazer et al. (2014)\nhave argued that most prominent applications of Big\nData are not properly scientific insofar as the sources of\ndata are unreliable. Specifically, they argue, the data\nthat serve as the basis for Big Data projects are not\nderived from scientific instruments (Lazer et al., 2014).\nBy contrast, philosophers of science have debated\nwhether computer-based methods generate models\nthat are closer to theoretical abstractions or to empir-\nical experiments (Barberousse and Vorms, 2014;\nof computational methods in science Paul Humphreys\n(2009) argues that the central problem is the mediation\nof our epistemic access to the phenomena of interest.\nThis is because computational methods can involve an\nineliminable ``epistemic opacity'' (Barberousse and\ndefines in the following way:\n``A process is epistemically opaque relative to a cogni-\ntive agent X at time t just in case X does not know at t\nall of the epistemically relevant elements of the pro-\nEpistemic opacity, understood in this sense is not a new\nfeature of scientific inquiry, nor is it unique to compu-\ntational methods. Humphreys recognizes that a parallel\nissue arose with the emergence of Big Science, i.e. when\nscientific inquiry became an ineliminably social endea-\nvor in which no individual was in control of the com-\nplete process of inquiry (Humphreys, 2009; Longino,\n1990). However, Humphreys regards the computational\nturn in science as generating a qualitatively different\nform of epistemic opacity. Some of the problems stem\nfrom lower level operational issues such as the seman-\ntics of computational processes. In a relatively obvious\nsense, human-level computer languages are already\nhighly mediated with respect to machine-level imple-\nmentation. This results simply from being compiled\nthrough several syntactic layers in order for code to\nbe accessible to human programmers. Another example\nat a higher level are unavoidable numerical discret-\nization choices that enable higher-order representa-\ntional features such as visualizations (Humphreys,\n2009). According to Humphreys, both features of\n8 Big Data & Society\ncomputational techniques represent novel instances of\nepistemic opacity.\nOne concrete example of how the social nature of\nsoftware contributes to epistemic opacity in a novel\nway is the effect of so-called ``legacy code''. This is\nprogramming code that has been built by engineers\neither using programming languages that have fallen\nout of favor or that for some other reason may be dif-\nficult for later programmers to understand. Coding is a\nhighly creative engineering task and although the code\nmay do its job appropriately there may, occasionally be\nno way for contemporary users to know exactly how it\nachieves its function (2009). As a matter of fact, legacy\ncode is common in computer science. One could argue\nthat certain analogous legacy methods or processes are\npart of traditional big-science projects. However, unlike\nsay a scientific instrument whose inner workings are\nwell-understood, it may not be evident how some\npiece of legacy software contributes to the functional\nrole of the whole piece of software. One could easily\nimagine being able to reverse-engineer the functionality\nof non-software aspects of a scientific project if one\nknew its function. However, it is not always the case\nthat one can understand the function of legacy code in\nsome large system.\nWhen dealing with legacy code it may prove easier\nand more viable to merely work around the already\nfunctioning code, even if no one actually understands\nit.29 In big, ongoing projects it is often economically\nunfeasible to discard the legacy code and begin from\nscratch (Holzmann, 2015). This is particularly the case\nwith critically important systems whose operation\ncannot be interrupted like flight control software. In\nsuch cases the system must be kept running as it is\nbeing patched or updated.\nThere are other distinctive sources of epistemic opa-\ncity resulting from the use of computational methods\nthat have no parallel in other aspects of conventional\nscientific inquiry. Consider weak emergence. Weak\nemergence is characterized by the emergence of unin-\ntended/non-programmed/unexpected behavioral pat-\nterns in running simulations (Humphreys, 2009).\nPatterns that were not known before the simulation\nwas turned on and ran (for more on this see Symons,\nterized, among other things, by their dependence on the\nactual running of a simulation. That is to say, there\nwould be no way of having found those patterns\napart from running the simulation itself. They are the\nproduct of the actual dynamics of the simulation and\ncannot be deduced from nor reduced to any of the\nReliance on computational methods involves a dis-\ntinct kind of epistemic opacity from the social epistem-\nology aspects of human-centered inquiry where the\ncentral issue is the bias and subjectivity inherent in\ninterpretation. The consequences of this epistemic opa-\ncity are not easily solved through some simple fix or\nrevision of appropriate methods to deal with them.\nComputational methods, as Humphreys argues are\n``essentially epistemically opaque'' (Humphreys, 2009).\nA process is essentially opaque in this way to an agent\nat ``if it is impossible, given the nature of X, for X to\nknow all of the epistemically relevant elements of the\nThis last formulation of epistemic opacity serves to\nelucidate the kinds of epistemic challenges at play in\nour discussion, namely those that are features of the\nsystems in questions and not merely contingent limita-\ntions of individual researchers or of teams of research-\ners. As such, it also serves to distinguish the general\nconcept of epistemic opacity from a related issue con-\ncerning the concept of black boxes in systems ana-\nlysis.30 Black box theory is in principle a\nmathematical approach that allows for schematization\nof non-linear functions between an input and a result\nwithout the need to know exactly what the internal\nstructure of the function is or without particular\nregard to the nature of the input or results (Bunge,\n1963). It was later adopted by emerging fields in the\nstudy of complex systems (Ethiraj and Levinthal,\n2004) and business related issues concerning organiza-\ntional structures and product design (Brusoni and\nPrencipe, 2001). Although black boxes and epistemic\nopacity are related in that both are issues concerning\ngaps in knowledge of a given system, they are very dif-\nferent concepts. In particular, black box theory is more\nof a pragmatic approach to an information system that\ncan function in a need-to-know basis. That is, it is an\nattempt to schematize in a formal manner an informa-\ntion system with the minimum amount of information\npossible being transmitted from one state to the next\nand to do so despite possible limitations. Epistemic\nopacity on the other hand is concerned with more\nthan just the pragmatic constraints associated with spe-\ncific methods or technologies. It is about the nature of\nknowledge per se and in particular about the ways in\nwhich knowledge can be conveyed or can fail to be so.\nBlack boxes are just one of the many instances of epi-\nstemic opacity. In other words, all black box problems\nare instances of epistemic opacity but not every\ninstance of epistemic opacity is a black box. But more\nimportantly, not all black boxes are instances of essen-\ntially opaque processes.\nError\nHumphreys' argument that computational methods\nsuffer from epistemic opacity is strengthened when we\nconsider the role of software error (see also\nSymons and Alvarado 9\nof error in software intensive systems and explain why\ntraditional approaches to handling error in a scientific\ncontext fall short. As briefly stated above, by error we\nsimply mean the many ways in which a software system\nmay fail. This may include erroneous calculations,\nimplementations, results, etc. The important point\nhere is not error per se but our epistemic relation to it\nin the context of inquiry.\nScientific claims are often--if not always--of a stat-\nistical nature (Mayo and Spanos, 2010). Increasingly\nsophisticated manipulation, interpretation, and accu-\nmulation of data have made the probabilistic aspect\nof scientific claims become more pressing (see Keller,\nstatistical nature of contemporary science Deborah\nMayo has called for a new philosophy of statistical sci-\nence in order to account for error and probability\ninherent in modern scientific inquiry (Mayo and\nSpanos, 2010). Mayo proposes what she calls `severe\ntesting'. A method by which a given hypothesis is said\nto have various degrees of reliability depending on how\nlikely it is to have been falsified by a test. Unlike trad-\nitional accounts of confirmation, error-based statistical\nassessments such as Mayo's measure the ability to\nchoose from one hypothesis over another by virtue of\nthe extent of error-detecting testing methods applied to\nit. The degree to which these tests are able to detect\nerror determines their severity. A hypothesis that is\ntested with methods that have a high likelihood of find-\ning errors in it is said to pass a severe test. Severity is\nformally defined as follows:\nA hypothesis H passes a severe test T with data x0\nif\nagrees with H, and\n2. with very high probability, test T would have pro-\nduced a result that agrees less well with H than does\n, if H were false or incorrect.\nInformally, the severity principle suggests that a\nhigh degree of trust is warranted in cases where a\nhypothesis is not shown to be wrong in the face of\ntests that have a high probability of finding it wrong if\nthe hypotheses were indeed false (Parker, 2008).\nFurther, Mayo suggests that concentrating on choos-\ning among highly probed hypotheses is crucially dis-\ntinct from those approaches that rely on highly\nprobable ones. In the former case we have a stronger\npositive account for falsification.\nWendy Parker (2008) argues that Mayo's error-sta-\ntistical approach, and in particular her severity prin-\nciple can help make the case for the epistemic import\nof computer-based methodology in science. This is\nbecause, according to her, Mayo explicitly accepts\nsimulations as a method that helps scientist assess\nwhether some source of error is absent in an experiment\nby estimating ``what they would be more or less likely\nto observe if [any] source of error were present in the\nexperiment.'' (Parker, 2008) Thus, we can have severe\ntesting of hypotheses concerning possible sources for\nerror in a particular experiment. For now, this first\nstep allows Parker to make the case that computer-\nbased methods are a reliable source of evidence at\nleast with respect to sources of error in experiments\ngiven Mayo's account. When computer-based methods,\nsuch as simulations, are about a system that is not a\nconventional experiment and for which we have no\nreal-world access the same approach can be applied\naccording to Parker. Parker appeals to Mayo's account\nin the following way.\nSimulation results are good evidence for H to the\ndegree that:\n(i) results fit the hypothesis, and\n(ii) the simulation wouldn't have delivered results that\nfit the hypothesis if the hypothesis had been false\nFor Parker one task is to ensure that (ii) holds. If (ii)\nholds then we can apply Mayo's notion of evidence to\nsimulation experiments. This is even if such simulations\nare of the kind that cannot be immediately compared to\nactual data from a system, like those simulations that\nhave to do with future states of a system. An example\nof these simulations could be computer experiments\nseeking to predict future weather patterns (Parker,\n2008). According to Parker, appeal to lower level sever-\nity tests, as explained above, can ensure that (ii) is the\ncase. That is, by making sure that errors that could\nhave been part of the simulation are absent from the\nsimulation we can then say that simulations are good\nsources of evidence and thus we can rely on them.\nParker offers a taxonomy of error to help supplement\nher point. Although this taxonomy in itself may have its\nParker thinks that while it is unclear that there are in\nfact procedures that allow us to assess the magnitude of\nsome error's impact,33 the list nevertheless provides evi-\ndence that ``we do have some understanding of the dif-\nferent sources of error that can impact computer\nPath complexity and Big Data\nAs discussed above, Big Data is a software-intensive\nscience. Given this dependence on software, as we will\nsee below, testing applications of Big Data using con-\nventional statistical inference theory (CSIT) is not an\noption. The reason for this is primarily due to the role\n10 Big Data & Society\nof conditionality in software (Horner and Symons,\nThe challenge is that for every conditional statement\nin piece of code the number of possible paths that must\nbe tested grows. Pieces of code frequently contain con-\nditional statements or their equivalents, that is, they\ntake the form of ``if. . .then/or else'' statements. Thus,\nif a 10 line-long program has a conditional of this kind\nthe lines to be tested would doubled to 20. Each of these\nconditionals augments the lines of code to be tested\nexponentially. Each conditional line of code alters the\nnumber of paths available to a given program. This\nincreases the program's path complexity. Assessment\nof error distribution directly relates to degrees of reli-\nability when testing software. Standard statistical tech-\nniques demand some degree of random distribution in\nthe sample of interest. This element of random distri-\nbution is not available in the context of software test-\ning. While random distributions are a reasonable\nassumption in natural systems, this is not the case in\nsoftware systems since it is not feasible ahead of time to\nexclude the possibility that the distribution of error is\ncaused by a non-random element in its constitution.\nThus there is simply no way, other than by assumption\nor by exhaustive testing, to know whether or not a par-\nticular error distribution in software is the product of a\nrandom element or not (Symons and Horner, forth-\ncoming). Thus, there is no way, other than by mere\n(unwarranted) assumption, to legitimately deploy stat-\nistical techniques that demand that the error distribu-\ntion in a system have some degree of randomness to it.\nAs exemplified by the discussion on path complexity,\nbrute force attempts at exhaustive testing, as Symons\nand Horner argue, for any conventional program is an\nimpractical task given meaningful time constraints.\nEven the simplest computer programs have 1000\u00fe\nlines of code and an average of one conditional state-\nment per every 10 lines. Thus, for example, the time\nresources required for testing a program with 1000\nlines of code with this average of conditionals would\nexceed many-fold the age of the universe.35 A program\nconsisting of 1000 lines of code would be a very small\nprogram for anything in the Big Data context. Most\ncomputer programs used in these context are large\nand in scientific applications more generally are com-\nmonly in the hundreds of thousands of lines of code\nThe most important consequence of the path-\ncomplexity catastrophe is the fact that statistical meth-\nods no longer apply in a straightforward manner to the\ndetection of error in software system.\nIt may be countered that modularity in software sys-\ntems may be a way to diminish the impact of path com-\nplexity and thus reduce the epistemic opacity related to\nit.36 Perhaps, it can be argued, by breaking a system\ninto epistemically manageable modules we may indeed\nbe able to carefully test each and every one of them\nindependently and thus have a reliable error assessment\nof the system as a whole. If this is the case then, we can\nindependently rely on each of them and by extension on\nall of them together. At first sight this sounds like a\nplausible approach to the problem of path complexity\nin particular and epistemic opacity in general.\nHowever, path complexity grows at catastrophic rates\neven given relatively small numbers of lines of code.\nThe interplay between modules will introduce untested\npaths even in cases where the modules themselves are\nreliable. The discussion above about the obstacles to the\ndeployment of conventional statistical methods shows\nthat even at a smaller scale the only truly available test-\ning technique for assessment of error distribution would\nbe an exhaustive brute force one. Even if we were to\ngrant that massive modularity and exhaustive testing\nwas a viable method for software design and testing,\nintegrating modules will result in epistemic opacity.\nAlthough modularity may indeed make black boxes\na bit more manageable, the dynamics among the mod-\nules would quickly evolve into a particularly complex\nsystem with its own problems. One immediate concern\nis the assumption that software (and indeed any other\nmodular system) develops as a cohesive, all-encompass-\ning unifying endeavor rather than as a patchwork\nWhile unification and modularity can be part of a\nprotocol for future software development, it is not cur-\nrently in place and the question remains as to whether it\ncan be implemented in scientific inquiry and the large\nsoftware systems that already underlie it. Take climate\nmodeling for example. When considering climate\nmodels, Winsberg (2010) cites at least three kinds of\nuncertainty that have to be taken into consideration:\nstructural uncertainty, parameter uncertainty, and\ndata uncertainty. The most important source of uncer-\ntainty for our current discussion is structural uncer-\ntainty of the model itself, which includes\nconsiderations regarding ``a plethora of auxiliary\nassumptions, approximations, and parameterizations\nall of which contribute to a degree of uncertainty\nabout the predictions of these models.'' (2010) Each\nof these assumptions, approximations, and parameter-\nizations is based upon segments, or modules of soft-\nware code that implement them. Let us for a second,\nin a very simplistic and rough way, think of each of the\nmany modeling layers that go into the software that\npredicts climate as modules. Even if we exhaustively\nspecify/test each module, the interactions among mod-\nules, their epistemic transparency, and therefore their\nreliability as a functioning system them won't be as\nstraightforward. Consider, for example, that after\n70\u00fe years of climate modeling the complexity\nSymons and Alvarado 11\nsurrounding the integration of so many different (one\nmay argue modular) systems/models has only allowed\nscientist to claim a degree of accuracy that averages\nmerely a day per decade (Bauer et al., 2015). That is,\nafter seven decades, and the use of the most sophisti-\ncated and powerful software, the integration of the\nmultiple modules of climate modeling is anything but\ndone. If anything this example elucidates the difficulty\nof managing integration of large complex simulations\nsystems. Furthermore, it exemplifies how modularity\nmay not even be an option in scientific practice.\nExample\nThere has been a recent trend in the past decade or so to\nuse the vast amount of data generated by internet\nsearches in attempts to create predictive models. These\nmodels range from prediction of American Idol winners\n(Ciulla et al. 2012), political election outcomes,\nunemployment rates, box-office receipts for movies,\nand song positions in charts (Goel et al., 2010). But per-\nhaps the best known among these attempts has been the\nflu tracker function: GFT. Researchers at Google\nexpected the data from accumulated queries to yield cor-\nrelational patterns that, all by themselves, would tell a\nstory about the presence and spread of the disease (Lazer\nthe spirit of so-called empiricist interpretation of Big\nData, discussed above. However, the researchers'\nhopes did not materialize. Although some correlations\nwere discovered, Google's flu tracker continued to con-\nsistently generate spurious correlations and, more ser-\niously, reporting false flu numbers (Lazer et al., 2014;\nGFT was designed to predict, in real time, the\nadvent of a flu epidemic. The innovative aspect of this\ntracker was its reliance on the relatively loose search\nqueries typed into Google's search engine. These data,\nthey hoped, could serve as the basis for predictions\nconcerning the behavior of the epidemic (Cukier and\nMayer-Schoenberger, 2013). The core idea behind this\nproject was to provide an alternative to conventional\nepidemiological surveillance and prediction systems\nwhich relied on medical reports of Influenza-like ill-\nnesses (ILI's) from regional clinics to the Centers for\nDisease Control and Prevention (CDC's). In particular\nit hoped to foresee an epidemic outbreak from search\nqueries that would indicate a strong presence of flu-like\nsymptoms based on specific flu-related words and com-\nbinations of these words typed into the search engine.\nThis, they argued, could be done if not in real time, at\nleast faster than reports from patients seeking care at\nlocal clinics, which could take a number of days.\nHowever, after its launch as an open tool for flu sur-\nveillance in 2008 there were two seriously embarrassing\nmoments for GTF. One of them was the fact that it\nThis led Google to actually modify its original algo-\nrithm in an attempt to get more accurate results.\nHowever, the second problem was that GTF suffered\nfrom general gross overestimation. In particular, it\net al., 2014) and it greatly overreported flu cases\nIt is by now well known that Google's flu tracker\nfailed to achieve what it was designed to do, namely\npredict and report ILI's better and faster than the con-\nventional surveillance tools available. It simply didn't\npredict at all or predicted erroneously. Because of this,\nthe project is often taken to exemplify ``Big Data\nhubris'' (Lazer et al., 2014), the often underlying\nassumption that large amounts of data and the patterns\nthat are discovered through its analysis can yield results\nindependently from or without the aid of principled\ntheoretical underpinnings.\nAlthough the disappointing errors of GFT have been\nrigorously documented and measured (Olson et al., 2013;\nSalzberg, 2014; see the supplemental material in Lazer\net al., 2014) what is most interesting to our discussion is\nthe ambiguity regarding their nature and source. Many\nof these studies focus particularly on the margin of error\nbut are not clear about what caused the errors. Some\nresearchers (Cook et al., 2011) for example, ascribe the\nerrors to issues like seasonality, the fact that outbreaks\nhappened outside of what is commonly thought to be flu-\nseason. This meant that common flu-related terms were\nless likely to have been used in queries to the search\nengine.37 Others ascribe the errors to differences of age\ndistribution and geographical heterogeneity occurring\nduring model fitting periods of GFT (Olson et al., 2013).\nLazer et al. (2014) offer two possible culprits. The\nfirst is due to neglect of traditional statistical tech-\nniques. Some of the error here can be fixed once GFT\nincorporates conventional statistical methods that can\nprovide correlational filters. These methods inform\nmodern pattern finding techniques in traditional\nresearch beyond Big Data (Lazer et al., 2014). If con-\nventional statistical methods were deployed along with\nthe GFT a reflective equilibrium between data from\nILI's surveillance and search terms could better cali-\nbrate GFT. Given the results presented by Horner\nand Symons the suggestion to take statistics seriously,\nwhile generally sensible, might have additional compli-\ncations that are beyond the scope of this paper.\nLazer et al. (2014) suggest that another possible\ncause for the errors in GFT is what they call algorithm\ndynamics undergone by Google's search algorithm.\nAlgorithm dynamics, according to them, are\n12 Big Data & Society\nmodifications to Google's search algorithms that are\nintroduced in order to enhance the functionality of\nthe search engine. They are of two kinds: blue team\ndynamics, those that the service provider deploys for\ngreater efficiency and usefulness of search results; and\nred team dynamics, those done by users of the service\nfor personal benefit such as prominence and visibility.\nAccording to them, blue team dynamics are what is\nmost likely behind GFT's errors. The evidence that\nthey cite is a correlation between reported changes to\nthe algorithm and the surge of predictive errors in\nGFT. According to the authors, what makes the\nsystem yield errors is the way in which search results\nskew the queries themselves, queries which are in turn\nused to extract the terms for the GFT to analyze. That\nis, the search results of Google's search engine influence\nthe prominence of search terms that users input and\nthese skewed inputs then are used by GFT to predict\nthe presence or absence of the flu (Lazer et al., 2014).\nThat results generated by the search engine can modify\nqueries and input from the very source that is supposed to\nbe furnishing the data for prediction is troublesome\nenough. However, there is something deeper going on.\nAlthough Lazer et al. (2014) define the blue team algo-\nrithm dynamics undergone by Google's search algorithms\nin the context of Google's particular business model, one\ncan extend the term beyond Google or GFT. Other social\nmedia platforms engage in algorithm dynamics too, in\nparticular blue team dynamics (Lazer et al., 2014). In\nfact, algorithm dynamics, insofar as they are defined as\nthe changes made to any software product by those\ndesigning it, affect all aspects of software production\nand development. These modifications include model fit-\nness processes and functional additions to the underlying\nsoftware that are necessary to its proper functioning.\nThus, algorithm dynamics are an essential feature of the\nkind of artifact that software is (Holzmann, 2015) and not\nmerely a product of arbitrary human intervention.\nGiven the extent and scope of these dynamics in Big\nData more generally, we have a bigger issue on our hands\nthan merely biased data gathering. In particular the\nissues discussed in ``The epistemic status of Big Data'',\n``Error'', and ``Path complexity and Big Data'' sections:\nepistemic opacity due to sheer volume of people, number\nof processes, legacy code, and path complexity catastro-\nphe given the number of lines of code involved in projects\nof such magnitude are at play. But the challenge is not\nmerely related to product development and modifica-\ntion. The epistemically relevant feature of this cycle of\nupdating software results from our inability to test for\nerror and our dependence on systems that are susceptible\nto it. In other words, this is an issue of knowledge acqui-\nsition, reliability, and, therefore, trust.\nThe software behind GTF and similar Big Data pro-\njects falls prey to the path complexity catastrophe as\ndescribed by Symons and Horner. Whatever efforts we\nintroduce to mitigate error in these systems will be under-\nmined by the fact that they incorporate a vast number of\nindividual machines and computational methods to yield\neven the simplest of results. And as discussed above, even\nif we characterize the problem in terms of modules, the\nprocess is highly unlikely to become less opaque.\nDiscussion\nIssues of path complexity and epistemic opacity are\nmore than merely abstract theoretical preoccupations.\nAs stated in the introduction to this article, some of the\nlimitations and risks involved in the use of computa-\ntional methods in public policy, commercial, and scien-\ntific contexts only become evident once we understand\nthe ways in which these methods are susceptible to\nerror. In the broader social and political context, a pre-\ncondition for understanding the potential abuses that\ncan result from the deployment of Big Data techniques\nby powerful institutions is a careful account of the epi-\nstemic limits of computational methods. A clear sense\nfor the nature of error in these systems is essential\nbefore we can decide how powerful they should\nbecome and how much trust we should grant them\n(see for example Paparrizos et al., 2016). By way of\nillustration, we have focused our attention on the limi-\ntations of GFT as a predictive tool that can be a sup-\nplement to ILI's surveillance. The consequences of\noverestimation in this context are not as immediately\ntroubling as the consequences for other systems that are\nin use in governmental and military contexts. For\nexample, if we relate our discussion to Software\nStudies research, such as that of Louise Amoore for\nexample, we can see the immediately troublesome\nimplications that a conventional account of epistemic\ntrust on Big Data systems could have. In her research\n(Amoore, 2011), Big Data systems are in charge of cal-\nculating and assessing the security risks posed by indi-\nviduals flying from one part of the world to another.\nWithout having a proper understanding of the nature\nof error inherent to these systems, assessing whether\nthey are flagging the right people, or the right number\nof people becomes ever more challenging.\nDebates concerning the epistemic status of Big Data\nin the Critical Data Studies literature must take account\nof the nature of error in software intensive contexts. We\nhave shown that an account of error management and\nreliability can be profitably introduced into the agenda\nof Critical Data Studies. Symons and Horner's concept\nof path complexity for example, highlights the limita-\ntions of testing given intrinsic features of software. The\nproblem of reliability and the changing character of trust\nin the context of Big Data projects pose an ongoing\nchallenge for Critical Data Studies.\nSymons and Alvarado 13\n"
}