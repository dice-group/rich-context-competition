{
    "abstract": "Abstract\nThis article analyses the rise of software systems in education governance, focusing on digital\nmethods in the collection, calculation and circulation of educational data. It examines how\nsoftware-mediated methods intervene in the ways educational institutions and actors are seen,\nknown and acted upon through an analysis of the methodological complex of Pearson Education's\nLearning Curve data-bank and its Center for Digital Data, Analytics and Adaptive Learning. This\ncalls for critical attention to the `social life' of its methods in terms of their historical, technical\nand methodological provenance; their affordances to generate data for circulation within the\ninstitutional circuitry of Pearson and to its wider social networks; their capacity to configure\nresearch users' interpretations; and their generativity to produce the knowledge to influence\neducation policy decisions and pedagogic practices. The purpose of the article is to critically survey\nthe digital methods being mobilized by Pearson to generate educational data, and to examine\nhow its methodological complex acts to produce a new data-based knowledge infrastructure for\neducation. The consequence of this shift to data-based forms of digital education governance by\nPearson is a challenge to the legitimacy of the social sciences in the theorization and understanding\nof learning, and its displacement to the authority of the data sciences.\n",
    "reduced_content": "European Educational Research Journal\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\neerj.sagepub.com\nDigital methodologies of education\ngovernance: Pearson plc and the\nremediation of methods\nBen Williamson\nSchool of Education, University of Stirling, UK\n Keywords\nData, data science, digital methods, methodology, Pearson, soft governance, software\nEmerging digital methods of data collection, calculation and communication are intervening in\nhow educational institutions and actors are seen, known and acted upon. This article provides an\nanalysis of the methodologies of Pearson plc's Learning Curve data-bank and its Center for Digital\nData, Analytics and Adaptive Learning. Pearson's mobilization of digital methods exemplifies a\nCorresponding author:\nBen Williamson, School of Education, University of Stirling, Pathfoot, Stirling FK9 4DS, UK.\nEmail: ben.williamson@stir.ac.uk\nArticle\nshift towards more software-based, computer-coded and algorithmically mediated techniques of\neducational governance, and is integral to the production and performativity of a powerful educa-\ntional data infrastructure. Pearson is examined as an important actor with the relevant digital meth-\nods of software-mediated statistical data collection, analysis, visualization and interactivity to\nmake contemporary education legible, intelligible, and therefore actionable through highly tar-\ngeted intervention. Its Learning Curve enables this through providing statistical indices and data\nvisualizations representing the global comparison of national education systems, while its Digital\nData, Analytics and Adaptive Learning centre focuses on mining patterns from individual learners'\ndata using advanced data analytics software in order to derive new insights into the learning pro-\ncess itself and then design new e-learning products based on those insights.\nThese are significant methodological accomplishments, and this article interrogates the `social\nlife of methods' (Savage, 2013) deployed by Pearson--the provenance of such methods, and the\nways they are inscribed with particular values and assumptions that then shape the insights they\ngenerate. The consequence of this shift to data-based methods of `digital education governance'\n(Williamson, 2015) by Pearson is a displacement of the legitimacy of the social sciences in the\ntheorization and understanding of learning to the authority of the data sciences. The `psy complex'\nof the psychological sciences (Rose, 1999a) dominated 20th-century attempts to model and clas-\nsify the learning process by implanting the gaze of `psychological eyes' in education (Popkewitz,\n2012). For Pearson it is the computational affordances of data science in combination with learning\nscience methods and theories that promise to produce the classifications and models by which\nlearning is to be understood and acted upon in the data scientific 21st century. The combination of\nthe methods of learning sciences with computer-based forms of data science is embryonic of an\nevolving `CompPsy' complex that hybridizes theories, concepts and practices from the computer\nsciences (CompSci) with those of the psy-sciences (Loveless and Williamson, 2013). The central\naim of the article is to trace the consequences emerging from the digital methods that Pearson is\nemploying in the classification and modelling of learning for the ways in which education might\nbe governed.\nSocializing methods\nAcross the social sciences, the production of social and human knowledge and theory is being\naffected by digital data analyses enacted through software (Kitchin, 2014). The emergence of big\ndata software and its algorithmic techniques of analysis is challenging conventional views about\nthe institutional practices and spaces of knowledge production (Burrows and Savage, 2014).\nInstead of social scientists, it is claimed, the new experts of the social world are `algorithmists'--\nexperts in computer science, mathematics and statistics, as well as policy, law, economics and\nsocial research--who can undertake big data analyses across commercial, political and scholarly\nsites (Mayer-Schonberger and Cukier, 2013). While the `everyday user performs their social life\nvia Facebook, Twitter et al.', these experts apply their `methodological techniques for spotting the\nThe emergence of `digital sociology' and `digital social research' reflects disciplinary anxieties\nabout the relevance of social science at a time when commercial social media companies, research\nand development (R&D) labs and think tanks are staking their claim to methodological expertise\n(Lupton, 2015). `Digital methods' that involve the use of digital devices to `perform a cultural and\nsocial diagnostics' (Rogers, 2013: 3) have the capacity to detect patterns in huge quantities of data\nand to augment how people and societies see and know themselves. As a consequence, there has\nbeen a `redistribution of social research'between human actors--researchers, software developers,\ndata analysts, commercial R&D labs and social media users--but also to non-human actors\nincluding databases, software, algorithms, platforms and other digital devices (Marres, 2012). The\nredistribution of research also entails a remediation of methods as social research is refashioned\nthrough digital data-processing technologies. The emergence of a new field of `educational data\nscience' (Piety et al., 2014) reflects how digital methodologies have been distributed into educa-\ntional research too.\nRecent research on `the social life of methods' has engaged with the plethora of digitally reme-\ndiated methods for performing social scientific research. By social life of methods, what is meant\nis a critical engagement with methodological devices that resists framing them simply as technical\ntools, but makes their affordances and capacities into the object of social scientific inquiry (Savage,\n2013). Firstly, methods are social because they are shaped by the social, cultural, economic and\npolitical circumstances in which they have been produced and of which they are a part (Law et al.,\n2011). Methods are designed for particular purposes, through the work of advocates, as devices for\nexamining, seeing, knowing and interpreting the social world. Secondly, methods are also social\nbecause they in turn help to shape that social world. The discoveries made by social scientific\nresearch conducted through digital devices, then, are not objective facsimiles of an existing world,\nbut are consequential for `social scientific ways of knowing' (Ruppert et al., 2013: 24). As part of\nthe material of contemporary ways of life, digital devices are fundamentally reconfiguring the\nways in which social science can be performed, and the kinds of analyses, interpretations and\ninsights into social worlds made possible. Digital data are therefore socially, technically and meth-\nodologically enacted `data practices' with their own social lives, and the generation of data is\ntherefore generative of particular effects and social implications; data are consequential to `what is\nknown,' and can influence decision-making and other activities (Ruppert et al., 2015). The notion\nof `socializing methods'registers this double process of socially enacting methods, and of mobiliz-\ning methods to make sense of the social world and wrap new social norms around it.\nDrawing on ideas about remediating and socializing methods as an analytical framework, I\nargue Pearson has become a significant methodological gatekeeper in the mobilization of digital\nmethods in education, thus in defining how and what can be seen and known about it, and what\nconsequently might be done to govern it. Methodological expertise in data science is increasingly\nbeing redistributed to organizations such as Pearson, and remediated through their digital methods,\nrather than enacted within either the methodological and theoretical apparatuses of research depart-\nments in universities or the data analysis agencies associated with government departments.\nPearson's current efforts around digital methods represent the embedding of a new form of classi-\nfication in the knowledge infrastructure of education. Whereas the process of learning was,\nthroughout the 20th century, largely the expert preserve of the psychological sciences, which trans-\nlated `the discourse of science to the imperatives and techniques of practice' (Rose, 1999a: 201)\nand acted as a relay between psychological models of development and the practices of the school,\nfor Pearson the process of learning is now to be mapped and known through the data sciences,\nalbeit twinned with learning science insights from cognitive and developmental psychology, neu-\nroscience and behavioural science. Pearson's hybrid science of data and learning, and the methods\nthat enact it, are consequential to ways of conceptualizing learning processes, measuring learner\nprogression and developing pedagogic products and practices. Through its methodological com-\nplex of psychological and data scientific ways of knowing and intervening in learning processes,\nPearson is seeking to derive new classifications and standard definitions of learning itself that can\nthen be relayed into practices by being coded into the e-learning software products that it inserts\ninto the pedagogic routines of the classroom.\nAs Bowker and Star (1999: 314) have argued, classifications and categories `touch people in a\nvariety of ways--they are assigned, they become self-chosen labels, they may be statistical arte-\nfacts'. The new digital methods and data practices enacted by Pearson are becoming active in the\nproduction of classificatory categories in relation to learning that have the potential to touch peo-\nple's lives by rendering new models and understandings of what constitutes learning itself. Pearson\nis seeking to embed such classifications in its recommendations for a new form of educational\ngovernance that focuses on personalizing the learning process--a task to be enacted by new data-\nproducing and -processing devices embedded in the pedagogic structure of the school--rather than\nsolely through the bureaucracy of education systems. In this sense, Pearson's digital methods are\nkey techniques in the generation of a data-derived classification system for learning, and are con-\nsequential to the production of new knowledge in relation to the ways that learning is known and\nlearners themselves are made amenable to being acted upon pedagogically. This reflects a struc-\ntural shift in the system of education governance from centralized bureaucracies to non-state and\nnon-public sector organizations (including commercial companies), and a discursive shift from\neducation to learning (Ozga et al., 2011). It is also part of a shift to focus on the subjectification of\nindividuals through diverse practices and `technologies of schooling' that are intended to shape\nThe soft(ware) governing techniques of Pearson plc\nPearson plc is the world's largest educational publisher. Originally established in 1844, it\nemployees (https://www.pearson.com/about-us.html). Following a re-structuring and re-branding\nexercise in 2014, represented by its strapline `always learning' and its goal `to help people make\nmeasurable progress in their lives through learning', Pearson has significantly broadened its field\nof operations to include major digital platforms for online publishing, testing and assessment,\ndata analysis and digital research, and has established an `affordable learning fund', a free-enter-\nprise model of low-cost private schools for low-income countries (Ball and Junemann, 2015). In\n2014 it also successfully tendered to provide the frameworks for the Organization for Economic\nCooperation and Development (OECD) Programme for International Student Assessment (PISA)\ntests scheduled for 2018; the frameworks define what will be measured in the test instruments,\nhow this will be reported and which approach will be chosen for the development of tests and\nPearson has therefore not only transformed itself from `a media holding company to an edu-\nbusiness', but also positioned itself as a `legitimate policy actor' and a `morally authoritative\nagency in educational matters' (Hogan et al., 2015: 49). It has also committed to measuring the\nlearning outcomes of its products and services in order to enable the company to demonstrate the\nextent to which any Pearson product has a measurable impact on improving the user's life through\nlearning. As part of this, it has established an `Open Ideas' database of reports `to help make the\nbest evidence and ideas about learning accessible to all, and to encourage open debate about what\nworks in education' (https://research.pearson.com/). In 2015 it launched two reports, one entitled\n`What Works in Education' and the other `What Doesn't Work in Education' (http://blog.pearson.\ncom/what-works-in-education-a-tough-love-message-from-john-hattie/), and it has also estab-\nlished an Efficacy Framework, a `tool that uses a tried and tested method to help understand how\nproducts or services can achieve their intended outcomes or results' (http://efficacy.pearson.com/\nefficacy-tool/). As a global `edu-business' with links to government, commercial and multilateral\nagencies, Pearson has become a `serious policy player' that can both define problems and solve\nthem but often `goes unnoticed in education policy analysis' (Ball, 2012: 128).\nIn this context, I analyse the role of two of Pearson's recent developments in identifying both\npolicy problems and solutions, focusing especially on its digitally mediated data-processing meth-\nodologies. These are the Learning Curve, a global databank and source of analysis on education\nlaunched in 2012 (http://thelearningcurve.pearson.com/), and its Center for Digital Data, Analytics\nand Adaptive Learning (CDDAAL), a R&D centre dedicated to the analysis and use of digital data\nfor educational improvement also established in 2012 as one of five centres in Pearson's Research\nand Innovation Network (http://researchnetwork.pearson.com/digital-data-analytics-and-adaptive-\nlearning). In combination, and supported by other Pearson documents, these resources emphasize\nPearson's transformation into a company that develops digital learning resources for use in schools,\nthe data-processing technologies and methods for analysing the data produced by them, and also\nthe data analytics and visualization tools required to measure and monitor the efficacy of whole\neducation systems.\nRecent studies have traced Pearson's networks of influence into the educational policy sphere\n(Hogan et al., 2015), and emphasized `Pearson's overall business ambitions ... to find new markets\nand to create new spaces of education for Pearson's products' (Ball and Junemann, 2015: 49). The\npolicy networks and commercial ambitions of Pearson are part of the argument in this article, but its\nnovel claim is that Pearson acts as a global methodological gatekeeper in defining and modelling\nwhat constitutes learning, and that this act of classification is consequential to how education sys-\ntems and individual learners alike will be governed in the future. Beyond its business plans, Pearson\nis participating in a reconfiguration of the methods by which learning is conceptualized, measured\nand understood, and seeking to secure consensus for its views through mobilizing techniques of data\nvisualization and human\u00adcomputer interaction (HCI). The aim of the article is to examine the scope\nof Pearson's methods to produce and circulate knowledge about learning, and the argument is that\nsuch knowledge may be redefining existing conceptualizations about learning and its measurement\nthat have previously been the preserve of social scientific forms of expertise. Pearson is not merely\nseeking new market niches, but redefining learning itself and seeking to mobilize its knowledge\nabout learning and cognition in the specification of new pedagogic applications and products.\nFocusing on the specific methodological and technical instruments it mobilizes is crucial to under-\nstanding how its policy and commercial ambitions are being operationalized and how its goal `to\nhelp people make measurable progress in their lives through learning' is materialized.\nAs such, the role of Pearson in influencing policy processes and pedagogic practice is part of a\nwider `governance turn' in European education and beyond (Ozga et al., 2011). Increasingly, gov-\nernance is conceived as a form of `soft power'realized through techniques of attraction, seduction,\npersuasion and the cultivation of support and shared interest across networks of loosely associated\nactors from across the public and private sectors (Moos, 2009), including `those conventionally\nconsidered peripheral to education governance' such as `commercial interests and technological\ninnovators' (Lawn and Grek, 2012: 82). The shift to soft governance is enabling a new kind of\ngoverning expert whose claim to authority rests on the methodological and technical capacity to\nknow, assess and act upon education through data collection, aggregation and analysis, and to pro-\nduce new kinds of `governing knowledge' (Fenwick et al., 2014). Websites and online portals that\npresent the data persuasively as the knowledge required to facilitate governing practices, and the\ndata practices employed to generate them, have therefore become the focus of recent research (e.g.\nuse of data in education systems (Lawn, 2013), such studies demonstrate how education govern-\nance is currently being accelerated by software-mediated processes of `datafication', including:\n...the conceptualisation and codification by which the pre-existing frames, categories and classifications\nshape the information that is constituted as data and which influence the possibilities for its usage and\neffects ...; the algorithmic treatment of data through which patterns and correlations are produced; and the\nre-representation of the world through data visualisation and the navigation of data by users. (Sellar, 2015:\nThe `data infrastructure' underpinning the production of governing knowledge is a sophisticated\ntechnical and methodological accomplishment. As Sellar (2014: 6) argues, the concept of data\ninfrastructure in relation to education governance can be defined as `an assemblage of material,\nsemiotic and social practices' that functions to translate things into numbers; enables the storage,\ntransmission, analysis and representation of data using algorithmic logics and computational tech-\nnologies; embeds data usage into other practices; and produces new kinds of spaces and social\npractices through practices of classification, measurement and comparison. Likewise, Edwards\net al. (2013: 5) refer to `knowledge infrastructures' as `networks of people, artifacts, and institu-\ntions that generate, share, and maintain specific knowledge about the human and natural worlds'.\nIt is useful to think of education as being orchestrated through infrastructures in which digital data\npractices are now embedded in the production of new governing knowledge about educational\ninstitutions, practices and spaces, and where data are increasingly perceived and accepted as a form\nof authoritative, objective and impartial knowledge. Yet, while the `social technologies' of soft\ngovernance appear to be `natural' or `neutral' tools (Moos, 2009), they are deeply inscribed with\nthe methodological values and styles of thinking of their designers and sponsors.\nMethodologically, I examine the Learning Curve and CDDAAL resources through detailed\ndocumentary analysis of the various websites, reports and visualizations produced to support them,\nfocusing explicitly on the digital methods involved in the generation of the Learning Curve and\nenacted by the CDDAAL. These methods constitute what I term Pearson's methodological com-\nplex--the technical and practical instruments for knowing and classifying learning. Drawing on\nsociomaterialist approaches from science and technology studies (STSs), and consistent with the\nsocial life of methods approach, a methodological complex does not only consist of the technical\ninstruments themselves, but a sociotechnical apparatus including the human and social actors\ndesigning and deploying them; the institutions promoting and sponsoring their use; the epistemo-\nlogical assumptions underpinning them; the underlying software, code and algorithms that enable\nthem to function; assumptions about the users for whom they are intended to produce data; and the\nproductivity of such an apparatus to exert material effects by shaping social practices and influenc-\ning decision-making. This ultimately represents what Bowker and Star (1999) have influentially\ndesignated as an infrastructural system of classifications, standards and categories that loops back\ninto the social world it represents. Pearson enforces a particular set of classifications and models of\nwhat learning is--as defined through its hybrid data science/learning science methodologies--\nwhich then informs policies, decisions and technical interventions that constitute those understand-\nings in particular material practices. This is a knowledge infrastructure consisting of data collected\nthrough learning processes; that produces knowledge about learning from analyses of these data;\nand that is consequential to ways of intervening in future learning processes and learners' lives.\nSuch an infrastructure is the material instantiation of Pearson's techniques of soft governance: a\nsystem delegated to non-government experts, with associated techniques that seek to activate the\ncapacities of subjects to act in new ways.\nDigital methods\nPearson does not possess the direct means to set education policy or determine pedagogy through\nhard regulative governance. Instead, it governs more softly and indirectly through seeking to attract\npolicymakers, practitioners and other publics to the insights, recommendations and advice it is able\nto derive from data. In this section I critically examine six categories of digital methods being\ndeveloped and deployed by Pearson, specifically analysing its role as a methodological gatekeeper\nwith the technical expertise to produce new classifications, categories and models of learning and\nto construct a new data/knowledge infrastructure for education, understandings that it can then\ncodify into software products that can be inserted into the pedagogic techniques and technologies\nof the classroom.\nStatistical methods\nMuch has been written in the field of education policy research on the process of `governing\nenumeration' (Hardy, 2015) are used to produce the knowledge required to enable education to\nbecome governable. Such studies contend that numbers are never merely factual, transparent or\ntheory-free conveyors of reality, but the product of particular languages, categories, interpretations\nand doctrines that result in the production of norms and expectations--such as what should happen\nin schools. `Commensuration' has become a particularly productive statistical method in the trans-\nformation and standardization of different qualities into a common metric as `evidence' required\nby policymakers for benchmarking, comparison, evaluation and decision-making purposes (Sellar,\nPearson's Learning Curve exemplifies the productivity of numbers to influence policy deci-\nsions. Launched in November 2012 under the leadership of Michael Barber (Pearson's chief educa-\ntion adviser, and a former head of the UK Prime Minister's Delivery Unit; see Barber and Ozga,\n2014), the Learning Curve consists of a vast databank of educational data aggregated together from\nover 60 datasets from around the globe. According to its website:\nThrough The Learning Curve we are contributing to the global conversation on learning outcomes; to help\npositively influence education policy at local, regional and national levels. The data and analysis on this\nwebsite will help governments, teachers and learners identify the common elements of effective education.\nThe data in the Learning Curve databank have been compiled into a global index of educational\nperformance that maps correlations between the inputs to and outputs of education, the inputs to\neducation and socio-economic environment indicators (as a proxy for wider society), and the out-\nputs of education and socio-economic environment indicators. These data are presented on the\nwebsite as ranked league tables, visual tools and also compiled into reports (to date reports are\nLearning Curve would become `an open, living database which we hope will encourage new\nresearch and ultimately enable improved ... evidence- informed education policy'.\nUnderpinning the Learning Curve is a complex of statistical methods utilized to ensure the com-\nmensurability and comparability of the data from the different datasets. As stated in the methodol-\nThe aim of the Data Bank was to include only internationally comparable data. Wherever possible, OECD\ndata or data from international organisations was used to ensure comparability ... and when possible, used\ninter- and extrapolations in order to fill missing data points. Different methods for estimations were used,\nincluding regression when found to be statistically significant, linear estimation, averages between regions,\nand deductions based on other research.\nElsewhere in the appendix, it is possible to identify a number of methodological commitments and\nepistemological assumptions. It refers to `objective quantitative indicators' and the normalization\nof statistical indicators into `z-scores' to indicate how many standard deviations an observation is\nabove or below the mean. Notably, the appendix features a number of cautions, such as that\n`because indexes aggregate different datasets on different scales from different sources, building\nthem invariably requires making a number of subjective decisions'. This is commensuration mate-\nrialized methodologically: a common metric derived from the transformation of different datasets\nand qualities into standardized form. It also registers what actor-network theorists have termed\n`qualculation'--how data are made to qualify for inclusion in calculations (see Edwards and\nFenwick, this issue).\nFrom a socializing methods perspective, it is especially significant that the Learning Curve is\nthe product of the Economist Intelligence Unit (EIU) (http://www.eiu.com/). Pearson itself owns\n50% of the Economist Group of which the EIU is the research and analysis division. The EIU field\nof expertise is in economic and market data; its `sound and transparent' methodologies include\neconomic, political and socio-demographic forecasting; quantitative, qualitative and synthetic\nindicators; innovative scoring systems; statistical index construction and global ranking; multi-\ndimensional comparison; data modelling and scenario analysis; industry and risk analyses; and\neconomic, political, cultural and locational benchmarking. The EIU enacts these methods in\nresearch for business as well as government. For the latter, its `team of analysts, economists, and\nregulatory specialists' are ` helping clients develop data-driven solutions to public policy chal-\nlenges', and it claims its `research programmes, always supported by reliable data and actionable\nresults, have helped governments, foundations, NGOs and business associations to understand and\novercome the challenges they face in the world of public policy'. Many of these methods are remo-\nbilized in the Learning Curve, in its global index of ranked countries; its production of country\nprofiles detailing their social, political and demographic indicators; its generation of visualizations\nto make the data easy to use and interpret; and its attempt to correlate the inputs to and outputs of\neducation with socio-economic environment indicators. These methods are drawn from the reper-\ntoire of business and market intelligence, and infuse educational data with economic logics of\ncalculation and forecasting. In particular, the EIU's expertise in country comparison and global\nbenchmarking infuses the design of the Learning Curve to promote cross-country comparison of\neducation systems.\nIts statistical methods permit Pearson to position itself as a `centre of calculation'in the govern-\nance of education, enabling it `to act as a centre by means of its centrality in the flows of informa-\ntion that \"re-present\" that over which it is to calculate and seek to programme' (Rose, 1999b: 211).\nBy turning education into numericized inscriptions, Pearson's methods render it visible as a calcu-\nlable space defined according to specific statistical methods and the norms, epistemologies and\nassumptions underlying them. There is a double social life to Pearson's statistical methods, in that\nthe data so generated do not merely inscribe a pre-existing reality but constitute education as a\ncalculable space in which institutions (schools, local authorities, government education depart-\nments) are incited to calculate about themselves in certain ways, and act to improve and optimize\nthemselves `because they are calculated about in certain ways by others' (Rose, 1999b: 213). The\nsocial life of the EIU's methods is, in other words, consequential to the ways in which education is\ncentred for counting and calculation in the Learning Curve.\nData science methods\nWhile statistical methods are a dominant aspect of the Learning Curve, Pearson is also developing\nmore digitally native methods from the field of data science to support its production of new\nknowledge about learning and skills. Building on established statistical methods and models, data\nanalytics technologies utilize advances in information management and storage, data handling,\nmodelling algorithms, machine intelligence and expert systems that can `automatically mine and\ndetect patterns and build predictive models' based on large datasets (Kitchin, 2014: 101). These\n`big data' methods are increasingly used in the analysis of governmental and business data,\nscientific analysis and the analysis of social and cultural trends, constituting a major development\nin the field of data science.\nData science methodologies infuse the approach of Pearson's CDDAAL. On the `meet the team'\npage of the CDDAAL website, for example, its director John Behrens is described as an expert in\nmeasurement and statistics, whose research focuses on how `the billions of bits of digital data\ngenerated by students' interactions with online lessons as well as everyday digital activities can be\ncombined and reported to personalize learning'. Staff are listed as `research scientists' with exper-\ntise in data mining, computer science, algorithm design, intelligent systems, HCI, data analytics\ntools and methods, and interactive data visualization. In a methodological report for the CDDAAL,\nBehrens (2013) claims that educational research is increasingly under pressure to adopt new com-\nputational and data science methods that enable data manipulation and data visualization, includ-\ning the mobilization of `big data' to enable continuous tracking and monitoring of streaming data,\nrather than the collection of data through discrete temporal assessment events; the move towards\n`population analytics' techniques that can handle enormous, scalable samples of many millions of\nrecords of research data; the use of `educational data mining'to extract patterns from it; and the use\nof statistical models for combining results from different datasets and to integrate new and existing\ndata and information.\nIn another CDDAAL publication, data science is a positioned as a `transformative'\nmethodology:\nOnce much of teaching and learning becomes digital, data will be available not just from once-a-year tests,\nbut also from the wide-ranging daily activities of individual students ... in real time. ... [W]e need further\nresearch that brings together learning science and data science to create the new knowledge, processes, and\nsystems this vision requires. (DiCerbo and Behrens, 2014)\nThe authors argue that combining `learning science' with data science methods will enable\nresearchers to `capture stream or trace data from learners' interactions' with learning materials;\nenable computer analysis to detect `new patterns that may provide evidence about learning'; pro-\nvide immediate feedback about performance on specific activities; construct data-based profiles\nand `better models of learners' knowledge, skills and attributes'; `tune' those models through con-\ntinuously updated streams of data to ensure the inferences drawn from them are accurate and valid;\n`to take a learner's profile of knowledge, skills and attributes and determine the best subsequent\nactivity'; and, finally, `to more clearly understand the micro-patterns of teaching and learning by\nindividuals and groups'.\nThe vision pursued in the report is explicitly modelled on the idea of tracing individuals' `activ-\nity streams' and is derived from social networking sites such as Facebook, where the activity\nstream is an integral part of the user interaction with the system--a constant trace of the user's\nproduction of content, status updates, comments, and so on. DiCerbo and Behrens (2014) expand\nthe notion of the activity stream to suggest that the `power of these streams lies in their ability to\nrecord change as it occurs', and that for the purposes of education `they have the potential to indi-\ncate changes in learning, motivation and other characteristics of interest as they happen'.\nPattern recognition methods\nAs the above examples indicate, the CDDAAL aims to mobilize techniques of social network\nanalysis to mine students' data for patterns, based on the understanding that, `faced with a very\nlarge number of potential variables, computers are able to perform pattern identification tasks that\nare beyond the scope of human abilities ... not only to collect information but also detect patterns\nwithin it'(DiCerbo and Behrens, 2014). The computational method of pattern recognition operates\nby taking log files of a user's activity and then subjecting it to detailed analysis using various meas-\nures; data captured from a single individual's log file can then be synthesized with other users' log\nfiles to see if they can be combined into generalizable indicators of aspects of learning. To do this,\nthe report details how pattern recognition analysis can be used to trace and match patterns in learn-\ners' activities:\nLearner interactions with activities generate data that can be analysed for patterns. ... Performance in\nindividual activities can often provide immediate feedback ... based on local pattern recognition, while\nperformance over several activities can lead to profile updates, which can facilitate inferences about\ngeneral performance. (DiCerbo and Behrens, 2014)\nThe methodological development of pattern recognition techniques is a major strand of the\nCDDAAL's R&D programme. CDDAAL researchers are even engaged in detecting patterns from\nyoung people's activities outside of formal education, in online videogaming environments and\nsocial networking sites. As learners interact with systems and with other people, `software records'\nevery aspect of their activity so that as learners interact in digital environments, in formal and\ninformal contexts, `actionable data can be drawn from both':\nThese developments have the potential to inform us about patterns and trajectories for individual learners,\ngroups of learners, and schools. They may also tell us more about the processes and progressions of\ndevelopment in ways that can be generalised outside of school. (DiCerbo and Behrens, 2014)\nThe promise of pattern recognition methods promoted by Pearson is therefore not simply of better\ntracking of learners, but also the generation of new generalizable theories and models of cognitive\ndevelopment and learner progression. Those insights can then be made actionable as new software-\nbased pedagogic products; Pearson is, of course, well positioned as an educational publisher to\ncodify these insights in its own software applications for schools.\nThe logics of pattern recognition mobilized by the CDDAAL owe much to social media analyt-\nics from the commercial domain, and to the specific methods developed to detect, classify and\nextract associations and patterns from large datasets (Kitchin, 2014). Methods including cluster\nanalysis, natural language processing, Bayesian networks, artificial neural networks and statistical\nanalysis can then be used to find relationships between data objects, identify trends and curves, and\nmake predictions about certain attributes on the basis of other attributes. CDDAAL researchers\nexplicitly mobilize such pattern recognition methods to reveal the hidden patterns of learning and\nbuild generalizable models of cognitive development. For example, in another CDDAAL paper on\nthe methodological challenges of analysing educational big data, Behrens (2013: 18) provides an\nupbeat assessment of how insights extracted from the generation of huge quantities of educational\ndata will challenge current theoretical frameworks for making sense of it, as `new forms of data\nand experience will create a theory gap between the dramatic increase in data-based results and the\ntheory base to integrate them'.\nFundamentally, the activities of the CDDAAL are premised on the big data epistemology that\npattern recognition methods and techniques can reveal meaningful connections, associations, rela-\ntionships, effects and correlations about human behaviours without the need for prior hypotheses,\ntheoretical frameworks or further experimentation. This assumes that `through the application of\nagnostic data analytics the data can speak for themselves free of human bias or framing, and that\nany patterns and relationships within big data are inherently meaningful and truthful' (Kitchin,\n2014: 132). Yet, data do not exist naturally as a `raw' or truthful representation of an underlying\nreality; they have to be brought into being through social, methodological and technical practices,\nand are constantly shaped as they move between human actors, software platforms and institu-\ntional structures and settings, all framed by social, political and economic contexts (Bowker, 2005).\nBased on the epistemological assumption that pattern recognition software can reveal truthful\nmodels of human action, the CDDAAL aims to develop computational theories of learning itself as\na means towards crafting better pedagogic techniques for governing learners.\nVisual methods\nMuch of the research undertaken by CDDAAL researchers is highly technical in nature, tra-\nversing learning science and data science methodologies for mapping and modelling the gener-\nalizable patterns of learning processes and cognitive development. In order to make the insights\nit has extracted from these patterns in the data persuasive and acceptable to wider publics of\npolicymakers, practitioners and even parents, Pearson has drawn significantly on data visuali-\nzation methods to `effectively reveal and communicate the structure, patterns and trends of\nvariables and their interconnections' (Kitchin, 2014: 106). With the massive growth of digital\ndata in education detailed by Pearson, visualization is employed to make visible and compre-\nhensible complex datasets that would otherwise be difficult to conceptualize, and to reveal\npatterns, structures and interconnections that might otherwise remain hidden. Michael Barber\nhas described how the Learning Curve supports `evidence-based policy' through data visualiza-\ntion `to make it easy for people ... to use quickly without undermining the integrity of the data'\nFor example, the Learning Curve mobilizes visual methods to reveal the patterns and associa-\ntions between educational input and output indicators on a global scale. It features a suite of\ndynamic and user-friendly mapping and time-series tools that allow countries to be compared and\nevaluated both spatially and temporally. Countries' educational performance in terms of educa-\ntional attainment and cognitive skills are represented on the site as semantically resonant heat maps\nand graphical time-series trend tools. It also permits the user to generate country infographic pro-\nfiles that visually compare multiple education input indicators (such as public educational expendi-\nture, pupil:teacher ratio, educational `life expectancy') with education output indicators (PISA\nscores, graduation rates, labour market productivity), as well as socio-economic indicators (gross\ndomestic product (GDP) and crime statistics).\nMoreover, the Learning Curve is used as a form of visual argumentation. Through the applica-\ntion of visual analytics algorithms, it allows the user to manipulate the images in order to reveal\npatterns and associations, to conduct comparisons by altering variables, and to build visual models\nand explanations. The logic of country comparison underpinning the Learning Curve at least partly\ndepends on the visual semiotics of the graphic presentation of patterns in the data. Visualization\nacts as a way of simplifying and reducing the complexity of the interaction of variables to graphical\nand diagrammatic form; it is an advanced semiotic technique of commensuration, whereby diverse\nquantities and qualities of educational data are transformed and standardized into a common visual\nmetric. The methodological notes on the Learning Curve website are carefully worded to detail the\ndata quality issues involved in aggregating its 60 different datasets; yet, its heatmaps, time-series\ntools and league tables smooth over these numerical problems to provide a glossy plane of graphi-\ncal commensuration through which comparisons can be made and to which evaluations might be\nattached.\nAs this would suggest, the visualization of data is no neutral or objective accomplishment.\nVisual methods give the numbers meaning; they translate numerical measurements into curves and\ntrends; they make the data amenable to being inserted into presentations and arguments that might\nbe used to produce conviction in others. In other words, data visualization gives numbers addi-\ntional pliability to be shaped and configured as powerful and persuasive presentations. A data visu-\nalization is assembled as it circulates around a network of offices and computer screens, as it is\nworked on by designers, visualizers, project managers, programmers and data analysts, and as it\nmoves between software programmes and hardware devices, `through which data are constantly\nmobile, shifting and proliferating, moving between different actors and media, ported and patched,\naltered and designed, collaged and commented on' (Rose et al., 2014: 401). The human eyes and\nhands, as well as software platforms and algorithms, involved in its display shape the interpreta-\ntions data visualization makes possible and the possible meanings that might be extracted from it.\nVisualization is thus also socially productive, in that it directs attention to correlations between\ndata variables and objects that might then be made actionable as insights for decision-making.\nThe Learning Curve ultimately visualizes a virtual reference space against which all education\nsystems might measure and monitor themselves; it constitutes a virtual comparator and a global\nbenchmark for educational evaluation, judgement and action. It is through such visual techniques\nthat Pearson seeks to attract various publics to the insights it has extracted from patterns of learning\nprocesses in its data, and to secure consensus that the models it has constructed from the data rep-\nresent learning as it really is rather than as abstracted theories constructed from pre-existing disci-\nplinary frameworks such as those associated with the social sciences.\nHuman\u00adcomputer interaction methods\nWhile visual methods of graphical data presentation enhance the plasticity of numbers, there is also\nconsiderable flexibility in its interpretation by users. The possible `interpretive flexibility' availa-\nble, however, is counterbalanced by the ways that data visualization itself `configures the user'\ninscribe their views of users and use in technological objects', thus constraining use of those tech-\nnologies in particular prescribed ways, but also highlight how users might `underwrite or reject and\nrenegotiate the prescriptions'.\nReflecting the tension between the configuration of the user by the designer and the users'recip-\nrocal reconfiguration of the designed object through resistant or unintended usages, the Learning\nCurve does not merely present ready-made visualizations, but also provides interactive tools to\nenable the user to conduct his or her own visual analyses through tweaking variables, selecting data\nsources and adjusting statistical weightings. Michael Barber has described it as a product of `co-\ncreation' that allows the public to `play' with the data and `connect the bits together' in a way that\nis more `fun' than preformatted policy reports (Barber with Ozga, 2014: 84). The Learning Curve\nfunctions as an exemplar of a `communication-based and information-based instrument'that privi-\nleges `audience democracy' (Lascoumes and Gales, 2007: 13\u00ad14), whereby public authorities are\nobliged to provide citizens with rights of access to the information they hold and citizens are\nrequired to play a reciprocal role in its interpretation and dissemination.\nHowever, the Learning Curve does not represent unconstrained audience participation. Within\nthe design constraints of the Learning Curve, users are guided towards particular forms of analysis\nthat privilege country comparison over other possible analyses, thus enabling and delimiting what\nusers can do with the data and what can be said about it. Consonant with the comparative methods\nof the EIU that designed it, global comparison and forecasting--and the values and methodologi-\ncal preferences that underpin such approaches--are structured into the user interface through its\nleague tables, heatmaps and time-series tools to shape interpretation, make visible particular edu-\ncational realities and encourage particular kinds of responses. The design of the Learning Curve\ninterface configures the research user as a comparative analyst and a data co-producer.\nApproached in terms of methods, the Learning Curve is the product of emerging HCI method-\nologies. HCI methods have developed significantly in the context of big data, as:\n...information providers conduct a great deal of research trying to understand, and then operationalize,\nhow humans habitually seek, engage with, and digest information. ... [In HCI], the understanding of\nhuman psychology and perception is brought to bear on the design of algorithms and the ways in which\nBig data itself has become a valuable resource for HCI researchers and developers, who are able to\nutilize masses of data about users' information processing practices to inform the design of new\nsoftware interfaces and functionality. Moreover, the aim of HCI in relation to commercial social\nmedia has been the optimization of the interaction in order to attract and seduce users to play a\nsignificant part in the continual production and circulation of informational and media content.\nThe logics of audience participation built in to the Learning Curve are not merely artefacts of a\ncommitment to data transparency and democracy, as Michael Barber claims. They are the product\nof HCI insights about human information processing, perception and capacity to comprehend large\ndata, twinned with the social media model of user interactivity and participatory content produc-\ntion. Through inviting user participation in the Learning Curve, it may even be possible for Pearson\nto track how users interact with the data--to monitor its efficacy, as its Efficacy Framework is\nintended--and then seek to optimize its tools to positively impact on future use. Yet, by including\nonly internationally comparable data, the user's interactivity is already pre-figured by the Learning\nCurve, leading to a subtle but significant reinforcement of the methodological assumptions under-\npinning its interface and interaction design.\nThe kinds of user interaction experiences enabled by HCI methods have the capacity to config-\nure the research user of the Learning Curve (the school leader, the practitioner, the policymaker,\nthe researcher), yet the methodological advances in the HCI field underpinning the presentation\nand circulation of educational data, or its productivity to configure the user of the data, remain\nunder-examined. This is a critical omission, as Pearson is rapidly developing the capacity to visual-\nize its datasets and to amplify the public accessibility of the models of learning it is developing\nthrough its research at the CDDAAL and enforcing through the Learning Curve. As a consequence,\nthe data-based models and classifications of learning and cognitive skills it is extracting from pat-\nterns in learning data have the potential to become more widely accepted as the reality of learning,\nand therefore to configure the practices of teachers and the decision-making of policymakers as its\nusers.\nMachine learning methods\nThe sixth part of Pearson's methodological complex is its capacity for prediction and pre-emption.\nIn 2014 Pearson published a report on using `intelligent software and a range of devices that facili-\ntate unobtrusive classroom data collection in real time'(Hill and Barber, 2014). Its authors promote\n`the application of data analytics and the adoption of new metrics to generate deeper insights into\nand richer information on learning and teaching', and to provide `ongoing feedback to personalise\ninstruction and improve learning and teaching' (Hill and Barber, 2014). Such systems, they argue,\ncould instantiate a revolution in education policy, shifting the focus from the governance of educa-\ntion through the institution of the school to `the student as the focus of educational policy and\nconcerted attention to personalising learning' (Hill and Barber, 2014). Here, Pearson's ambitions\nare most starkly realized: it aims to make its emerging insights about learning, derived from pat-\nterns in masses of learners' data and translated into generalizable models of learning, into the\ncentral focus of a predictive mode of educational practice and policy driven by machine-based\nintelligence. Pearson has even supported R&D in the area of artificial intelligence in education,\nclaiming that `artificial intelligence is increasingly present in tools such as adaptive curricula,\nonline personalised tutors, and teachable agents' (Pearson College London, 2015).\nThe technical developments underpinning such an anticipatory approach are premised on cur-\nrently emerging idea from technical R&D in `learning analytics'. Notably, Pearson has partnered\nwith Knewton, a major learning analytics provider, to power its digital content:\nThe Knewton Adaptive Learning PlatformTM uses proprietary algorithms to deliver a personalized\nlearning path for each student.... `Knewton adaptive learning platform, as powerful as it is, would just be\nlines of code without Pearson,'said Jose Ferreira, founder and CEO of Knewton. `You'll soon see Pearson\nproducts that diagnose each student's proficiency at every concept, and precisely deliver the needed\ncontent in the optimal learning style for each. These products will use the combined data power of\nmillions of students to provide uniquely personalized learning.'(http://www.knewton.com/press-releases/\npearson-partnership/)\nBased on artificially intelligent machine learning algorithms, learning analytics software platforms\nlike Knewton are designed to enable individual students to be tracked through their digital data\ntraces in real time and to provide automated predictions of future progress (Siemens, 2013). Here,\nmachine learning algorithms and the predictive analytics and prescriptive analytics they enact, are\nsignificant. Through machine learning techniques, `programmers construct models that predict\nwhat people will do' by `transforming data on events, actions, behaviours, beliefs and desires' into\nprobabilistic predictions of the future that then can be used to decide on action to be taken in the\npresent (Mackenzie, 2013: 399). Predictive learning analytics are one material instantiation of\nmachine learning. Prescriptive analytics can then be mobilized as `recommender systems' for per-\nsonalized pedagogic intervention.\nConsonant with the social life of the methods approach, it is important to acknowledge that\nlearning analytics is itself a field of methodological inquiry, part of an `emerging field of Educational\nData Science' (Piety et al., 2014). The predictive models generated by the machine learning algo-\nrithms of learning analytics are therefore the product of complex social, technical and trans-\ndisciplinary practices and are embedded in the methodological commitments, assumptions, values\nand styles of thinking of their designers. As Piety et al. (2014) acknowledge in relation to educa-\ntional data science, its `architectures can encode various theories of learning that manifest them-\nselves in the data the tools provide'.\nPearson's director of the CDDAAL, John Behrens, is a key voice in the field of educational data\nscience (Piety et al., 2013). Shaped by the methodological commitments and constraints of the\neducational data science field, the predictive learning analytics techniques being developed through\nPearson's partnership with Knewton anticipate a form of future-tense educational management\nthrough machine learning methods. These analytics capacities not only complement existing large-\nscale database techniques of governance conducted at discrete temporal intervals through large-\nscale testing, but also accelerate the timescales of governing by numbers. They make the collection\nof enumerable educational data, its processes of calculation and its consequences into a real-time\nand recursive process operationalized up close from within the classroom and regulated at a dis-\ntance by new centres of statistical calculation, data analytics, pattern recognition, interactive visu-\nalization and prediction. Data are therefore being used `to govern by activating the capacities of the\nindividual' (Ozga et al., 2011: 88), a strategy accomplished through digital methods that capture,\nprocess and display highly granular detail on individual learners, their performances and their pre-\ndicted progress in real time, and that then prescribe or recommend pedagogic interventions directly,\nrather than merely capturing snapshots of national systems at discrete temporal intervals.\nAs an `open, living database', as Barber has described it, future iterations of the Learning Curve\nmight also feature the kind of fine-grained individualized data on learners' cognitive skills that is\nbecoming available through developments enacted by the CDDAAL too, enabling users to interact\nwith the data aggregated from individuals'activity streams as well as conducting comparisons with\ntime-series data from large-scale assessments. This would be consistent with Pearson's ambitions\nto shift the policy focus from large-scale testing to individuals' learning, and with its analytics\ncapacities to generate generalizable models of cognitive skills development. The Learning Curve\nalready commensurates diverse data sources to tabulate and visualize a common cognitive skills\nmetric. If governing by numbers has been concerned with processes of commensuration and com-\nparison across geographical regions, the digital governing methods of Pearson's analytics-based\napproach amplify its focus on the comparison of individual learners' cognitive skills within a\nglobal database that itself contains a generalized model of cognitive skills derived from massive\npopulations of learner data.\nWithin this approach, then, Pearson is actively intervening in the classifications and categories\nby which learning is known, conceptualized and acted upon, enabling individual learners to be\ncompared against algorithmic norms and globally standardized classifications of learning progres-\nsions. Its claim to be filling the gap between data-based results and the theory base to integrate\nthem, leading to the production of new generalizable models of learning processes and progres-\nsions, means that its theoretical understandings and models of learning might then be transcoded\ninto the pedagogic resources that Pearson itself produces and promotes to schools, particularly its\npersonalized and adaptive learning applications. It is seeking to mobilize machine learning meth-\nods, as a form of artificial intelligence, to accomplish this task.\nGoverning methods\nMethods socialize the objects of analysis. The methodological maximalism enacted by Pearson\nreveals the extent to which remediated digital methods derived from overlapping disciplinary tradi-\ntions and epistemological perspectives are combining as a set of operational practices for the gov-\nerning of education. By hybridizing the methodologies of data science and learning science (and\ntheir parent disciplines of CompSci and psy), Pearson has not only socialized these methods in the\nsense of normalizing them in educational inquiry, but also as a means towards wrapping new social\nnorms, understandings and interpretations around education itself. The central contribution of this\narticle has been to explore the consequences of Pearson's digital methods in terms of how its gen-\neration of new models and classifications of learning might then loop back into the pedagogic\nmachinery of the classroom by being codified in software products. Four key points emerge from\nthis analysis.\nFirstly, Pearson's methodological complex constitutes a significant set of data practices for the\nproduction and performativity of an educational data/knowledge infrastructure, and influences the\ngoverning knowledge it produces. Through the work of both the Learning Curve and the CDDAAL,\nit enacts the enumeration of national and individual performance; the analysis and presentation of\ndata; the embedding of data into practices, including not only policymaking but also pedagogic\nroutines; the production of a virtual reference space that acts as a comparator model for other edu-\ncational spaces; and the production of new practices, such as participatory data visualization and\nprediction. This infrastructure of data-based knowledge production is being constructed gradually,\nmethod by method, as well as line by line, out of software code and algorithms. The coding prac-\ntices of programmers, software developers, algorithm designers and other technical experts are\ncombined with data scientific research methods expertise in the development of Pearson's meth-\nodological complex and the data infrastructure it enacts.\nSecondly, as an increasingly prominent actor in the production of a global educational data\ninfrastructure, Pearson is positioning itself as an institutionalized governing expert, peopled by\nalgorithmists with access to the methodological complex of software, algorithms, analytics and\nvisualization tools required to analyse and make sense of the growing mass of data becoming avail-\nable as education is digitized. Its remediated methods allow it to see, know and interpret aspects of\neducation in ways that displace the authoritative knowledge of the educational psychologist, soci-\nologist, historian or philosopher to the knowledge produced by the data scientist or even by auto-\nmated machine intelligence. This reflects social scientists' anxieties about their authority to record\nand report on social phenomena at a time when big data methods have been elevated to methodo-\nlogical supremacy in commercial, cultural and political contexts, and social research has been\nredistributed to data science laboratories. Pearson is making data science methods into a new mode\nof expertise for knowing and intervening in learning, and is even seeking to transform the bureau-\ncratic organs of official policymaking by emphasizing the personalization of learning at the indi-\nvidual level--directly through embedding adaptive software products in the pedagogic routines of\nthe classroom--over incremental improvement to education systems.\nThirdly, through its redistribution and remediation of methods, Pearson is making educational\ndata available to the eyes, hands and minds of educational policymakers and practitioners world-\nwide. Underpinning its approach is a data scientific commitment to data as a theory-free window\non to educational realities, twinned with the epistemological assumption of the learning science\nfield that education can be calculated objectively and visualized as existing facts about learning\nprocesses and cognitive development. However, its methods also subtly direct the gaze and guide\nthe fingers to make sense of those data in particular ways, not just by simplifying the complexity\nof the data but by amplifying the perceptibility of certain features and reducing others to construct\na new `virtual world of educational data' (Lawn, 2013). Within this virtual world of data, learning\nis being reconfigured by Pearson in terms of commensurable data, patterns, predictions and visu-\nalizations that are themselves shaped by the social contexts in which they are produced. Its new\ndata-derived models of learning and cognitive development have the potential to shape how peda-\ngogic practitioners and policymakers understand what learning is and how to activate it through\nspecific pedagogic resources, approaches and applications--all of which Pearson is itself posi-\ntioned to provide as the world's largest publisher of educational resources and technologies:\nPearson is involved both in seeking to influence the education policy environment, the way that policy\n`solutions' are conceived, and, at the same time, creating new market niches that its constantly adapting\nand transforming business can then address and respond to with new `products'. (Ball and Junemann,\nInformed both by the methods of data science and learning science, its products provide the means\nfor the personalization of learning but `at the same time can demonstrate impact in the form of\nmeasurable (learning) outcomes' (Ball and Junemann, 2015: 31).\nFourthly, however, these developments need to be understood not just in terms of commercial\nbusiness models, but also as technical and methodological accomplishments that are both socially\nenacted and socially productive, and that are redefining how learning is understood and how\nlearners are to be made amenable to pedagogic intervention through the technologies of school-\ning. The consequence is that Pearson is poised to exert a kind of `looping effect' (Hacking, 2007)\non learners' subjectivities, where the data-derived model acts to shape and `make up' the people\nthat it purports to measure and represent. In other words, the combined learning science and data\nscience methods of Pearson could become highly consequential to the formation of new models\nof learning, and thereby to `making up' students as new `kinds of people' who are understood in\nterms of the data and encouraged through the pedagogic apparatus of the adaptive classroom to\nrelate to their own learning in novel ways. These developments amount to the production of a new\ndata/knowledge infrastructure in education: the generation of classifications and categories of\nlearning, measured and monitored in terms of the models produced by learning and data science\npractitioners, that might then `touch people' (Bowker and Star, 1999: 314) by looping back into\nthe classroom as pedagogic software applications. Methodologically, it is a matter of transcoding\nclassifications of learning into the lines of code that constitute Pearson's e-learning software\nproducts--software code that then activates students' capacities in accordance with the codes of\nconduct contained in the classification.\nConclusion\nPearson plc has become an important policy actor with a `network of interests and objectives' that\nstretch `beyond corporate boundaries and into spaces of policy, academic research and philan-\nthropy', through which it is strengthening its role `across all aspects of the education policy cycle,\nfrom agenda setting, through policy production and implementation to evaluation' (Hogan et al.,\n2015: 62). Its high-profile personnel, such as Michael Barber, give Pearson policy credibility and\nleverage, whilst its presence in the field of educational data science through John Behrens positions\nit at the forefront of an emerging academic field of methodological inquiry and discovery. This\narticle has surveyed key components of the methodological complex Pearson is inserting into the\npolicy cycle--as well as into pedagogic practice--and explored the social life of its methods to\nunderline its capacity to produce the data, analyses and knowledge required for the soft governance\nof education. It is achieving this through employing a range of data-based digital methods to pro-\nduce new knowledge about learners and their learning processes, leading towards the production\nof a knowledge infrastructure within which new understandings and models of learning can be\ncirculated and codified into pedagogic software products and recommendations. Pearson is also\ncommitted to monitoring the efficacy of its products, thus producing a highly recursive feedback\nloop that includes modelling and classifying cognitive learning processes, developing new peda-\ngogic products to activate these processes, and then testing their effectiveness in terms of\noutcomes.\nThese activities are embryonic of an emerging hybrid of computer science-based data practices\nand psychological learning sciences that Loveless and Williamson (2013: 13) term a `CompPsy\ncomplex', which `assembles and encodes a particular representation of learner subjectivity' into\nspecific technologies that are designed to `elicit, promote, facilitate and foster the capacities, capa-\nbilities and qualities of such a pedagogic subject'. Mobilizing such a CompPsy complex, Pearson's\nLearning Curve, the CDDAAL, and other products such as its Efficacy Framework are key\nresources--each underpinned by very particular methodological techniques--for operationalizing\nthe data infrastructure within which new and powerful classifications of learning derived from the\nlearning sciences and computed through data will be produced and circulated as a kind of govern-\ning knowledge.\nPearson's work in education demonstrates how methods themselves are socially produced: they\nhave past lives in disciplinary traditions; they are made by human hands with particular interests,\nguided by epistemological assumptions; they are selective, partial and always framed by the cul-\ntural, political and economic contexts in which they are deployed. Digital methods have become\nsocialized as expert ways of knowing, seeing and evaluating social phenomena. As the social prod-\nuct of human endeavours, methods are also socially productive: they frame a problem to be seen in\na particular way; they visualize results to shape interpretation; they produce classifications; and\nthey direct attention towards particular patterns and translate associations and connections into\ninsights that might be acted upon through particular forms of social action. The social production\nand social productivity of new software-mediated digital methods now being deployed from the\nCompPsy laboratories of methodological experts such as Pearson require detailed interrogation as\nthey exert social, political and material effects in digital education governance. Pearson's methodo-\nlogical complex is integral to its construction of new models and classifications of learning--a new\nknowledge infrastructure for knowing and acting in educational institutions--and represent the\ndisplacement of social scientific ways of knowing and intervening in the learning that takes place\nin schools to emerging data scientific modes of collecting, calculating and classifying learning.\nDeclaration of conflicting interest\nThe author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or\npublication of this article.\nFunding\nThe author(s) disclosed receipt of the following financial support for the research, authorship, and/or publica-\ntion of this article: This work was supported by the Economic and Social Research Council (grant ref: ES/\nReferences\nBall S (2012) Global Education Inc. New Policy Networks and the Neoliberal Imaginary. London: Routledge.\nBall S and Junemann C (2015) Pearson and PALF: The mutating giant. Brussels: Education International.\nAvailable at: http://download.ei-ie.org/Docs/WebDepot/ei_palf_publication_web_pages_01072015.pdf\nBarber M with Ozga J (2014) Data work: Michael Barber in conversation with Jenny Ozga. In: Fenwick T,\nMangez E and Ozga J (eds) Governing Knowledge: Comparison, Knowledge-Based Technologies and\nExpertise in the Regulation of Education. London: Routledge, pp.75\u00ad85.\nBehrens J (2013) Harnessing the currents of the digital ocean. In: the annual meeting of the American\nEducational Research Association, San Francisco, CA, April 2013.\nBowker G (2005) Memory Practices in the Sciences. London: MIT Press.\nBowker GC and Star SL (1999) Sorting Things Out: Classification and its Consequences. London: MIT Press.\nBurrows R and Savage M (2014) After the crisis? Big Data and the methodological challenges of empirical\nsociology. Big Data & Society 1(1): 1\u00ad6.\nDavies W (2015) The return of social government: From `socialist calculation' to `social analyt-\nics'. European Journal of Social Theory. Epub ahead of print 15 April 2015. DOI: http://dx.doi.\nDecuypere M, Ceulemens C and Simons M (2014) Schools in the making: mapping digital spaces of evi-\nDiCerbo KE and Behrens JT (2014) Impacts of the Digital Ocean. Austin, TX: Pearson.\nEdwards P, Jackson S, Chalmers M, et al. (2013) Knowledge Infrastructures: Intellectual Frameworks and\nResearch Challenges. Ann Arbor, MI: Deep Blue.\nEdwards R and Fenwich T (this issue) Exploring the role of smart technologies on professional responsibili-\nties and education. European Educational Research Journal.\nFenwickT,MangezEandOzgaJ(2014)GoverningKnowledge: Comparison, Knowledge-Based Technologies\nand Expertise in the Regulation of Education. London: Routledge.\nGillespie T (2014) The relevance of algorithms. In: Gillespie T, Boczkowski PJ and Foot KA (eds) Media\nTechnologies: Essays on Communication, Materiality and Society. London: MIT Press, pp.167\u00ad193.\nGorur R (2013) My school, my market. Discourse: Studies in the Cultural Politics of Education 34(2):\nHardy I (2015) A logic of enumeration: the nature and effects of national literacy and numeracy testing in\nHill P and Barber M (2014) Preparing for a Renaissance in Assessment. London: Pearson.\nHogan A, Sellar S and Lingard B (2015) Network restructuring of global edu-business: The case of Pearson's\nefficacy framework. In: Au W and Ferrare JJ (eds) Mapping Corporate Education Reform: Power and\nPolicy Networks in the Neoliberal State. London: Routledge, pp.43\u00ad64.\nKitchin R (2014) The Data Revolution: Big DATA, OPEN DATA, DATA INFRASTRUCTURES and their\nConsequences. London: Sage.\nLascoumes P and le Gales P (2007) Introduction: Understanding public policy through its instruments--from\nthe nature of instruments to the sociology of public policy instrumentation. Governance 20(1): 1\u00ad21.\nLaw J, Ruppert E and Savage M (2011) The double social life of methods. CRESC Working Paper No. 95,\nOpen University.\nLawn M (2013) The rise of data in education. In: Lawn M (ed.) The Rise of Data in Education Systems:\nCollection, Visualization and Use. Oxford: Symposium, pp.7\u00ad25.\nLawn M and Grek S (2012) Europeanizing Education: Governing a New Policy Space. Oxford: Symposium.\nLoveless A and Williamson B (2013) Learning Identities in a Digital Age: Rethinking Creativity, Education\nand Technology. London: Routledge.\nLupton D (2015) Digital Sociology. London: Routledge.\nMackenzie A (2013) Programming subjects in the regime of anticipation: Software studies and subjectivity.\nMarres N (2012) The redistribution of methods: On intervention in digital social research, broadly conceived.\nMayer-Schonberger V and Cukier K (2013) Big Data: A Revolution that will change how we Live, Work and\nThink. London: John Murray.\nMoos L (2009) Hard and soft governance: The journey from transnational agencies to school leadership.\nOudshoorn N and Pinch T (2003) How users and non-users matter. In: Oudshoorn N and Pinch T (eds). How\nUsers Matter: The Co-construction of Users and Technology. London: MIT Press, pp.1\u00ad25.\nOzga J, Segerholm C and Simola H (2011) The governance turn. In: Ozga J, Dahler-Larsen P, Segerholm\nC et al. (eds). Fabricating Quality in Education: Data and Governance in Europe. London: Rouledge,\nPearson College London (2015) Intelligence Unleashed: How smarter digital tools can improve learning.\nPearson College London, 7 September 2015. Available at: https://www.pearsoncollegelondon.ac.uk/\nPiattoeva N (2015) Elastic numbers: national examinations data as a technology of government. Journal of\nPiety PJ, Behrens J and Pea R (2013) Educational data sciences and the need for interpretive skills. American\nEducational Research Association, 27 April\u00ad1 May 2013, San Francisco. Available at: http://www.slide-\nshare.net/PhilipPiety/education-data-sciences-and-interpretive-skills (accessed 24 September 2015).\nPiety PJ, Hickey DT and Bishop MJ (2014) Educational data sciences--framing emergent practices for analyt-\nics of learning, organizations and systems. In: LAK `14, 24\u00ad28 March 2014, Indianapolis, IN. Available\nat: http://edinfoconnections.com/wp-content/uploads/2014/01/Educational-Data-Sciences-Feb-9.pdf.\nPopkewitz TS (2012) Numbers in grids of intelligibility: Making sense of how educational truth is told.\nIn: Lauder H, Young M, Daniels H, et al. (eds) Educating for the Knowledge Economy? Critical\nRogers R (2013) Digital Methods. London: MIT Press.\nRose G, Degen M and Melhuish C (2014) Networks, interfaces and computer-generated images: learning\nfrom digital visualizations of urban regeneration projects. Environment & Planning D: Society & Space\nRose N (1999a) Governing the Soul: The Shaping of the Private Self. 2nd ed. London: Free Association\nBooks.\nRose N (1999b) Powers of Freedom: Reframing Political Thought. Cambridge: Cambridge University Press.\nRuppert E, Law J and Savage M (2013) Reassembling social science methods: The challenge of digital\nRuppert E, et al. (2015) Socializing big data: from concept to practice. CRESC Working Paper no. 138.\nAvailable at: http://www.cresc.ac.uk/medialibrary/workingpapers/wp138.pdf (accessed 24 September\nSavage M (2013) The `social life of methods': A critical introduction. Theory, Culture & Society 30(4): 3\u00ad21.\nSellar S (2014) Data infrastructure: A review of expanding accountability systems and large-scale assess-\nments in education. Discourse: Studies in the Cultural Politics of Education 36(5): 765\u00ad777.\nSellar S (2015) A feel for numbers: Affect, data and education policy. Critical Studies in Education 56(1):\nSiemens G (2013) Learning analytics: The emergence of a discipline. American Behavioral Scientist 57(10):\nWilliamson B (2015) Digital education governance: data visualization, predictive analytics and `real-time'\npolicy instruments. Journal of Education Policy. Epub ahead of print 15 April 2015. DOI: http://dx.doi.\nWoolgar S (1991) Configuring the user: The case of usability trials. In: Law J (ed.) A Sociology of Monsters:\nEssays on Power, Technology and Domination. London: Routledge, pp.57\u00ad99.\nAuthor biography\nBen Williamson is a lecturer in the School of Education at the University of Stirling. His research focuses on\ndigital data, software code and algorithms in the technologies and practices of education governance, and on\nthe influence of think tanks and policy innovation labs in education policy. His recent articles have been pub-\nlished in the Journal of Education Policy, Critical Policy Studies, and the Journal of Educational Administration\nand History."
}