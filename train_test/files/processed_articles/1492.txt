{
    "abstract": "We develop a new Gibbs sampler for a linear mixed model with a Dirich- let process random effect term, which is easily extended to a generalized lin- ear mixed model with a probit link function. Our Gibbs sampler exploits the properties of the multinomial and Dirichlet distributions, and is shown to be an improvement, in terms of operator norm and efficiency, over other com- monly used MCMC algorithms. We also investigate methods for the estima- tion of the precision parameter of the Dirichlet process, finding that maximum likelihood may not be desirable, but a posterior mode is a reasonable ap- proach. Examples are given to show how these models perform on real data.",
    "reduced_content": "The Annals of Statistics\n\u00a9 Institute of Mathematical Statistics, 2010\n \nWe develop a new Gibbs sampler for a linear mixed model with a Dirich-\nlet process random effect term, which is easily extended to a generalized lin-\near mixed model with a probit link function. Our Gibbs sampler exploits the\nproperties of the multinomial and Dirichlet distributions, and is shown to be\nan improvement, in terms of operator norm and efficiency, over other com-\nmonly used MCMC algorithms. We also investigate methods for the estima-\ntion of the precision parameter of the Dirichlet process, finding that maximum\nlikelihood may not be desirable, but a posterior mode is a reasonable ap-\nproach. Examples are given to show how these models perform on real data.\nOur results complement both the theoretical basis of the Dirichlet process\nnonparametric prior and the computational work that has been done to date.\n1. Introduction. Linear and generalized linear mixed models have become\nimportant statistical tools bringing more flexibility through the addition of random\neffects to the more traditional linear models. A general mixed effects model can be\nwritten as\n(Y1,...,Yn)  f (y1,...,yn\n|,1,...,n) =\ni\nf (yi\n|,i),\ni\n G, i = 1,...,n,\nwhere f and G are often taken to be normal. The addition of a link function turns\nthis into a generalized linear mixed model; we will discuss that below. The restric-\ntion of G to a normal distribution is sometimes thought to be limiting. For example,\nresearchers in the social sciences are uncomfortable with this assumption, wanting\na more flexible, possibly nonparametric structure here [Gill and Casella (2009)].\nAnother troublesome fact, as noted by Burr and Doss (2005), is that random ef-\nfects, unlike error terms, cannot be checked (there are no residuals). Thus we are\ntotally dependent on this uncheckable model assumption.\nOne way of relaxing this assumption is with a richer, nonparametric model for\n with a popular alternative being the Dirichlet process\ni\n DP(m,0), i = 1,...,n,\nKey words and phrases. Linear mixed models, generalized linear mixed models, hierarchical\nmodels, Gibbs sampling, Bayes estimation.\nwhere DP is the Dirichlet process with base measure 0 and precision parame-\nter m. By moving to this model we not only relax the normal assumption, but also\nprovide a richer model for the random effects. Such a model has potential for cap-\nturing more types of variability in those effects with the possible end result of more\nprecise estimates of the fixed effects. Here we will look at ways to fit such models,\nfocusing on methods of estimating the precision parameter m, and evaluating and\nimproving on Gibbs samplers for the models.\n1.1. Background. Dirichlet process mixture models were introduced by Fer-\nguson (1973) who defined the process and investigated basic properties. Black-\nwell and MacQueen (1973) showed that the marginal distribution of the Dirichlet\nprocess is equal to the distribution of the nth step of a P\u00f3lya urn process. In partic-\nular, they proved that for 1,...,n i.i.d. from G  DP, the joint distribution of\n is a product of successive conditional distributions of the form\ni\nm\n(l\n= i),\nwhere  denotes the Dirac delta function.\nOther work that characterizes the properties of the Dirichlet process includes\nimportance for our development is that of Lo (1984), who derives the analytic\nform of a Bayesian density estimation that is generated by convoluting a known\ndensity kernel with a Dirichlet process, and Liu (1996), who derives an identity for\nthe profile likelihood estimator of m.\nThe implementation of the Dirichlet process mixture model has been made\nfeasible by modern methods of Bayesian computation and efficient algorithms.\noped estimation techniques and sampling algorithms, and Neal (2000) provided an\nextended and more efficient Gibbs sampler.\nNote that the representation (1) induces clusters in the random effects since with\npositive probability the value of i is equal to one of the previous values. McCul-\nlagh and Yang (2006) showed that the marginal distribution of these Dirichlet clus-\nters can be derived using cycles of integers and exchangeability based on Pitman\n(1996), and an exchangeable cluster process can be generated by a standard Dirich-\nlet allocation scheme. Quintana and Iglesias (2003) show that the joint marginal\ndistribution of the Dirichlet observations can be expressed as a product partition\nmodel. Such models were introduced by Hartigan (1990) and Barry and Hartigan\n(1992), and are based on modeling random partitions of the sample space.\nIt is interesting to note that with n observations from a Dirichlet process with\nprecision parameter m, the marginal distribution of a partition {n1,n2,...,nk\n},\nwhere j\nnj\n= n,nj\n 1, is given by\n(m)\n(m + n)\nmk\nk\n(nj ),\nwhich is a normalized probability distribution on the set of all partitions of n ob-\nservations. This is the same distribution derived by McCullagh and Yang (2006)\nusing cycles of integers (which can be related to the partitions) and has been used\ndistribution on clusters for a Bayesian clustering algorithm.\nMost of theoretical and computational work for the Dirichlet process mixture\nmodels focus on the efficient estimation of 0. For the Dirichlet prior, we also need\nto consider the precision parameter m because it strongly influences the number\nof distinct components, which is the distribution of the underlying random effects.\nThe estimation of m has had difficulties in implementation due to computational\nintractability. The number of distinct components is not known in practice and it\ncan be changed if new data are observed. Doss (2008) notes that the precision\nparameter is typically the most difficult to estimate or defend as a fixed value.\nThe approach of Liu (1996), for the estimation of m, is to use sequential im-\nputation to estimate m using maximum likelihood and treating the subclusters as\nbut did not investigate the properties of the solution. Our approach is based on us-\ning marginal or profile likelihood for m, and using the Gibbs sampler to estimate\nthe model parameters. This is a variation of Casella (2001) who showed that, using\nan empirical Bayes approach, the hyperparameters can be estimated in a computa-\ntionally feasible way.\n1.2. Summary. In this paper, for a mixed Dirichlet random effects model, we\ndevelop algorithms for estimation of the precision parameter and MCMC algo-\nrithms for fitting the models. We focus on linear models but also show how to ex-\ntend our results to a generalized Dirichlet process mixed model with a probit link\nfunction. In Section 2 we develop our model and its probit extension using a new\nparameterization of the Dirichlet mixed model. Section 3 shows how to estimate\nthe precision parameter, m. There we see that there is, in fact, not much informa-\ntion in the data about m (as it only depends on the size of the subclusters). We find\nthat the MLE of Liu (1996) can be unstable and show how to obtain a more stable\nposterior mode estimate. Section 4 derives a Gibbs sampler for the model para-\nmeters and the subclusters of the Dirichlet process, and we use our new parame-\nterization of the hierarchical model to derive a new Gibbs sampler that more fully\nexploits the structure of the model and mixes very well. We can adapt the results\nof Hobert and Marchev (2008) to establish that our sampler is an improvement,\nin terms of operator norm and efficiency, over other commonly used algorithms.\nSection 6 given details for the estimation of the precision parameter, and Section\n7 contains illustrative applications. Section 8 summarizes these contributions and\nadds some perspective, and there are a number of technical appendices.\n2. Models and likelihoods. A general random effects Dirichlet model can be\nwritten as\n(Y1,...,Yn)  f (y1,...,yn\n|,1,...,n) =\ni\nf (yi\n|,i),\ni\n DP(m,0), i = 1,...,n,\nwhere DP is the Dirichlet process with base measure 0 and precision parame-\nter m. The vector  contains all of the model parameters which we will address a\nbit later.\nApplying the Blackwell\u00adMacQueen formula (1), we can calculate the likelihood\nfunction, which by definition is integrated over the random effects, as\nL(|y) = f (y1,...,yn\n\u00b7\u00b7\u00b7 dn,\nwhere\n(1,...,n) =\nn\nI(j\n= i)\n.\nTheorem 1, we can evaluate this integral to get\nL(|y) =\n(m)\n(m + n)\nn\nmk\nC:|C|=k\nk\n(nj ) f y(j)\n|,j 0(j )dj ,\nwhere C defines the subclusters, y(j) is the vector of yis that are in subcluster j and\nj is the common parameter for that subcluster. There are Sn,k different partitions\nC, the Stirling number of the second kind.\nA partition C clusters of the sample of size n into k groups, k = 1,...,n, and\nwe call these \"subclusters\" since the grouping is done nonparametrically rather\nthan on substantive criteria. That is, it is likely that any real underlying clusters\nwould be broken up into multiple subclusters by the nonparametric fit since there\nis little penalty for over-separation. Recall that the regular GLMM assumes that\nthe random effect is are independent and identically distributed with the normal\n\n). However, the subclustering assigns different normal para-\nmeters across groups and the same parameters within groups; cases are i.i.d. only\nif they are assigned to the same subcluster.\n2.1. A matrix representation of subclusters. Each partition C can be associ-\nated with an n \u00d7 k matrix A defined by A = (a1\n,...,an\n) where ai is a 1 \u00d7 k\nvector of all zeros except for a 1 in one position that indicates which group the ob-\nservation is from. Note that the column sums of A are (n1,n2,...,nk), the number\nof observations in the groups, and there are, of course, Sn,k such matrices. Specifi-\ncally, if the partition C has groups {S1,...,Sk\n}, then if i  Sj , i\n= j and the ran-\ndom effect can be rewritten as  = A. For example, if S1\n\n\n\n\n\n.\n.\n.\n\n\n\n\n\n=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n .\nWe then have\nC:|C|=k\nk\n(nj ) f y(j)\n|,j 0(j )dj\n=\nk\n(nj ) f (y|,A)0()d,\nwhere Ak is the set of all matrices A and j\n 0, independent. If we define\nf (y|,A) = f (y|,A)0()d,\nthe likelihood function is\nL(|y) =\n(m)\n(m + n)\nn\nmk\nk\n(nj )f (y|,A).\nNote that if the integral in (6) can be done analytically, as will happen in the nor-\nmal case discussed next, we have effectively eliminated the random effects from\nthe likelihood, replacing them with the A matrices, which serve to group the ob-\nservations.\n2.2. Marginalizing the random effects. Start with a normal linear model,\nY|  N(X + ,2I),\nwhere  = (1,...,n) , i\n DP(m,N(0,2)), i = 1,...,n, independent.\nWhen we introduce the A matrices we get the models\nY|,A  N(X + A,2I),   Nk(0,2I),\nand marginally, Y is multivariate normal with\nEY = X, VarY = 2I + 2AA .\nMoreover, in this model we can analytically marginalize out some of the model\nparameters. See Section 4.\n2.3. Consistency. We briefly discuss posterior consistency of these models,\nnoting that they have been extensively examined by Ghosal and co-authors [see,\n(2009) defines a Mixture of Dirichlet Process (MDP), where the Dirichlet process\nis the error distribution, with possibly additional hyperparameters for the base mea-\nsure. Alternatively, there is the Dirichlet Process Mixture (DPM)3 where, for a\nparametric density, we assume there is a latent variable from a unknown and un-\nrestricted distribution, modeled with a Dirichlet process prior. The famous incon-\nsistency result of Diaconis and Freedman (1986), and the results of Doss (1985a,\n1985b), are for the MDP model while our Dirichlet random effects model is an\nexample of DPM with a normal density for the observations.\nGhosal (2009) showed that for the MDP model, the conditions of the general\nconsistency theory of Schwartz (1965) are not satisfied due to the discreteness\nof the resulting Dirichlet likelihood; thus the posterior from the MDP model can\nbe inconsistent. However, the DPM likelihood is smooth enough to satisfy the\nconditions of Schwartz's theorem, and thus posterior consistency holds. Therefore,\nit follows that the posterior of our proposed Dirichlet random effects model is\nconsistent.\nWe conducted a small simulation study to examine the behavior of the posterior\nestimates of the coefficients in a linear DPM model. In this simulation, we fix\nthe number of subclusters, k, to be 20% of sample size, and the concentration\nparameter m by using (18). Other parameters are fixed at 2 = 1, 2 = 4 and  =\n(0,1) = (1,2) . From Table 1 we see that the standard deviations get smaller as\nsample size bigger, and the estimates are getting close to the true value as n gets\nbigger, illustrating the posterior consistency for the linear Dirichlet random effects\nmodels.\nEstimation of the coefficient parameters: Posterior means and standard deviations\n3It appears that the terms Mixture of Dirichlet Process and Dirichlet Process Mixture and not used\nunambiguously in the literature.\n2.4. A probit mixed Dirichlet random effects model. A generalized linear\nmixed model (GLMM) can be specified to accommodate outcome variables con-\nditional on mixtures of possibly correlated random and fixed effects [Breslow and\nClayton (1993)]. For example, assume that there is a Bernoulli selection process\nwhere we observe Yi according to\nYi\n Bernoulli(pi), i = 1,...,n,\nwhere yi is 1 or 0; thus pi\n= E(Yi) is the probability of a success for the ith\nobservation. Moreover, using a link function g(\u00b7), we can express the transformed\nmean of Yi as a linear function,\ng(pi) = Xi + i,\nwhere Xi are covariates associated with the ith observation,  is the coefficient\nvector, and i is a random effect accounting for subject-specific deviation from\nthe underlying model. The is are usually assumed to be distributed as N(0,2\n\n).\nVariations of GLMMs were used by Dorazio et al. (2008) to model spatial het-\nerogeneity in animal abundance, and Gill and Casella (2009) modeled political\nscience data by using a GLMM with an ordered probit link. For Bayesian infer-\nence, Albert and Chib (1993) used a Gibbs sampler by introducing a latent variable\ninto the model and noted that the probit model on the binary response is connected\nwith the normal linear model on a continuous latent data response. Mukhopadhyay\nand Gelfand (1997) used a fully Bayesian approach to fit generalized linear models\nwith Dirichlet random effects, and Ghosh et al. (1998) proposed hierarchical Bayes\ngeneralized linear models for longitudinal GLMMs in small area estimation, and\nprovided a general theorem that ensures the propriety of posteriors under diffuse\npriors.\nAlthough we can proceed to develop estimators with a general link function (9),\nchoosing g to be the probit link greatly simplifies things, which is what we will\ndo. If we introduce the latent variable\nUi\n N(Xi + i,2),\nthen defining Yi\n= I(Ui > 0) results in the probit model. Thus, if we consider the\nhierarchy to start with the Ui, we are in exactly the model of Section 2.2. To fit\nthe probit model we then add an additional step to the Gibbs sampler to generate\nthe Ui conditional on the remaining parameters. This results in a truncated normal\nrandom variable generation, with details in Appendix A.2.\n3. Estimating the precision parameter. In this section we look at the per-\nformance of the maximum likelihood estimates of m using both profile likelihood\nand marginal likelihood. We find that these estimates can be unstable, and hence\nsuggest an alternative based on a posterior mode, which we ultimately implement\nwith importance sampling in Section 6.\n3.1. Maximum likelihood estimates. From (7) define\nLk(|y) =\nk\n(nj )f (y|,A).\nThen, as in Liu (1996) or Doss (1994), we can obtain a profile likelihood estimate\nof m. That is, for each fixed value of  we can differentiate the log of (7) and set it\nequal to 0 to obtain the profile MLE as the solution to\nn\nkmk-1Lk(|y)\nn\nmkLk(|y)\n=\nn\nwhich defines ^\nm(), the profile MLE.\nThis development can be easily modified to obtain the marginal likelihood es-\ntimate of m, ^\nm. Under the usual regularity conditions, when ^\nm and ^\nm( ^\n) are both\nconsistent estimates, we would expect that the marginal MLE would be a more\nstable estimate of the precision parameter.\nStarting from (11) we can also integrate over  to obtain the marginal likelihood,\nLk(y) = Lk(|y)d.\nThe same development as above will lead to the expression (12) with Lk(|y)\nreplaced by Lk(y).\nWe also note that expressions for the approximate variances of either ^\nm or ^\nm( ^\n)\ncan be easily attained from the second derivative of the appropriate log likelihood\nevaluated at the estimate.\n3.2. Solving the likelihood equations. Note that, for either profile or marginal\nlikelihood, we can write the log likelihood function as\n(m) = log\nn\nmkck\n-\nn\nlog(i - 1 + m),\nwhere ck\n= Lk(|y) for profile likelihood and ck\n= Lk(y) for marginal likelihood.\nThe derivative of the log-likelihood is given by\n\nm\n(m) =\nn\nn\nmkck\n-\nn\n.\nIt is straightforward to show that\nlim\n\nm\n(m) =\n-\nn\n,\nlim\nm\nsgn\n\nm\n(m) = sgn\nn\ncn\n,\nwhere sgn(\u00b7) is the sign of the function. Note that limm\n\nm\n(m) = 0, but the\ndirection of approach is important. The signs of the derivatives at the extremes are\nonly functions of the ratios of the ck, the pieces of the likelihood. (We can assume,\nwithout loss of generality, that k\nck\nWe would typically expect the sign of the derivative at m = 0 to be positive.\nOtherwise one of two cases could occur. If the derivative starts out negative and\nnever changes sign, it will be the case that ^\nm = 0 which implies that the Dirichlet\nmodel collapses back to the base model. If not, the derivative would cross from\nnegative to positive, implying that a solution to the likelihood equations could\nresult in a minimum, and thus we must exercise more care in finding the MLE.\nIn either case, the sign of the derivative at m = 0 depends on the values of the\nlikelihoods with partition sizes k = 1 and k = 2.\nWe would also like the limiting sign as m   to be negative, otherwise it\ncould be the case that ^\nm =  or, depending on the sign at 0, there could be multi-\nple interior extrema. If cn is close to 0, then the limit can be positive. Note also that\nthe first term of the sign of the derivative equals (n-1)(n-2)\n, so for the sign of the\nderivative to negative, cn-1\ncn\nshould be greater than (n-1)(n-2)\n, a number that grows\nrapidly with the sample size. Thus, the sign of the derivative at m =  depends\non the likelihoods for partition sizes k = n - 1 and k = n, and the sample size n.\nAs an illustration of the possible shapes of the likelihood function, we consider\na number of simple cases for n = 6:\n\nm\n(m) is  so m = ;\n\nm\n(m) is  so m = 0;\n\nm\n(m) is - + - so there is a minimum\nand maximum;\n\nm\n(m) is - + so there is a unique minimum;\n\nm\n(m) is + - so there is a unique maximum.\nThus the likelihood function can have a variety of shapes, which are also illustrated\nin Figure 1. Moreover, we also note that the likelihood of m can be very flat,\nmeaning that there is insensitivity to the value of the MLE. Referring to (15), the\nMLE actually corresponds to finding the m that solves this equation. Liu (1996)\nreferred to this as the equating of prior and posterior means (where the expectation\nis taken using the discrete distribution with weights mkck) as the right-hand side\nof this equation can be interpreted as a prior number of clusters. For example, if\nFIG. 1. Log likelihood functions for a selection of configurations of component likelihoods given\nin (17). The vectors c are the configuration of the marginal likelihoods.\nwe define  by\n =\nn\nm\n,\nthen  is the expected number of prior clusters. Even though ^\nm can be quite vari-\nable, there is less variability in ^\n = n\n^\nm\n^\n.\n3.3. Posterior mode estimates. Since the likelihood of the precision parameter\ndepends on the likelihoods from the sub-cluster size k = 1,2,n - 1 and n, this\nreflects an insensitivity to the likelihood and the sample size n. For example, when\nLk are equal for all k, limm0\n\nm\n(m) > 0, but limm\n\nm\n(m) > 0. Thus, in\nthis case, we easily get the MLE as ^\nm = .\nGiven the potential problems with using and MLE for m, we consider a prior\non m that results in a unique value of the posterior mode. One of the candidates is\na gamma distribution with the shape parameter a and scale parameter b. Using the\nprior g(m) = 1\n(a)ba\nma-1e-m/b we have\nL(|y) =\n(m)\n(m + n)\ng(m)\nn\nmk\nk\n(nj )f (y|,A).\nIf we now take logs and differentiate, this amounts to adding the factor a-1\nm\nb\nto (15). The result of this is that the derivative of this log posterior increases from\nm = 0 and decreases as m  , guaranteeing an interior global maximum. If we\nintegrate (19) we then get the marginal posterior for m, which behaves in a similar\nway.\n3.4. Simulation study of a linear Dirichlet mixed model. Using the normal\nlinear model of Section 2.2, we conducted a simulation study with a gamma prior,\nm  gamma(a,b) to study the behavior of the estimates of m. We take n = 6,\n = 3,  = 0, 2 = 1 and  = (1,2,3). The Dirichlet process on the random effect\n has precision parameter m and base distribution G0\n= N(0,2). We simulated\n100 data sets by generating X1 and X2 from N(0,1) using the fixed design matrix\nto generate Y.\nIn this setting, k = 1,...,6, and from the calculation of the Stirling number\nThe matrices A associated with these subclusters can be generated, and we sum\nup the likelihood with all possible subclusters for each k for the profile likeli-\nhood.\nWe estimate m with various settings of the prior mean and variance. With n = 6,\n = 3 and m = 5, the solution of m from equation (18) is m = 1.70. The numerical\nsummary is given in Table 2 and the histogram of the estimated k is given in\nFor n = 6,  = 3, m = 5 and various values of the prior parameters, we estimate the precision\nparameter m and its transformed value k. Standard errors are in parentheses\nCondition m \nFIG. 2. A histogram of the estimated k with prior mean 2 and variance 10.\nFigure 2. For the estimation of , we use the posterior mean of m, m and calculate\n^\n by using equation (18).\nFrom Table 2, we observe that if the prior mean ab is close to m = 1.70, we get\na good estimate of  that is close to the fixed  = 3. However, if the prior variance\nis too big, then the estimate of  is less precise. Also, from Figure 2, we observe\nthat the histogram of the estimated  with ab = 2 is almost symmetric at  = 3.10\nwith small variance.\n4. A Gibbs sampler for the model. We describe a general Gibbs sampling\nscheme that iteratively generates A matrices and then model parameters assuming\nthat m is fixed at either the MLE or posterior mode. Details on the estimation of m\nare in Section 6.\nStart with the joint likelihood,\nL(,A|y) =\n(m)\n(m + n)\ng(m)mk\nk\n(nj )f (y|,A).\nWith a flat prior on A and (), we get the joint posterior distribution as\n(,A|y) =\nmkf (y|,A)()\nA\nmkf (y|,A)()d\n.\nBased on (21), the full conditional posteriors of  and A are\n(|A,y) =\nmkf (y|,A)()\nmkf (y|,A)()d\n=\nf (y|,A)()\nf (y|,A)()d\n,\n(A|,y) =\nmkf (y|,A)()\nA\nmkf (y|,A)()\n=\nmkf (y|,A)\nA\nmkf (y|,A)\n.\nWe now outline a Gibbs sampler that will generate from these conditionals by\ngenerating n \u00d7 n A matrices and recovering the subcluster size through marginal-\nization.\nFor t = 1,...,T , at iteration t:\n1. Starting from ((t),A(t)),\n(t+1)   |A(t),y .\n,...,q(t+1)\nn\n Dirichlet n(t)\n+ r1,...,n(t)\nk\n+ rk,rk+1,...,rn ,\nA(t+1)  mk f y|(t+1),A\nn\n\u00b7\u00b7\u00b7nn\nn\nj\nnj ,\n+\u00b7\u00b7\u00b7+nn\n= n with k of the nj\n> 0. Sampling of the model parameters \nin (22) is straightforward (Appendix A.1), so we will concentrate on the sampling\nof A and q.\nThe matrix A is n \u00d7 n with column sums n1,...,nn, and the columns with zero\nsums will be removed to obtain an n \u00d7 k matrix, according to Appendix B. Here\nwe keep the rj as a general choice, but we will see in Section 5.2 and Appendix C.1\nthat we should choose rj\n= 1 for all j.\nThe transition kernel of this Markov chain is\nk((,A),( ,A )) = ( |A,y)\nQ\nP(A |q, )f (q|A)dq\nwith\nP(A|q,) =\nmkf (y|,A) n\nn\nqnj\nj\nA\nmkf (y|,A) n\nn\nqnj\nj\nand\nf (q|A) =\n(n + n\nrj )\nk\n(nj\n+ rj ) n\n(rj )\nk\nqnj +rj -1\nj\nn\nj\n.\nNow we take rj\n= 1, and then we can express the multinomial as\nn\n\u00b7\u00b7\u00b7nk\nk\nqnj\nj\n=\nn\n(nj\nn\nqnj\nj\nbecause the zero valued nj s take care of themselves. With this choice of rj , the\ntransition kernel has (,A|y) as its stationary distribution; details are given in\nAppendix C.1.\n5. Generating the subclusters. In this section we discuss two aspects of gen-\nerating the subclusters. First, we address how to generate according to (23). Then\nwe examine convergence rates and establish that our sampler is an improvement,\nin terms of operator norm and efficiency, over commonly used algorithms.\n5.1. Generating the matrix A. Generation of the matrix A can be accom-\nplished by using a Gibbs sampler on the rows of A. Recall that ai,i = 1,...,n\nare the rows of A. Define A-i to be the matrix A with the ith row removed and\na( )\ni\nto be a vector of zeros with a 1 in the th position. The matrix (a( )\ni\n,A(t)\n-i\n) has\ncolumn sums n( )\nj\nwith k( ) of n( )\nj\n> 0. Then for i = 1,...,n,\nP ai\n= a( )\ni\n|A(t)\n-i\n\nmk( )\nf (y|,(a( )\ni\n,A(t)\n-i\n)) n\nn( )\n\u00b7\u00b7\u00b7n( )\nn\nn\nj\n]n( )\nj\nn\nmk( ) f (y|,(a( )\ni\n,A(t)\n-i\n)) n\nn( )\n\u00b7\u00b7\u00b7n( )\nn\nn\nj\n]n( )\nj\n,\nwhere we update A(t)\n-i\nin the usual (Gibbs sampling) way.\nAlternatively, we can use a Metropolis\u00adHastings algorithm with a candidate\ntaken from a multinomial/Dirichlet as described in Appendix B.2. Based on the\nvalue of the qj in (23) we generate a candidate A from the multinomial and then\nremove the columns with column sum 0. That is, generate an n \u00d7 n matrix where\neach row is a multinomial, and the effective dimension of the matrix, the size of\nthe subclusters, are the non-zero column sums. Deleting the columns with column\nsum zero is a marginalization of the multinomial distribution. The probability of\nthe candidate follows from Appendix B.2, and the Metropolis\u00adHastings step is\nthen done.\n5.2. Convergence properties. From (23), we see that given the subclusters,\nthe sampling of the model parameters from (|A,y) is straightforward. Thus,\nin investigating convergence we are only concerned with the convergence of the\nMarkov chain on the subclusters. Clearly, if convergence is improved for this part\nof the chain it will then transfer to the entire chain.\nIf we ignore the model parameters, then we are concerned with convergence of\nthe chain to the stationary distribution (2), that is,\n(A) = (n1,...,nk) =\n(n)\n(n + m)\nmk\nk\n(nj ),\nand first we derive the full conditionals in the following way. Start with (n1,...,\nnk) with sum n - 1, and generate a new row of the A matrix. The matrix A is\nn \u00d7 k, and when we generate a new row, either the dimension will remain n \u00d7 k or\nwe will increase to n \u00d7 k + 1. If we write a = {aj\n}, then\nP(aj\n(n)\n(n + m)\nmk (nj\nk\nj =j\n(nj\n) for j = 1,...,k,\nP(aj\n(n)\n(n + m)\nk\n(nj\n) for j = k + 1,\nP(n1,...,nk) =\n(n - 1 + m)\nmk\nk\n(nj ).\nThis results in\nP(aj\n\n\n\n\n\nnj\n, for j = 1,...,k,\nm\n, for j = k + 1,\nwhich are exactly the full conditionals derived by Neal (2000), his equation (3.6)\nignoring the model parameters, using a limit argument starting from a finite-\ndimensional Dirichlet. The Gibbs sampler based on (26) is the basis for most of\nthe eight algorithms that he describes; some of which were originally developed\nby other authors.\nThe Gibbs sampler resulting from (23), ignoring the model parameters, is\nP(A|q) =\n( (n)/ (n + m))mk k\n(nj ) n\nk\nqnj\nj\nA\n( (n)/ (n + m))mk k\n(nj ) n\nk\nqnj\nj\nand\nf (q|A) =\n(n + n\nrj )\nk\n(nj\n+ rj ) n\n(rj )\nk\nqnj +rj -1\nj\nn\nj\n,\nand a similar argument shows that the full conditionals from this chain are\nP(aj\n\n\n\n\n\n\n\nnj\nnj\nqj\n, for j = 1,...,k,\nm\nqj , for j = k + 1,...,n.\nNotice that for qj\n= nj\n+ 1, j < k and qj\n= 1,j > k (the normalization does\nnot matter), we see that Neal's Gibbs sampler (26) is the same as (28). We can\ntherefore write the transition kernel of (26) as\nKN (A,A ) = P(A |q0)g(q0|A),\nwhere g(q0|A) is a point mass. In this notation, the kernel of (28) is\nK(A,A ) =\nQ\nP(A |q)f (q|q0)g(q0|A)dq,\nwhere f (q|q0) is the same as f (q|A) in (27). The vector q0 merely serves to pass\nthe nj .\nWe are now in the setup of Hobert and Marchev (2008) and can use their Theo-\nrem 3 to establish the superiority of K(A,A ) over KN (A,A ).\nTHEOREM 1. For the transition kernels KN (A,A ) and K(A,A ), both with\nstationary distribution given by (25):\n1. K(A,A ) dominates KN (A,A ) in operator norm;\n2. K(A,A ) dominates KN (A,A ) in the efficiency ordering of Mira and Geyer\nfunction h, the asymptotic variance is smaller using K(A,A ) than using\nKN (A,A ).\nPROOF. In the terminology of Hobert and Marchev (2008), KN (A,A ) is in\nthe form of a Data Augmentation (DA) algorithm, and K(A,A ) is a parameter-\nexpanded version of KN (A,A ). The theorem will be established if we can show\nthat K(A,A ) is reversible. This is straightforward as K(A,A ) is, itself, a DA\nalgorithm since K(A,A ) =\nQ\nP(A |q)f (q|A)dq. To be specific, if we take\nrj\n= 1, then K(A,A ) satisfies the detailed balance condition K(A,A )(A) =\nK(A ,A)(A ) (Appendix C.2).\nTherefore, in the estimation of any square integrable function h, using (28) will\nresult in a smaller variance than obtained by using (26).\n5.3. Assessing the improvement. The results of Section 5.2 show that our sam-\npler should mix better than \"Stickbreaking\" as defined by (26). Although we do not\nknow the amount of potential improvement, the results of Roy and Hobert (2007)\nsuggest that there are substantial gains to be had.\nTo assess the amount of improvement of the Gibbs sampler, the following sim-\nulation study was done. For the linear Dirichlet mixed effects model described in\nSection 2.2 we simulated four data sets. For n = 100 we took A matrices corre-\nFIG. 3. For n = 100, comparison of variance estimates using the \"Stickbreaking\" algorithm of (26)\n(S-B, dashed line) and the algorithm given in (28), the \"Dirichlet Random Effects Model\" (DREM,\nsolid line). The four plots correspond to four underlying distributions of 1, 5, 25 and 100 groups.\nTwenty Markov chains were run, and the variance of the 20 estimates was calculated at each of the\n500 iterations. At each iteration we calculated the variance of the 20 cumulative\nmeans which are displayed in Figure 3.\nAs can be seen, the improvement over the \"Stickbreaking\" algorithm can be\nquite substantial; in most cases we see almost a 50% percent improvement. Al-\nthough we are not claiming that this will hold in all cases, we have a clear indica-\ntion that substantial reduction in Monte Carlo variance can be attained.\n6. Importance sampling the precision parameter. To estimate the precision\nparameter m we want to work with a marginal likelihood function in the form\nof (7). Based on the development in the previous sections, we start with the mar-\nginal posterior,\n(m|y) =\n(m)\n(m + n)\ng(m)\nn\nmkfk(y),\nwhere\nfk(y) =\nk\n(nj )f (y|,A)()d.\nTo take advantage of this functional form for the estimation of m, we want to cal-\nculate fk(y) for each k = 1,...,n. However, this strategy is difficult to implement\nfor a number of reasons. First, it would necessitate running a full Gibbs sampler\n(or other MC technique) for all k = 1,...,n. Second, the implementation is prob-\nlematic. For example, consider using an importance sampler based on simulating\nfrom the model\n  f (y|,A),\nai\n Multinomial(1,(q1,...,qk)), independent,\nq = (q1,...,qk)  Dirichlet(,...,),\nwhich leads to the joint posterior distribution\n,A  f (y|,A)\n(k)\nk\n()\nk\nj\ndq\n= f (y|,A)\n(k)\n(n + k)\nk\n(nj\n+ )\n()\n,\nwhere nj\n= i\naij . Unfortunately, there is no guarantee that nj > 0, and samples\nwith nj\n= 0 will have to be discarded.\nHowever, we can proceed as in (23), and modify (30) to use an n-dimensional\nDirichlet,\nq = (q1,...,qn)  Dirichlet(,...,),\nand then generate ai independently from this Dirichlet. We then eliminate from the\nA matrix all columns whose sum is zero. The resulting value of k has the distri-\nbution given in (31) because of the marginalization properties of the multinomial\nand Dirichlet.\nThe simulation strategy is the following. For t = 1,...,T we generate A(t)\naccording to (30) but using the n-dimensional Dirichlet, and then marginalize to\nthe number of nonzero nj . We then gather the A(t) according to their values of k.\nThen, for each k, if there are Tk matrices A of that size, we estimate fk by\nfk(y) =\nk\n(nj )f (y|,A)()d\n=\nk\n(nj )f (y|,A)()\nf (y|,A)()( (k)/ (n + k)) k\n( (nj\n+ )/ ())\n\u00d7 f (y|,A)()\n(k)\n(n + k)\nk\n(nj\n+ )\n()\nd\n\n(n + k) ()k\n(k)\nTk\nTk\nk\n(n(t)\nj\n)\n(n(t)\nj\n+ )\n= ^\nfk(y),\nwhere we see very clearly that m only depends on the nj . We now use ^\nfk(y) in\n(29) to obtain the marginal MLE of m.\nWe can further reuse these random variables for all k < k by randomly choosing\ntwo columns and adding them together. This results in an A matrix of one fewer\ndimension. Details are given in Appendix B.\n7. Application. In this section we use the Gibbs sampler for a generalized\nlinear mixed model with a Dirichlet process random effect term and probit link\nto analyze survey data from Scotland. On September 11, 1997, an overwhelming\n74.3% of Scottish voters approved of the establishment of the first Scottish na-\ntional parliament in nearly three hundred years, and on the same ballot the voters\ngave strong support, 63.5%, to granting this parliament taxation powers. This vote\nrepresents a watershed event in the modern history of Scotland which was a free\nand independent country separate from England until 1707. This vote is part of the\nLabour government's decentralization program and there is still uncertainty about\nthe future role of Scottish government with the United Kingdom and the Euro-\npean Union. What we are interested in here are those who subsequently voted for\nthe Conservative (Tory) party in Scotland and whether such a vote is intended to\nmitigate Labour's devolution program in Scotland.\nThe data come from the British General Election Study, Scottish Election Sur-\nfrom an interview with a Scottish national after the election. Our outcome vari-\nable of interest is their party choice in the UK general election for Parliament\nwhere we collapse all non-Conservative party choices (abstention, Labour, Liberal\nDemocrat, Scottish National, Plaid Cymru, Green, Other, Referendum) to one cat-\negory which produces 104 Conservative votes. The chosen explanatory variables\nare intended to explain this choice and include two measures of political efficacy:\nPOLITICS, which asks how much interest the respondent has in political events\n(increasing scale: none at all, not very much, some, quite a lot, a great deal), and\nREADPAP, which asks about daily morning reading of the newspapers (yes = 1 or\nno = 0). It is also important to establish party identity separate from vote choice,\nPTYTHNK, and how strong that party affiliation is for the respondent (categorical\nby party name), IDSTRNG (increasing scale: not very strong, fairly strong, very\nstrong).\nWe also look at respondents' views on various policy issues. The variable TAX-\nLESS asks if \"it would be better if everyone paid less tax and had to pay more\ntowards their own healthcare, schools and the like\" (measured on a five point in-\ncreasing Likert scale). DEATHPEN asks whether the UK should bring back the\ndeath penalty (measured on a five point increasing Likert scale). LORDS queries\nwhether the House of Lords should be reformed (asked as remain as is coded as\nzero and change is needed coded as one). The question SCENGBEN asks how eco-\nnomic benefits are distributed between England and Scotland with the following\nchoices: England benefits more = -1, neither/both lose = 0, Scotland benefits\nmore = 1. The important question INDPAR asks which of the following repre-\nsents the respondent's view on the role of the Scottish government in light of the\nnew parliament: (1) Scotland should become independent, separate from the UK\nand the European Union; (2) Scotland should become independent, separate from\nthe UK but part of the European Union; (3) Scotland should remain part of the\nUK, with its own elected parliament which has some taxation powers; (4) Scot-\nland should remain part of the UK, with its own elected parliament which has no\ntaxation powers and (5) Scotland should remain part of the UK without an elected\nparliament. Relatedly, SCOTPREF1 asks, \"should there be a Scottish parliament\nwithin the UK?\" (yes = 1, no = 0).\nFinally, we use three demographic explanatory variables: RSEX, the respon-\ndent's sex, RAGE, the respondent's age, RSOCCLA2, the respondents social class\n(7 category ascending scale), TENURE1, whether the respondent rents (0) or owns\n(1) their household and a categorical variable for church affiliation, measurement\nof religion is collapsed down to one for the dominant historical religion of Scotland\n(Church of Scotland/Presbyterian) and zero otherwise and designated PRESB.\nWe set 2 = 1 to establish the scale and provide an intuitive (standard) probit\nmetric. This decision appears to have little influence on the resulting posteriors and\nallows the  specification sufficient latitude to draw nonparametric information\nfrom the data. The parameters in the priors on  and 2 are chosen to make the\npriors sufficiently diffuse to allow the random effect to do its work. In previous\nwork [Gill and Casella (2009)], we observed little sensitivity to hyperparameter\nvalues.\nWe ran the Gibbs sampler for 5000 iterations disposing of the first 2000. All of\nthe common diagnostics (Geweke, Brooks\u00adGelman\u00adRubin, Heidelberger\u00adWelsh,\nFIG. 4. Cumulative mean plot, Scotland voting model.\ngraphics) point toward convergence of the Markov chain to its stationary distribu-\ntion. Figure 4 is a cumulative mean plot for each of the dimensions for the entire\nTable 3 provides quantiles for the posterior marginal distributions. We observe\nthat an interest in politics and regular reading of the newspapers increases the\nprobability of voting Conservative as does (not surprisingly) supporting less taxes\nand the return of the death penalty. We see the same positive effect for men versus\nwomen, older versus younger and homeowners versus renters. Those with stronger\nparty attachments are also more likely to vote for the Conservative party. Reform-\ning the House of Lords, Presbyterians and those affiliated with more liberal parties\nare less likely to vote Conservative.\nTwo results are surprising. First, those that think that economic policies bene-\nfit Scotland more than England are more likely to vote for the Conservative party\nwhich is much more aligned with English voters than Scottish voters in general.\nPerhaps there is a sense that Conservative voting brings attention to Scottish is-\nsues from the party. More surprisingly, favoring an independent party is positively\nassociated with voting Conservative through the model. This was our key variable\nof interest and the relationship is not in the direction expected. The Conservative\nparty is not generally favorable to devolution issues, so these voters are clearly\ncross-pressured. It is important to keep in mind that the new parliament has tax-\nation powers and thus diminishes the power of local council authorities who are\noverwhelmingly associated with the Labour and Scottish National parties. Thus a\nPosterior model quantiles, voting model\nConservative voter may welcome a more centralized taxation program with possi-\nbly less influence from these parties, at least at the local level.\nIn terms of model fit, notice that, aside from the constant, only two marginal\nposteriors do not have 90% HPD intervals bounded away from zero. It turns out\nthat by every common measure of fit the generalized linear mixed model with a\nDirichlet process random effect term outperforms a simple Bayesian probit model\nwith diffuse uniform prior distributions on the parameters. Indeed, when we com-\npare the lengths of credible intervals in Figure 5, we find that the Dirichlet model\nresults in uniformly shorter intervals than those of a normal random effects model.\nThus as we anticipated in Section 1, the richer random effects model is able to\nremove more extraneous variability resulting in tighter credible intervals. We take\nthis as evidence that the new procedure is capturing nonparametric information of\ninterest.\n8. Discussion. Our interest in models with Dirichlet random effects grew\nfrom modeling social science data, where scientists expressed concern over\nBayesian models that used informative priors. The Dirichlet process random ef-\nfects model helps to balance the information from the data and the belief of the\nresearcher while still allowing a normal-type interpretation (in terms of means and\nvariances). As noted previously, we are in agreement with the sentiment of Burr\nand Doss (2005), who note that random effects, unlike error terms, can not be\nchecked (there are no residuals). Thus a model with normal random effects is a\nmodel of convenience, and moving to a richer model such as the Dirichlet process\nFIG. 5. Comparison of 90% credible intervals for the Dirichlet random effects model (black) to\nthose from a normal random effects model (blue).\nis a step in relaxing unverifiable assumptions. In particular, the subclustering struc-\nture of the Dirichlet process may capture extra variation in the random effects that\nescape the normal random effects models. The fact that data analysis with the\nDirichlet random effect model often differs substantially from the normal random\neffects model, as noted in Section 7, supports this claim.\nRepresenting the subclustering structure through the symmetric binary matrix\nA is not new. For example, such an equivalence representation was noted by Mc-\nCullagh and Yang (2006). Here, the representation has proved useful not only in\nderiving alternative forms of the model but also in leading to an improved Gibbs\nsampler. The influence of the random effects, as modeled with the parameters \nand , is only felt through the matrix A, and in some cases these parameters may\nnot have to be generated (see the representation in Section 2.2). This again will\nlead to a more efficient Gibbs sampler.\nThe improvement in the Gibbs sampler, as described in Sections 5.2 and 5.3,\nappears to come with an increase in computational effort, as we want to start each\niteration with an n \u00d7 n matrix A. However, due to the binary structure of A, such\na matrix need never be generated. In particular, we can use the correspondence\nbetween a multinomial random variable and a discrete random variable to represent\nthe n \u00d7 n matrix A as an n \u00d7 1 vector . If X  Multinomial(1,(p1,...,pn)),\nwe create a discrete random variable X satisfying P(X = j) = pj . We then\nuse X to generate the rows of A. For example, if n = 6 and six samples of X\ngives the vector (2,2,1,1,3,1), this represents a matrix A with row 1 having a\n1 in column 2, row 2 having a 1 in column 2, etc., with the full matrix being the\nmatrix A in (5).\nWe started this project to investigate generalized linear models with Dirichlet\nrandom effects but quickly realized that dealing with m is of prime importance\nand concentrated on linear models to better understand the estimation. As we have\nseen, ordinary likelihood could be problematic which may be a result of the fact\nthat there is really very little information about m coming from the data. As we\nsaw in Section 3, the information in the model about m is only contained in the\nsubclusters, which makes it relatively important to check that the results of the\nmodel as somewhat insensitive to the value of the estimated m.\nFinally, we note that although we have concentrated on linear and probit models,\nthe results will apply directly to a wider class of generalized linear models. There\nare implementation problems with the Gibbs sampler that arise with models such\nas the logit, where one needs to use either a slice sampler, a Metropolis\u00adHastings\nstep or a demarginalization with the Kolmogorov\u00adSmirnov distribution [Andrews\nand Mallows (1974)]. We have looked at these implementations in Kyung, Gill and\nCasella (2009). However, these are all variations on the model and, when any gen-\neral link function such as (9) is used, the Gibbs sampler for A and the estimation\nof m will remain the same.\nA.1. A linear model. For given A, the likelihood function is given by\nWe add the following normal and inverted gamma (IG) priors:\nThen for fixed m and A, with A = 1\nA A, a Gibbs sampler of (,2,2,)\nis\nk\nn + p\n|y - X - A|2 +\nIf we marginalize out , the joint posterior distribution of (,2,2) is\n\nwhich leads to an alternate Gibbs sampler.\nA.2. A probit model. Here we need to consider the latent variable Ui such\nthat\nUi\n= Xi + i\n+ i, i\nand\nYi\n=\ni = 1,...,n.\nIt can be shown that Yi are independent Bernoulli random variables with the prob-\nability of success, pi\n= (Xi-i\n\n) where is the cdf of the standard Normal.\nFor given A, the likelihood function of model parameters and the latent variable\nis given by\n=\nn\n{I(Ui > 0)I(yi\n= 1) + I(Ui\n\u00d7\nwhere U = (U1,...,Un) and leads to the Gibbs sampler\nk\nn + p\n|U - X - A|2 +\nfor the model parameters. For the latent variable U, for i = 1,...,n,\nUi\n N Xi + (A)i,2 I(Ui > 0) if yi\nUi\n N Xi + (A)i,2 I(Ui\nHere, we can marginalize out  such that\n=\nn\n{I(Ui > 0)I(yi\n= 1) + I(Ui\n\u00d7\nwhere A = 1\nA A.\nIn this appendix we give the details for the calculation of marginal densities of\nthe Dirichlet, multinomial and their mixture.\nB.1. Dirichlet. Starting with an n-dimensional Dirichlet distribution, the mar-\nginal distribution of any k components is also Dirichlet. This corresponds to ex-\ntracting the rows with non zero column sums in the A matrix in (23). That is, if\n(q1,...,qn)  Dirichlet(r1,...,rn), then for k  n\nj=k\nqj ,qk+2,...,qn\n Dirichlet r1,...,rk-1,\nj=k\nrj ,rk+2,...,rn\nA special case of this result is the combining of two rows which is the marginal-\nization that we use in the calculation of the estimate of m (see Section 6).\nIf q = (q1,...,qn)  Dirichlet(r1,...,rn), then for any k and k + 1  n\nj=k\nqj ,qk+2,...,qn\n Dirichlet r1,...,rk-1,\nj=k\nrj ,rk+2,...,rn .\nB.2. Multinomial. For (X1,...,Xn)  Multinomial(1,(q1,...,qn)), mar-\nginalization of the Xis is compatible with the Dirichlet results of the previous\nsection. That is,\nn\nj=k\nXj\nqj .\nWe also have a similar result for the combining of two elements of the vector,\nthat is, if (X1,...,Xn)  Multinomial(1,(q1,...,qn)), then\nj=k\nXj ,Xk+2,...,Xn\nqj ,qk+2,...,qn .\nB.3. Multinomial\u00adDirichlet. Lastly, we see that these marginalization pat-\nterns persist when we combine the multinomial and Dirichlet. Let the matrix An\u00d7n\nhave each row be an independent multinomial as follows:\n(ai1,...,ain)  Multinomial(1,(q1,...,qn)), i = 1,...,n,\n(q1,...,qn)  Dirichlet(r1,...,rn),\nand then create the matrix A by adding together rows k + 1,...,n, marginalizing\n(q1,...,qn) in the same way. Then\nP(A) = P(A|q)f (q)dq\n=\n( n\nrj )\nn\n(rj )\nqnj\nj\nqj\nnk n\nj\ndqj ,\nwhere nj\n= i\naij ,1,...,k - 1 and nk\n= i\nn\nj=k\naij .\nIf we add together rows k and k +1 in the matrix A to obtain A, a similar result\nholds:\nP(A) =\n( n\nrj )\n(rj ) ( k+1\nj=k\nrj )\n(nj\n+ rj ) (nk\nj=k\nrj )\n(n + n\nrj )\n.\nC.1. Stationary distributions of (,A). From the transition kernel of (,A)\nA\nK((,A),( ,A ))(,A|y)d\n=\nA\n( |A,y)\nQ\nP(A |q, )f (q|A)dq(,A|y)d\n=\nQ\nmkf (y| ,A ) n\nk\nq\nnj\nj\nA\nmkf (y|,A) n\nk\nqnj\nj\n\u00d7\nA\nf (q|A)\nf (y| ,A)( )\nf (y| ,A )( )d\n\u00d7\nmkf (y|,A)()\nA\nmkf (y|,A)()d\nd dq,\nand note that the integral cancels f (y| ,A )( )d inside the sum over A.\nSo the integral becomes\nQ\nmkf (y| ,A ) n\nk\nq\nnj\nj\nA\nmkf (y|,A) n\nk\nqnj\nj\nA\nmkf (q|A)f (y| ,A)( )\nA\nf (y|,A)()d\ndq.\nNow take j\n= 1 for all j = 1,...,n so that\nf (q|A) =\nk\n(nj\nk\nqnj\nj\n=\nn!\nn\n\u00b7\u00b7\u00b7nk\nk\nqnj\nj\n.\nThis cancels out the denominator sum to leave\nn!\nQ\nmkf (y| ,A )( ) n\nk\nq\nnj\nj\ndq\nA\nmkf (y|,A)()d\n,\nand evaluating the integral over q gives\nn!\nn\n\u00b7\u00b7\u00b7nk\nn\n(nj\nwhere we do the n dimensional integral with q0\nj\nfor j > k . So\nA\nK((,A),( ,A ))(,A|y)d =\nmkf (y| ,A )( )\nA\nmkf (y|,A)()d\n= ( ,A |y).\nC.2. Detailed balance. We have from (27)\nK(A,A )(A) =\nQ\n( (n)/ (n + m))mk k\n(nj\n) n\n\u00b7\u00b7\u00b7nk\nk\nq\nnj\nj\nA\n( (n)/ (n + m))mk k\n(nj ) n\nk\nqnj\nj\n\u00d7\nk\n(nj\nk\nqnj\nj\n\u00d7\n(n)\n(n + m)\nmk\nk\n(nj ) dq\n=\nQ\n( (n)/ (n + m))mk k\n(nj ) n\nk\nqnj\nj\nA\n( (n)/ (n + m))mk k\n(nj ) n\nk\nqnj\nj\n\u00d7\nk\n(nj\nk\nq\nnj\nj\n\u00d7\n(n)\n(n + m)\nmk\nk\n(nj\n) dq\n= K(A ,A)(A ).\nREFERENCES\nANDREWS, D. F. and MALLOWS, C. L. (1974). Scale mixtures of normal distributions. J. Roy.\nALBERT, J. H. and CHIB, S. (1993). Bayesian analysis of binary and polychotomous response data.\nBARRY, D. and HARTIGAN, J. A. (1992). Product partition models for change point problems. Ann.\nBLACKWELL, D. and MACQUEEN, J. B. (1973). Ferguson distributions via P\u00f3lya urn schemes.\nBOOTH, J. G., CASELLA, G. and HOBERT, J. P. (2008). Clustering using objective functions and\nBRESLOW, N. E. and CLAYTON, D. G. (1993). Approximate inference in generalized linear mixed\nBURR, D. and DOSS, H. (2005). A Bayesian semi-parametric model for random effects meta-\nCROWLEY, E. M. (1997). Product partition models for normal means. J. Amer. Statist. Assoc. 92\nDIACONIS, P. and FREEDMAN, D. (1986). On the consistency of Bayes estimates (with discussion).\n(2008). Modelling unobserved sources of heterogeneity in animal abundance using a Dirichlet\nDOSS, H. (1985a). Bayesian nonparametric estimation of the median. I: Computation of the esti-\nDOSS, H. (1985b). Bayesian nonparametric estimation of the median. II: Asymptotic properties of\nDOSS, H. (1994). Bayesian nonparametric estimation for incomplete data via successive substitution\nDOSS, H. (2008). Estimation of Bayes factors for nonparametric Bayes problems via Radon\u00ad\nNikodym derivatives. Technical report, Dept. Statistics, Univ. Florida.\nESCOBAR, M. D. and WEST, M. (1995). Bayesian density estimation and inference using mixtures.\nFERGUSON, T. S. (1973). A Bayesian analysis of some nonparametric problems. Ann. Statist. 1\nGHOSH, M., NATARAJAN, K., STROUD, T. W. F. and CARLIN, B. P. (1998). Generalized linear\nGHOSAL, S. (2009). Dirichlet process, related priors and posterior asymptotics. In Bayesian Non-\nparametrics in Practice (N. L. Hjort et al., eds.). Cambridge Univ. Press. To appear.\nGHOSAL, S., GHOSH, J. K. and RAMAMOORTHI, R. V. (1999). Consistent semiparamet-\nric Bayesian inference about a location parameter. J. Statist. Plann. Inference 77 181\u00ad193.\nGILL, J. and CASELLA, G. (2009). Nonparametric priors for ordinal Bayesian social science models:\nHOBERT, J. P. and MARCHEV, D. (2008). A theoretical comparison of the data augmentation, mar-\nKORWAR, R. M. and HOLLANDER, M. (1973). Contributions to the theory of Dirichlet processes.\nKYUNG, M., GILL, J. and CASELLA, G. (2009). Sampling schemes for generalized linear\nDirichlet random effects models. Technical report, Dept. Statistics, Univ. Florida. Available at\nwww.stat.ufl.edu/~casella/Papers.\nLIU, J. S. (1996). Nonparametric hierarchical Bayes via sequential imputations. Ann. Statist. 24\nLO, A. Y. (1984). On a class of Bayesian nonparametric estimates: I. Density estimates. Ann. Statist.\nMACEACHERN, S. N. and M\u00dcLLER, P. (1998). Estimating mixture of Dirichlet process models.\nMCCULLAGH, P. and YANG, J. (2006). Stochastic classification models. In International Congress\nMIRA, A. (2001). Ordering and improving the performance of Monte Carlo Markov chains. Statist.\nMIRA, A. and GEYER, C. J. (1999). Ordering Monte Carlo Markov chains. Technical Report 632,\nSchool of Statistics, Univ. Minnesota.\nMUKHOPADHYAY, S. and GELFAND, A. E. (1997). Dirichlet process mixed generalized linear mod-\nNEAL, R. M. (2000). Markov chain sampling methods for Dirichlet process mixture models. J. Com-\nNASKAR, M. and DAS, K. (2004). Inference in Dirichlet process mixed generalized linear models\nNASKAR, M. and DAS, K. (2006). Semiparametric analysis of two level bivariate binary data. Bio-\nPITMAN, J. (1996). Some developments of the Blackwell\u00adMacQueen urn scheme. In Statistics,\nProbability and Game Theory (T. S. Ferguson, L. S. Shipley and J. B. MacQueen, eds.) 30 245\u00ad\nQUINTANA, F. A. and IGLESIAS, P. L. (2003). Bayesian clustering and product partition models. J.\nROY, V. and HOBERT, J. P. (2007). Convergence rates and asymptotic standard errors for Markov\nchain Monte Carlo algorithms for Bayesian probit regression. J. Roy. Statist. Soc. Ser. B 69 607\u00ad\nUSA\nE-MAIL: kyung@stat.ufl.edu\ncasella@stat.ufl.edu\nUSA\nE-MAIL: jgill@wustl.edu"
}