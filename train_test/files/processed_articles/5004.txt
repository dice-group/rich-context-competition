{
    "abstract": "Abstract\nThough full of promise, Big Data research success is often contingent on access to the newest, most advanced, and often\nexpensive hardware systems and the expertise needed to build and implement such systems. As a result, the accessibility\nof the growing number of Big Data-capable technology solutions has often been the preserve of business analytics. Pay as\nyou store/process services like Amazon Web Services have opened up possibilities for smaller scale Big Data projects.\nThere is high demand for this type of research in the digital humanities and digital sociology, for example. However,\nscholars are increasingly finding themselves at a disadvantage as available data sets of interest continue to grow in size and\ncomplexity. Without a large amount of funding or the ability to form interdisciplinary partnerships, only a select few find\nthemselves in the position to successfully engage Big Data. This article identifies several notable and popular Big Data\ntechnologies typically implemented using large and extremely powerful cloud-based systems and investigates the feasi-\nbility and utility of development of Big Data analytics systems implemented using low-cost commodity hardware in basic\nand easily maintainable configurations for use within academic social research. Through our investigation and experi-\nmental case study (in the growing field of social Twitter analytics), we found that not only are solutions like Cloudera's\nHadoop feasible, but that they can also enable robust, deep, and fruitful research outcomes in a variety of use-case\nscenarios across the disciplines.\n",
    "reduced_content": "Original Research Article\nBig Data solutions on a small scale:\nEvaluating accessible high-performance\ncomputing for social research\nDhiraj Murthy1 and Sawyer A Bowman2\n Keywords\nBig Data, social media research methods, Big Data research methods, digital humanities, digital sociology, Twitter\nIntroduction\nOver the past decade, there has been an exponential\nincrease in the amount of quantitative social trace\ndata--statistical data pertaining to sociological phe-\nnomena--available to researchers across the globe.\nFacebook boasts 1.32 billion active monthly users\n(Associated Press, 2013) while Twitter, the increasingly\npervasive microblogging service, has 271 million active\nmonthly users generating over 400 million tweets a day\n(Holt, 2013). Other technology companies are part of a\nrush to bring a wide variety of broad-based and niche\nsocial media services, products, and ecosystems into the\nglobal online marketplace. For example, Instagram, a\nsocial media site for sharing photos that debuted in\n(Instagram, 2014). As a result of this rapid growth,\nthere has been an increasing demand for systems and\nmethods that allow the collection, storage, and analysis\nof these vast troves of social trace data. Big Data typ-\nically refers to data sets so large that they challenge the\nabilities of more traditional software tools and systems\ntypically used in data collection, storage, and analysis\n(Manovich, 2011). As the desire and need to efficiently\ncollect and store such large data sets have grown, many\nresearchers have turned towards distributed cloud and\ncluster-based data storage and retrieval systems that\nefficiently process Big Data by spreading the data and\n1Goldsmiths, University of London, London, UK\n2Bowdoin College, Brunswick, ME, USA\nCorresponding author:\nDhiraj Murthy, Goldsmiths, University of London, Lewisham Way, New\nCross, London, UK.\nEmail: d.murthy@gold.ac.uk\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY-NC: This article is distributed under the terms of the Creative Commons Attribution-NonCommercial\n3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use, reproduction and distribution\nof the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages (http://www.uk.\nsagepub.com/aboutus/openaccess.htm).\nprocessing tasks across many computing nodes (Ruflin\nIncreasingly, many forms of computational work in\na wide variety of fields in research pertain to tackling\nlarge data sets. Heralded as pioneering technology\nwhich will ``transform how we live, work, and think''\n(Mayer-Schonberger and Cukier, 2013), Big Data\nremains a loosely defined, often nebulous term, for\nlarge data sets that require complex technologies for\nthe capture, storage, and analysis procedures\n(Manovich, 2011). Despite the growing trend of mar-\nketing and media hyperbole on the value of such data\nto society as a whole, Big Data does have significant\napplications to research. According to a recent study,\nthe creation and replication of digital information per\nyear were found to have a growth factor of 44 (Gu\net al., 2011). Additionally, rapidly growing emerging\nmarkets and the steep increase in web and mobile tech-\nnologies suggest this growth trend will continue (Baru\net al., 2012). As a result of this incredible growth, the\nneed for Big Data technologies in many disciplines is\nmore pressing than ever.\nOne of the specific areas this article seeks to contrib-\nute to is the increasing importance of large collections\nof social trace data in traditionally low-technology\nresearch fields, including the humanities and the\nsocial sciences. Moreover, this article emphasizes\nsmall-scale solutions that can leverage the power and\npotential of Big Data technologies that can be repli-\ncated, implemented, and maintained with low cost\nand minimal expertise. While large-scale Big Data\nresearch has been conducted (e.g. using the full\nFirehose Twitter stream), there is little information\navailable about scaling these solutions down to fit the\nneeds of individual researchers or smaller research labs.\nCost and ease of setup are limiting factors with regard\nto large-scale solutions. Therefore, solutions that pos-\nsess the same tools to process Big Data but on a smaller\nscale, smaller budget, and with the ability to scale up,\nempower individual researchers or researchers in non-\ncomputational fields to pursue research questions that\nwere previously unfeasible due to limits imposed on the\ndata set by price, experience with technology, and size\nof the data. Solutions that are capable of handling large\nvolumes of data while addressing the limitations of cost\nand familiarity with technology are needed for social\nscience and humanities fields to take advantage of Big\nData methods.\nFor individual researchers, there may be a variety of\nconsidered pros and cons associated with their decision\nto attempt to work with and synthesize large data sets.\nThere is little question that more complete social data\nprovide more opportunity of discovery. Businesses\nlooking to market new products, for example, see the\nutility of Big Data social analytics as a vital means\ntowards understanding their audiences and to better\ntarget advertisements. The medical field has used Big\nData to better understand novel drugs, the relation-\nships between chemicals, and the potential impact any\none chemical may have on the human body (Joshi and\nsocial media Big Data has been used for trend detection\n(Preotiuc-Pietro et al., 2012). Similarly, Big Data can be\nused as a means of studying social forces. Speaking\nfrom our own research, we used Twitter data to inves-\ntigate urban American social media use (Murthy et al.,\nforthcoming).\nSocial media have grown enormously. As a result of\nthe increased use of these technologies, the ways in\nwhich people interact and connect on a daily basis\nhave changed fundamentally. Social research stands\nto benefit from analyses of society's deep engagement\nin technologically mediated culture.\nQuantitative sociology has been traditionally driven\nby manageable, structured data sets. Digital soci-\nology--the sociology of online networks, communities,\nand social media--is now quickly emerging as a major\nfield due to the rise of social networking sites like\nFacebook and Twitter. Big Data from these social\nmedia sites has been used to study social behavior\nonline (Gold, 2012). With the large amount of data\nthat is potentially available, one should not have to\nsacrifice size over the quality of the data set or vice\nversa (Manovich, 2011). As a result of the increased\navailability and user-friendliness of analytic techniques\nand jumps in processing power and storage capabilities,\na variety of disciplines including, but not limited to, the\ndigital humanities, social sciences, and information sys-\ntems are becoming increasingly interested in capturing,\nstoring, and analyzing large data sets that were previ-\nously inaccessible to most.\nThough the literature abounds with Big Data's\npromise, the very nature of its size represents a signifi-\ncant challenge. The exponential increase in size of avail-\nable data sets holds the potential for developing a richer\nunderstanding of online social formations. However, it\ncan be difficult, expensive, and time consuming to store\nand process this amount of data. Most make a cost-\nbenefit decision to limit or filter the scope of their\nresearch to data sets of a size they know they can\nhandle. This introduces a bias on the direction of\nresearch studies within a field. Acquisition of the data\npresents yet another difficulty. With the ever-decreasing\ncost of data storage, it is retrieval and organization of\ndata that represent the biggest obstacle. Dimensions\nsuch as the height (the number of records), width (the\nnumber of variables recorded per record), and diversity\npose considerable challenges in making sense of the\ndata (Heer and Kandel, 2012). In addition, the appli-\ncation and study of Big Data in the humanities and\n2 Big Data & Society\nsocial sciences represent several new and formidable\nchallenges. These disciplines face a steep technological\nlearning curve as they must develop new means for\nunderstanding larger sample sizes. Ultimately, however,\nthe stakes are worth it, as these fields are important not\nonly for theory generation around Big Data but also\nfor social critiques leveraging Big Data methods.\nThe purpose of this article is to explore the feasibility\nand suitability of emergent and established Big Data on\n``small-scale'' systems. This article presents specific\ninsights gleaned from our experimental case study in\nbuilding and piloting a small-scale, Big Data collection\nand analysis engine that collects publicly available\nstreaming data from the Twitter application program-\nming interface (API). We first acknowledge and address\nthe unique challenges and advantages of common dis-\ntributed Big Data storage and analysis engines in com-\nparison to more traditional approaches. We then\ndiscuss our findings and summarize a number of lead-\ning Big Data solutions with a focus on which ones\nmight have the most utility when implemented using a\nminimal configuration and low-cost hardware.\nUltimately, we center our attention on Cloudera's\nHadoop, a distribution of Hadoop that maximizes per-\nformance in storage, retrieval, and analysis for a limited\nbudget. We detail the design, testing, and evaluation of\nour small-scale Big Data system while commenting on\nthe strengths and limitations of the systems we piloted.\nLastly, we present an overview of the types of data we\ncollected. The integration of small data technologies\nwith research in the humanities and social sciences\nhas been met largely with success, but the rise of Big\nData poses new challenges to these established meth-\nods. Consequently, we feel that our discovery process\ncan serve as a model and proof of concept in many\ninterdisciplinary fields of social research.\n``Big Data'' challenges in storage,\nretrieval, and analysis\nIn understanding the role and potential of Big Data in\nquantitative social research, it is important to under-\nstand and identify the challenges inherent in the collec-\ntion and use of such data sets. Chief among these\nconcerns is the choice and implementation of technol-\nogies that are capable of efficient information storage,\nretrieval, and analysis at the Big Data scale. While trad-\nitional relational databases have been successfully used\nwith large-scale social research, it is important to con-\nsider how Big Data generates a new set of challenges\nthat often render these older techniques obsolete or\ninefficient at best. For instance, traditional database\nsystems may be fully capable of the storage and index-\ning of large data sets, given available storage space, but\ndepending on the goals of the research, processing\nefficiency may degrade significantly with the increased\nsize of the data set. Almost all data storage and retrie-\nval engines not specifically designed with Big Data and\ndistributed processing in mind have some fail points in\na variety of use-case scenarios beyond which the pro-\ncessing of data becomes untenable. It is important to\nunderstand specific processing needs and the capabil-\nities of any chosen storage technology prior to begin-\nning a project. Otherwise, it is possible to end up with\nvast amounts of data that are unfeasible to successfully\nnavigate. It can often become difficult to simply trans-\nfer the collected data into a more appropriate system in\ncertain situations.\nAll databases have different underlying methods for\nthe storage and retrieval of data, and these differences\nare best understood through the CAP theorem, which\nexplains three factors that make up the ideal database:\nconsistency, availability, and partition tolerance.\nConsistency allows for all clients to have the same\nview of the data. Availability grants each client read\nand write capabilities. Partition tolerance maintains\ngood system performance in spite of physical network\npartitions (Hurst, 2013). A strict interpretation of the\nCAP theorem would argue that no database has been\nable to satisfy all three, but recent interpretations reveal\nthat modern system designers make trade-offs to\neach database. Furthermore, some suggest that ``there\nis actually space to outmaneuver the constraints\nimposed by the CAP theorem with clever design''\nIn this section, we explore the three key functions of\nany database system: storage, retrieval, and analysis,\nwhile giving consideration to the differences, with\nregard to CAP, between more traditional relational\ndatabase management systems (RDBMS) and emerging\ndistributed systems and how each responds to the\nunique challenges posed by Big Data.\nStorage\nThe RDBMS is the most widely used database across\nthe world. The strengths of RDBMS are consistency\nand availability of the data. This means all clients\nhave the same view of the data and each client can\nperform read and/or write operations (Padhy et al.,\n2011). Data are traditionally stored as tables, and\nRDBMS stores these data by forging relations between\npieces of data. In other words, the RDBMS links dif-\nferent pieces of information together by assigning\ntables to keys (Padhy et al., 2011). However, relational\ndatabases are generally not ideal for the storage of Big\nData. As the size of a data set continues to grow, the\ndatabase must also scale. Although relational databases\nexhibit great vertical scalability, they have restrictions\non just how far they can scale. The large amount of\nMurthy and Bowman 3\ndata makes it extremely difficult for the storage of the\ndata on a single machine or cluster (Ruflin et al., 2011).\nIt is possible to add more space on that machine or\ncluster, but this vertical scalability has limits. Big\nData lends itself to newer techniques that utilize elastic\nscaling, scaling out, or horizontal scalability. This pro-\ncess entails distributing the data across several hosts or\nservers, which allows for an easier and more dynamic\nstorage of Big Data (Nance et al., 2013).\nRetrieval\nThe biggest challenges posed by Big Data are the ability\nto retrieve, sort, and filter large data sets. Typically,\nthese tasks are aided through partitions and indexes.\nPartitions physically store data files in different loca-\ntions based on the range of values of some defined vari-\nable. Queries that seek to filter results based on the\npartitioning variable only need access to the data bins\ndefined in the query, thus speeding up retrieval. Indexes\ntypically evaluate the diversity of values of a given vari-\nable and create a reference structure in memory (like a\nbinary search tree). This allows for fast identification\nand retrieval of data when queries ask for records with\na variable exactly matching one or more specific values.\nWith large data sets in RDBMS systems, performance\nof indexes and partitions typically degrades as the\nnumber of records grows. Every table insertion can\ncause the index to be rewritten; over time, this can\nadd up to many additional processing cycles. In distrib-\nuted database systems, the responsibility for indexes\nand partitions is distributed across all the nodes in\nthe system. Technically, they would still be subjected\nto the performance drag of RDBMS, but the effects\nmay not be noticeable until one had collected orders\nof magnitude more data records than was possible\nwith RDBMS.\nData processing\nIn the social sciences, social trace data are often com-\nposed of many different data types (Ruflin et al., 2011).\nThis provides a considerable challenge to relational\ndatabases, which have a static schema. This quality\nenhances performance with structured data, but it\nproves to be a limitation in other scenarios (Padhy\net al., 2011). For instance, with the introduction of\nsocial data that is semi-structured or unstructured,\ndatabases that can adapt, change, and accommodate\nnew data types become more desirable than those\nwith rigid schema (Ruflin et al., 2011). For some\nresearch questions, non-relational databases provide\nfor less stringent data model restrictions (Padhy et al.,\n2011). Furthermore, they allow for easy incorporation\nof new data types, which is valuable to research\nsituations where the data is in flux. These systems\noffer new forms of flexibility, especially in terms of stor-\ning new, diverse, and high-volume data.\nCommon Big Data technologies\nThe use of non-relational database systems has risen\nsubstantially over the past few years due to benefits\nsuch as scalability, high availability, fault tolerance,\nand a compatibility with heterogeneous data (Shi\net al., 2010). While each database strives to be flexible\nyet robust, the application and implementation tend to\nvary significantly. As a result of the differences in per-\nformance between each database, we have selected\nthree databases for comparison. In this section, we\nquickly compare MongoDB, Riak, and Hadoop and\ninvestigate their overall performance, applicability to\nthe collection and analysis of social data, and their limi-\ntations. It should be noted that the purpose of this is\nnot an in-depth comparison, but rather a brief\noverview.\nMongoDB is one popular NoSQL database. The\nschema is flexible and largely uses document structure\nand storage. The documents are of JavaScript Object\nNotation (JSON)-style, a type of text-based data that\noffers both simplicity and power (Dede et al., 2013)\n(JSON is used by the Twitter API and many other\nsocial media platforms). Additionally, indexes allow\nfor the quick organizing of documents, particularly\nones corresponding to frequent queries. MongoDB cre-\nates a replica set of documents that ensures an auto-\nmated failover. This also provides for redundancy and\nhigh availability. MongoDB also scales through shard-\ning, a process that partitions a collection of documents\nand then stores each segment on a different machine\n(Dede et al., 2013). This creates a balanced load\nacross the machines. MapReduce is also a critical com-\nponent of MongoDB. This command is meant to\nhandle complex aggregation jobs. The map function\nensures each instance is created and the reduce function\ncreates ``sorted groups of instances that share a\ncommon key'' (Borkar et al., 2012). GridFS, another\nkey component, is used to store and retrieve files that\nexceed the BSON--the binary representation of JSON\ndocuments--document size limit. It achieves this by\ndividing a document into parts and creating multiple\nnew documents. While MongoDB represents a power-\nful database technology, there are many limitations,\nparticularly with the user interface. For example,\nMongoDB tends to be significantly slower--nearly\nfive times slower--than the Hadoop Distributed File\nSystem (HDFS) for large data input (Dede et al., 2013).\nRiak is another popular NoSQL database. It places\nemphasis on availability achieved through replication.\nAdditionally, data are retrieved so that read and write\n4 Big Data & Society\noperations can be called even during failure conditions.\nIn the case of a network or hardware failure, loss of\naccess to nodes can occur without data loss. Adding\nmachines to the cluster can be done easily, and the\ndata in that cluster are automatically distributed\nthrough hashing. Each node is the same, and this sets\na foundation for fault tolerance and scalability. Riak\nuses a key/value model for object storage. Any type of\ndata can be stored as an object. Much like MongoDB,\nRiak uses MapReduce for aggregation tasks,\nthough Riak has its own search and index system.\nWhile Riak is able to rebalance automatically due to\ndividing data space into equal partitions, this process of\nequal partitioning is overwhelmed by a high load of data\n(Konstantinou et al., 2011). Consequently, Riak does not\ncompare with HDFS in high-request rate scenarios.\nHadoop, a popular open-source platform for data-\nintensive applications, has a software library that is\nused to process large data sets (White, 2012). Like\nMongoDB, it also uses the MapReduce model. This\nis done through the use of several nodes, which make\nHadoop both reliable and highly available (Dede et al.,\n2011). In comparison to MongoDB, Hadoop outper-\nforms in read and write operations as well as scalability\n(Dede et al., 2013). Another project-specific goal we\nconsidered included cost. As a result, we had to con-\nsider how Hadoop would perform as a single node of\nlow-node cluster on low-cost hardware. Hadoop's per-\nformance over MongoDB and its ability to handle a\nhigher load of data than Riak make Hadoop a great\ncandidate for social research. The configuration\noptions, availability of support, and the active develop-\nment for scalability were also factors that were weighed\nin our decision. Specifically, a significant amount of Big\nData social research is Hadoop-based and online sup-\nport is readily available. Though there are several dif-\nferent implementations of Hadoop, the most relevant to\nsocial data projects is the Cloudera open-source distri-\nbution of Hadoop (Cloudera Inc., n.d.). Essentially,\nthis version of Hadoop not only possesses the same\nmethods, functions, and general properties that\nHadoop has, but it also incorporates other Big Data\ntools to effectively sort social data objects, such as\ntweets. Specifically, Flume, HDFS, Oozie, and Hive\nare all used in this control flow as a means of storing,\nsorting, and analyzing the data.\nTable 1 summarizes the three databases discussed in\nthis section. This table highlights key differences\nbetween the three databases for NoSQL implementa-\ntions. Of the three prominent databases explored in this\nsection, Apache Hadoop is the most widely used and\nperhaps most applicable to the common needs of Big\nData research in the social sciences (Khuc et al., 2012;\nLee et al., 2012). For example, Hadoop has been\nsuccessfully implemented in a social media\ncloud-computing application (Kim and Lee, 2011)\nand in the Lydia TextMap system which enables\nsocial scientists to study the intersections between\nblogs, newspapers, patents, scientific abstracts, and\nlegal documents (Bautin et al., 2010). This is largely\ndue to Hadoop offering a higher read rate, which is\nessential for processing the large amounts of data that\nmust be read into the database's storage (Ruflin et al.,\n2011). Additionally, the combined scalability (elasti-\ncity) and relative speed for a high throughput of data\nmake Hadoop a good fit (Dede et al., 2013;\nExperimental case study: Twitter data\ncollector using hive\nThough there is a desire on the part of researchers\nacross the disciplines to implement powerful distributed\ndatabase architectures, little information is available to\nsuggest how this should be accomplished at the level of\nindividual researchers (rather than well-staffed labs).\nEnterprise-level Big Data storage solutions are designed\nand optimized to be deployed on large cloud-based\nserver farms consisting of hundreds to thousands of\nnodes. This is entirely appropriate for a system of pro-\nviding database services simultaneously to many con-\nnected clients. Little information is available about how\nsuch technologies can usefully be leveraged by individ-\nual researchers or small teams with one or two large\ndata sets that they would like to be able to analyze.\nEven less information is available about performance\nof such systems when implemented at a small scale (1\u00ad5\nnodes). The lack of information about such systems\nimplemented on small scales serves as a critical barrier\ntowards researchers attempting to use such technolo-\ngies. Often, they will not be able to determine if their\nproject is feasible, possible, or if it will perform better\nor worse than their current data management system.\nAs a result, most stick with what they know: possible\ninvesting, constant upgrades, workarounds, hacks, and\nconsultants to get their legacy data systems to meet\ntheir basic needs. A sad consequence of this is that\nTable 1. Comparison between NoSQL data collection\ntechnologies.\nDifficulty levela Language Querying Storage\nMongo DB Easy C\u00fe\u00fe JavaScript Document\nRiak Moderate Erlang & C Riak Search Key-Value\nHadoop Moderate Java Hive Column\naLevel of difficulty/complexity of each, taking into special consideration\nsetup of required hardware, databases, and querying languages. In fields\nthat are not traditionally associated with technology, it is important to\nconsider the learning curves associated with these different tools.\nMurthy and Bowman 5\nthe divide between business and academic expertise and\napplications of Big Data methods continues to grow.\nSpecifically, most information available about the\ndesign and implementation of distributed storage and\nretrieval systems focuses on large, multi-node systems,\nwhich are likely overkill for most academic social\nresearch case scenarios. This has the effect of influen-\ncing those researchers determined to take the leap to\ncloud-based storage and analysis engines to perhaps\nover-invest in hardware, personnel, and support for\ntasks which could have been accomplished with a\nmuch lower investment. Both of these problems poten-\ntially contribute to an unnecessary drain of research\nfunding pools, and funds are often misallocated or\nover-allocated. Our approach to this case study was\nto choose a popular and well-documented distributed\nstorage engine and implement it at the smallest reason-\nable scale. We would then populate the system with Big\nData and see how it performs in comparison to more\ntraditional approaches. The value of this approach is\nthe potential to determine what is possible at the lowest\nlevel (lowest entry barrier), while at the same time\ndetailing a system which can easily be incrementally\nscaled up to grow with a live project's data and per-\nformance needs.\nTowards this end, the following criteria were set as\ngoals for our experimental case study. The first goal\nwas to develop a system using commonly available\ndecided to implement the freely available packaged dis-\ntribution of a common distributed-data storage engine,\nin this case the Cloudera distribution package of the\nApache Hadoop ecosystem. Our case study was a\nsystem capable of collecting up to one year's worth of\nTwitter data from a 1% sample of all tweets. (While our\nsystem has indeed collected a year's worth of data, we\nonly test on three months of data in this article.)\nAdditional considerations were given towards creating\na system that would be fault tolerant and provide\nacceptable retrieval and analysis performance in at\nleast some common use cases.\nResources\nThe backbone of the system we designed was a stock\nDell PowerEdge T320 server. This machine has twelve\n1.9 GHz processing threads on six cores. We added 32\nGB of RAM and four 2 TB hard drives in a RAID 3\nconfiguration, providing 5.4 TB of usable storage space\non the single node. It should be noted that in multi-\nnode systems, data replication can be employed across\nthe node, making RAID unnecessary. This system was\nconfigurations met our goals, this one is entirely middle\nof the road, readily available, and easily extensible.\nThis represents considerable savings over traditional\n``I/O hungry'' RDBMS node clusters, which can cost\n2012). In the next two sections, we discuss using\nTwitter as a data source, and we outline our approach\nin setting up a single-node Hadoop database.\nData source\nWith a user base of over 500 million, Twitter represents\none of the most popular social media platforms and one\nwhich is seen as a valuable source for business intelli-\ngence (Culnan et al., 2010). The number of users con-\ntinues to grow rapidly, and the amount of data\ngenerated by this user base is on the scale of Big\nData. For example, the full Twitter Firehose, a paid\nAPI source that allows for the capture of every tweet,\nWhile our low-cost system is not appropriate for the\ncapture and storage of the entirety of Twitter through\nFirehose, it can capture Twitter samples well into the\nBig Data level. There are a total of 112 metadata fields\nof Twitter data grouped into four different categories:\nTweets, Users, Entities, and Places (Twitter, 2013).\nSocial researchers have been particularly interested in\nthe collection of Twitter data because of the ease of\naccessibility and richness of data. It is possible for the\nresearcher to select particular metadata fields that best\ncontribute towards the support of a specific hypothesis.\nIn our introduction, we highlighted the obstacles\nthat traditionally limit Big Data research in the social\nsciences. One classic trade-off is size versus quality of\nthe data set. Without Big Data technologies, research-\ners are forced to choose between breadth and depth.\nOften, the way researchers filter introduces biases in\nthe collected data set. However, our system allows for\na broad collection of items--tweets in our case\nstudy--as well as all of the associated metadata. One\ncan then comb through these data with HiveQL queries\nto find data that are relevant to their particular case\nstudy, rather than starting with small data. We were\nable to answer a variety of sociological research ques-\ntions with this data set (particularly regarding demo-\ngraphic attributes of users and the use of Twitter via\nmobile versus web clients). We were also able to detect\ntweet patterns. For example, using the time zone and\ntime of tweet metadata, we were able to create graphs\nthat visualized the tweeting patterns and behaviors in\ndifferent time zones around the world. Not only did\nthese graphs demonstrate differences in online behavior\naround the world, but they also revealed abnormal fre-\nquency spikes that could be linked to current events.\nAlso, by pairing the matching of words with emotional\ncontent within tweets with the device a user tweeted\nfrom (i.e. mobile or web), we were able to evaluate\n6 Big Data & Society\nwhether mobile and web-based tweets were likely to be\nmore positive or negative. As Twitter introduces more\nmetadata fields, the flexibility of our architecture would\nenable us to handle these changes gracefully.\nTo facilitate collection, Twitter provides an API that\nallows researchers to connect to the stream. The API\nallows the researcher to request data via either the\nREST, an architectural style that relies on HTTP and\nXML, or the stream. Both provide numerous data col-\nlection options. The Spritzer stream collects approxi-\nmately 1% of the total stream flow in real time\n(Natkins, 2012b). Other options include location\nstreams, keyword, or REST calls for individual pieces\nand/or blocks of information. The data arrive in JSON\nformat and contain data associated with the corres-\nponding tweet in a text-based format (Bo, 2010).\nWhat makes Twitter especially interesting from a socio-\nlogical perspective is not just the amount of data gen-\nerated but also the relationships, trends, and social\nmeaning that can be studied with these data. In particu-\nlar, Twitter data allow for the aggregation of opinions,\nideas, and trends by socio-demographic characteristics\nsuch as location, time of day, and pace of tweeting\n(Sankaranarayanan et al., 2009). In this sense, Twitter\nis increasingly seen by some social researchers as\nan important way of visualizing relationships and\nsocial communication between people (Boyd and\ntraditional means of communication, Twitter pro-\nvides an environment of almost synchronous feedback\n(Sankaranarayanan et al., 2009). As a result, Twitter\nacts as a source for the distribution of news and infor-\nmation (Park and Chung, 2012), which can provide\nimportant social insights.\nSetup and configuration\nThere are several setup and configuration issues which\nshould be considered at the start of any Big Data social\nresearch project or proposal. The first is data ingestion.\nThis typically involves establishing a connection to a\ndata source, a possible transformation step, and\nmoving the data to a final storage location. The next\nconsideration is the management and organization of\ndata within the data store itself. Organization typically\ninvolves partitioning data, the indexing of variables\nwithin data records, and other concerns which help\nease retrieval and analysis tasks. The main concerns\nin the retrieval and analysis step are to plan what\nways one will connect to the data, store and request\ndata, possibly process, transform, and/or analyze the\nresults, and finally deliver those results to the requester\nvia some method or format. Figure 1 illustrates the\narchitecture of our single-node Hadoop database.\nThere are two main components to our data inges-\ntion component. These are the Twitter API itself and a\nFigure 1. Our architecture implementing Hadoop.\nMurthy and Bowman 7\nFlume data source. Flume is a highly configurable\nopen-source data ingestion system that will connect to\none or more user-defined data sources and push the\ndata to one or more data sinks, via data channels\n(Natkins, 2012b). Flume's sources, channels, and\nsinks allow for the definition of complex data flows.\nIn this case, the data source used the open source twit-\nter4j Java library to connect to the Twitter statuses/\nsample stream and push these data to an HDFS.\nOnce our data had been loaded into the HDFS, we\nused the Oozie workflow automation tool to automat-\nically partition these data by hour and prepare these\npartitions to be accessible to storage and retrieval\nrequests. Oozie is a component of the Apache\nHadoop ecosystem that allows for the definition of\npotentially complex and repeating automated work-\nflows (Natkins, 2012a). In this case, Oozie was config-\nured with parameters that ran a script to automatically\ncreate data partitions and prepare these data for access\nvia the Hive database system.\nAs Figure 1 illustrates, we used Hive as our tool for\nenacting queries on data stored in the HDFS. Hive is a\ndata warehouse system that works in coordination with\nHadoop to achieve easy data summarizing, ad-hoc\nqueries, and analysis of large data sets (Apache\nHadoop, 2013). It accesses the data in Hadoop through\nan SQL-like language. The performance of data load-\ning and range queries in Hive is strong and surpasses\nthe performance of alternative tools such as Cassandra\nand HBase in these categories (Shi et al., 2010). Hive is\neffective because it can handle unstructured, semi-struc-\ntured, and poly-structured data (Natkins, 2012c). Hive\nhas the ability to define a fully dynamic data serializa-\ntion and deserialization interface (SerDe). The previous\ntwo features allow the storage of data in its native\nformat, which allows for the processing of only the\nqueried data as opposed to each piece of incoming\ndata. This, in turn, makes insert speeds faster. Data\ncan be searched and records processed while only par-\nsing the data from its original format as needed\n(Natkins, 2012c). This is a particularly powerful tech-\nnique for semi-structured data like XML or JSON, and\none which is immediately applicable to the JSON\nTwitter records which are returned via the Twitter\nAPI. This workflow illustrates how the individual\ntools discussed throughout this section compose an effi-\ncient and accessible architecture. This combination of\ntools also enabled us to efficiently analyze our collected\nTwitter data.\nCollection, retrieval, and analysis\nFor this case study, our Twitter collection and analysis\nIt ran without any system-caused failures. Analytic\ntools bundled with the Cloudera distribution of\nHadoop indicated that the system was not overly\ntaxed and was in general good health over the research\nperiod, even while performing processor intensive\nqueries alongside new data ingestion. In the first\nmonth, over 150 million metadata-enriched tweets\nwere collected (approximately 5 million tweets per\nday), which consumed approximately 300 gigabytes\nworth of storage space. The collector read and stored,\nconfirms the Spritzer stream's advertised 1% sample\nrate as the total volume of worldwide tweets was\naround 500 million the time of data collection (Holt,\nHiveQL is the language utilized by Hive and is mod-\neled after SQL for the sake of familiarity and includes\nSQL commands like ``from clauses, joins, group bys,\naggregations, and create table as select'' functions\n(Stewart et al., 2011). However, not all SQL functions\nare implemented in HiveQL and vice versa. For exam-\nple, HiveQL lacks some of the functions and indexing\ncapabilities that are available in SQL, but HiveQL also\noffers extensions such as multi-table inserts, transform,\nmap, and reduce functions (White, 2012).\nWe performed a variety of tests involving select vari-\nables from the Twitter data to investigate potential\nmethods of data analytics. For example, we used the\nHive ngrams function to create four different tables\nconsisting of the top 10,000 unigrams, bigrams, tri-\ngrams, and quadgrams per language across the entire\nmonth. We were also able to generate tables of the top\n1000 ngrams per day and evaluate the change in fre-\nquency for a particular ngram over the period of the\nmonth. Regular expression extractions allowed us to\nfilter for a particular word that occurred after a\ncommon phrase. For example, we searched for the fre-\nquency of words that followed ``I love'' and ``I want to\nbuy.'' These queries and extractions provided a closer\nlens for examining the patterns and use of phrases as\nwell as the exchange of information on Twitter.\nEvaluation\nIn working with Hadoop and Twitter data, we experi-\nenced success as well as unanticipated challenges.\nUltimately, we found that this architecture presented\na suitable solution to overcoming previous difficulties\nwith storing and filtering Big Data. In order to context-\nualize our single-node implementation of a Hadoop\ndatabase within the sphere of Big Data research, we\ncompared the performance of our Hive-Hadoop setup\nwith a more traditional RDBMS database that we pre-\nviously created. Our Hive-Hadoop database performed\na full table scan of 150 million records in approximately\n2\u00ad2.5 h on average. On the other hand, our RDBMS\n8 Big Data & Society\ndatabase performed a full table scan of 250 million rec-\nords, from a different database but using the same\nTwitter source, in over 14 hours. Taking the difference\nin size of each data table and the amount of time taken\nto perform a full table scan into consideration, our\nHive-Hadoop database still demonstrates a 300%\nincrease in performance over a similarly structured\nand queried MySQL database. This finding is notable\nfor several reasons. First, it demonstrates that our\nHive-Hadoop solution functions well for the collection\nand processing of Twitter Spritzer stream data. Second,\nit exhibits Hadoop's ability to offer an increase in per-\nformance over RDBMS solutions on certain tasks.\nLastly, our Hadoop solution represents a low-cost\n(under $5000) database that can be set up with minimal\nexpertise required. Our low-cost commodity hardware,\nsingle-node Hive-HDFS solution was not only designed\nand implemented with minimal Big Data technical\nexpertise, but it also met and usually exceeded the per-\nformance of RDBMS in many situations. Our case\nstudy also affirms the accessibility of Hadoop in Big\nData research, even in small-scale single-node\nimplementations.\nThe most significant performance gains were seen on\nthe largest data queries. Hive was strongest in retrieving\nand processing large chunks of data, as it can distribute\nthe load across many nodes and processors. Despite\nthis, Hive is relatively weak for seek operations to\nfind specific pieces of information. This is mostly\nbecause of the large amount of overhead associated\nwith performing any one query. A query (of any size)\nrequires the start-up and initialization of at least one\nJVM, Java Virtual Machine, per node to process the\nthread and manage the job's execution. This can take\nseveral seconds per initialization and it does not signifi-\ncantly benefit from increased storage or processing cap-\nacity. Thus, even for a query on an indexed variable,\nthere is a fixed start-up cost even if the query will even-\ntually execute in sub-second time. This situates Hive-\nbased Big Data solutions as one of the better options\nfor very large table scan type analyses where time to\nresult is less important and critically inadequate for\nqueries where a near real-time response is desirable or\nrequired. A happy solution may be to use tools like\nOozie to stage recent or historical data incrementally\nover time in other distributed data engines, which can\nprovide real-time query response at the cost of memory,\nlike Cassandra.\nAnother potential benefit that we identified in our\narchitecture is the addition of nodes. This implementa-\ntion of multiple nodes--as opposed to our single-node\nsystem--would increase the performance of the system\nas a whole. For example, both Cassandra and HBase\nexhibit significant improvements in the speed of most\nevery operation from five nodes to 19 nodes (Shi et al.,\n2010). Due to the marked improvement of both\nCassandra and HBase, we hypothesize that our Hive-\nHadoop system would also result in similar increases in\nperformance. The real benefit of the system structure\nwe have tested is that storage or processing capacity can\nbe incrementally added to obtain desired performance.\nLimitations\nOne potential limitation we identified for Hive users\nrelying on previous experience with SQL-like languages\nis that HiveSQL only implements a subset of the\nadvanced functions and features of the SQL language\nspecification. Many of these features may be recreated\nas Hive may be extended to include user-defined func-\ntions or one may be forced to reformulate their query\nusing only the implemented features. In almost every\ncase, there would be an acceptable work around. But\nthose considering using Hive should be aware of this\nlimitation to avoid encountering potential stumbling\nblocks.\nAnother limitation is that any system of any size\neventually will be constrained by memory or storage\nlimitations. For instance, a single-node Hadoop\nsystem is likely not appropriate for collecting the full\nTwitter Firehose, unless you are only interested in\ndoing so for a short period of time. It would only be\nable to ingest data for about four days before available\nstorage resources were consumed. In any event, the\nbudgetary restrictions of most academic social media\nresearch limit researchers to freely available data\nrather than paid data like Firehose. Using our system\narchitecture, we could collect Twitter's free streamed\ndata for more than a year. It should be noted that\nonce storage capacity is exhausted, one would still be\nable to fully utilize the retrieval functions of their\nchosen database system, and additional storage and\nprocessing capacity can be added at any time via the\naddition of new nodes.\nConclusion\nBig Data, with its promise of ``complete'' data sets,\ncomes with an enormous level of complexity both in\nterms of storage and data analysis. Of course, this pre-\nsents barriers of cost and technical expertise both\nwithin traditionally technical disciplines as well as\nwithin the humanities and social sciences. That being\nsaid, the high levels of social data being created as part\nof our online social media footprint have attracted the\nattention of social scientists in Big Data. Furthermore,\nthe digital humanities cut their teeth on the digitization\nof large corpuses of books, and the field saw immediate\npayoff in Big Data analytics. Both traditionally tech-\nnical fields as well as disciplines which have been\nMurthy and Bowman 9\nhistorically less technical see two major challenges in\nmoving into Big Data research: cost and technical\nexpertise. This article has sought to ameliorate these\ntwo challenges by presenting our evaluation of Big\nData solutions for a Twitter-based social research\ndata project. Using a small-scale yet extensible hard-\nware setup, we used Apache Hadoop and Hive to pro-\nvide an efficient and cost-effective storage and analytics\nsolution for the collection of approximately 150 million\ntweets/month.\nUltimately, the increase of database solutions to\nhandle Big Data is often confusing, as it is challenging\nfor one to understand what platforms are most suitable\nfor newer forms of data such as social media data. In an\nattempt to find a suitable database for the collection,\nstorage, and analysis of large amounts of Twitter data\nfor our study, we have compared and contrasted prom-\ninent database solutions. Like others, we sought a data-\nbase that could provide horizontal scalability, a flexible\ndata schema for unstructured social data, a familiar\nlanguage like SQL, an intuitive user-interface, fast\nread and write capabilities, a reliable architecture, par-\ntitioning capabilities, and a sound method for analysis.\nRDBMS and NoSQL solutions were introduced and\nexplored, though we ultimately chose to implement\nHadoop. While Hadoop has major limitations, its per-\nformance in key categories outstrips the other data-\nbases we tested it against. As a result, we adapted\nCloudera's version of Hadoop for our Big Data\nTwitter project. With the use of Big Data tools includ-\ning Flume, Oozie, and Hive, we were able to effectively\nplug into the Twitter API, stream the unstructured\nJSON data into a distributed file storage system, auto-\nmatically process work flows, and organize the previ-\nously unstructured data into partitions loaded into a\nquery-able data table.\nThe experimental results discussed demonstrate not\nonly the power of this solution but also its ability to be\nused for innovative forms of hypothesis generation for\nsocial research. The overhead associated with large-\nscale Big Data technologies often hinders individual\nresearchers from pursuing hypotheses that require the\nsize and quality of Big Data sets. However, the minimal\ntechnical expertise it takes to set up and utilize a small-\nscale Big Data technology compensates for lack of pre-\nvious Big Data experience. The results detailed in this\narticle merely scratch the surface when it comes to pos-\nsible queries one could make through Hive. Our work\nnot only evaluates and demonstrates the effectiveness of\nHadoop in handling the challenges of Big Data, but it\nalso critically addresses its limitations. There is great\npotential for Hadoop in terms of social media data col-\nlection. Ultimately, we found that our Hadoop-based\narchitecture enabled us to implement a cost-effective\ndata collection and analysis framework for a large\nTwitter-derived data set. Our aim is to provide a\nmodel for practitioners across the disciplines who are\ncurrently evaluating Big Data solutions but are on a\nbudget and may have limited sets of expertise. We\nalso feel that the gap in Big Data research methods\nbetween business and social research applications has\nbeen growing, and this article seeks to bridge some of\nthis divide by opening the hood to accessible Big Data\nmethods.\n"
}