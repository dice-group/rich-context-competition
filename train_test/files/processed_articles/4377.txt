{
    "abstract": "Abstract\nWe are living in an algorithmic age where mathematics and computer science are coming together in powerful new ways\nto influence, shape and guide our behaviour and the governance of our societies. As these algorithmic governance\nstructures proliferate, it is vital that we ensure their effectiveness and legitimacy. That is, we need to ensure that they are\nan effective means for achieving a legitimate policy goal that are also procedurally fair, open and unbiased. But how can\nwe ensure that algorithmic governance structures are both? This article shares the results of a collective intelligence\nworkshop that addressed exactly this question. The workshop brought together a multidisciplinary group of scholars to\nconsider (a) barriers to legitimate and effective algorithmic governance and (b) the research methods needed to address\nthe nature and impact of specific barriers. An interactive management workshop technique was used to harness the\ncollective intelligence of this multidisciplinary group. This method enabled participants to produce a framework and\nresearch agenda for those who are concerned about algorithmic governance. We outline this research agenda below,\nproviding a detailed map of key research themes, questions and methods that our workshop felt ought to be pursued.\nThis builds upon existing work on research agendas for critical algorithm studies in a unique way through the method of\ncollective intelligence.\n",
    "reduced_content": "Original Research Article\nAlgorithmic governance: Developing a\nresearch agenda through the power of\ncollective intelligence\nJohn Danaher1, Michael J Hogan1, Chris Noone1,\nRo\n\u00b4na\n\u00b4n Kennedy1, Anthony Behan2, Aisling De Paor3,\nHeike Felzmann1, Muki Haklay4, Su-Ming Khoo1, John Morison5,\nMaria Helen Murphy6, Niall O'Brolchain1,\nBurkhard Schafer7 and Kalpana Shankar8\n Keywords\nAlgorithmic governance, Big Data, algocracy, collective intelligence, interactive management, public participation\nIntroduction\nWe are living in an algorithmic age where mathematics\nand computer science are coming together in powerful\nways to influence, shape and guide our behaviour and\nthe governance of our societies. With the spread of sur-\nveillance technologies and the growth of the internet of\nthings, we are creating a vast interconnected network of\ndata collection devices (Greengard 2015; Kellermeit\nand Obodovski, 2013). This network produces ever-\nlarger datasets of potentially useful information,\nupdated in real time (Kitchin, 2014; Mayer-\nSchonberger and Cukier, 2013). No human being can\n1National University of Ireland \u00ad Galway, Ireland\n2IBM, Ireland\n3School of Law and Government, Dublin City University, Ireland\n4Department of Geography, University College London, UK\n5School of Law, Queen's University, Belfast, UK\n6School of Law, Maynooth University, Ireland\n7School of Law, Old College, University of Edinburgh, UK\n8University College Dublin, Ireland\nCorresponding author:\nJohn Danaher, National University of Ireland \u00ad Galway, University Road,\nGalway, Ireland.\nEmail: john.danaher@nuigalway.ie\nCreative Commons Non Commercial CC BY-NC: This article is distributed under the terms of the Creative Commons Attribution-\nNonCommercial 4.0 License (http://www.creativecommons.org/licenses/by-nc/4.0/) which permits non-commercial use, reproduction\nand distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nBig Data & Society\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\njournals.sagepub.com/home/bds\nmake sense of this data unassisted. Hence there is sig-\nnificant reliance on algorithms to mine, parse, sort and\nconfigure the data into useful packages. Oftentimes\nthese systems are maintained and tweaked by human\ndesigners and engineers, and the information is then\nutilized by humans in traditional corporate and bureau-\ncratic decision-making systems. But there is also a\ngrowing willingness to outsource decision-making\nauthority to algorithm-based decision-making systems.\nSome even dream of creating `master algorithms' that\nwill be able to learn and adapt to any decision-making\nsituation without the need for human input or control\nWe can refer to these converging trends by using the\nlabel `algorithmic governance' or, perhaps more pro-\n2016). Whether we like it or not, algorithms are increas-\ningly being used to nudge, bias, guide, provoke, control,\nmanipulate and constrain human behaviour. Sometimes\nthis is beneficial; sometimes benign; sometimes prob-\n2016). To ensure that it is more the former than the\nlatter, an algorithmic governance system ought to be\ndesigned and implemented in a way that ensures both\nits effectiveness and its legitimacy (Peter, 2017). That is\nto say, we should ensure that it is an effective means for\nachieving some policy goal, whilst remaining procedur-\nally fair, open and unbiased. But how can we ensure that\nalgorithmic governance systems are both?\nThis article shares the results of a collective intelli-\ngence (CI) workshop we ran at NUI Galway in March\n2016 that addressed exactly this question. The work-\nshop brought together a multidisciplinary group of\nscholars to consider (a) barriers to legitimate and effect-\nive algorithmic governance and (b) the research meth-\nods needed to address the nature and impact of specific\nbarriers. An interactive management (IM) workshop\ntechnique (Warfield and Cardenas, 1994) was used to\nharness the CI of this multidisciplinary group. This\nmethod enabled participants to produce a framework\nand research agenda for those who are concerned about\nalgorithmic governance. We outline this research\nagenda below. We start by explaining some of the back-\nground to our workshop, placing its results in the con-\ntext of the current literature on algorithmic governance.\nWe then explain the methods and results of our work-\nshop. Finally, we close by offering reflections on the\nresearch agenda proposed by the group. The proposed\nagenda is then provided in Table 2.\nContext: Understanding algorithmic\ngovernance\nThe technological trends alluded to in the opening\nparagraph are relatively recent, but they have a\ndeeper history. The phenomenon of algorithmic gov-\nernance is part of a longer historical trend toward the\nmechanization of governance. Sociologists since the\ntime of Weber have highlighted ways in which the\nlegal-bureaucratic organization of the state is subject\nto the same modernising trends as the design of indus-\na system of governance that is machine-like in nature:\ntasks are subdivided and roles are specialized so as to\nperform the business of governance as efficiently as pos-\nsible. This has always depended on the collection of\ndata about the society and citizens to whom the\nsystem applies (Hacking, 2006), and from the dawn of\nthe computer age attempts have been made to automate\nsome or all of the process. Key figures in the cybernetics\nmovement, for example, advocated the use of compu-\nterized systems of data collection, processing and deci-\nsion-making in social governance (Medina, 2011;\nThis does not mean that algorithmic governance is\nnothing new. The systems we consider in this paper and\nthat we considered at our workshop are different from\ntheir historical forebears. The differences are largely a\nmatter of degree and not of type. The technologies that\nfacilitate the automation of governance certainly build\non top of the pre-existing structures, thereby taking\nadvantage of previous mechanistic innovations. But\nthe speed, scale and ubiquity of the technologies that\nmake algorithmic governance possible are grander now\nthan they were in the past. Advances in machine learn-\ning and data collection enable the automation of pro-\ncesses that previously would not have been possible.\nThey also enable far more efficient processing and\nhandling of the data. Couple that with the fact that\ntechnologies of surveillance have become more deeply\nintegrated into our everyday lives, and it seems that we\nare at a significant inflection point for the future of\nalgorithmic governance. Many scholars have started\nto pay attention to this emerging reality and this has\ngiven rise to a burgeoning academic literature on the\ntopic of algorithmic governance.\nOur workshop aimed to contribute to and build upon\nthis literature. Three aspects of the literature were of\nparticular concern to us, specifically, the aspects focus-\ning on (a) the forms/modes of algorithmic governance,\n(b) the problems of algorithmic governance and (c) the\nmethods for studying algorithmic governance. We\nbriefly describe these aspects below, noting how they\nshed light on the phenomenon of algorithmic govern-\nance, and how our study tries to build upon them.\nThe forms of algorithmic governance\nThe first trend concerns the classification of different\nalgorithmic governance systems. Considerable work\n2 Big Data & Society\nhas been done on identifying the key properties of the\nBig Data systems that enable contemporary algorith-\nmic governance. Back in 2001, Doug Laney of Gartner\nproposed the now-classic `three Vs' framework for\nunderstanding the emergence of Big Data systems.\nThis framework suggested that Big Data was `big' in\nterms of its volume, velocity and variety. Since then,\nmore complex frameworks have been proposed\n(Kitchin and McArdle, 2016). Kitchin, for instance,\nhas argued that there are at least seven dimensions to\nbigness, adding exhaustivity, resolution and indexicality,\nrelationality and extensionality and scalability to the\nthree Vs (Kitchin, 2014). These elaborations are\nuseful insofar as they help us to grasp the properties\nof Big Data and better understand the challenges and\nopportunities it poses in the context of the design of\nalgorithmic governance systems. That said, any classi-\nfication system of this sort is prone to being value-laden\nand under- or over-inclusive (Kitchin and McArdle,\n2016). This comment applies equally well to the other\nclassification systems to which we refer below, and has\nimplications for the definition and scope of algorithmic\ngovernance systems.\nAnother aspect of the ongoing literature relates to\nthe classification of algorithms themselves. New algo-\nrithms are designed all the time, but they typically fall\ninto a set of general types that have been exhaustively\ncategorized by computer scientists (for a basic introduc-\ntion see Cormen, 2013; for a more comprehensive one,\nsee Cormen et al., 2009). For example, there are search-\ning and sorting algorithms that break down into sub-\ntypes such as binary search, selection sort, insertion\nsort, merge sort and quicksort. Understanding these\ndifferent types is important when it comes to assessing\nthe social and normative properties of algorithmic gov-\nernance systems. For instance, one of the most import-\nant high-level shifts in the design of algorithms in recent\nyears is the move from `top-down' algorithms (in which\na programmer or team of programmers exhaustively\ndefines the ruleset for the algorithm) to `bottom up'\nmachine-learning algorithms (in which the algorithm\nis given a learning rule and trained on large datasets\nin order to develop its own rules). This shift is import-\nant because the use of bottom-up algorithms creates\ncertain problems when it comes to the transparency\nand opacity of algorithmic governance systems, par-\nticularly when such algorithms are incorporated into\nalready-opaque governance structures. Awareness of\nthis problem was part of the original motivation for\nour workshop and something that was repeatedly high-\nlighted by the participants. A major goal, consequently,\nwas to develop a research agenda that could address the\nconsequences of this shift.\nA final aspect of the ongoing literature on the clas-\nsification of algorithmic governance systems is more\nexplicitly evaluative in nature. It is primarily underta-\nken by ethical and legal theorists and focuses on iden-\ntifying the key stages in the process of algorithmic\ngovernance and seeing how they relate to pre-existing\ngovernance systems. Four stages are identified by most\ncontributors to this literature. They are collection, pro-\ncessing, utilization and feedback and learning (Citron\nThese stages are often said to define a governance\n`loop': the system acquires information, processes it,\nuses it and then feeds back on itself by learning from\nwhat it has done (Carr, 2015; Citron and Pasquale,\nernance system functions like a quasi-intelligent and\nadaptive system. From a normative perspective, one\nof the key concerns is to figure out how humans are\ninvolved in the different stages. Human participation in\nand comprehension of governance is typically deemed\nto be an important determinant of social and political\nlegitimacy. And, of course, the impact of such systems\non human behaviour is often key to their ethical and\nnormative evaluation. Consequently, considerable\nattention has been paid to classifying systems on the\nbasis of the type and degree of human involvement.\nCitron and Pasquale (2014), for instance, adopt a\nclassification system utilized in military contexts to dis-\ntinguish between systems in which humans are in the\nloop, on the loop or off the loop.\nIdentifying the problems with algorithmic\ngovernance\nAlthough we did not seek to add additional complexity\nto these classificatory systems with our study, we found\nthem valuable when it came to understanding and iden-\ntifying the potential shortcomings or problems in the use\nof algorithmic governance systems. This is the second\nmajor trend in the current literature and the one to\nwhich we tried to contribute more directly. There are\nmany proposed benefits to algorithmic governance,\nincluding speed, efficiency, comprehensiveness and fair-\nness (Domingos, 2015; Mayer-Schonberger and Cukier,\ncritical algorithm studies which seeks to locate the\nsocial, ethical, political and legal problems that may\nbe produced or reinforced by these systems (Gillespie\nand Seaver, 2016). There is a large and well-known lit-\nerature on the privacy and data protection issues asso-\nciated with the surveillance systems that underlie\nalgorithmic governance (Polonetsky and Tene, 2013).\nThere are several studies highlighting potential biases\nin the collection and utilization of data (Crawford,\nother studies expressing concerns about the emerging\n`Big Data divide' which ensures that only large\nDanaher et al. 3\ninstitutions can realize the benefits of the data revolu-\ntion (Mittelstadt and Floridi, 2016). There are many\nscholars talking about the opacity and lack of transpar-\nency that might be inherent in algorithmic governance,\nparticularly when the governance system is driven by\nwhen it is protected by a network of secrecy laws pro-\ntecting such algorithms (Pasquale, 2015). There are also\nconcerns about the inaccuracies, inefficiencies and unin-\ntended consequences of these systems. All of these\nproblems threaten to undermine the effectiveness and\nlegitimacy of algorithmic governance.\nZarsky (2016) provides a taxonomy for classifying\nall the problems discussed in the literature to date. He\nargues that algorithmic decision-making systems have\ntwo key properties: they are potentially opaque and can\nbe automated. He then argues that these two properties\ngive rise to a particular taxonomy of objections. This\ntaxonomy divides the problem space into two major\nbranches: (i) an efficiency branch (which covers prob-\nlems arising from the inaccuracy of decisions made on\nfoot of algorithmic assistance); and (ii) a fairness\nbranch (which covers problems arising from the\nunfair treatment of people under algorithmic govern-\nance systems). These branches break down into related\nsub-branches (prediction problems, bad data problems,\nunfair wealth transfer problems, arbitrariness problems\nand so on), allowing us to map out a reasonably com-\nprehensive space of problems that could arise from\nalgorithmic governance (see Figure 1). Being cognizant\nof these potential problems could, according to Zarsky,\nbe a boon to future research.\nWhile Zarsky's work on this is both helpful and\ninsightful, it is largely the product of his personal,\nalbeit highly-informed, perspective on the topic. One\nof things we sought to do through our CI workshop\nwas to harness the insights of a group of scholars with\ndiverse academic, applied and industry experience in an\neffort to map out a comprehensive problem space, spe-\ncifically focused on barriers to legitimate and effective\nalgorithmic governance. We expected that the barriers\nidentified by our participants would complement those\nidentified by Zarsky but would also provide a more\ndisciplinarily diverse perspective on the problem\nspace. Furthermore, we were conscious of the fact\nthat Zarsky did not link his taxonomy of problems to\nan explicit research agenda for overcoming barriers to\nlegitimate and effective algorithmic governance. This is\nsomething we explicitly attempted through our CI\nmethodology.\nFigure 1. Zarsky's taxonomy of objections to algorithmic decision-making.1\n4 Big Data & Society\nMethods for studying algorithmic governance\nThis brings us to the final trend in the current literature,\none to which we also sought to contribute more dir-\nectly. This one relates to the identification of key\nresearch questions and methods that could further\nenhance our understanding of algorithmic governance\nand advance the critical algorithm studies agenda.\nThere has been relatively little systematic work done\non this topic to date. The most concerted effort is\nthat of Kitchin (2017). He argues that a major goal of\ncritical algorithm studies should be to better under-\nstand the processes through which algorithmic govern-\nance systems are designed and implemented. In\nparticular, he argues that attention be paid to the\n`translation' problems that arise when policy goals\nneed to be converted into computer code. He then iden-\ntifies three major challenges facing anyone who wishes\nto understand these processes and six potential meth-\nodological approaches for overcoming them. Each of\nthese methodological approaches brings with it a\nnumber of plausible research methods. The result of\nthis is the framework illustrated in Table 1. Note as\nyou read it that the six methodological approaches do\nnot map directly on to the three challenges but instead\nsuggest a range of potentially useful research methods\nthat might help to overcome those challenges.\nKitchin thus provides a useful starting point and\nframework for anyone wishing to do serious research\nin this area. Once again, however, the framework\nderives from the mind of a single scholar and is not\nthe product of diverse disciplinary perspectives.\nFurthermore, it is not directly linked to a more compre-\nhensive, categorised and coherent map of the problem\nspace associated with algorithmic governance. Drawing\nupon the CI of a group of scholars, we hoped to provide,\nvia our workshop, a more diverse, innovative and com-\nprehensive framework of research questions and meth-\nodologies that is directly linked to a map of the problem\nspace. This, we believe, will help to build upon and com-\nplement Kitchin's framework, and suggest ways to pro-\ngress research in the area of algorithmic governance. We\nnow turn to the methodology of our workshop and the\nresults we obtained from it.\nMethods \u00ad Collective intelligence\nFor our workshop, we used a CI methodology known\nas `interactive management' (Warfield and Cardenas,\n1994). This provided a systematic approach when\nworking with the participants at the workshop to iden-\ntify barriers to legitimate and effective3 algorithmic\ngovernance and to develop a research agenda that\nwould help to address these barriers.\nIM was originally designed to assist groups in deal-\ning with complex issues (Warfield, 1976). The theoret-\nical constructs that inform IM draw from both\nbehavioural and cognitive sciences, with a strong\nbasis in general systems thinking. The IM approach\ncarefully delineates what are known as `content' and\nResearch focus\n Translation problem: How is policy converted into code?\nResearch challenges\n Black-boxing: Algorithms are often proprietary. They are owned and controlled by companies and governments and their precise\nmechanisms are often hidden from view.\n Heterogeneity and contextual embedding: Algorithms are often created by large teams, assembled from pre-existing packages of code\nand embedded into complex networks of other algorithms.\n Ontogenetic and performative: Algorithms are not static and unchanging. They are often modified and adapted in response to user\ninteractions; they develop and change in uncontrollable and unpredictable ways.\nMethodological approaches and methods\n Examining code: Deconstruct code by sifting through documentation, map out algorithm genealogies, examine how the same task is\ntranslated into separate coding languages and run across different platforms.\n Reflexively producing code: The auto-ethnographic method, i.e. reflect on how you would convert the problem into a ruleset and\nassociated code.\n Reverse engineering: Select dummy data and see what is outputted under various scenarios (e.g. testing Google's Pagerank or\nFacebook's Edgerank), follow debates among users, interview those who try to game algorithmic systems and so on.\n Interviews and ethnographies of coders: Carefully observing and interviewing members of coding teams as they construct an algo-\nrithm.\n Unpacking the socio-technical assemblage: i.e. discursive analysis of company documents, industry material, procurement tenders,\nlegal standards and frameworks.\n Studying real world effects: Conducting user experiments, user interviews and/or ethnographies and otherwise exploring the social\neffects of algorithms.\nDanaher et al. 5\n`process' roles, assigning to participants to the work-\nshop responsibility for contributing content ideas, and\nto the facilitator of the workshop responsibility for\nchoosing and implementing selected processes, specific-\nally, methodologies for generating, clarifying, structur-\ning, interpreting and amending ideas. In an IM\nworkshop, emphasis is given to balancing behavioural\nand technical demands of group work (Broome and\nChen, 1992) while honouring design laws concerning\nvariety, parsimony and saliency (Ashby, 1958;\na variety of situations to accomplish many different\ngoals, including developing instructional units (Sato,\n1979), designing a national agenda for paediatric nur-\nsing (Feeg, 1988), creating computer-based information\nsystems for organizations (Keever, 1989), assisting city\ncouncils in making budget cuts (Coke and Moore,\n1981), improving the US Department of Defense's\nacquisition process (Alberts, 1992), promoting world\npeace (Christakis, 1987), improving Tribal governance\nprocess in Native American communities (Broome,\n1995) and training facilitators (Broome and Fulbright,\n1995). IM has also been recently used in a variety of\nbasic science applications, for example, to design a\nnational well-being measurement system (Hogan\net al., 2015b), to understand the adaptive functions of\nmusic listening (Groarke and Hogan, 2015), critical\nthinking skills (Dwyer et al., 2014) and entrepreneurial\nIn a typical IM session, a group of participants\nwho are knowledgeable about a particular situation\n(a) develop an understanding of a situation they\nface, (b) establish a collective basis for thinking\nabout their future in relation to that situation and\n(c) produce a framework for effective action. IM util-\nizes a set of group methodologies, matched to the dif-\nferent phases of group interaction and the\nrequirements of the situation. These include the nom-\ninal group technique (NGT), ideawriting, interpretive\nstructural modelling and field and profile representa-\ntions. The first two methodologies are used for gener-\nating ideas that are then structured using one or more\nof the latter three methodologies. Our workshop used\na combination of NGT, ideawriting and field represen-\ntations (see `The process' section). To our knowledge\nthis represents the first application of IM to examine\nbarriers to legitimate and effective algorithmic govern-\nance and research that would help to address these\nbarriers.\nParticipants\ndiverse academic and industry backgrounds were\ninvited to attend a CI workshop as part of an\nAlgorithmic Governance conference at NUI Galway.\nAll expenses relating to the event, including travel\nand accommodation, were funded by the Whitaker\nInstitute at NUI Galway and the Irish Research\nCouncil. Participation was on an invitation-only\nbasis. Participants were selected by the first and\nfourth authors. They were selected on the basis of\ntheir research interests and expertise, with a view to\nsecuring a reasonably diverse set of disciplinary back-\ngrounds, and to achieving some reasonable balance rep-\nresentation from both genders. We did not succeed in\nachieving ideal balance in the latter regard but did suc-\nceed in achieving a minimal target of one-third female\nparticipants. More participants were invited (20) than\ncould attend and participate (15), and two of the par-\nticipants were self-selecting (we advertised the event\nwithin our own institution and to the colleagues of\nother invited participants and asked that people who\nwere interested express their interest via email before\nreceiving an invitation to attend). The backgrounds of\nthe participants included computer science, law, library\nscience, philosophy, geography, psychology, data sci-\nence, political science and information systems. One\nof the participants was a former elected official, and\nseveral had some previous experience in the civil ser-\nvice. Three of the participants had a background in\ncomputer programming and had worked in industry.\nOne of the participants was at the time employed by\na company with a commercial interest in the technol-\nogy, but this was declared and it was made clear that he\nwas participating on a personal not a commercial basis.\nAll participants were informed about the study proced-\nure and gave their informed consent.\nThere are questions to be asked regarding the repre-\nsentativeness of the group. We sought participants on\nthe basis of their academic and technical expertise\nrather than on the basis of other criteria. We reflect\non some of the issues this may raise for the results of\nthe workshop in the concluding section.\nThe process\nThere were four steps involved in the IM process: (1)\nparticipants were asked to generate and clarify barriers\nto legitimate and effective algorithmic governance using\na modified NGT method, (2) the facilitators categorised\nthese barriers to create a field representation of barriers,\n(3) the participants engaged in multiple rounds of idea-\nwriting and group discussion to generate and clarify\nresearch options in response to barriers, (4) the result-\ning idea-writing sheets were transcribed and analysed to\nsynthesise research options for a proposed research\nagenda.\nThe nominal group technique (NGT; Delbeq et al.,\n1975) is a method that allows individual ideas to be\n6 Big Data & Society\npooled. A modified version of the standard face-to-face\nNGT method was used in the current study, with an\ninitial pool of ideas gathered via email. The NGT\nmethod involved four steps: (a) participants were pre-\nsented with a context statement and stimulus question\n(the question was `what are the barriers to legitimate\nand effective algorithmic governance?') via email; (b)\nthe participants generated five responses to this stimu-\nlus question by working alone and then sent their\nresponses to the facilitators via email; (c) the facilitators\nrecorded these ideas for posting on the walls surround-\ning the group at the workshop; (d) the participants\nengaged in a serial discussion of the listed ideas for\nthe sole purpose of clarifying their meaning (during\nthe first hour of the workshop).\nThe field representations were generated in advance\nof the face-to-face meeting and workshop with partici-\npants using the paired comparison method (Rezaei\nZadeh et al., 2016) to compare barriers in pairs and\nidentify categories of related barriers. Category labels\nwere generated after related ideas had been grouped,\nwith three interdependent coders working together to\ncategorise a total of 57 ideas.\nThe ideawriting method (Paulus and Yang, 2000)\nwas then used by participants to propose research\nideas, along with stated methods and methodologies,\nwhich could facilitate understanding and help to\naddress barriers to legitimate and effective algorithmic\ngovernance. Group members wrote their ideas on\nsheets of paper and exchanged them across an idea\ntable, silently reading one another's ideas and adding\nto the idea set, prior to group discussion on the full set\nof ideas, and round-robin presentation of ideas to the\nfacilitation team. Two trained IM facilitators facilitated\nthe ideawriting session, which lasted for 2 hours.\nResults \u00ad Barriers, research questions\nand research methods\nParticipants identified 12 major categories of barriers to\neffective and legitimate algorithmic governance (see\nFigure 2), and an additional challenge posed by the\ninterdisciplinary nature of the topic. They also identi-\nfied a wide range of research themes (see Figure 3) and\nmethods (see Figure 4) that could be used to address\nthose barriers. What follows is a description of all 13\nbarriers along with the set of research questions and\nmethods participants proposed to address and over-\ncome these barriers. This constitutes the research\nagenda proposed by the workshop.\nOpacity of algorithms\nOne major concern that has emerged in the literature\nabout algorithmic governance is the actual and\npotential opacity of such systems. The participants\nagreed that this is a problem, highlighting in particular\nhow the lack of public and governmental understand-\ning worked alongside intrinsic and manufactured opa-\ncity in the construction of algorithmic governance\nstructures.\nTo address these problems, participants suggested\nthat we try to get a better understanding of how algo-\nrithmic systems work: how they are coded and how they\ncan be de-coded. Several research methods were recom-\nmended. Some participants suggested that we study\ncoders as they programme and develop algorithms `in\nreal time', for example by following the coding process\nthrough live video-streaming services such as Periscope,\nor by `crashing' (i.e. attending and observing) hacka-\nthons. Others recommended forensic analysis and\nethnographic case studies to develop an understanding\nof how systems are developed, with a particular focus on\nhow machine learning systems develop rulesets that end\nup being used in decision-making systems. Participants\nalso recommended that we map out the stakeholder\nunderstanding of algorithmic systems, paying particular\nattention to the gap between public and expert under-\nstanding of how these systems work. This could be\nachieved through a combination of methods, including\nsurveys, case studies, citizen science approaches, inter-\nviews and visual qualitative methods that graph what\nalgorithmic systems `look like' to affected communities.\nParticipants recommended that we study more closely\nhow these systems are used in governance and the\nimpact they have on targeted communities and social\ngroups. Again, interviews, ethnographic studies, com-\nparative analysis and longitudinal studies were sug-\ngested as appropriate methods for this. Finally, one\nparticipant suggested that we develop an index of algo-\nrithmic transparency that could be used to scrutinize\nand assist in the development of algorithmic governance\nsystems. The index could be modelled on similar indexes\nfor political transparency developed by Transparency\nInternational and developed in partnership with such\nan organisation.\nTechno-utopianism\nAnother barrier raised by participants concerned nai\u00a8ve\ntechno-optimism or utopianism among politicians and\ntechnological stakeholders. This could lead them to\nrush into the widespread adoption of algorithmic gov-\nernance systems without properly reflecting on their\npotential biases and negative effects. In addressing\nthis barrier, participants felt there was a need to\nassess how widespread such techno-utopianism was\nand clarify its causes. The use of discussion groups\nand observational studies was identified as a way to\nmap current attitudes toward these technologies.\nDanaher et al. 7\nFigure 2. Categories of barriers to legitimate and effective algorithmic governance, including sample barrier statements.\n8 Big Data & Society\nOne participant suggested that we analyse speeches\nmade by politicians and other key decision-makers to\nsee how frequently they take-up `popular science' ideas\nin their discussions of the technology. Careful analysis of\ncultural depictions and representations of the technolo-\ngies was also suggested as being important when it came\nto understanding causes and origins of techno-utopian-\nism. Participants felt it was important to understand the\nlong-term effects of techno-utopianism with one person\nsuggesting that a longitudinal corpus analysis could be\nused to map changes in emotional attitudes toward the\ntechnology over time. Observational and comparative\ncase studies of coders, developers and relevant govern-\nment agencies during the design and implementation of\nalgorithmic governance systems was also suggested as a\nway to identify the limitations and biases that might\nresult from techno-utopianism. In addition to this, par-\nticipants felt we should explore various ways in which to\nraise awareness of techno-utopianism, perhaps through\nthe use of video games and comics. In reflecting on these\nsuggestions after the workshop, we would suggest that in\norder for these research methodologies to be effective,\nresearchers would need a definition and clear measure of\n`techno-utopianism'.\nFigure 3. Key research themes in response to barriers to legitimate and effective algorithmic governance.\nDanaher et al. 9\nTechno-pessimism\nContrasting with the previous barrier, several of our par-\nticipants suggested that pessimism regarding advances\nin technology could be a barrier to effective and legitim-\nate algorithmic governance. They proposed several lines\nof research to understand more about such techno-\npessimism. They identified a need to form a common\nunderstanding of what constitutes techno-pessimism,\nwhen it may be justified, and under what conditions it\napplies to one specific technology or towards technology\nin general. Participants suggested that we look at\nwhether harms due to the proliferation of technological\ndevices in young peoples' lives are evident. This could\nshed light on whether generalised techno-pessimism is\njustified. Investigations into the possible harms that\nalgorithmic governance presents, such as defamation\nand damage to credit were also suggested when examin-\ning a more narrow type of techno-pessimism. Another\nsuggestion was to examine analogous historical experi-\nences of techno-pessimism in governance.\nFollowing the development of a coherent definition\nof techno-pessimism, participants suggested that\ninvestigations into the prevalence of techno-pessimism,\nparticularly amongst regulators, would be appropriate.\nThis would require the development and validation\nof measures of techno-pessimism. The development of\nsuch measures would facilitate two further lines of\nresearch suggested by our participants, one of which\nwould focus on the cause of techno-pessimism and\nthe other on its effects. Participants mentioned specific\ncauses of techno-pessimism that might warrant investi-\ngation. These were: negative experiences of technology,\ndata protection hype, lack of knowledge, age effects,\nlevel of political engagement and the speed of release\nof technologies. Participants also mentioned possible\neffects of techno-pessimism which might warrant inves-\ntigation, including the possible disadvantaging of cer-\ntain sectors of society due to their fear of technology.\nBoth quantitative and qualitative methodologies were\nproposed as suitable for progressing these lines of\nresearch, although a broader range of qualitative meth-\nodologies were suggested including interviews, case stu-\ndies, focus groups and observational methods. Surveys\nand experiments were proposed as possible quantitative\napproaches.\nA \u00ad Case Study\nB \u00ad Survey\nC \u00ad Interview\nD \u00ad Action Research\nE \u00ad Discourse Analysis\nF \u00ad Experiment\nG \u00ad Focus Group\nH \u00ad Systems Modelling\nI \u00ad Review\nJ \u00ad Ethnography\nK \u00ad Legal Case Analysis\nL \u00ad Historical Stud y\nM \u00ad Observation\nN \u00ad Visual Analysis\nO \u00ad Mental Model Analysis\nP \u00ad Forensic Analysis\nQ \u00ad Cross-national Comparison\nR \u00ad Secondary Data Analysis\nS \u00ad Economic Cost Analysis\nT \u00ad Intervention\nU \u00ad Process Tracing\nFigure 4. Frequency count of proposed research methods.\n10 Big Data & Society\nTechnological uncertainty\nThe apparent uncertainty regarding the effects of\ntechnological development was seen as a significant\nbarrier to effective and legitimate algorithmic govern-\nance. Participants thought that the uncertainty had\nboth subjective and objective dimensions.\nOn the objective side, it was suggested that, in gen-\neral, we have a poor understanding of the contingent\nways in which technology develops. This was linked in\nour participant discussions to the experimental and eth-\nically uncertain nature of all new technologies.\nProposed research strategies included investigating the\neffects of technological development from both a his-\ntorical and ethical standpoint. The historical approach\nwas seen as useful for examining the unintended conse-\nquences of new information systems, and for mapping\nthe dynamic evolution of technology. One suggestion\nwas to acquire all documents on a completed algorith-\nmic governance system using a Freedom of\nInformation request in order to examine how that\nsystem changed from development to implementation.\nOn the subjective side, technological uncertainty was\nseen as a matter of perception, caused by either a lack\nof technological understanding or lack of interest\namong the public and policy-makers. Participants sug-\ngested that our research strategies focus on ascertaining\nthe extent of such uncertainty and its causes. They felt\nthat research could be done to see how best to address it\nthrough interviews, focus groups and awareness-raising\nprojects. Participants also felt that the lack of clarity\nregarding the contribution of government officials to\nalgorithm development in the public sector was a\nsignificant source of technological uncertainty. This\nsuggested there was a need to investigate how major\ne-government systems are developed and it was sug-\ngested that this could be done using ethnographic meth-\nods. Another suggested line of research in this vein was\nthe investigation of how policy-makers work with\nexperts and how governments ensure that specific\nexpertise is factored into algorithmic decision-making.\nCapacity/Knowledge among technologists\nTechnical experts wield a lot power when it comes to\nthe design of effective and legitimate governance sys-\ntems, but several of our participants worried that tech-\nnical experts lack knowledge of the legal (and other)\ngovernance systems with which they interact. They\nalso worried that such experts might be unaware of\ntheir own implicit biases and how they affect the\ncoding process, and might be hostile to outsiders who\nlack their technical expertise.\nTo address these barriers, participants suggested that\nwe get a clearer sense of the attitudes of programmers\ntowards themselves, their work and those outside their\ndiscipline. In particular, they felt that researchers\nshould figure out the extent to which technologists are\ndeveloping their own internal culture and groupthink,\nthe extent to which they resist critical outside perspec-\ntives, and their overall awareness of implicit biases.\nDeveloping this understanding could be achieved\nthrough case studies, surveys and interviews.\nFurthermore, since attitudes toward, and awareness\nof, law was a particular concern, our participants felt\nthat research should focus on legal knowledge and reac-\ntions to law among technologists. This could include\ntests of legal knowledge, analysis of the extent to\nwhich programmers incorporate legal changes into\ntheir code, and the extent to which coders try to\ndefend themselves from litigation in their coding deci-\nsions. Surveys, case studies and interviews were again\nsuggested as the preferable methods. Finally, several\nparticipants recommended that we examine the educa-\ntional background of technologists and consider the\nbenefits of a broad life-long learning model for\ntechnologists.\nCapacity among public servants and\nrepresentatives\nIn a similar vein, several of our participants were con-\ncerned that a lack of competence amongst politicians\nand public sector workers could be a barrier to effective\nand legitimate algorithmic governance. The concerns\nvaried from questions about mathematical abilities\namong politicians, to questions about procurement\nand information systems-capacities in public sector\norganisations. To research these barriers, participants\nsuggested, in the first place, that we have some clear\nsense of the definitions of `capacity' and `competence'\nin the public service context. Clarity could be achieved\nby recruiting focus groups of public service workers to\ncluster and refine definitions. Understanding the extent\nof the capacity problem in the public sector was also\ndeemed to be an important research priority.\nParticipants said that research should focus on identify-\ning the levels of competence needed by key decision-\nmakers. This could be achieved through historical\ncase studies of public sector organisations, qualitative\nand narrative interviews with key actors, interviews and\nfocus groups analysing understandings of uncertainty,\nand comparative studies of different government agen-\ncies and private sector workers. Getting a better handle\non politicians' competencies with mathematical reason-\ning and information systems was also deemed to be key\nresearch strategy. Finally, participants recommended\nthat research should focus on how problems with com-\npetence arise and get managed within the public sector.\nIn particular, they suggested that studies be conducted\non who gets recruited to the public sector, who gets\ndecision-making authority, what is the relationship\nbetween the public and private sector, and how are\npeople held to account when things go wrong. A\nnumber of methods were recommended in this regard,\nincluding actor-network theory methods, to map out\nrelationships between individuals and social organisa-\ntions, comparative and cross country analysis of gradu-\nate destinations, freedom of information requests to\ncollate relevant information, and detailed case studies.\nCapacity among lawyers and legal systems\nIf algorithmic governance is something that is ultim-\nately made possible and held to account through law,\nthen it is important to understand the capacity of legal\nsystems and legal actors to manage it. Our participants\nwere sensitive to this need, with several expressing con-\ncerns about the training of lawyers, the inflexible and\nconservative nature of legal codes, and the possibility of\nregulatory gaps opening up which prevent people from\nchallenging the negative outcomes of algorithmic gov-\nernance. To overcome these barriers, participants sug-\ngested that we survey the extent to which legal\nproblems are already arising from algorithmic govern-\nance systems. This could be achieved by literature\nreviews and interviews with key legal figures (e.g. pros-\necutors or regulators). The creative exploration of pos-\nsible legal problems was also recommended, with one\nparticipant suggesting that this could be achieved\nthrough experimental moot courts (mock trials).\nParticipants also suggested that we identify capacity-\nrelated problems by analysing existing code and cate-\ngorizing errors that emerge from this code according to\nwhether they are `technical' or `legal' in nature. This\ncould be enabled through case studies that combine\ninterviews with document analysis.\nLegal and institutional complexity\nComplexity and a lack of transparency about algo-\nrithms are often deemed to be barriers to legitimate\ngovernance. Our participants expressed some concerns\nabout how complexity in legal-bureaucratic systems\nthat implement algorithmic governance systems could\ncontribute to these problems. Several distinct worries\nwere expressed. Some participants worried about the\ncomplexity of bureaucratic systems in themselves;\nsome worried about the increased complexity resulting\nfrom the use of ICT within those systems; and some\nworried about the ways in which laws contribute to the\nlack of transparency associated with algorithmic gov-\nernance systems. Participants recommended that\nresearch be undertaken to address each of these three\nconcerns. Some participants suggested that we\ninvestigate previous and existing complexities in bur-\neaucratic systems, using historical case studies, visual\ncartographies of the relationships between different\norganisations, process tracing, and comparative stu-\ndies. One participant suggested that we take advantage\nof our experience with existing regulatory regimes that\nrequire the collection and tracking of environmental\ninformation and conduct a detailed study of how insti-\ntutions respond to regulatory change. Others suggested\nthat we focus on how ICT is adopted and deployed\nwithin bureaucratic systems. This could be done by\ncomparing use of proprietary and open source systems\nin data-management, and by conducting a mental\nmodel analysis of key stakeholders that compare how\nthey think ICT systems work with their actual oper-\nation. Finally, one participant recommended that we\ninvestigate the way in which algorithmic governance\nsystems are described and framed in legal cases invol-\nving privacy and data protection issues. There are many\ncases on these issues already and they provide insight\ninto how legal systems might cope with algorithmic\ngovernance more generally.\nCommercial and public interests\nThe lack of balance between private profit-driven inter-\nests and public socially-driven interests was seen as\nanother significant barrier. One participant noted that\nthis lack of balance has led to a gap between the pace of\nthe commercial development of algorithmic decision-\nmaking systems and the more limited applications in\nthe public sector. The participant suggested that\npeople in the private sector were reluctant to slow\ndevelopment in order to ensure effectiveness, as this\nwould curb commercial success. Underlying this con-\ncern is the broader conflict between the values and\ngoals of private and public bodies, which another par-\nticipant said was not sufficiently acknowledged. They\ngave the example of the lack of regulation put in place\nfollowing the privatisation of sensitive areas such as\nhealthcare. A number of research strategies for investi-\ngating both the balance between commercial and public\ninterests and the balance between commercial and indi-\nvidual interests were recommended. Regarding the\nformer, participants suggested that the differences in\nperspective between government IT departments and\nprivate sector contractors should be explored through\ncontent analysis of e-government policy documents and\nthe brochures and websites of vendors. This could help\ninform the development of a model of the goals and\nvalues of private and public actors. Regarding the bal-\nance between commercial interests and individual\nrights, participants recommended that we explore cur-\nrent attitudes and perspectives toward algorithmic gov-\nernance and regulation, and raise awareness of\n12 Big Data & Society\nalgorithmic governance. Participants identified a\nnumber of secondary data methods that could advance\nthis research strategy. These included case studies and\ntraditional legal and economic analysis research meth-\nods. Participants also identified a number of primary\ndata methods, including public consultation, inter-\nviews, focus groups and surveys. The final research\nstrategy recommended by participants in relation to\nthis barrier was to examine the effects that a greater\nfocus on privacy might have on the legitimacy and\neffectiveness of algorithmic governance. Participants\nsuggested that the practice of `privacy by design' be\nresearched, and potentially followed up by awareness-\nraising training in commercial settings. Case studies of\ndata overload could also be used to shed light on\nwhether more data is always better. Furthermore,\nmethods from economics could establish whether\nthere are benefits to more discerning data collection.\nEffective governance versus individual rights\nParticipants felt that, in considering the development of\nalgorithmic decision-making technologies, care must be\ntaken to ensure that a focus is maintained on individual\nrights and fair treatment, rather than solely on effective\ngovernance. For example, participants suggested that\nwe prioritise research into the ways in which inequality\nand bias can be embedded in algorithms. A number of\ncase studies already exist looking at this and it was felt\nthat more should be undertaken. Participants also sug-\ngested that a key research priority was to examine the\ncompetition between efficiency and fairness in govern-\nance by surveying the public about their conception of\neffective governance. This could be combined with an\ninvestigation of into how technology can be employed\nto enhance government transparency and citizen par-\nticipation. This strategy was suggested in response to\nthe perceived apathy towards participative governance\npractices among political leaders. One participant felt\nthat research into smart cities and their potential to\nenable participative governance would be a useful con-\ntribution to this research agenda. The piloting of block-\nchain technology as an alternative governance\nmechanism was also suggested as a means of increasing\ntransparency and participation.\nEthical awareness (or lack thereof)\nIn relation to ethical awareness, participants high-\nlighted inadequate consideration of ethical concerns\nwith algorithms and failure to integrate deep social\nand ethical thinking into technology education as sig-\nnificant barriers. They also highlighted the lack of\nawareness of ethical implications of apparently neutral\nalgorithms and the failure to recognize the political\nethical dimensions of Big Data. In response to these\nbarriers, participants suggested that it would be import-\nant to examine current levels of awareness and know-\nledge of ethical issues in the area of algorithmic\ngovernance among coders, politicians and the public.\nThey highlighted the value of historical analyses and\ncase study analyses in shedding light on how under-\nstanding of ethical issues developed in analogous\ndomains (e.g. the development of medical ethics in\nthe field of medicine) and how analysis of specific\ncases of algorithmic governance can shed light on key\nethical issues in political decision-making. Participants\nalso highlighted the importance of directly analysing\nthe ethical consequences of algorithmic governance,\nfor example, in areas such as predictive policing and\nprofiling, where there is potential to perpetuate ethnic\nbias and social dynamics within communities. Related\nto this is the need for an analysis of the language of\npolitics and Big Data studies, and how ethics are\nreflected in the language of agency, depoliticisation,\nand hegemony; and how political bias might be\nreflected in Big Data decisions. Analyses of ethical\nframeworks and existing codes of practice used in\nboth technology education and Big Data applications\nwas also seen as important, alongside an analysis of the\nethical decision-making practices of data scientists and\nthe ethical deliberation of politicians. From a techno-\nlogical point of view, participants proposed that we\nanalyse the extent to which ethical development can\nbe incorporated into machine learning. Participants\nalso recommended some applied research goals, such\nas trying to write an open-source charter of algorithmic\nethics and to investigate how research ethics commit-\ntees are currently handling algorithms.\nPrivacy and informed consent\nIn relation to privacy and informed consent, partici-\npants highlighted inadequate privacy protections, fail-\nure to adequately protect human rights, dissonance\nbetween algorithmic systems and regulatory/legislative\nframeworks, conflict between the private and public\ninterests, and failure to fully inform the citizenry of\nthe multiple and myriad uses of their data as barriers.\nIn response to these barriers, participants highlighted\nthe importance of exploring the public's understanding\nof informed consent, their knowledge of the uses of\ntheir data and the uses of algorithms using surveys,\ninterviews, vignette studies and laboratory studies.\nThey also proposed case studies and survey studies\ndesigned to examine public attitudes toward conveni-\nence versus protection of rights (i.e. how willing are\npeople to sacrifice privacy rights in return for cheaper\nand more effective services?). They highlighted the need\nto review the literature on privacy as defined in law and\nthe need to adopt cross-discipline studies that compare\nstandards of consent in different legal fields.\nThe challenge of interdisciplinarity\nThe landscape of interdependent barriers outlined\nabove, and the range and scope of research needed to\nunderstand and overcome those barriers highlighted, to\nall participants, the need for interdisciplinary cooper-\nation. This was felt by most to be a generic challenge,\npresent in many cross-disciplinary fields of research,\nbut one that should not be ignored. Participants\nnoted a range of issues that may have a negative\neffect on progress in this regard, including the dismissal\nof different perspectives in the field, a continuing know-\nledge gap and the lack of shared perspectives between\ntechnologists and others, which results in the under-\ntheorised nature of both approaches to algorithmic\ngovernance. Participants highlighted a separation\nbetween domain-level experts and developers that led\nto systems that fail to match ethical and legal require-\nments. They noted that inadequate communication and\nlack of common language and cause among academics,\npolicy makers and private sector actors, and lack of\nopportunities for deep multidisciplinary engagement,\nwere also barriers to effective and legitimate algorith-\nmic governance. In response, participants highlighted\nthe value of survey, interview and ethnographic studies\nexploring competing attitudes of technologists and\nsocial scientists and the language used by different dis-\nciplines to describe similar phenomena. They proposed\nresearch focused on the nature of successful collabor-\nation, and case study, document, and interview analysis\nof existing multidisciplinary projects and the iterative\ncollaborative development of understanding. They\nhighlighted the need for a review of available curricula,\nand interviews and surveys of students and teaching\nprofessionals to examine the form of training under-\ntaken in different disciplines and how different educa-\ntional practices may perpetuate difficulties associated\nwith interdisciplinary communication. Participants\nhighlighted the potential to theorize and establish a\ncommon approach to multiperspectivalism using col-\nlaborative writing methodologies. They also proposed\nthe establishment of new networks of cross-disciplinary\nresearchers who can work together to overcome inter-\ndisciplinary challenges and advance understand of\neffective and legitimate algorithmic governance.\nConclusion: Mapping the research\nagenda\nWe present the full map of barriers, research questions\nand research methods/strategies that was produced\nthrough our CI workshop in Table 2. We want to con-\nclude by explaining the contribution we believe this\nmap makes to the existing literature on algorithmic\ngovernance, highlighting the limitations of what we\ndid, and identifying ways in which the research\nagenda we produced could be developed and enhanced\nin the future.\nWe can explain the contribution most easily by com-\nparing what the workshop produced with the existing\nframeworks we discussed in `Context: Understanding\nalgorithmic governance' section. Consider, for instance,\nthe frameworks that have been put forward by Zarsky\n(Figure 1) and Kitchin (Table 1). Zarsky's was a tax-\nonomy of the problems that arise from the use of algo-\nrithmic governance systems. This taxonomy focused on\ntwo major categories of problems (efficiency and fair-\nness), and broke those down into a series of sub-pro-\nblems. The barriers identified in our CI workshop\ncovered much of the same territory, with participants\nalso highlighting specific concerns about inefficiencies,\nbiases, lack of transparency and unfairness in the\nimplementation of these systems. But the participants\nwent much further than Zarsky, highlighting how the\nefficiency and fairness problems were connected to\nother problems in education, public understanding,\ntechnical competence, recruitment, institutional com-\nplexity, gaps in legal standards and more. The result\nis a much richer understanding of the problem space\ninvolved in this debate. On top of this, our participants\nlinked these problems to specific research questions and\nmethodologies and thus identified ways in which we\nmight better understand these problems and contribute\nto their solution.\nSomething similar is true when we compare the\nresults of our workshop with the research framework\nproposed by Kitchin. Where he highlighted one major\nfocus, three research challenges, and six research stra-\ntegies, our participants identified 12 major barriers to\neffective and legitimate governance and an additional\nchallenge relating to multidisciplinary study, at least\n48 distinct research questions, and 65 research applica-\ntions. There is, consequently, a different level of com-\nprehensiveness and breadth to the results of our\nworkshop than is currently found in the literature. To\nbe sure, our participants identified similar challenges\nand methods to Kitchin, but their collective efforts pro-\nduced a more fine-grained analysis of the challenges,\nand a more complete mapping of the research that\nneeds to be done to address each of these challenges.\nOn top of this, we think they hit upon some interesting\nand novel research methods, including the use of live-\nstreaming video to study coders as they code, partner-\ning with political transparency organizations to create\nmeasures of algorithmic transparency, the construction\n14 Big Data & Society\nof visual cartographies and the use of actor-network\ntheory to better understand institutional and legal com-\nplexity, among many other proposed methods.\nOf course there are limitations to what we produced.\nIt is important that anyone proposing to use our\nagenda is aware of that. One obvious limitation con-\ncerns the representativeness of the group involved. The\nresearch agenda we developed was the product of a\nparticular group of people, working together over a\nparticular period of time. There is no doubt that\nimportant perspectives were missing from what we\ndid. As we mentioned in the `Context: Understanding\nalgorithmic governance' section, the participants were\ninvited on the basis of academic and technical expertise\nand interest. Only one of the 15 was currently\nemployed in industry, the remaining 14 were all cur-\nrently employed in academic institutions. Some of\nthose currently employed in academia had back-\ngrounds in industry and government, and this was\none of the selection criteria, but their current form of\nemployment no doubt limited their perspective on the\nissue. On top of this, although there was some attempt\nto achieve disciplinary and gender diversity, other\nforms of representativeness were not sought. As the\ncurrent research literature and our research agenda\nitself suggests, algorithmic governance systems may\nembed certain forms of bias and may disproportion-\nately affect members of minority groups (ethnic,\nracial, sexual, disability-related, etc.). While some of\nour participants may belong to such groups, we did\nnot select them for that reason and hence the absence\nof a more explicit recognition and engagement with\nminority perspectives means that there could well be\ngaps in what we have produced. We would defend\nthe appropriateness of our academic-oriented selection\ncriteria given that our aim was to produce a research\nagenda that would be useful to academic researchers,\nbut there is certainly room for others to repeat the\nexercise with different groups and compare the results\nwith what we have produced.5\nAllied to this, the fact that the research agenda was\nproduced by a particular group on a particular day\nmeans that our participants will undoubtedly have\noverlooked or ignored other possible research questions\nand methods. More work needs to be done to add-in\nthe missing perspectives and fill-in the gaps, perhaps by\nreconciling and cohering our agenda with those already\nprovided. A particular concern in this regard, and one\nraised by several of the reviewers on this paper, was the\napparent absence of more critical/radical perspectives\non the topic of algorithmic governance from our\nresearch agenda. It is worth noting that such perspec-\ntives are not entirely absent from what has been pro-\nduced. The barriers originally identified by the\nparticipants consisted in statements/propositions,\nwhich we then grouped together and reduced to\nsimple descriptive labels (such as `opacity', `techno-\npessimism, `public vs. private interests' and so on).\nSeveral of these statements6 \u00ad particularly those relating\nto techno-utopianism, inequality vs. rights, and public\nvs. private interests \u00ad were quite explicitly radical/crit-\nical in their focus, challenging the more mainstream\nliberal political focus adopted by others. These state-\nments were displayed to all participants on the day of\nthe workshop on the walls of the room in which the\nsession took place. Furthermore, the workshop itself\ntook place after a more traditional academic conference\nconsisting of short paper presentations. Several of these\npapers adopted a more radical and critical perspective\non the topic and those perspectives continued to be\ndiscussed in the workshop session itself.7 The research\nagenda we have produced (see Table 2) may seem to be\nshorn of those perspectives, but we would argue that\nthis is not necessarily the case. We have reduced the\ndiscussions and conversations from the day to a series\nof reasonably concrete research questions and methods.\nWe would argue that several of these questions and\nmethods are open to those who wish to pursue a\nmore radical/critical research agenda. That said, we cer-\ntainly acknowledge that the way in which we framed\nthe workshop (asking participants to focus on the ques-\ntions of legitimacy and effectiveness) had a mainstream\nliberal/political orientation. We encouraged partici-\npants not to take this framing for granted in their con-\ntributions, but this could have affected the results we\nproduced. We also acknowledge that we tried to facili-\ntate dialogue at the workshop that represented the full\nrange of perspectives of participants, including those\nthat were negative or critical of the possibilities for\nalgorithmic governance.8\nDespite these limitations, we would argue that by\nharnessing the power of CI, we have produced the\nmost comprehensive mapping of the research agenda\nto date \u00ad something that researchers can begin to use\nand develop right now. But no research agenda is ever\ncomplete and final. They are and should be subject\nto critique, iterative change and development. Future\nCI workshops of this sort could be used to facilitate\nfurther interdisciplinary collaborations on this import-\nant topic, perhaps by trying to represent different\ngroups in the conversation and discussion. This is\nlikely to be made necessary anyway by the fluid and\nrapidly-changing nature of the technologies underlying\nalgorithmic governance structures. Nevertheless, we\nthink the methodology we adopted to produce this\nresearch agenda, and the agenda that was actually pro-\nduced, provide a firm platform on which future\nresearchers can build.\nTable 2. A research agenda for algorithmic governance\nBarriers to effective\nand legitimate\nalgorithmic\ngovernance Potential research questions Potential research applications\nOpacity of\nalgorithms\nHow are algorithms coded?\nCan they be decoded?\nHow are they understood by those\naffected?\nHow are they used in governance?\nHow do they affect relevant commu-\nnities?\nCan we measure the transparency of\nalgorithms?\nStudy coders as they programme in real time using video-streaming\nservices like periscope or at hackathons.\nForensic analysis of algorithms (source code; pseudo-code, etc.)\nEthnographic case studies of programmers\nSurveys, case studies, citizen science, interviews and visual qualitative\nmethods to find out how stakeholders understand these systems.\nInterviews, ethnographic studies, comparative analysis and longitudi-\nnal studies of communities affected by algorithmic governance.\nDevelop an index of algorithmic transparency, working in consulta-\ntion with organisations who measure political transparency\nTechno-utopianism How widespread is techno-utopianism?\nHow frequently are appeals made to\ntechno-utopian ideals in political cir-\ncles?\nWhat are the long-term effects of\ntechno-utopianism?\nWhat are the limitations and biases that\nresult from techno-utopianism?\nDiscussion groups and observational studies to map out current\nattitudes toward the technology\nAnalysis of political speeches and cultural representations of tech-\nnologies.\nLongitudinal corpus analysis to map changes in emotional attitudes\ntowards technology over time.\nObservational and comparative studies of coders, developers and\nrelevant government agencies during the design and implementa-\ntion of algorithmic governance systems.\nAwareness-raising exercises of techno-utopianism through the use of\nvideo games and comics.\nTechno-pessimism Can we form a common understanding\nof techno-pessimism?\nWhen is techno-pessimism justified?\nUnder what conditions is techno-pes-\nsimism narrowly focused on one\ntechnology or broadly focused\ntowards technology in general?\nHow prevalent is techno-pessimism?\nWhat are the causes of techno-\npessimism?\nEmpirical investigations (surveys. interviews, experiments, etc.) of\nharms to people due to the proliferation of algorithmically-\nmediated devices.\nInvestigation of specific harms resulting from techno-pessimism such\nas defamation and harm to credit.\nHistorical case analysis of periods of techno-pessimism in governance.\nSurveys, experiments, interviews, observational studies and focus\ngroups with regulators and other key figures involved in algorith-\nmic governance in order to determine prevalence and causes of\ntechno-pessimism.\nTechnological\nuncertainty\nDo people understand the contingent\nand uncertain ways in which tech-\nnology develops?\nWhat historical examples are there of\nunintended consequences arising\nfrom technological uncertainty? Can\nwe learn from such examples?\nIs there a lack understanding and\ninterest in technological develop-\nment among the public and policy\nmakers?\nFOI requests and document analysis on all documents relating to the\nconstruction of an algorithmic governance system in order to\nexamine how the system changed from development to imple-\nmentation.\nInterviews, focus groups and ethnographic studies of policy-makers\nand technological experts as systems are developed.\nInterviews, focus groups, ethnographic studies and awareness-raising\nprojects in order to track lack of understanding and the gap\nbetween expert and public knowledge.\nCapacity/\nKnowledge\namong\ntechnologists\nDo technical experts lack knowledge of\nthe legal and governance systems\nwith which they interact?\nAre technical experts aware of their\nown implicit biases and how these\nmight affect the coding process?\nAre technical experts hostile to outsi-\nders who lack their technical\nexpertise?\nCase studies, surveys and interviews directed at:\n- Assessing overall awareness of implicit bias\n- Internal culture and groupthink among organisations building algo-\nrithmic governance systems\n- Understanding and knowledge of governance systems.\n- Figuring out the extent to which coders try to defend themselves\nfrom litigation in coding decisions\nTests and quizzes of legal and regulatory knowledge.\n(continued)\n16 Big Data & Society\nTable 2. Continued\nBarriers to effective\nand legitimate\nalgorithmic\ngovernance Potential research questions Potential research applications\nCapacity among\npublic servants\nand\nrepresentatives\nWhat does capacity and competence\nmean in the public service context?\nDo politicians and public servants\nunderstand how algorithmic govern-\nance systems work?\nWhat levels of competence are needed\namong key decision-makers when it\ncomes to algorithmic governance?\nHow do competence problems arise\nand get managed within the public\nsector?\nWhat is the relationship between the\npublic and private sector?\nHow are actors held to account when\nsomething goes wrong?\nWho gets hired in the public service to\ndeal with algorithmic governance?\nFocus groups of public service workers to cluster and refine defini-\ntions of capacity and competence.\nHistorical case studies on capacity-related problems in public service.\nNarrative interviews and focus groups to assess needed levels of\ncompetence.\nComparative studies of competence and understanding across gov-\nernment agencies and private sector workers.\nActor network theory methods to map out relationships between\nindividuals and social organisations.\nComparative and cross country analysis of graduate destinations.\nFreedom of information requests combined with detailed case studies\nof competence failures/successes.\nCapacity among\nlawyers and legal\nsystems\nWhat legal problems are arising from\nthe emergence of algorithmic gov-\nernance systems?\nAre regulatory gaps opening up as a\nresult of algorithmic governance?\nIs the training of lawyers adequate to\ndeal with the challenges emerging\nfrom algorithmic governance?\nCase studies and literature reviews of problems emerging in existing\nlitigation.\nInterviews with key legal actors, e.g. prosecutors and regulators.\nExperimental moot courts to explore new problems and gaps that\nmight be arising.\nAnalyse existing code and categorising errors that emerge according\nto whether they are technical or legal in nature. Combine this with\ninterviews and document analysis.\nInstitutional and\nlegal complexity\nHow complex are bureaucratic systems\nin themselves?\nDoes the level of complexity increase\nas a result of increased use of ICT\nwithin bureaucratic systems?\nDo laws contribute to a lack of algo-\nrithmic transparency?\nHistorical case studies of bureaucratic complexity\nVisual cartographies to map relationships between different organi-\nsations, combined with process tracing and comparative studies.\nStudy systems that already track and collate information (e.g. envir-\nonmental regulatory systems) and conduct detailed analyses of\nresponse to regulatory change.\nCompare use of proprietary and open source systems in data-man-\nagement.\nConduct mental model analysis of key stakeholders in regulatory\nsystems to compare how they think ICT systems work with their\nactual operation.\nInvestigate the ways in which algorithmic governance systems are\ndescribed and framed in existing legal case law on privacy and data\nprotection.\nClash between\ncommercial and\npublic interests\nIs there a reluctance to slow techno-\nlogical development down in order\nto ensure effectiveness and legiti-\nmacy?\nIs there a failure to acknowledge the\nclash of values between public and\nprivate bodies, particularly when\nalgorithmic governance systems are\nbeing created?\nAre we/Can we balance commercial\ninterests and individual rights?\nWhat are the commercial/public effects\nof a greater focus on privacy?\nContent analysis of e-government policy documents and brochures\nand websites of commercial service providers.\nEconomic and legal case studies of attitudes toward and use of\nalgorithmic governance systems.\nCost\u00adbenefit analysis regarding risks/rewards of greater data collec-\ntion.\nPublic consultation, interview, focus groups and surveys to explore\nexisting attitudes toward and understanding of the values under-\nlying algorithmic governance systems.\nEmpirical investigation (interviews, ethnographies, surveys) of `privacy\nby design' practices.\n(continued)\nDeclaration of conflicting interests\nOne of the authors (Behan) works for a company (IBM) with\na commercial interest in the type of technology being dis-\ncussed. However, his contribution to this article is strictly in\na personal capacity.\nFunding\nThe authors would like to acknowledge the funding of the\nIrish Research Council (New Horizons Grant) and the\nWhitaker Institute, NUI Galway, without whose support\nthis paper would not have been possible.\nNotes\n1. This figure was created by the lead author of the current\npaper and is based on the discussion in Zarsky (2016). It\nwas approved by Zarsky in correspondence with the lead\nauthor.\nTable 2. Continued\nBarriers to effective\nand legitimate\nalgorithmic\ngovernance Potential research questions Potential research applications\nEffective govern-\nance versus indi-\nvidual rights\nIs effective governance being pursued at\nthe expense of individual rights and\nfairness?\nHow can inequality and bias be\nembedded in algorithmic govern-\nance?\nHow can technology enhancement\ngovernment transparency and citizen\nparticipation?\nMore case studies on biased effects of algorithmic governance sys-\ntems.\nSurveys of public opinion of what they understand by effective gov-\nernance.\nCase studies of existing programmes (e.g. smart cities) to see how\nthey facilitate transparency and participative governance.\nPilot studies of alternative governance systems such as blockchain\ntechnologies to see if they facilitate greater transparency and\nparticipation.\nEthical awareness\n(or lack thereof)\nWhat are the current levels of ethical\nawareness and knowledge among\ncoders, politicians and the public?\nHow are biases and ethical problems\ncurrently created through algorith-\nmic governance systems?\nCould ethical codes for algorithmic\ngovernance be developed?\nHistorical and case study analyses of how understanding of ethical\nissues developed in analogous domains (e.g. medical ethics)\nSurveys, interviews and tests of ethical awareness among coders,\npoliticians and public.\nEmpirical studies (experiments, surveys, ethnographies) of the ethical\nconsequences of algorithmic governance, e.g. predictive policing\nsystems.\nLinguistic analysis of the language used in discussions of Big Data and\nalgorithmic governance.\nForensic analysis of existing codes of practice in technology education\nand Big Data applications.\nEmpirical examination of ethical decision-making practices among\ndata scientists and politicians.\nDevelop machine learning models that can incorporate ethical\nlearning \u00ad measure their effectiveness.\nWrite an open source charter of algorithmic ethics.\nPrivacy and\ninformed\nconsent\nHow do the public understand\ninformed consent?\nDo the public know how their data is\nused in algorithmic governance sys-\ntems?\nAre privacy and informed consent\nprotocols effective in protecting\nhuman rights abuses?\nCase studies, surveys and interviews on public understanding of\ninformed consent.\nCase studies and surveys of public attitudes toward convenience of\ntechnology versus protection of rights.\nLiterature reviews on privacy and informed consent protections in\nlaw.\nComparative analysis of consent standards across different areas of\nlaw.\nResearch challenge Key question Strategies for addressing the challenge\nInterdisciplinarity How do we ensure successful interdis-\nciplinary collaboration on problems\nof this sort?\nCase study, document analysis and interviews of existing multidisi-\nplinary research agendas.\nReview curricula and interview and survey students and teachers\nacross existing disciplines.\nCollaborative writing methodologies to theorise a common approach\nto multiperspectival research.\nEstablish new networks of cross-disciplinary researchers.\n18 Big Data & Society\n2. This table was created by the lead author based on the\ndiscussion in Kitchin (2017). It does not appear in\nKitchin's original paper.\n3. We focused on the concepts of `legitimacy' and `effective-\nness' due to the fact that (a) they seemed sufficiently cap-\nacious to cover a number of concerns one might have\nabout this technology, (b) were similar to concepts used\nin the pre-existing literature on the problems of algorith-\nwere broadly acceptable within mainstream liberal-demo-\ncratic theory (Peter, 2017). We were conscious, however,\nthat focusing the inquiry on these two concepts could bias/\nnarrow the results of our study since they may be thought\nto exclude more radical and critical theoretical perspectives\non algorithmic governance. To overcome this problem we\nencouraged participants in our workshop to feel free to\nquestion the conceptual framework that we used. We\nreflect on our successes and failures in this regard in the\nconcluding section.\n4. Groupwork methodologies have their problems. For\nexample, the different processes used by a group to gener-\nate, critique and refine an idea set can lead to excessively\nconvergent thinking and be excessively reliant on common\nknowledge (and hence not on the knowledge provided by\nthe unique individual perspectives that are present in the\ngroup). We were conscious of these issues in the design of\nour workshop and the IM techniques we used (NGT, idea-\nwriting, field representations) have been designed to over-\ncome some of the common pitfalls of groupwork. For a\nlonger discussion of the problems with groupwork and the\ntechniques that can be used to address them, we recom-\nmend that the reader consult some of the second author's\nwell as the very comprehensive discussion in Straus et al.\n5. As one of our reviewers pointed out, there are issues even\nwithin academia concerning the representativeness of our\ngroup. After all, not everyone has the time to travel to\nattend and participate in a workshop of this sort. We\ntried to mitigate against this to some extent by ensuring\nthat the event took place outside of teaching time (for the\nIrish participants) and during the ordinary working day.\nJust over a third of the participants were locally-based (i.e.\nattending an event that took place at their ordinary place\nof employment) so they should not have faced any greater\ndifficulties in attending than they would ordinarily have\nfaced in attending work. For those travelling from other\ninstitutions, difficulties in taking time out to attend would\ncertainly be a greater issue and was the stated reason for\nmost of the rejected invitations.\n6. These statements are on file with the authors.\n7. Approximately half of the papers were published in an\nabbreviated form in the September/October 2016 edition\nof the journal Computers and Law. The more radical/crit-\nical perspective is on display in at least one of these pub-\nlished papers: Morison `Algorithmic Governmentality:\nTechno-Optimism and the Move to the Dark Side' \u00ad avail-\nable at https://www.scl.org/articles/3714-algorithmic-gov-\nernmentality-techo-optimism-and-the-move-towards-the-\ndark-side\n8. Another issue, as one of the reviewers to the paper pointed\nout, has to do with the `algorithmic' nature of the collect-\nive intelligence method. It might seem ironic and odd that\nwe have used a quasi-algorithmic method for producing a\nresearch agenda about algorithmic governance. We appre-\nciate this irony. But we think there are broad (any rule-\nfollowing process) and narrow (a computerized, auto-\nmated rule-following system) interpretations of what it\nmeans for something to be `algorithmic'. Our research\nmethod was algorithmic in a broad sense, but we are not\nsure that it is algorithmic in the narrow sense and the\nnarrow sense is the one covered by the proposed research\nagenda. Nevertheless, there is an interesting `meta'\nresearch question to be posed about the method and\nwhether it compounds or alleviates concerns about algo-\nrithmic (in the narrow sense) governance.\nReferences\nAlberts H (1992, March) Acquisition: Past, present and\nfuture. Paper at the meeting of the Institute of\nManagement Sciences and Operations Research Society,\nOrlando, FL.\nAneesh A (2006) Virtual Migration. Durham, NC: Duke\nUniversity Press.\nAneesh A (2009) Global labor: algocratic modes of organiza-\nAshby WR (1958) Requisite variety and its implications for\nthe control of complex systems. Cybernetica 1(2): 1\u00ad17.\nBoulding KE (1966) The Impact of the Social Sciences. New\nBrunswick, NJ: Rutgers University Press.\nBroome BJ (1995) Collective design of the future: Structural\nanalysis of tribal vision statements. American Indian\nBroome BJ and Chen M (1992) Guidelines for computer-\nassisted group problem-solving: Meeting the challenges\nBroome BJ and Fulbright L (1995) A multi-stage influence\nmodel of barriers to group problem solving. Small Group\nBurrell J (2016) How the machine thinks: Understanding opa-\ncity in machine learning systems. Big Data and Society.\nCarr N (2015) The Glass Cage: Where Automation is Taking\nUs. London: The Bodley Head.\nChristakis AN (1987) Systems profile: The Club of Rome\nCitron D and Pasquale F (2014) The scored society: Due\nprocess for automated predictions. Washington Law\nCoke JG and Moore CM (1981) Coping with a budgetary\ncrisis: Helping a city council decide where expenditure\ncuts should be made. In: Building City Council\nLeadership Skills: A Casebook of Models and Methods,\nWashington, DC: National League of Cities, pp. 72\u00ad85.\nCormen T (2013) Algorithms Unlocked. Cambridge, MA:\nMIT Press.\nCormen T, Leiserson CE, Rivest RL, et al. (2009)\nIntroduction to Algorithms, 3rd ed. Cambridge, MA:\nMIT Press.\nCrawford K (2013) The hidden biases of Big Data. Harvard\nBusiness Review. Epub ahead of print 1 April 2013.\nDanaher J (2016) The threat of algocracy: Reality, resistance\nand accommodation. Philosophy and Technology 29(3):\nDomingos P (2015) The Master Algorithm: How the Quest for\nUltimate Machine Learning will Remake Our World. New\nYork, NY: Basic Books.\nDwyer CP, Hogan MJ, Harney OM, et al. (2014) Using inter-\nactive management to facilitate a student-centred concep-\ntualisation of critical thinking: A case study. Educational\nFeeg R (1988) Forum of the future of pediatric nursing:\nLooking toward the 21st century. Pediatric Nursing 14:\nGillespie T and Seaver N (2016) Critical algorithm studies: A\nreading list. Available at: https://socialmediacollective.\norg/reading-lists/critical-algorithm-studies/ (accessed 1\nGreengard S (2015) The Internet of Things. Cambridge, MA:\nMIT Press.\nGroarke JM and Hogan MJ (2015) Enhancing wellbeing: An\nemerging model of the adaptive functions of music listen-\nHacking I (2006) The Emergence of Probability, 2nd ed.\nCambridge, MA: Cambridge University Press.\nHogan MJ, Harney O and Broome B (2014) Integrating argu-\nment mapping with systems thinking tools \u00ad Advancing\napplied systems science. In: Okada A, Buckingham\nShum S and Sherborne T (eds) Knowledge Cartography:\nSoftware Tools and Mapping Techniques. London:\nSpringer.\nHogan MJ, Harney O and Broome B (2015a) Catalyzing col-\nlaborative learning and collective action for positive social\nchange through systems science education. In: Wegerif R,\nKaufman J and Li L (eds) The Routledge Handbook of\nResearch on Teaching Thinking. London: Routledge.\nHogan MJ, Johnston H, Broome B, et al. (2015b) Consulting\nwith citizens in the design of wellbeing measures and poli-\ncies: lessons from a systems science application. Social\nKanter RM (1991) The future of bureaucracy and hierarchy\nin organizational theory. In: Bourdieu P and Coleman J\n(eds) Social Theory for a Changing Society. Boulder, CO:\nWestview.\nKeever DB (1989, April) Cultural complexities in the partici-\npative design of a computer-based organization informa-\ntion system. Paper presented at the International\nConference on Support, Society and Culture: Mutual\nUses of Cybernetics and Science, Amsterdam, The\nNetherlands.\nKellermeit D and Obodovski D (2013) The Silent Intelligence:\nThe Internet of Things. DND Ventures LLC.\nKitchin R (2014) The Data Revolution: Big Data, Open Data,\nData Infrastructures and their Consequences. London:\nSage.\nKitchin R (2017) Thinking critically about and researching\nalgorithms. Information, Communication and Society\nKitchin R and McArdle G (2016) What makes Big Data, Big\nData? Exploring the ontological characteristics of 26 data-\nsets. Big Data and Society. Epub ahead of print 2016. DOI:\nKraemer F, van Overveld K and Peterson M (2011) Is there\nan ethics of algorithms? Ethics and Information Technology\nLaney D (2001) 3D data management: Controlling data\nvolume, velocity and variety. Gartner.\nMayer-Schonberger V and Cukier K (2013) Big data: a revo-\nlution that will transform how we live work and think.\nLondon: John Murray.\nMedina E (2011) Cybernetic Revolutionaries: Technology and\nPolitics in Allende's Chile. Cambridge, MA: MIT Press.\nMiller GA (1956) The magical number seven, plus or minus\ntwo: Some limits on our capacity for processing informa-\nMittelstadt B and Floridi L (2016) The ethics of Big Data:\nCurrent and foreseeable issues in biomedical contexts.\nMorozov E (2014) The planning machine: Project cybersyn\nand the origins of the Big Data nation. The New Yorker.\nAvailable at: http://www.newyorker.com/magazine/2014/\nO'Neil C (2016) Weapons of Math Destruction. New York:\nCrown Publishers.\nPasquale F (2015) The Black Box Society. Cambridge, MA:\nHarvard University Press.\nPaulus PB and HC Yang (2000) Idea Generation in Groups:\nA basis for creativity in organizations. Organizational\nPeter F (2017) Political legitimacy. In: Zalta EM (ed.)\nStanford Encyclopedia of Philosophy. Available at:\nhttps://plato.stanford.edu/entries/legitimacy/ (accessed 15\nPolonetsky J and Tene O (2013) Privacy and Big Data:\nRezaei Zadeh M, Hogan M, O'Reilly J, et al. (2016)\nCore entrepreneurial competencies and their inter-\ndependencies: Insights from a study of Irish and Iranian\nentrepreneurs, university students and academics.\nInternational Entrepreneurship and Management Journal\nSato T (1979) Determination of hierarchical networks of\ninstructional units using the ISM method. Educational\nStraus S, Parker A, Bruce J, et al. (2009) Group matters: A\nreview of the effects of group interaction processes and\noutcomes in analytic teams. RAND Working Paper.\nAvailable at: http://www.rand.org/content/dam/rand/\n20 Big Data & Society\nWarfield JN (1976) Societal Systems: Planning, Policy, and\nComplexity. New York, NY: Wiley.\nWarfield JN and Cardenas AR (1994) A Handbook of\nInteractive Management, 2nd ed. Ames: Iowa State\nUniversity Press.\nWeber M (1947) The Theory of Social and Economic\nOrganzation. New York, NY: The Free Press.\nZarsky T (2012) Automated predictions: perception, law and\nZarsky T (2013) Transparent prediction. University of Illinois\nZarsky T (2016) The trouble with algorithmic decisions: An\nanalytic road map to examine efficiency and fairness in\nautomated and opaque decision making. Science,"
}