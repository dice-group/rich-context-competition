{
    "abstract": "Abstract\nSchool curriculum change processes have traditionally been managed internally. However, in Queensland, Australia, as a\nresponse to the current high-stakes accountability regime, more and more principals are outsourcing this work to external\nchange agents (ECAs). In 2009, one of the authors (a university lecturer and ECA) developed a curriculum change model\n(the Controlled Rapid Approach to Curriculum Change [CRACC]), specifically outlining the involvement of an ECA in the\ninitiation phase of a school's curriculum change process. The purpose of this article is to extend the CRACC model by\nunpacking the implementation phase, drawing on data from a pilot study of a single school. Interview responses revealed\nthat during the implementation phase, teachers wanted to be kept informed of the wider educational context, use data to\nconstantly track students, relate pedagogical practices to testing practices, share information between departments and\nprofessional levels, and own whole school performance. It is suggested that the findings would be transferable to other school\nsettings and internal leadership of curriculum change. The article also strikes a chord of concern: Do the responses from\nteachers operating under thecurrent accountability regime live their professional lives within this corporate and globalized\nideology whether they want to or not?\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nArticle\nIntroduction\nOver the last decade, many nations have tied their educational\nsystems to funding to establish a degree of accountability\nfor invested public money (Lingard & McGregor, 2013).\nEducation in Australia has been subjected to this escalating\nagenda of accountability measures by stakeholders, most nota-\nbly governments.According to Lingard and McGregor (2013),\nthese measures have led to a realigning of the curriculum to\nfocus on literacy and numeracy and other market-oriented\nreforms. InAustralia, nation-wide high-stakes testing is a rela-\ntively recent phenomenon compared with other Western and\nnewly industrializing nations. Introduced in 2008, literacy and\nnumeracy tests commonly referred to as National Assessment\nProgram--Literacy and Numeracy [NAPLAN]) are manda-\ntory in all Australian states and territories where Years 3, 5, 7,\nand 9 are tested annually. In addition to justifying government\nfunding allocations, national high-stakes test data are used to\ncompare schools' and states' educational performance. In this\nmarketplace mentality, governments increasingly expect posi-\ntive returns for their funding outlay in terms of what students\nknow and can demonstrate as a result of their learning (Corson,\n2002). Inevitably, this has put pressure on schools to produce\npublicly acceptable test data.\nAs well as national testing in the primary and lower sec-\nondary schools, Australian states conduct their own Year 12\nexit testing programs. In Queensland, this examination is\nknown as the Queensland Core Skills (QCS) Test and is\nundertaken by final year students from both independent and\nstate schools. The QCS is based on the Common Curriculum\nElements1 (CCEs) underpinning the senior school sylla-\nbuses. Results from this test are combined with school-based\nassessment data to give students their exit results. The mean\nof the combined students' exit results (known as the school\nresult) are released to the public through the major statewide\nmedia outlet. This practice carries high-stakes consequences\nfor schools as even a minimal risk of atypically poor annual\nacademic results could have negative effects on the most\nstable institutions (Kasperson et al., 1988).\n1Queensland University of Technology, Brisbane, Australia\n2North LakesState College, Brisbane, Australia\nCorresponding Author:\nJudy Smeed, School of Cultural and Professional Learning, Faculty of\nEducation, Queensland University of Technology, Victoria Park Road,\nKelvin Grove, Queensland 4059, Australia.\nEmail: j.smeed@qut.edu.au\nTesting Times for the Implementation\nof Curriculum Change: Analysis and\nExtension of a Curriculum Change Model\nJudy Smeed1, Terri Bourke1, Julie Nickerson1, and Tracy Corsbie2\n Keywords\nCRACC model, curriculum change, external change agent, high-stakes testing, implementation\n2 SAGE Open\nOne risk management strategy or market-oriented reform\nis mirroring part of the school's curriculum to the require-\nments of the high-stakes test. By using such a strategy, some\nprincipals are demanding immediate and focused changes to\nAccording to some writers such as Fullan (2005), Fullan\nand Hargreaves (1991), and Gross, Giacquinta, and Bernstein\n(1971), attempts to change school curricula have historically\nmet with limited success. Wetton (2010) specifically notes\nthat one of the reasons that past reforms failed is because of\nthe actual change process. Prior to heightened accountability\nregimes, curriculum change processes in schools tended to\nbe slow, collaborative, cumbersome, activities-centered, and\nmanaged in-house (Brady & Kennedy, 2003). In contrast to\nthis, the Controlled Rapid Approach to Curriculum Change\n(CRACC) developed by one of the authors of this article\n(Smeed, 2008) promoted change that was externally facili-\ntated, tightly focused, results-driven (Schaffer & Thomson,\n1992), and timely in implementation. It outlined specific\npractices that needed to be considered for controlled rapid\nchange in the initiation phase (Fullan, 2007) of a curriculum\nchange process. The streamlined model (Smeed, 2008)\nallowed the completion of the initiation phase in 10% of the\nallocated project time, leaving 90% for the implementation\nphase. However, it is often at this transition point from initia-\ntion to implementation that change processes flounder\n(Fullan, 2007). To counteract this, the authors thought it\ntimely to revisit and reexamine the model giving consider-\nation to the essential practices of the implementation phase\nof a change process, specifically one in response to high-\nstakes testing accountability and led by an external change\nagent (ECA). The authors acknowledge that such a change\nprocess responds to a neoliberal agenda and may leave some\nreaders feeling uneasy as a \"teach to the test\" approach\nsprings to mind. However, this is neither the practice nor the\nintention. Rather, what is being posited is using the neolib-\neral paradigm, in particular, data from high-stakes testing, in\na focused, results-driven change process. Alternative change\nprocesses that are more activities-based (see Pertuz\u00e9, Calder,\nachieve success, but this is not the context here, and the writ-\ners make no apology for the stance taken. With these caveats\nin mind, this article outlines the required practices for the\nimplementation phase as spoken by the participants, by\naddressing single case study research findings from a school\nthat undertook a CRACC in recent years. In developing the\nextended model, the authors critically reflect on whether\nteachers embrace curriculum change or have become reform\nweary (Mansell, 2012), feeling forced to accommodate the\naccountability agenda whether they want to or not.\nThe opening gambit of this article provides a discussion on\nhigh-stakes testing and how this accountability regime can\ndrive curriculum change. This is followed by a detailed out-\nline of the CRACC model originally developed by (Smeed,\n(2008). Current literature relevant to the implementation\nphase of curriculum change is then considered. The research\nmethodology is explicated, including limitations, before find-\nings from semistructured interviews are critically examined\nto extend the CRACC model, detailing the implementation\nphase.\nHigh-Stakes Testing and Curriculum\nChange\nEven though high-stakes tests in schools tend to be standard-\nized, they do differ greatly depending on their focus. For\nexample, in Queensland, some high-stakes tests are skills-\nbased, whereas others test knowledge. An important point\nabout high-stakes tests is not so much what is tested but that\nthe results are used to make significant educational decisions\nsuch as funding allocations, student movement through year\nlevels, teacher competency, student enrollment, enrollment\nscreening, rewarding and sanctioning of institutions, and\nnarrowing and targeting of specific aspects of the curriculum\n\"second order ways of using [assessment] evidence\" (p. 4).\nIn contrast, first-order ways relate to how teachers use assess-\nment data to make judgments about what students do or do\nnot know. She argues that using assessment data for second-\norder reasons not directly related to learning is questionable.\nRegardless of this, governments worldwide are increasingly\nKostogriz, 2012) as it relates to the curriculum through stu-\ndent and school performances in standardized tests, which\nare often high stakes (Amrein & Berliner, 2002).\nThe literature on the use of high-stakes testing to drive\ncurriculum change is divided, albeit a one-sided debate.\nCritics of high-stakes testing highlight the following themes\nabout their use: a narrowing of the curriculum, performance-\nrather than learning-oriented schools, increased drop-out\nrates, increased teaching to the test, lowering of teacher\nmorale and defection from the profession, promoting cultural\nbiases, increased teacher and student stress, increased pres-\nsure to cheat, negative and discriminating effects on life\nchances particularly for minorities, and the marginalization\nof subjects that are not explicitly tested, such as the humani-\nties and physical education (Amrein & Berliner, 2002;\nin the United States takes this even further, suggesting that\nstandards are in fact lowered as districts downgrade bench-\nmarks to attract more funding.\nHowever, proponents of high-stakes standardized testing\npurport that such tests reduce educational inequality, increase\nobjectivity in assessment, increase accountability, allow\nfunding to be directed where needed, and ensure consistent\ncomparison between international education systems\n(Dreher, 2012). An example where the use of high-stakes test\ndata is seen as helpful is Greene et al.'s (2003) work on the\nState of Florida's testing program. Greene et al. compare\nresults from high- and low-stakes (school-based) testing and\nSmeed et al. 3\nfind that improved performance in high-stakes testing does\ntranslate into improved performance in low-stakes testing.\nHowever, many, such as Amrein and Berliner (2002), argue\nthat there are no clearly identified links between these tests\nand increased student learning performance.\nRegardless of what side of the debate you straddle, it is\nclear that high-stakes testing is a global phenomenon placing\never increasing pressure on schools (Fullan, 2001) demanding\nconsiderable time and energy in schools' curriculum agendas\n(Pinar, 2004). How these pressures are responded to at an indi-\nvidual and school level varies according to the personnel and\nthe context. One such strategy has been mirroring the curricu-\nlum to the demands of the test with the change brought about\nby an ECA. The CRACC model underpinned by Schaffer and\nThomson's (1998) results-driven improvement program relies\non an incremental approach to change building on what works\nand discarding what does not. Targets are set through the vari-\nous stages, and measurable goals are achieved. This model is\nsimilar to the Program Logic Model (PLM; Cooksy, Gill, &\nAustralia, to expand teachers' pedagogical knowledge. The\nPLM provided a framework to measure outputs against prede-\ntermined goals where ECAs worked with students and teach-\ners. The CRACC model comes from a similar mind-set, the\nframework outlining time, stages, and involvement in curricu-\nlum change, and is now outlined in detail.\nThe CRACC Model\nTraditionally, curriculum change models in education have\nbeen activities-centered (Smeed, 2008). Such models con-\ncentrate on activity, rather than results or impact. Teachers\nare continually bombarded with change programs for\nimprovements in areas such as literacy and numeracy; how-\never, according to Pertuz\u00e9 et al. (2010), change processes\noften focus on the program and not on the results. Writers\nutilizing such an approach leads to inevitable failure, and the\nschool naively moves on to a new activities-based model.\nAlternatively, the CRACC model (Smeed, 2008) was\ndesigned to resemble change models more closely aligned\nwith the corporate world (Beer & Nohria, 2000; Bennis,\nresults-driven. Schaffer and Thomson (1992) conduct exten-\nsive research in the corporate world and claim that managers\noften falsely assume that results will materialize if activities-\ncentered programs are initiated. Their research illustrated\nthat change that is not results-driven rarely leads to improve-\nments. With these thoughts in mind, the CRACC model\n(Smeed, 2008) for curriculum change in education was\ndeveloped and is shown in Figure 1.\nThe model is divided into three facets, namely, \"time,\"\n\"stages,\" and \"involvement.\" As the name suggests, the\n\"time\" facet details the amount of time allocated to initiation\nand implementation during the curriculum change. The\n\"stages\" facet details sequential practices or activities, and\n\"involvement\" indicates which members of the teaching\nteam are involved and their relative degree of involvement.\nThese three facets are given further consideration below.\nTime\nIn the current climate of accountability, school principals can-\nnot afford to let a change process meander aimlessly. This\npressure on school leaders was given serious attention in the\ndevelopment of the original CRACC model (Smeed, 2008)\nand is discussed in further detail in Smeed's later work\n(Smeed, 2010). In reality, ECA-facilitated change processes\nare restricted by time as well as budget limitations (Fullan,\n2007), so one of the key principles underpinning the original\nmodel was the development of an appropriate timeline. The\nmodel's timeline was divided into two phases--initiation\nand implementation--terminology borrowed from Fullan\n(2007). These phases were represented sequentially following\nsuggestions by Rogers (1983) where the initiation phase was\nconsidered to be a process through which an individual, or\nanother decision-making unit or organization (such as a\nschool), must initially pass. As the transition between initia-\ntion and implementation is where curriculum change pro-\ncesses can often falter (Fullan, 2007), the CRACC model\nrecommended quick movement through initiation with con-\ntact mainly limited to the principal and deputy principal. This\nenabled the energies of middle-management and classroom\nteachers to be preserved for implementation.\nStages\nThe CRACC model outlined sequentially what the ECA had\nto achieve within the initiation phase. The ECA was required\nto (a) connect with the principal (connection), (b) conduct an\nindependent situational analysis (decipher), (c) set goal(s),\n(d) outline the development of the process, and (e) share\ninformation with staff (inform). These five stages are further\nelaborated.\nStage 1: Connection. This stage highlighted the importance of\nthe initial connection with the principal. In essence, an exter-\nnally led curriculum change process does not occur without\nthe principal's support of the ECA and the process itself. For\nprincipals, the ECA's knowledge and track record in leading\ncurriculum change were crucial. Once the principal was\ncomfortable with the external person, the ECA then moved\nto deciphering the situation or the situational analysis.\nStage 2: Decipher. The deciphering stage allowed the ECA to\nmake independent judgments about the school's current con-\ntext. Prior to the construction of the CRACC model, the ECA\nhad always relied on the perceptions of the principal to gain\nan understanding of the schools'needs. In conducting the rel-\nevant research for the CRACC model's development, it\nbecame clear that the ECA should seek information from\nwider sources. In response to this, the deciphering stage of\n4 SAGE Open\ninitiation in the CRACC model involved conducting an inde-\npendent situational analysis that incorporated the voices of\nmore than just the school leader.\nStage 3: Goal setting. In line with the thoughts of Hall and\nHord (1987), any long-term change success needs support\nfrom the leader. Therefore, the goal of the change process\nhad to be one that the principal was committed to. In addi-\ntion, the \"goal setting\" stage of the CRACC model also\nassisted in driving the process. The ECA had to bring about\nthe desired changes quickly, so it was important that she or\nhe knew exactly what she or he had to achieve. This goal\nsetting stage involved the ECA in consultation with the prin-\ncipal, or principal and deputy principal only.\nStage 4: Process development.In the original research\nundertaken to develop the CRACC model, the participants\nstrongly articulated the desire for a simple process. By\nthis, they meant that it was easy to follow and that the\nrequirements were specific. Therefore, the ECA developed\na process that was easy for participants to follow and could\nbe overseen closely by the external person. Once this pro-\ncess had the principal's support, it was then shared with\nstaff.\nFigure 1. CRACC model.\nNote. CRACC = Controlled Rapid Approach to Curriculum Change; ECA = external change agent.\nSmeed et al. 5\nStage 5: Share.The CRACC model recommended that staff\nshould be informed about the reasons for change, as well as\nthe goal(s), the process, and the timeline, but not necessarily\nuntil the final stage of initiation. This is contrary to advice\nfrom various writers and academics (Brady & Kennedy, 2003;\nLuke, 2007) who suggest that decisions should be made in col-\nlaboration with staff. However, findings from the development\nof the original model refuted this claim. Some teachers went as\nfar as to say that they just wanted to be told what to do. This\ncould be seen as managerial professionalism (Sachs, 2001),\nbut as the findings here reveal, the term compliance profes-\nsionalism (Bourke & Smeed, 2010) is more fitting. The com-\npliant professional works with the objectives of external\naccountability regimes, but does so in a way that provides\nspace or \"wriggle room\" (Hoyle & Wallace, 2009) for their\nown professional judgment and practice. The teachers in the\noriginal study did not mind who set the goal, but did want to\nknow what the goal was and, more importantly, how it would\nbe accomplished. Therefore, the CRACC model responded to\nthis request by limiting the involvement of middle managers\nand classroom teachers until the sharing stage of the process.\nTheir only involvement prior to this was voicing their opinions\nto assist the ECA in deciphering the situation (Stage 2).\nInvolvement\nDuring the change process, the ECA worked with different\nprofessional levels of the school hierarchy, moving from one\nlevel to the next. By \"stepping\" the involvement of the ECA,\nthe process could be closely managed and controlled. As the\nECA moved from one professional level to the next, her time\nwith the previous level decreased. In Figure 1, the CRACC\nmodel depicts the amount of time spent with each profes-\nsional level by a widening or narrowing of the arrows and\nheavier shading where the main involvement occurred.\nFigure 1 also shows that the first contact was with the princi-\npal.After the initial meeting, the deputy principal was invited\ninto the process. The middle managers were then introduced\njust before the implementation phase. Finally, the classroom\nteachers joined their leaders and managers.\nThis time spent working with the ECAwas considered pro-\nfessional development. Desimone (2009) points out that the\nkey characteristics of effective reform-oriented professional\ndevelopment include a focus on content, active learning,\ncoherence, duration, and collective participation. This imple-\nmentation project concurs with Desimone's incorporation of\nsuch key characteristics. Furthermore, it concurs with Hattie's\n(2009) findings that teachers are the most valuable resource in\na school. Therefore, their professional development as part of\nthe process was crucial. However, not all change processes are\nstraightforward; there is always a degree of resistance. This is\nillustrated by the jagged lines as shown in Figure 1.\nThe details of the initiation phase of the original CRACC\nmodel have been outlined. Some may see this as a top-down\napproach to change. However, the authors argue that rather\nthan top-down, the focus is results-driven (Schaffer &\nThomson, 1992) and based on the use of evidence or data\ngathered from the ECA working collaboratively with various\nlevels across the school at specific times (see \"Involvement\"\nsection). Over 20 years ago and with little contradictory evi-\ndence in the academic literature since, commentators such as\ndown approaches as barriers to change, forcing people to\ncomply and being sanctioned if they did not. Print maintains\nthat under such conditions, participants had little ownership\nof, or motivation to ensure, the success of the change. The\nCRACC model counteracts these criticisms by identifying\nthe needs based on data and working collaboratively with all\nstakeholders during the situational analysis.\nAs already mentioned, curriculum change processes often\nfail between initiation and implementation, so this article's\npurpose is to elaborate and expand the CRACC model to\nincorporate details for the implementation phase of a curricu-\nlum change process led by an ECAin response to high-stakes\ntesting accountability. The article also critically analyzes\ncriticisms apply to change in this marketplace mentality.\nFigure 2 provides a graphic explanation of the purpose of\nthis research.\nImplementation\nThe implementation phase follows on sequentially and\nimmediately from initiation. Fullan (2007) states that this\nphase \"consists of the process of putting into practice an\nidea, program, or set of activities and structures new to the\npeople attempting or expected to change\" (p. 84).\nDuring this phase, participants are asked to think and act\ndifferently. Lewin (1951) refers to this as \"cognitive restruc-\nturing.\" It is at the point of implementation that resources\nand knowledge are reorganized to give new meaning to what\nis happening on the ground.\nHarris (2001) claims that the implementation phase\nshould start after a decision to undertake change is made.\nHowever, the CRACC model argues that the decision to\nundertake change is made by the principal at the beginning\nof the initiation phase, and therefore, the implementation\nstage follows sequentially without the need for further deci-\nsion making at this point in time. In addition, some authors\nto begin, those in the organization need to develop the under-\nstanding gained in the initiation phase to the point where\nthey feel comfortable to work actively toward the change.\nHowever, once again, the CRACC model challenges this\nnotion in that the teachers are informed of the change process\nat the end of the initiation stage. The research for the devel-\nopment of the CRACC model strongly indicated that apart\nfrom having their say in the deciphering phase, teachers did\nnot want to be involved with the initial development. They\njust wanted explicit direction and strong leadership.\nAccording to Fullan and Stiegelbauer (1991), once the\nimplementation phase is underway, the success or failure of\n6 SAGE Open\nthe project relies upon the type and amount of quality assis-\ntance that is provided for the participants. Various writers\nsuggest that this assistance should be in the areas of skill\ndevelopment, resources, motivation (Darkenwald & Merriam,\n1982), encouragement and problem solving, conflict resolu-\ntion, and day-to-day support (Booz Allen Hamilton, 2004;\nVisser, 2004). In addition, other authors postulate that during\nthe implementation phase, continued education and training\nFullan (2007) stresses the importance of clarity during\nand throughout the period of implementation, stating that a\nlack of such can \"represent a major problem\" (p. 89). He\nmaintains that quite often teachers and others find that the\nproposed changes are \"not very clear . . . in practice\" (Fullan,\n2007) which he maintains can occur if the people involved\nperceive and interpret the change process in an oversimpli-\nfied manner. Therefore, care must be taken so that the change\nprocess provides teachers with clear, specific details about\nwhat is required, and that changes are not implemented on a\nsuperficial basis (Fullan, 2007). With clear expectations,\nteachers are more likely to own the process (Smith & Lovat,\nFigure 2. Research position.\nNote. ECA = external change agent.\nSmeed et al. 7\nThe length of any implementation period varies depend-\ning on the project and the setting.According to Harris (2001),\nmaintaining momentum has been identified as an inherent\nproblem in this phase. Fullan and Stiegelbauer (1991) con-\ncur, stating that momentum is sometimes lost as the initial\nexcitement wears off and the hard work and reality of mak-\ning the change begins.\nThis research draws on the findings of a single case in a\nrecent curriculum change process to outline essential prac-\ntices within the implementation phase similar to what had\nalready been developed for the initiation phase of the\nCRACC model. Hence, the extension and elaboration of the\nimplementation phase of the CRACC model are needed to\nfurther chances of curriculum change success.\nMethod\nAqualitative case study approach was chosen for this study to\nanswer the following question: What essential practices are\nneeded for the implementation phase of a curriculum change\nprocess in times of high-stakes accountability? According to\nFreebody (2003), \"case studies focus on one particular\ninstance of educational experience and attempt to gain theo-\nretical and professional insights from a full documentation of\nthat instance\" (p. 81). He says, in its most general form, the\ngoal of a case study is \"an inquiry in which both researchers\nand educators can reflect upon particular instances of educa-\ntional practice\" (Freebody, 2003, p. 81); here, the practices\nuseful in a curriculum change process in response to high-\nstakes testing. The boundaries are set by the school, thus\nmeeting Merriam's (1998) requirement that unless the\nintended phenomenon is bounded, it is not a case study. As\nwell as having a strong sense of place, this study also has a\nstrong temporal dimension. Yin (1994) maintains that a case\nstudy is an empirical inquiry that investigates a contemporary\nphenomenon within its real-life context. Changing curricu-\nlum in response to high-stakes testing is currently a global\nphenomenon, and as Hamel, Dufour, and Fortin (1993) point\nout, these local cases can be a reflection of a much larger\nglobal educational force. The study gives voice to the partici-\npants (Hatch, 2002), is based on a contemporary phenomenon\n(Yin, 1994), and focuses on a particular educational instance\n(Freebody, 2003), thus justifying the case study approach.\nTwo of the authors were employed as ECAs by the case\nstudy school in Queensland,Australia, with the aim of chang-\ning the curriculum in the school to meet the demands of high-\nstakes testing. The school was a large state-owned, preschool\nto Year 12 college on the urban fringe of Brisbane, the capital\ncity of Queensland. When the research was conducted, the\nschool had only been open for around 10 years with an\nenrollment of approximately 3,000. In Australia, all schools\nare scaled according to socioeducational advantage, which is\nbased on socioeconomic data, parent qualification data, per-\ncentage of indigenous and percentage of language back-\nground other than English students, and, finally, whether the\nschool is metropolitan, rural, or remote. This particular\nschool services a predominantly White, middle-class clien-\ntele and could be classified as slightly socioeducationally\nadvantaged. The college is owned by the state government,\nbut the day-to-day management is the responsibility of the\nexecutive principal (EP).\nIt should be noted that this school was not one of the\nschools involved in the development of the original CRACC\nmodel in 2008. The EP, like many other school leaders, had the\nunenviable task of improving student and school performances\nwithin a relatively short time frame, so there was a degree of\nurgency for curriculum change to accommodate the perfor-\nmance agenda. This is a common reality for principals who\nlead schools within systems that prioritize improvement in\nperformance, and the state school system inAustralia has been\novertly focusing on an improvement agenda since 2007\n(Lingard & McGregor, 2013). He addressed this challenge by\ninviting an external team to facilitate a curriculum change pro-\ncess using the CRACC model. Prior to this, attempts to change\nthe school's curriculum had been facilitated \"in house\" and led\nby a member of the school's leadership team.\nThe majority of schools in Queensland follow the sylla-\nbuses of the Queensland Curriculum and Assessment\nAuthority (QCAA), formerly known as the Queensland\nStudies Authority (QSA), and the college in this study is no\nexception. The QCAA is a statutory body of the Queensland\nGovernment, responsible for the provision of a range of cur-\nriculum services and materials relating to syllabuses, testing,\nassessment, moderation, certification, accreditation, voca-\ntional education, tertiary entrance, and research.\nSix months into the 12-month project, the EP was pro-\nmoted to a new position. The new appointee quickly became\nengaged with the project and conducted interviews with staff\ninvolved, and it is these data that this article draws on.\nInterviews were conducted with nine staff members who\nwill be identified as A, B, C, and so on. In Queensland, the\npeople responsible for managing school curriculum are heads\nof departments (HODs) or middle managers. Five of the nine\npeople interviewed in this research came from this level.\nThey worked extensively with the ECAs, evaluating data and\nimplementing programs based on these data. Of the other four\npeople interviewed, three were deputy principals and one was\nthe principal of one of the school's three subschools.\nInterviews took place at the college over a 4-week period.\nThis qualitative method featuring semistructured interviews\nwas undertaken by the new EP. As the interviews took place\nwithin the first 2 weeks of her taking up the position, she had\nno ownership of the project and was therefore attempting to\ncome to an understanding of the program without biases.\nInterviews were 60-min long and conducted on a one-on-one\nbasis. They were recorded and then transcribed verbatim.\nTranscripts were read by three different researchers who\nworked independently to identify recurrent themes revealing\nthe practices and activities that the teachers found useful in\nthe curriculum change process. The findings were then used\nto detail the implementation phase of an externally facili-\ntated curriculum change process.\n8 SAGE Open\nAs previously stated, interviews were conducted by the\nnew EP who is one of the authors of this article. This may be\nperceived by some as the first limitation of the research.\nHowever, in line with participative research as outlined by\nDenzin and Lincoln (2005), genuine collaboration was val-\nued. It was also in this principal's interests to make sure the\nprocess that someone else had initiated was going to work\nwith her vision for the school. In addition, three of the authors\nhave had experience on school leadership teams and used a\ntechnique called \"dissociation\" in their everyday dealings\nwith staff, students, and teachers. Dissociation occurs when\na person emotionally distances herself or himself from inci-\ndents while conducting an investigation or an analysis.\nA second limitation could be the choice of a single case\nstudy so that the findings may not be transferable to other\nschool settings. As already mentioned, this was a pilot proj-\nect to extend a curriculum change model. An evaluation of\nthe successes or failures of the findings continue to be tried\nand tested in other school settings. These are not commented\non here, as this is not the focus of this article.\nFinally, to further limit bias, interview data were ana-\nlyzed, and recurrent themes established by using category\nconstruction (Merriam, 1998). The researchers undertook\nthis analysis in isolation, as it was necessary to establish a\nstrong degree of confirmability. While it is unrealistic to pre-\nsume an absence of all bias, as the discussion shows, many\nstrategies were put into place to limit its occurrence.\nFindings and Discussion\nFive themes revealing the practices of the implementation\nphase emerged from the analyzed data. These were as\nfollows:\n1. Keep teachers informed of the wider educational\ncontext through professional development activities,\n2. Use data to constantly track students to achieve better\noutcomes,\n3. Ensure that changes improve classroom pedagogical\npractices,\n4. Share information between departments and profes-\nsional levels, and\n5. Encourage ownership of full school performance.\nThese five themes are given further consideration below.\nKeep Teachers Informed of the Wider\nEducational Context\nParticipants' responses revealed the importance of being\nkept informed of the wider educational context. By this, the\nparticipants were not only referring to overarching federal\nand state educational policies and their associated agendas\nbut also what other schools in their area and beyond were\nincorporating into their practices to improve performance.\nAgain, the authors acknowledge that this could be buying\ninto the neoliberal agenda of increasing competitiveness, but\non the flip side, it could also be looked upon as a thirst for\nknowledge of the local educational landscape. Teacher A\ncomments,\nWe wanted to know what those other high-performing schools\nwere doing because we wanted to get there.\nIn line with Teacher A, Teachers D and C also benefited\nfrom this type of information, stating,\nI found comparisons to other schools really helpful. It helped me\nunderstand where we were going. (Teacher D)\nBy finding out what was happening in other schools, we could\nsee the whole picture, therefore we had a much better\nunderstanding. (Teacher C)\nIn addition, the participants showed an appreciation of the\nECA's knowledge of this type of information. Teacher B\ncomments,\n[The ECA] presents in a good way--including giving us\nexamples what other schools are doing both in Australia and\nbeyond.\nThis theme reveals an element of competition between\nteachers in different schools within districts--\"we wanted to\nget there.\" In line with the Schaffer and Thomson's (1992)\nresults-driven model, the \"there\" which Teacher A refers to is\nthe explicit targets that were set as part of the change process.\nTeachers could see examples of how other national and inter-\nnational high-performing schools transferred policy into\npractice with successful results so that their understanding of\nwhat they were doing was enhanced--\"it helped me under-\nstand,\" \"we had a much better understanding.\" Teachers\ncould easily utilize these examples to translate and own the\nate school context. As well as the element of competition\n(perhaps keeping the change process thriving and ongoing),\naccording to participants, resources, motivation (Darkenwald\n& Merriam, 1982), and continual education and training\nwere also integral to driving the process. In addition, Burridge\nand Carpenter (2013) recognize that programs collabora-\ntively implemented by schools with external providers can\nexpand teachers' teaching practice. In this program, the\nECAs worked directly with both teachers and students to\nachieve an outcome.\nRazik and Swanson (1995) recognize the importance of the\nknowledge of the ECA when leading a curriculum change\nprocess. The original CRACC model highlighted the ECA's\nknowledge and prior experience as essential for the initiation\nphase, and as the findings here reveal, this link to the wider\neducational context, whether for theories and practices from\nSmeed et al. 9\noverseas or for local area performance data knowledge, is\nequally important for any chance of success through imple-\nmentation. However, this competitive spirit could simply\nreveal concerns over the publication of data and where their\nschool performance sits in relation to surrounding schools.\nHowever, as an example of O'Neill's (2013) primary use of\nassessment data, the focus was on improving learning rather\nthan on second-order ways of using assessment data.\nUse Data to Constantly Track Student\nPerformance\nThe second theme that emerged from the analyzed data was\nthe practice of using data to track student performance.\nParticipants articulated the need for ongoing engagement\nwith achievement data for the benefit of their students. This\nnotion is revealed by the following comment by Teacher I:\nI loved the fact that we have kids as our focus . . . We never lost\nsight of that. We were working for the kids--even when . . .\ntalking pathways it was always about the individual student.\nBy constantly engaging with data, the participants were\nable to identify not only students who were performing well\nbut also those who might be having problems. Teacher B\ncomments,\nTo have all our focus on a kid at risk was incredible . . . Our kids\nhave been advantaged by the process.\nThis reveals how teachers deemed the process worth-\nwhile, as they could see how it benefited their students.\nFullan (2000) argues that \"continual assessment of student\nprogress\" (p. 16) or \"monitoring of student progress and per-\nformance\" (p. 17) is a central component of the implementa-\ntion phase. In this study, the teaching group met every week\nand discussed and reported on every student within the\ncohort. In addition, the information collated from tracking\nwas used by teachers as an effective teaching device to\nencourage further motivation and increased achievement.\nOnce again, this is in line with O'Neill's (2013) use of pri-\nmary assessment data. Teacher C reveals that she could \"give\nbetter advice to kids--more accurate than previously.\" The\ndata provided achievement knowledge and enabled more\ninformed feedback to be given, as Teacher A suggests,\nThe most significant learning has been for me as HOD. It has\ngiven me a process for how to have discussions with kids who\naren't succeeding.\nUsing educational data to track student performance was\nnew to the participants. First, they had to come to an under-\nstanding of the data and, second, make decisions about the\nstudent which were supported by the data. This required\nthose involved to act and think differently or restructure cog-\nnitively (Lewin, 1951). Not only were the participants\nputting into place new ideas, programs, or activities that\nFullan (2007) maintains are essential processes for curricu-\nlum change, they were engaged in a learning process them-\nselves, which Teacher E describes as a \"massive learning\ncurve.\" Furthermore, the use of words such as \"loved,\"\n\"focus,\" and \"incredible\" reveals that the teachers involved\nembraced the change process.\nEnsure Changes Improve Pedagogical Practices\nThe third theme was the improvement of pedagogical prac-\ntices in the classroom and how such could lead to improved\nassessment outcomes. It was quite obvious that many of the\nparticipants prior to this research felt negatively about high-\nstakes testing regimes (Mansell, 2012), and indeed, these\nsentiments are echoed by many academics as outlined ear-\nlier. However, as can be seen from the comments below,\nteachers were encouraged to think about these testing regimes\nin a different light after instruction and demonstration from\none of the ECAs:\n[The ECA] opened my eyes on how to teach kids to do multiple\nchoice . . . Made me think about how to incorporate the practices\nshe was doing into our school program. (Teacher A)\nFurthermore, Teacher A added,\nWe need to use [the practices of the ECA] in the everyday\nlearning in our classrooms, including doing the explanation of\n\"why.\"\nTeacher B said, \"Our teaching will be better because of\nthis,\" and Teacher D commented on how \"the process made\nme think more.\"\nThese comments illustrate how, by watching the ECAs in\naction, the participants considered their own practice as\nreflective practitioners (Shulman, 1987) and could easily see\nhow they could do things differently. Not only did teachers\nreceive professional development, but the explicit modeling\nof high expectations through direct observation and the\ndeconstruction of tasks to encourage higher order thinking\nwas also an integral part of the change process. Lawless and\nPellegrino (2007) argue the most important impact a profes-\nsional development activity has on a teacher is that of peda-\ngogical practice change. In other words, teachers change\ntheir classroom practices as a result of the professional\ndevelopment. In addition to self-reflection, the sharing of\npractice between departments and professional levels was\nalso encouraged, which is revealed in the next theme that\nemerged from the interview responses.\nSharing Information Between Departments and\nProfessional Levels\nMany of the participants commented on the level of sharing\nthat was engendered as a result of the change process\nsuggesting that collaboration is key for change. Each week,\nthe group met for 1 hr and focused on student and school\nperformance. The vast majority of participants commented\npositively on this. They particularly liked learning about\nwhat was happening in other departments and other areas of\nthe school.\nWe can learn really exciting things from the other areas [other\ndepartments in the school]--invaluable . . . Collective listening\nand seeing other HODs' skills . . . I have loved it. (Teacher I)\nThey felt a strong sense of everyone working together for a\ncommon purpose as clearly articulated by Teachers G and H:\nThe collegiality from everyone meeting together is powerful.\n(Teacher G)\nEveryone was on task--same boat. (Participant H)\nThe collegiality was not limited to the participants in the\nWednesday group meeting, but became part of rich profes-\nsional dialogue among other members of the teaching staff.\nTeachers B and D commented,\nI have been able to share my learnings with my staffroom.\n(Teacher B)\nI talk to my faculty about what we do--or at the lunch table.\n(Teacher D)\nAs the comments above demonstrate, there was a sense of\nworth and enjoyment in working together as a team.\nHowever, as Teachers B and D further comment, collabora-\ntion was also an important method for gaining more informa-\ntion about individual students. They say,\nIt is essential to continue Wednesday practices as it is the only\ntime we look beyond seeing our kids in just our faculty . . . we\ncan see the student as a whole. (Teacher B)\nI've realized that while a student can have very studious habits\nin one subject, they may not be the same in another subject.\n(Teacher D)\nTherefore, the sharing and collaboration worked on two\nlevels, promoting stronger working relationships as well as\nhaving more knowledge about individual students. Fullan\n(2007) maintains that these notions are critical for successful\nchange:\nChange involves learning to do something new, and interaction\nis the primary basis for social learning. New meanings, new\nbehaviours, new skills, and new beliefs depend significantly on\nwhether teachers are working as isolated individuals or are\nexchanging ideas, support, and positive feelings about their\nwork. The quality of working relationships among teachers is\nstrongly related to implementation. (p. 97)\nFurthermore, the comment, \"it is essential to continue\nWednesday practices,\" reveals a desire for sustainability of\nthe process.\nIn earlier writings, Hargreaves (1994) refers to the bal-\nkanization of staff, particularly in secondary schools. He\nsuggests that balkanization \"restricted the opportunities for\nteachers to learn from each other--particularly across sub-\npants' comments make it obvious that balkanization once\nexisted within this school, but the change process had served\nas a tool to \"debalkanize.\" The debalkanization not only\nenabled teachers to interact outside their own areas of exper-\ntise but also promoted ownership of whole school perfor-\nmance, which is the fifth and last theme that emerged from\nthe interview responses.\nEncourage Ownership of WholeSchool\nPerformance\nSeveral participants commented on the importance of all\nteachers owning the school performance agenda referring to\n\"a culture of collective ownership\" (Teacher F) or \"a whole\nof college approach\" (Teacher I). They felt that all members\nof the community benefited from such an approach, Teacher\nH saying,\nWe pulled together, the process gave structure and empowered\nthe Administration and even empowered the students which is\nvery important.\nThis comment is in line with Block's (2004) thoughts\nwhere he asserts that if change is to be successful, people\nneed to feel powerfulness, rather than powerlessness. He\ndeveloped this notion further by suggesting that ownership is\nthe cornerstone of accountability and, without it, people will\nnot want to take responsibility for any change or change pro-\ncess. Teacher I comments,\nDiscussing the information and data has given everyone\nownership to do something about it.\nIn this case study, according to participants, all stakehold-\ners, the administration team, the classroom teachers, and the\nstudents became involved in the implementation phase of the\nchange process and felt a sense of ownership of the college's\nthat ownership is essential for successful change.\nHowever, although not articulated in this study, some\ncomments from the original study strike a chord of con-\ncern--\"just tell us what to do and we will do it\" and \"we just\nwant to be told, explicit direction and strong leadership.\"\nThese statements could be indicative of a loss of teacher\nautonomy/agency, reform weariness, or a general acceptance\nof the current regime where their teacher professional identi-\nties are shaped by the performance agenda. They either have\nno choice, do not see a choice, or do not articulate a choice.\nThey comply. The focus in this change process was improve-\nment through learning, but there are airs of competition and\nstudents as data products, all that accommodate the present\naccountability regime. Perhaps there is a degree of docility\nand rather than challenging, teachers have just found ways of\ncoping with the demands.\nThe five themes that emerged from the analyzed data are\nnow used to reconfigure the CRACC model, revealing the\nuseful practices and processes for the implementation phase\nof curriculum change, specifically change led by an ECA in\nresponse to high-stakes testing. The reconfigured model is\nshown in Figure 3.\nConclusion\nThe aim of this study was to inform the practices of schools\nand ECAs by extending the CRACC model through more\nFigure 3.CRACC-I.\nNote. CRACC-I = Controlled Rapid Approach to Curriculum Change\u00adImplementation; ECA = external change agent.\nexplicitly mapping the implementation phase of a curriculum\nchange process. This pilot case study illustrates how a single\nschool using the CRACC model navigated this reality and,\nwith the help of two ECAs, implemented change with both\nthe interests of the teachers and the students in mind. The five\npractices--keep teachers informed of the wider educational\ncontext through professional development activities, use data\nto constantly track students to achieve better outcomes, ensure\nchanges relate to pedagogical practices, share information\nbetween departments and professional levels, and encourage\nownership of full school performance--have been added to\nthe CRACC model and used to underpin the creation of the\nCRACC\u00adImplementation (CRACC-I) model. This model is\ndifferent from previous curriculum change theories in that it\nis results-driven, was implemented within a shorter time\nframe, and did not wait for all staff to come onboard. In\npursuit of fulfilling a neoliberalist agenda to achieve better\nstudent outcomes, teachers created a positive learning envi-\nronment where self-reflection, compliance professionalism,\nin situ professional learning, sharing practice, enhanced peda-\ngogy, and collaboration were engendered. Moving this from a\npilot to an extended study will yield further findings and elab-\norate our understanding of curriculum change to take into\naccount high-stakes testing accountability.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nNote\n1. Common Curriculum Elements (CCEs) are the 49 skills that\nunderpin the senior syllabuses in Queensland, Australia.\nReferences\nAmrein, A. L., & Berliner, D. C. (2002). High-stakes testing, uncer-\ntainty, and student learning. Retrieved from http://epaa.asu.\nBarth, R. (1990). Improving schools from within. San Francisco,\nCA: Jossey-Bass.\nBasom, R. E., & Crandall, D. P. (1991). Implementing a rede-\nsign strategy: Lessons from educational change. Educational\nBeer, M., & Nohria, N. (2000). Cracking the code of change.\nBoston, MA: HarvardBusinessSchool Press.\nBennis, W. G., Benne, K. D., & Chin, R. (1961). The planning of\nchange. New York, NY: Holt, Rinehart & Winston.\nBerliner, D. C. (2009). MCLB (Much Curriculum Left Behind):\nA U.S. calamity in the making. The Educational Forum, 73,\nBooz Allen Hamilton. (2004). Ten guiding principles of change\nmanagement. New York, NY: Author.\nBourke, Theresa, & Smeed, Judy L. (2010) Let's take a closer look\nat teacher professionalism. Leadership in Focus: The journal\nBrady, L., & Kennedy, K. (2003). Curriculum construction (2nd\ned.). Sydney, Australia: Pearson.\nBurridge, P., & Carpenter, C. (2013). Expanding pedagogical\nhorizons: A case study of teacher professional development.\nChin, R., & Benne, K. S. (1969). General strategies for effecting\nchange in human systems. In W. G. Bennis, K. Benne, & R.\nChin (Eds.), The planning of change (2nd ed., pp. 32-59). New\nYork, NY: Holt, Rinehart & Winston.\nChin, R., & Benne, K. S. (1976). General strategies for effecting\nchanges in human systems. In W. Bennis, K. Benne, & R. Chin\n(Eds.), The planning of change (3rd ed., pp. 23-45). New York,\nNY: Holt, Rinehart & Winston.\nCooksy, L. J., Gill, P., & Kelly, P. A. (2001). The Program Logic\nModel as an integrative framework for a multimethod evalua-\nCorson, D. (2002). Teaching and learning for market-place utility.\nInternational Journal of Leadership in Education, 5, 1-13.\nCothran, D. (2001). Curricular change in physical education:\nSuccess stories from the front line. Sport, Education and\nDarkenwald, G., & Merriam, S. (1982). Adult education founda-\ntions of practice. New York, NY: Harper & Row.\nDenzin, N., & Lincoln, Y. (Eds.). (2005). The SAGE handbook of\nqualitative research (3rd ed.). Thousand Oaks, CA: SAGE.\nDesimone, L. M. (2009). Improving impact studies of teachers'\nprofessional development: Toward better conceptualizations\nDreher, K. (2012). Tests, testing times and literacy teaching.\nDweck, C. S. (1999). Self-theories: Their role in motivation, per-\nsonality and development. Ann Arbor, MI: Psychology Press.\nDwyer, J. J., & Makin, S. (1997). Using a Program Logic Model\nthat focuses on performance measurement to develop a pro-\nFlores, M. A. (2005). Teachers' views on recent curriculum\nchanges: Tensions and challenges. The Curriculum Journal,\nFreebody, P. (2003). Qualitative research in education: Interaction\nand practice. London, England: SAGE.\nFullan, M. (1992). Successful school improvement: The imple-\nmentation perspective and beyond. Buckingham, UK: Open\nUniversity Press.\nFullan, M. (1993). Change forces: Probing the depths of educa-\ntional reform. London, England: The Falmer Press.\nFullan, M. (2000). The return of large-scale reform. Journal of\nFullan, M. (2001). The new meaning of educational change (3rd\ned.). New York, NY: Teachers College Press.\nFullan, M. (2005). Leadership and sustainability: System thinkers\nin action. Thousands Oaks, CA: Corwin Press.\nFullan, M. (2007). The new meaning of educational change (4th\ned.). New York, NY: Teachers College Press.\nFullan, M., & Hargreaves, A. (1991). What's worth fighting\nfor: Working together for your school. Toronto, Canada:\nOntarioPublic School Teachers' Federation.\nFullan, M., & Stiegelbauer, S. (1991). The new meaning of educa-\ntional change. London, England: Cassell.\nGlickman, C. (1993). Renewing America's schools. San Francisco,\nCA: Jossey-Bass.\nGreene, J. P., Winters, M. A., & Forster, G. (2003). Testing high\nstakes tests: Can we believe the results of accountability tests?\n(Civic Report No. 33). New York, NY: Manhattan Institute for\nPolicy Research.\nGross, N., Giacquinta, J. B., & Bernstein, M. (1971). Implementing\norganizational innovations: A sociological analysis of planned\neducational change. New York, NY: Basic Books.\nHall, G. E., & Hord, S. M. (1987). Change in schools: Facilitating\nthe process. Albany: StateUniversity of New York Press.\nHamel, J., Dufour, S., & Fortin, D. (1993). Case study methods.\nNewbury Park, CA: SAGE.\nHargreaves, A. (1994). Changing teachers, changing times:\nTeachers' work and culture in the postmodern age. New York,\nNY: Teachers College Press.\nHarris, A. (2001). Building the capacity for school improvement.\nHatch, J. A. (2002). Doing qualitative research in educational set-\ntings. Albany: State of University of New York Press.\nanalyses relating to achievement. London, England: Routledge.\nHoyle, E., & Wallace, M. (2009). Leadership for professional practice.\nIn S. Gewirtz, P. Mahony, I. Hextall, & A. Cribb (Eds.), Changing\nteacher professionalism: International trends, challenges and\nHuberman, K., & Miles, M. (1984). Innovation up close. New\nYork, NY: Plenum.\nIles, V., & Sutherland, K. (2001). Organisational change: A review\nfor health care managers, professionals and researchers.\nLondon, England: National Coordinating Centre for Service\nDelivery and Organisation(NCCSDO).\nIngersoll, R. M. (2003). Is there really a teacher shortage?Seattle,\nWA: Center for the Study of Teaching and Policy, University\nof Washington.\nKasperson, R. E., Renn, O., Slovic, P., Brown, H. S., Emel, J.,\nGoble, R., . . .Ratick, S. (1988). The social amplification of\nKemmis, S., & McTaggart, R. (Eds.). (1988). The action research\nplanner. Waurn Ponds, Australia: DeakinUniversity.\nKostogriz, A. (2012). Accountability and the affective labour\nof teachers: A Marxist-Vygotskian perspective. Australian\nLawless, K. A., & Pellegrino, J. W. (2007). Professional devel-\nopment in integrating technology into teaching and learning:\nKnowns, unknowns, and ways to pursue better questions and\nLewin, K. (1951). Field theory in social sciences. New York, NY:\nHarper & Row.\nLingard, B. (2010). Policy borrowing, policy learning: Testing\ntimes in Australian schooling. Critical Studies in Education,\nLingard, B., & McGregor, G. (2013). High-stakes assessment and\nnew curricula: A Queensland case of competing tensions in\ncurriculum development. In M. Priestley & G. Biesta (Eds.),\nReinventing the curriculum: New trends in curriculum policy\nAcademic.\nLouis, K., & Miles, M. (1990). Improving the urban high school:\nWhat works and why. New York, NY: Teachers College\nPress.\nLuke, A. (2007, February). Conference welcome. Paper pre-\nsented at the National Testing Curriculum Seminar, Brisbane,\nAustralia.\nMansell, W. (2012). A race to the bottom? No country is pursu-\ning education reform with such speed and breadth as England.\nIs this because of stagnant achievement or politics? Phi Delta\nceptions of accountability. Paper presented at the American\nEducational Research Association Annual Meeting, Seattle,\nMerriam, S. B. (1998). Qualitative research and case study appli-\ncations in education. San Francisco, CA: Jossey-Bass.\nO'Neill, O. (2013). Intelligent accountability in education. Oxford\nParkay, F. (2006). Curriculum and instruction for becoming a\nteacher. Boston, MA: Allyn & Bacon.\nPertuz\u00e9, J. A., Calder, E. S., Greitzer, E. M., & Lucas, W. A.\n(2010). Best practices for industry-university collaboration\nCambridge: Massachusetts Institute of Technology.\nPinar, W. (2004). What is curriculum theory?Mahwah, NJ:\nLawrence Erlbaum.\nPrint, M. (1993). Curriculum development and design. Sydney,\nAustralia: Allen & Unwin.\nRazik, T., & Swanson, D. (1995). Fundamental concepts of edu-\ncational leadership and management. Englewood Cliffs, NJ:\nMerrill.\nRogers, E. (1983). Diffusion of innovations (3rd ed.). New York,\nNY: The Free Press.\nRogers, E. (1995). Diffusion of innovations (4th ed.). New York,\nNY: The Free Press.\nSachs, J. (2001). Teacher professional identity: Competing dis-\ncourses, competing outcomes. Journal of Educational Policy,\nSarason, S. (1990). The predictable failure of educational reform.\nSan Francisco, CA: Jossey-Bass.\nSchaffer, R. H., & Thomson, H. A. (1992, January-February).\nSuccessful change programs begin with results. Harvard\nSchaffer, R. H., & Thomson, H. A. (1998). Successful change\nprograms begin with results. In J. P. Kotter (Ed.), Harvard\nHarvardBusinessSchool Press.\nShulman, L. S. (1987). Knowledge and teaching: Foundations of\nSmeed, Judy Lillian (2008) A controlled rapid approach to curricu-\nlum change : addressing the pressures of increased test-based\naccountability on schools(PhD thesis), Queensland University\nof Technology.\nSmeed, Judy L. (2010) Accountability through high-stakes testing\nSmith, D., & Lovat, T. (1995). Curriculum: Action on reflection\nrevisited (3rd ed.). Wentworth Falls, Australia: Social Science\nPress.\nSmith, D., & Lovat, T. (2003). Curriculum: Action on reflection\n(4th ed.). Tuggerah, Australia: Social Science Press.\nStrebel, P. (1996). Why do employees resist change? In Harvard\nHarvard Business School Press.\nVisser, K. (2004). Implementing peer support in schools: Using a\ntheoretical framework in action research. Journal of Community\nWetton, M. (2010, March). Some lessons for system-level reform-\ners. Perspectives on Educational Leadership, pp. 1-2.\nYin, R. K. (1994). Case study research: Design and methods (2nd\ned.). Newbury Park, CA: SAGE.\nAuthor Biographies\nJudy Smeed is a senior lecturer in Education at the Queensland\nUniversity of Technology, Brisbane, Australia. Her teaching,\nresearch and consutancies are in the areas of school performance,\nleadership and management.\nTerri Bourke is a lecturer in Education at the Queensland\nUniversity of Technology, Brisbane, Australia. She teaches and\nresearches in the areas of Geography, Geography Education,\nProfessionalStandards for Teachers and Foucault.\nJulie Nickerson is a research assistant at the Queensland University\nof Technology, Brisbane, Australia. She is the author of severalchil-\ndrens' books and many scholarly papers.\nTracy Corsbie is a regional director for Education Queensland\nin the Sunshine Coast Educational Region. She has been a princi-\npal of several schools and led many state-wide educational\ninitaitives."
}