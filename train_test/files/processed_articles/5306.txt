{
    "abstract": "Annandale, NJ  Cancer and noncancer health effects have traditionally been handled differently in quantitative risk assessment. A threshold (i.e., safe exposure) has been assumed for noncancer health effects, and low-dose linearity without a threshold has been assumed for cancer. \"Harmonization\" attempts to rec- oncile these contrasting assumptions under one paradigm. Recent regulatory initiatives suggest that the U.S. Environmental Protection Agency may be leaning toward a harmonized, probabilistic/linear approach for noncancer health effects. Proponents of this approach cite variability in human sus- ceptibility as an argument against thresholds (i.e., some individuals may be exquisitely sensitive at exposures well below threshold levels). They also cite the results of epidemiological models that suggest low-dose linearity for noncancer health effects. We will discuss the implications of these arguments and compare them to what is known about human biological variability in general. We will also touch on the regulatory implications of hormesis within this framework.",
    "reduced_content": "Annandale, NJ\n Cancer and noncancer health effects have traditionally been handled differently in quantitative\nrisk assessment. A threshold (i.e., safe exposure) has been assumed for noncancer health effects, and\nlow-dose linearity without a threshold has been assumed for cancer. \"Harmonization\" attempts to rec-\noncile these contrasting assumptions under one paradigm. Recent regulatory initiatives suggest that\nthe U.S. Environmental Protection Agency may be leaning toward a harmonized, probabilistic/linear\napproach for noncancer health effects. Proponents of this approach cite variability in human sus-\nceptibility as an argument against thresholds (i.e., some individuals may be exquisitely sensitive at\nexposures well below threshold levels). They also cite the results of epidemiological models that suggest\nlow-dose linearity for noncancer health effects. We will discuss the implications of these arguments\nand compare them to what is known about human biological variability in general. We will also\ntouch on the regulatory implications of hormesis within this framework.\nKeywords. nonlinearity, harmonization, risk assessment\nINTRODUCTION\nCancer and noncancer health effects have traditionally been handled dif-\nferently in quantitative risk assessment (QRA). For noncancer effects, there\nis the assumption of a \"safe\" exposure threshold, below which no effects are\nseen (i.e., the no-adverse-effect level). This is in keeping with the histori-\ncal toxicological paradigm that \"the dose makes the poison.\" Cancer risk\nassessment has used a linear, no-threshold assumption, because cancer can\nbe produced through a genetic mechanism, suggesting that even a single\ngenetic error, if perpetuated, could lead to tumor formation. There is reg-\nulatory interest in harmonizing these two approaches under a single set of\nprinciples/paradigms (Bogdanffy et al., 2001). Harmonization is very attrac-\ntive from a regulatory perspective, because it simplifies the process by permit-\nting the use of standardized methodology for all QRA. However, any harmo-\nnized QRA approach still needs to reflect the biology of the various diseases\ninvolved.\nRecent regulatory initiatives suggest that the U.S. Environmental Pro-\ntection Agency (USEPA) may be leaning toward a probabilistic, linear (i.e.,\nAddress correspondence to John A. Bukowski, ExxonMobil Biomedical Sciences, Inc., 1545\nbukowski@exxonmobil.com\n4 J. A. Bukowski and R. J. Lewis\nno-threshold) approach for noncancer health effects. For example, the QRA\napproach for particulate matter (PM) assumes linear, no-threshold effects for\nfor lead accepts that there are linear decreases in children's IQ from even very\nlow blood lead levels (USEPA, 2003). There is also evidence of movement\ntoward increasingly stringent regulatory conservatism, so that even when a\nthreshold is assumed, safe levels would be so low as to represent a de facto\nno-threshold approach. For example, the heightened conservatism associ-\nated with regulations such as the Food Quality Protection Act (FQPA) pro-\nduces a level of uncertainty (often 1000-fold or more) that results in extremely\nlow \"safe\" levels, consistent with \"practical\" low-dose linearity.\nThe current article explores the practical/logical implications of some\nof the assumptions inherent in low-dose linearity for noncancer effects. The\nlogic behind these assumptions is examined and compared to real-world\n(e.g., clinical) examples to see if they are coherent with our current state\nof knowledge. Although we recognize that no-threshold extrapolations are\nnot always linear and linear extrapolations are not always without a thresh-\nold, these concepts are similar, from a practical standpoint, and will be used\nsomewhat synonymously in the current discussion.\nThe linear, no-threshold assumption for cancer risk has a theoretical basis,\neven though it does not address genetic repair (especially from low doses)\nor carcinogens that act through nongenetic mechanisms. However, linear,\nno-threshold extrapolation for noncancer health effects runs contrary to the\nhistorical toxicological principle that all things are hazardous at some upper\ndose and nonhazardous at some lower dose. Instead, arguments in favor of\nlinear low-dose extrapolation substitute theoretical assumptions of extreme\nvariability in the range of human sensitivity and/or the results of complex\nmathematical models. These arguments also tend to ignore or marginalize\nthe growing body of evidence that low-level exposures to many potentially\nhazardous agents may actually have positive health benefits (i.e., hormesis).\nThese issues are discussed in more detail in the following sections.\nExtreme Variation in Sensitivity\nArguments in favor of linear and/or no-threshold low-dose extrapolation\nfor noncancer QRA are often predicated on the assumption of extreme vari-\nability in human sensitivity to environmental exposures, such that at least\none individual is harmed by even minute exposures to a given agent. For ex-\nample, in a draft of the PM Criteria Document, the USEPA suggested that a\nno-threshold linear model could approximate acute morbidity and mortality\nNonlinear Effects in Risk-Assessment Harmonization 5\nfrom PM. This was partly based on an assumption that \"since individual\nthresholds would vary from person to person due to individual differences\nin genetic level susceptibility and pre-existing disease condition, it would be\nalmost mathematically impossible for a threshold to exist in the population\"\n(USEPA, 2001). However, there are practical and logical limitations to the\npreceding theoretical argument.\nOn a fundamental level, this extreme sensitivity argument ignores the fact\nthat individuals respond to stresses biologically, not \"mathematically.\" It pro-\nvides no reasonable clinical/biological mechanism by which trivial exposure\nto an environmental agent would cause serious harm to even compromised\nindividuals. It is tantamount to saying that one can cook an egg in three min-\nutes at 212F, so there should be some egg somewhere that can cook at 40F,\ngiven enough time.\nThe extreme sensitivity argument ignores the fact that the range of sus-\nceptibility to physical/biological stress is generally finite (truncated), rather\nthan some theoretical, infinite continuum. In fact, there are many clini-\ncal examples where extreme biological variation is incompatible with exis-\ntence. Table 1 presents the range of normal values for several commonly\nmeasured blood parameters. Although it is recognized that some individu-\nals may normally lie outside this range and that others may have abnormal\nvalues due to pre-existing medical conditions, there is usually a practical\nlimit to these distributions. For example, random blood glucose levels ex-\nist in a range that brackets a mean of approximately 100 mg/dl (Table 1).\nThere is arguably no individual not experiencing an extreme health emer-\ngency who has a blood glucose of either 10 or 1000 mg/dl, as such extremes\nare incompatible with continuing existence. Similarly, resting adult heart\nrates vary between perhaps 40 and 100 beats per minute, with values of 4\nand 1000 being incompatible with long-term existence under any reasonable\nsituation.\nVariability arguments also often assume default extreme sensitivity for\nparticular groups, such as children or the elderly. While it is true that these\ngroups may have physiological differences that influence sensitivity to some\nagents, this is typically neither universal nor extreme. For example, Table 2\nshows that dosage rates are usually quite similar between children and adults\nfor drugs commonly used in both groups. This is true for relatively benign\nTABLE 1 Range of Normal Laboratory Values for\nBlood urea nitrogen 7\u00ad30 mg/dL\n6 J. A. Bukowski and R. J. Lewis\nTABLE 2 Estimated Dosages for Selected Pharmaceuticals (PDR, 1999)\nDrug Type Adult dose Pediatric dose Other considerations\nAzithromycin Antibiotic 7 mg/kg d1\nSimilar pharmacokinetics\nin young adults, and\nchildren\nZyrtec Antihistamine 5\u00ad10 mg/d\n50% longer half-life in\nhepatic disease\nCytoxan Cancer\nchemotherapy\n40\u00ad50 mg/kg Same Reduce if low WBC or if\ncombined with other\nagents\nVinblastine Cancer\nchemotherapy\ndisease\nAssumes 70-kg adult and child weight appropriate for midrange age (USEPA, 1997).\nantihistamines and antibiotics, as well as for more toxic cancer chemother-\napeutic agents. Of course, there are drugs that have not been approved for\nuse in children and some that are contraindicated because of adverse reac-\ntions, such as tooth mottling from tetracycline. However, even these drugs\nare unlikely to exert adverse effects at levels 100 times below the therapeutic\nrange, as suggested by the FQPA. A possible exception to this might be rare\ninstances of anaphylactic allergy, but these have not been demonstrated for\ntrace exposures to environmental chemicals.\nExtreme sensitivity arguments sometimes exhort the need to protect de-\nbilitated individuals who suffer from organ impairment, such as those with\npulmonary disease, liver disease, or immune suppression. Although these\narguments have some merit in general, here again the matter is one of de-\ngree. Drug warning labels provide a case in point. These have cautionary\nstatements about the need to reduce dosages with liver or kidney impair-\nment, but suggest reductions of perhaps 2-fold, not 10\u00ad100-fold (Table 2).\nOne also needs to consider if populations of extremely debilitated individ-\nuals are even at risk from ambient environmental exposures. Those with\nsevere impairment are likely to already live in protected environments or\nutilize protective equipment, such as oxygen supplies for pulmonary impair-\nment or heightened air/water filtration for severe immune suppression. In\nsuch situations, slight changes in ambient pollutant levels are likely to be\nirrelevant.\nMost risks addressed by conservative regulations such as the FQPA are\nderived from laboratory animal studies. In this situation, the QRA typically\nassumes a 10-fold uncertainty for animal-to-human extrapolation, an addi-\ntional 10-fold uncertainty to address within human sensitivity, and an addi-\ntional 10-fold uncertainty for extreme sensitivity (Kimmel, 2001). This ratio-\nnale presupposes extrapolation from an average rodent to an average person,\nso that both routine and extreme human variation still need to be addressed.\nNonlinear Effects in Risk-Assessment Harmonization 7\nHowever, QRA already begins with results in the most sensitive, rather than\naverage, species of laboratory animal. Furthermore, these laboratory animals\nare highly inbred strains that lack the outbred vigor of wild animals and\nare often selected specifically for enhanced sensitivity to disease and envi-\nronmental insult. In fact, researchers have noted that problems related to\ngenetics and artificial laboratory environments have resulted in highly un-\nthrifty laboratory animals with reduced life spans (Palazzolo, 1995). These\nfactors suggest that laboratory animals are already highly sensitive individu-\nals, so that extrapolation from animals to people might better be viewed as\n\"sensitive to sensitive\" rather than \"average to average.\" The conservatism of\nregulations such as the FQPA would therefore seem to be excessive.\nReliance on Statistical Models\nSupport for low-dose linear extrapolation increasingly rests on weak, but\nstatistically significant, epidemiologic results produced with complex statis-\ntical models. For example, risk of acute mortality from fine (<10\u00ad2.5 \u00b5m\nin diameter) PM is based not on biological models of PM toxicity, but on\ncomplex time-series models suggesting a linear association between daily\ngenerally produce relative risks (RRs) in the range approximately 1.005\u00ad1.05\nweak associations have been considered to be \"outside the resolving power of\nthe epidemiologic microscope\" (Shapiro, 1994), especially when not accom-\npanied by strong mechanistic support (Angell, 1990). There are also serious\nlogical/practical limitations that argue against reliance on these results for\ndecisions related to public health and resource allocation.\nFor one, statistical models are only as accurate as their assumptions. These\nmodels assume that data follow particular distributional patterns (e.g., nor-\nmal, Poisson, etc.) and behave in predictable ways. Modeling assumptions\nare not always tested and, even when they are, traditional statistical tests\n(e.g., goodness-of-fit tests) can only detect relatively large departures from\nthe assumptions. Small departures are difficult to detect but may still influ-\nence results, especially if these results suggest only tiny increases in risks.\nOne common model assumption is linearity, which is assumed in virtually all\nof the models used to predict either acute/chronic illness/death associated\nwith PM or decreased IQ associated with low-level lead exposure. Therefore,\nit should not be a surprise that these models suggest linear effects at low levels\nof exposure, given that they begin with that assumption.\nAnother unresolved limitation is the potential for residual bias. Epidemi-\nology studies represent observational data, which have a much higher poten-\ntial for bias than do experimental data. Multivariate statistical models attempt\nto correct for one type of bias by adjusting for potentially confounding risk\nfactors. Historically, such adjustment has been adequate, because adjustment\n8 J. A. Bukowski and R. J. Lewis\nremoves enough confounding to allow moderately strong associations (e.g.,\nRR>2) to stand out. However, modern statistical techniques and enhanced\ncomputing power have permitted ever more complex statistical models that\ncan measure weaker and weaker mathematical associations. Unfortunately,\nmany researchers and regulators have continued to rely on routine statistical\nadjustment, even when interpreting increasingly weak associations.\nA major problem with using traditional statistical adjustment when as-\nsessing vanishingly low risks is that most of the confounders included in\nmultivariate models are only crude surrogates for more complex variables.\nFor example, a simple variable such as years of maternal education may be\nused as a surrogate for the complexities associated with socioeconomic sta-\ntus. Similarly, crude weather variables, such as average daily ambient tem-\nperature measured at a single, remote location (e.g., an airport), have been\nused as estimates of the heat/humidity stress experienced by individuals in\ndiverse urban microenvironments. Crudely measured covariates permit only\nimperfect adjustment for major risk factors and result in a certain degree\nof residual confounding. Furthermore, many minor risk factors are unmea-\nsured and/or not included as covariates in complex statistical models. Only\na relatively small amount of residual confounding would be needed to in-\nfluence the very weak associations suggested by many recent investigations\nof low-level environmental exposures. In fact, as risks become smaller and\nsmaller, it becomes increasingly difficult to tease out true associations from\nthose related to the influence of other factors (Lumley and Sheppard, 2003).\nThis implies the potential for a practical threshold, due to statistical limita-\ntions, in addition to a biological one.\nOn a more fundamental level, reliance on statistical adjustment to elu-\ncidate the true risk posed by low-dose exposures assumes that we can both\nknow all the factors that influence subtle, multifactorial health effects, and\nthat these factors can be accurately measured. For example, reliance on\nstatistical coefficients to determine the subtle, adverse neurodevelopmen-\ntal (e.g., IQ) impacts caused by low-level chemical exposures suggests that\nwe can sufficiently explain complex human behavior and can predict\nlowered intelligence and future life success using only a crude estimate\nof environmental exposure and a few crudely measured covariates. If this\nwere true, the disciplines of psychiatry, psychology, and sociology would be\nsuperfluous.\nAnother argument against undue reliance on statistics as an arbiter of\nlow-dose health effects is the issue of multiple statistical comparisons. The\npower of modern computing and statistical modeling have allowed smaller\nand smaller associations to be detected, but have not changed the funda-\nmental limitations inherent in the significance testing used to detect those\nassociations. This testing assumes that one, and only one, test is being con-\nducted, so that the p -value is a reflection of the probability of a chance\nNonlinear Effects in Risk-Assessment Harmonization 9\nevent. In actuality, all epidemiological investigations perform scores to hun-\ndreds of statistical tests, so that individual p -values are meaningless and do\nnot reflect the true probability of finding the result by chance alone. This\nmultiple-comparison problem is compounded when investigators rely on the\ndata to guide them to the most important models (i.e., those that are most\nFinally, statistical significance says nothing about the clinical significance\nof the results. Statistical models may indicate that each \u00b5g/dl of lead exposure\nis significantly associated with a loss of 0.3 IQ points, but such a change has no\nmeasurable clinical impact on intelligence or social functioning (Kaufman,\n2001). Similar concerns surround minor pulmonary function fluctuations,\nsubtle changes in heart rate variability, and other trivial \"effects\" that are\nwell within normal clinical variability for a population. This again suggests a\npractical threshold, even when a statistical one is not readily apparent.\nHormesis\nThe most fundamental assumption of low-dose linearity is that environ-\nmental exposures cannot be inherently beneficial. This ignores the growing\nbody of evidence in support of hormesis, the notion of beneficial effects\nfrom small exposures to agents that are toxic at higher doses. This theory is\ngaining credibility as a possibly universal phenomenon. There are a consid-\nerable number of laboratory examples demonstrating chemical and radia-\ntion hormesis among microbial organisms, plants, and mammals (Calabrese,\ning the hormetic effects of alcohol, micronutrients, exercise, and calorie/fat\nintake. All are beneficial at low to moderate levels, but harmful in the extreme\n(Bukowski, 2000). Hormesis suggests that conservative regulatory decisions\nbased on low-dose linearity may not just be incorrect, but may actually be\ndetrimental to public health.\nCONCLUSIONS\nComplex statistical modeling results and speculation on extreme bio-\nlogical sensitivity do not provide sufficient evidence of low-dose linearity to\noverturn the traditional threshold paradigm for noncancer QRA. In fact, the\nempirical evidence cited earlier supports the traditional paradigm and sug-\ngests that nonlinear exposure\u00adresponse relationships (i.e., thresholds and\nhormesis) may well be the norm at low doses. Therefore, an assumption of\nlow-dose linearity does not appear to be appropriate for noncancer QRA.\nOne could even argue that a realistic range of both human sensitivity to\ncarcinogens and background repair of genetic damage go against low-dose\nlinearity for most cancer risk assessment as well.\n10 J. A. Bukowski and R. J. Lewis\nREFERENCES\nBogdanffy MS, Daston G, Faustman EM et al. Harmonization of cancer and noncancer risk assessment:\nBukowski JA, Lewis RJ. Hormesis and health: A little of what you fancy may be good for you. Southern\nCalabrese EJ, Baldwin LA. Hormesis: The dose-response revolution. Annu Rev Pharmacol Toxicol. 2003a;43:\nChatfield C. Model uncertainty, data mining and statistical inference. Journal of the Royal Statistical Society\nKaufman AS. 2001. Do low levels of lead produce IQ loss in children? A careful examination of the\nKimmel CA. 1999 Warkany Lecture: Improving the science for predicting risks to children's health.\nLumley T, Sheppard L. Time series analyses of air pollution and health: Straining at gnats and swallowing\nMerck Manual, 17th ed. Rahway, NJ: Merck Research Laboratories; 1999.\nPalazzolo MJ. Decreasing life span of rats poses problems in labs. Emphasis (Corning Hazleton). 1995;6(3):\n/my.execpc.com/jwolf/ratlong.pdf).\nPhysicians Desk Reference (PDR). 53rd ed. Montvale, NJ: Medical Economics Company; 1999.\nUSEPA (U.S. Environmental Protection Agency). Exposure Factors Handbook. Vol. 1, EPA/600/P-95/002Fa.\nWashington, DC: Office of Research and Development; 1997.\nUSEPA (U.S. Environmental Protection Agency). Second External Review Draft of the Particulate Matter Air\nQuality Criteria Document. EPA/600/P-99/002bB. Washington, DC: Office of Research and Develop-\nUSEPA (U.S. Environmental Protection Agency). Third External Review Draft of the Particulate Matter Air\nQuality Criteria Document. EPA/600/P-99/002aC. Washington, DC: Office of Research and Develop-\nUSEPA (U.S. Environmental Protection Agency). Integrated Risk Information System. Lead and compounds"
}