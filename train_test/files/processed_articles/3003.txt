{
    "abstract": "Abstract\nThis study examines adoption and implementa-\ntion of the US Department of Education's new\npolicy, the `Principles of Effectiveness', from a\ndiffusion of innovations theoretical framework.\nIn this report, we evaluate adoption in relation to\nPrinciple 3: the requirement to select research-\nbased programs. Results from a sample of 104\nschool districts in 12 states indicate that many\ndistricts appear to be selecting research-based\ncurricula, but that the quality of implementation\nis low. Only 19% of the responding district\ncoordinators indicated that schools were imple-\nmenting a research-based curriculum with\nfidelity. Common problems included lack of\nteacher training, lack of requisite materials, use\nof some but not all of the required lessons and\nteaching strategies, and failure to deliver lessons\nto age-appropriate student groups. This study\nrepresents the first attempt to assess the quality\nof implementation of research-based programs\nas required by the Principles of Effectiveness.\nWe conclude that low levels of funding, inad-\nequate infrastructure, decentralized decision\nmaking and lack of program guidance have\ncontributed to the slow progress in improving\nschool-based prevention.\nDepartment of Maternal and Child Health, School of\n",
    "reduced_content": "Will the `Principles of Effectiveness' improve\nprevention practice? Early findings from a\ndiffusion study\nD. Hallfors and D. Godette\n Introduction\nA critical issue in substance abuse prevention is\nthe gap between what is known to be effective and\nwhat is actually being done in practice. The primary\ncontext for providing substance abuse prevention\neducation to adolescents has been through the\nschools (Ellickson, 1995). The US Department of\nEducation provides the largest single source of\nfederal prevention funding through its Safe and\nDrug Free Schools (SDFS) program (US Depart-\nment of Education, 1998b). A new Department of\nEducation policy has created an opportunity to\nclose the gap by increasing research-based sub-\nstance abuse prevention education in schools. The\nnew policy requires school districts to follow the\n`Principles of Effectiveness' or run the risk of\nlosing their SDFS funding. The potential impact\nof the policy is significant since almost all school\ndistricts in the country currently receive such\nfunding (US Department of Education, 2000).\nThe new policy requires school districts to: (1)\nconduct needs assessments, (2) set measurable\nobjectives, (3) choose research-based programs\nand (4) evaluate progress towards objectives (US\nDepartment of Education, 1998b). The present\npaper focuses on data from the first of two success-\nive district surveys aimed at evaluating the impact\nof the policy. For this report, only the third Principle\nof Effectiveness, the requirement to choose\nresearch-based programs, will be assessed.\nThe Department of Education's non-regulatory\nguidance instructs grantees to `design and imple-\nment its programs for youth based on research or\nevaluation that provides evidence that the programs\nD. Hallfors and D. Godette\nused prevent or reduce drug use, violence or\ndisruptive behavior among youth' [(US Department\nwhether school districts appear to be meeting this\nstandard in their choice of prevention curricula.\nFurther, it examines the quality of implementation\namong those that report choosing research-based\nprograms.\nThe policy represents an innovation, i.e. a new\nprocess of selecting school-based prevention pro-\ngrams, based on rational planning. Adoption is the\ndecision to make full use of an innovation as the\nbest course of action (Rogers, 1995). If the policy\nis successful and school districts are adopting\nthe Principles, we expect that they are selecting\nresearch-based programs and dropping programs\nthat have not shown evidence of effective preven-\ntion. Implementation occurs when the organization\nactually puts the innovation into use. Important\naspects of prevention program implementation\ninclude teacher training, and the use of specified\nlesson plans and teaching methods (Smith et al.,\nmate goal of the policy is to improve prevention\npractices in school districts, it is important to assess\nkey issues in the selection process, the programs\ndistricts are choosing and the quality of their\nimplementation.\nDespite a concentrated effort over the past\ndecade by federal agencies to promote effective\nprevention strategies (National Institute on Drug\nAbuse, 1997; Center for Substance Abuse Preven-\ntion, 2000; Division of Adolescent and School\nHealth, 2000), schools have continued to select\nheavily marketed curricula that have not been\nevaluated, have been evaluated inadequately or\nhave been shown to be ineffective in reducing\nsubstance use (Rohrbach et al., 1996; Tobler and\ncommonly relied on untested `homegrown' preven-\ntion curricula (Hansen and McNeal, 1999). The\nDepartment of Education's new policy thus repres-\nents a bold attempt to change the way districts\nchoose programs, with an embedded financial\nincentive for compliance.\nTheoretical framework\nDiffusion theory provides a useful framework for\nevaluating the impact of this policy. Diffusion is\nthe process by which members of a social system\nlearn about, decide about and act on ideas, practices\nor objects that they perceive as new (Rogers,\n1995). The diffusion of innovations in schools has\nbeen characterized as a four-stage process: (1)\ndissemination, or planned efforts to make school\ndistricts aware of a program and encourage its\nadoption; (2) adoption, or the encouragement of\ndistricts to make a commitment to initiate a pro-\ngram; (3) implementation, or interventions to assist\nteachers or other appropriate personnel to deliver\nthe program in accordance with its original design;\nand (4) maintenance, or the encouragement of\nschool administrators and teachers to continue\nusing the program (Rohrbach et al., 1996).\nThis paper reports on information related to\nadoption and early implementation of one aspect\nof the Principles of Effectiveness. The policy was\nenacted in July 1998 and data were gathered\nin Fall 1999. Given that school districts have\ntraditionally not used rational planning in selecting\nschool districts in the US, we assumed that this\nassessment would represent a relatively early point\nin the diffusion process. Other studies have shown\nthat schools have difficulty implementing research-\nbased strategies, even under the most supportive\nconditions (Gottfredson, 1997; Hansen and\nMcNeal, 1999). Our intent was to assess whether\nthe policy had actually begun to prompt selection\nof research-based programs and, if so, how well\nthey were being implemented.\nSeveral diffusion theory constructs are particu-\nlarly salient for the present analyses. First, diffusion\ntheory suggests that preventive innovations are\nslow to be adopted, because individuals have\ndifficulty perceiving their relative advantage\n(Rogers, 1995). Relative advantage refers to the\neconomic and social rewards that are thought\nto follow adoption of an innovation. Prevention\nrewards tend to be distant in time and there\nis uncertainty whether prevention activities are\nDiffusion study\nactually needed (i.e. maybe students would not\nuse drugs even without the program). Financial\nincentives can help to increase relative advantage\nand the Department of Education's policy provides\nan incentive for adoption. If districts do not adopt\nthe Principles of Effectiveness, they risk losing\ntheir longstanding SDFS funding. On average,\nschools receive about $6/student/year; because\nmost school districts are small, almost 60% receive\ndistricts can receive substantially more and, based\non state-determined criteria of need, certain dis-\ntricts receive larger per pupil allocations ($12).\nAlthough funding allocations may be relatively\nsmall, many districts report that their prevention\nefforts rely heavily on SDFS funding, which is by\nfar their largest source of drug abuse prevention\nIncentives are useful, but can be a double-edged\nsword. Incentives can increase the rate of adoption,\nand motivate individuals and organizations that\nwould otherwise not adopt, but commitment to\nthe decision may be low, limiting the intended\nconsequences of adoption (Rogers, 1995). For\nexample, when districts select effective curricula,\nthey may not purchase adequate curricula materials\nor they may neglect to train teachers in essential\nmethods for effectively teaching the curricula\n(Smith et al., 1995). Teachers, in turn, may only\nteach a portion of the lesson plans, resulting in an\ninsufficient `dose' to students (Pentz and Trebow,\nassociated with relative advantage, we hypothes-\nized that there would be widespread adoption of\nthe Principles and evidence that school districts\nwere selecting research-based programs, but that\ninitial implementation would be of poor quality.\nCompatability is a related diffusion construct\nthat can also influence the adoption of effective\nprevention programs. Compatibility indicates the\ndegree to which the innovation is consistent with\nexisting values, past experiences and needs of\npotential adopters (Rogers, 1995). If the innovation\nis perceived as an extreme change, then it will not\nbe compatible with past experiences and will be\nslow to diffuse. School districts are known to base\ntheir prevention decisions on compatibility with\npast experiences and satisfaction with programs\nalready in place (Hantman and Crosse, 2000).\nSince programs such as the DARE program have\nlong enjoyed widespread name recognition and\ncommunity support, we expected that districts\nwould be slow to replace DARE and other familiar\nprograms.\nComplexity is another construct that can influ-\nence the diffusion process. Innovations that are\nrelatively more complex are less likely to be\nadopted, and more likely to be re-invented and\nsimplified if adopted (Rogers, 1995). Re-invention\nof prevention curricula often results in a lowered\ndose (i.e. reducing the number or content of lesson\nplans) and has generally been found to decrease\neffects in prevention science (Pentz, 1994; Botvin\net al., 1995). Positive program effects are attributed\nto carefully crafted activities linked to theories of\nmoderators and mediators of behavior (MacKinnon\net al., 1991), and re-invention may occur because\nof lack of knowledge about the concepts underlying\nthe prevention activity (Hansen and McNeal,\n1999). We hypothesized that complex prevention\nprograms would be less likely to be adopted and\nmore frequently re-invented than simpler curricula-\nbased programs.\nMethods\nSample\nA mailed questionnaire was sent to the SDFS\nthe District of Columbia. Sample districts were\nselected because of their location in communities\nparticipating in an existing study (Hallfors et al.,\ncoordinators (78%) in 11 states responded to the\nsurvey. As can be seen in Table I, our sample of 81\nschool districts has a much greater representation of\nlarge, urban districts, than the country as a whole.\nDistricts responding to the survey did not differ\nfrom non-responders on either size or urbanicity.\nMeasurement and data collection\nA survey was developed to capture key constructs\nof diffusion theory predicting innovation adoption.\nD. Hallfors and D. Godette\nTable I. Comparison of national percentages to study sample\nStudy (%) US (%)a Children attending\nin the US (%)a\naBased on data obtained from the US Department of\nIn order to test convergent validity, the instrument\ncontained questions (including questions 2\u00ad6,\nbelow) from a separate study that achieved a\nnationally representative sample of school districts\n(Ringwalt et al., 2000). The survey was reviewed\nfor construct validity by diffusion experts and also\ntested for reliability through a series of cognitive\ninterviews with school district coordinators who\nwere not included in the sample. Cognitive inter-\nviews encourage respondents to think out loud,\nprompted by probes to determine whether questions\nare understood and measure the construct as inten-\nResponses to a subset of survey questions were\nanalyzed for the present report. These questions\nincluded:\n(1) When did you start working to implement the\nPrinciples of Effectiveness (more than 1 year\nago, more than 1 month but less than 1 year,\nhave not started yet but will soon, do not\nplan to)?\n(2) In your district, how much input does each\nof the following persons or groups have in\nselecting your substance abuse curricula (a\ngreat deal of input, some input, not too much\ninput, no input)?\n(3) In your school district, can individual schools\ndecide whether or not they will implement\nspecific substance use prevention programs\n(yes, no)?\n(4) Which, if any, of the following substance use\nprevention curricula, available commercially\nor because of participation in a research study,\ndoes your district use (responses described\nbelow)?\n(5) Has your district adapted or combined any of\nthe above curricula for use within your school\ndistrict (yes, no)?\n(6) Does your district use a written substance use\nprevention curriculum (or set of materials) that\nwas developed locally by your state, county,\nschool district or one of your schools (yes, no)?\nResponse categories for the fourth question\nincluded a comprehensive list of 59 curricula\ncompiled and reviewed by a private non-profit\nresearch institute, Drug Strategies (Drug Strategies,\n1999). District coordinators could select more than\none curriculum from the list. Since curricula are\nimplemented at the school level, it is possible that\ndifferent schools in the same district use different\nprograms. In addition, curricula are grade-specific,\nso we expected that districts would use different\nprograms for elementary, middle and high school.\nSix programs on the curricula list were consid-\nered to be research-based, having been evaluated\nin one or more randomized control trials. These\nprograms were: Alcohol Misuse Prevention Pro-\ngram (Dielman et al., 1989), Life Skills Training\nProgram (Botvin et al., 1995), Project ALERT\n(Ellickson and Bell, 1990), Project STAR or\nI-STAR (Pentz et al., 1989), Reconnecting Youth\n(Eggert et al., 1994) and Project Northland (Perry\net al., 1996). If any of the sampled SDFS district\ncoordinators reported use of one or more of the\nsix curricula, we conducted a follow-up telephone\ninterview with the coordinator, using a standard\nprotocol. In many cases (38% of contacts), we\nspoke with a teacher or another person in the district\nto whom the coordinator referred us, because that\nperson had more direct knowledge about program\nimplementation. The interviewer probed for (1)\nconfirmation that the program was the actual pro-\ngram of interest, (2) teacher access to official\ncurriculum and materials, (3) teacher use of curric-\nulum and the extent to which he or she followed\nprogram protocols, (4) teacher training in the use\nof the curriculum, and (4) program delivery at the\nappropriate grade level.\nDiffusion study\nAll of the research-based programs have a pub-\nlished curriculum with a set number of lesson\nplans that include exercises for student interaction.\nAll require teacher training for proper implementa-\ntion. Three of the six programs, however, are\nmore complex than the others, since they include\nessential elements that go beyond these basic\nfeatures. The three programs are Project STAR,\nProject Northland and Reconnecting Youth. Project\nSTAR and Project Northland are comprehensive\nprograms, with required features such as mass-\nmedia events, parent activities and community\norganizing to augment the school-based curricula.\nReconnecting Youth is a program for high-risk\nstudents, which requires schools to use specific\ncriteria to identify eligible students and to identify\nteachers with special characteristics to teach the\nclass. In contrast, the other programs were designed\nfor all students at a given grade level. The inter-\nviewer asked additional questions for the three\ncomplex programs, related to specific components.\nFor example, the interviewer asked how students\nwere identified and invited to participate in Recon-\nnecting Youth.\nAll aspects of the research plan were reviewed\nand approved by the Institutional Review Board.\nResults\nSelection of prevention programs\ncompleted the questionnaire reported that they\nhad received information about the Principles of\nEffectiveness, all but one of the others appro-\npriately skipped the question `When did you start\nworking to implement the Principles of Effect-\niveness?'. Of those that responded to this question,\nonly one indicated that they had no plans to\nimplement the Principles of Effectiveness. Thirty-\neight coordinators (63%) reported that they had\nbeen working on implementation for more than\n(15%) had `not started yet, but will soon'.\nAll other study questions were answered by all\nrespondents. In response to the question `In your\ndistrict, how much input does each of the following\npersons or groups have in selecting your substance\nabuse curricula', 70% indicated that the coordinator\nthemselves had `a great deal of input' in selection,\ncompared to district-level substance use advisory\ngroups (39%), classroom teachers (31%), district\nadministrators (26%), school principals (24%),\ndistrict school boards (16%), community coalitions\n(10%) and students or parents (8% each). Neverthe-\nless, individual schools decided whether they\nwould implement specific substance abuse preven-\ntion programs in over half (53%) of the districts.\nWhen asked to select the curricula that their\ndistrict was using, most respondents checked more\nthan one program from the list (range  1\u00ad20;\nmean  6). Table II lists the programs that were\nchecked by at least 10% of study districts. As can\nbe seen, more than half of the sample checked\neach of three programs--DARE, Here's Looking\nat You, and McGruff Drug Prevention and Child\nProtection. All three programs have been commer-\ncially marketed, with a long history (10\u00ad15 years)\nof development and dissemination. Forty-eight\ncoordinators (59%) reported use of one or more\nof the six research-based programs in district\nschools, although none reported use of either\nProject Northland or the Alcohol Misuse Preven-\ntion Program.\nFinally, 53% of school coordinators who\nresponded to the survey said that they were using\nlocally developed curricula or materials that were\nnot on the survey list. Fifty-two percent of respond-\nents indicated that they had adapted or combined\ncurricula from the list.\nImplementation of research-based\nprograms\nFollow-up telephone calls were made to the 48\ncoordinators that checked one or more of the\nresearch-based programs. Since some districts\nchecked more than one program, the total number\nof possible interviews was 84. Fifty-eight inter-\nviews (69%) were completed (see results in Tables\nIII\u00adVI). As can be seen in the tables, coordinators\noften reported that they were not actually currently\nD. Hallfors and D. Godette\nTable II. Drug prevention curricula and percentage of use\nProgram name Percentage (n)\nof district\nreporting use\nMcGruff's Drug Prevention and Child 52 (42)\nProtection\nSunburst Drugs and Alcohol Curriculum 25 (20)\nModules\nComprehensive Health for the Middle Grades 15 (12)\nTalking with Your Student about Alcohol 15 (12)\nLearning about Alcohol and other Drugs 12 (10)\nTeenage Health Teaching Modules 10 (8)\nusing any of the effective curricula. Only a few of\nthe coordinators or secondary informants could\nconfirm that district teachers engaged in quality\nimplementation of the programs. Common quality\nproblems included lack of teacher training, lack of\nrequisite materials for every class and lack of\nstudent exposure to the entire (or even majority\nof) the curriculum (see Tables III\u00adVI). In some\ncases, curricula were used at the wrong grade\nlevel or in alternative schools rather than regular\nclassrooms.\nDiscussion\nOur findings supported two of our three hypotheses.\nFirst, we expected that most districts would recog-\nnize the relative advantage of adopting the Prin-\nciples, that they would begin selecting research-\nbased programs, but that many would implement\nthem with low quality. Indeed, almost every coord-\ninator that knew about the Principles indicated that\nthey had adopted them and the majority of these\ncoordinators reported that they had been working\non implementation for 1 year or more. In relation\nto Principle 3, the majority indicated that they\nwere selecting one or more research-based pro-\ngrams from our list; coordinators usually selected\nthe research-based programs along with several\nother programs that were not research-based.\nFollow-up telephone interviews, however, showed\nthat implementation quality of research-based pro-\ngrams was generally poor.\nSecond, we expected that well-known programs\nsuch as DARE would be used more frequently\nthan research-based programs, because they are\ncompatible with past practices. The data supported\nthis assumption even more strongly than anticip-\nated. Two programs appear to be most prominent\nin school districts: DARE and Here's Looking at\nYou. Unfortunately, neither is supported by peer-\nreviewed evaluation studies (Hallfors et al., 2001).\nDARE has been widely studied and found to have\nrelatively small short-term effects with no long-\nterm benefits on substance use behavior (Ennett\nat You has never been rigorously tested. DARE, in\nparticular, has observable qualities, i.e. uniformed\npolice officers, bumper stickers, tee-shirts and\nrelated paraphernalia that facilitated its rapid dif-\nfusion (Rogers, 1995). Both programs have been\nheavily marketed, and closely aligned with `preven-\ntion principles' (National Institute on Drug Abuse,\n1997) and requirements of the SDFS policy.\nIn contrast, the programs that have been rigor-\nously tested and found to be effective have been\nmuch slower to diffuse. Most of the research-based\nprograms have not been marketed to the same\nextent as commercial programs. Two of the six\nthat were included in the survey were not used by\nany of the 81 districts. A third, Project STAR, was\nactually used in only one or two districts and not\nfaithfully implemented. Both Project STAR and\nDiffusion study\nTable III. Implementation of Life Skills Training\nNumber of (%) of Quality of curriculum implementation n  19 (of 33 districts reporting use of LST)\nschool districts\n8 (42) The districts adhered to the protocol for implementation and delivery of the curriculum; all teachers had\nreceived appropriate training. Some coordinators noted that there may be variation in implementation in\nthe schools.\n1 (5) The district was using Life Skills Training in one school in the district; teachers used the curriculum\n`more often than not', but had not received formal training.\n1 (5) The district was using the program at the high school level. (The program was intended for the middle\nand junior high school levels.)\n4 (21) The districts were not currently implementing the curriculum but planned to implement it in the Fall of\n4 (21) The districts were using different programs with the name `Life Skills'.\n1 (5) The district was not using any curricula called Life Skills Training. (Error in survey response.)\nTable IV. Implementation of Project Alert\nNumber of (%) of Quality of curriculum implementation n  15 (of 25 districts reporting use of Project Alert)\nschool districts\n4 (27) The districts adhered to the protocol for implementation and delivery of the main curriculum; all teachers\nreceived ALERT training.\n2 (13) The districts used specific lessons from the curriculum, not the entire curriculum; all teachers received\nALERT training.\n1 (7) The district implemented the curriculum at the high school level in an alternative school. (The program\nwas designed for middle school/junior high.) Teachers received ALERT training.\n1 (7) The district borrowed the guidelines/manuals from neighboring districts `as needed'. Teachers did not\nreceive ALERT training.\n3 (20) The districts were unable to comment on how closely the in-school staff followed the curricula\nguidelines/manuals. Teachers in two of the sites received ALERT training. The coordinator in the third\nsite had no information on whether or not the staff had received the ALERT training.\n2 (13) The district could not verify any use of Project ALERT in the district.\n2 (13) The districts were not using the Project ALERT curriculum. (Error in survey response.)\nTable V. Implementation of Reconnecting Youth\nNumber (%) of Quality of curriculum implementation n  7 (of nine districts reporting use of Reconnecting Youth)\nschool districts\n3 (43) The districts adhered to all protocols for implementation and delivery of the curriculum; all teachers had\nreceived training.\n2 (29) The districts confirmed access to the curriculum but did not follow all protocols regarding use of\ncurricula, student selection and teacher training.\n1 (14) The district reported that a school nurse in one site selected lessons from the curriculum for various\ngroup activities.\n1 (14) The district discontinued use of the curriculum prior to the survey.\nD. Hallfors and D. Godette\nTable VI. Implementation of Project STAR\nNumber (%) of Quality of curriculum implementation n  17 (of 17 districts reporting use of Project STAR)\nschool districts\n1 (6) The district confirmed they are using the University of Southern California STAR curriculum, but not the\ncommunity organizing or parent involvement components.\n1 (6) The district coordinators believed that some of their schools were using the STAR curriculum, but could\nnot confirm.\n7 (41) The district sites were using different programs with the STAR acronym or something very similar.\n8 (47) The districts were not using any STAR programs: two reviewed the curriculum but never implemented,\none tried the curriculum 2 years ago and five are not using any STAR curriculum.\nProject Northland have been under continued\ndevelopment and testing, and are only now begin-\nning to be marketed. Two programs did appear to\nhave substantial market penetration: Life Skills\nand Project Alert. Both curricula are universal\nprograms aimed at middle schools, which appears\nto be the most popular venue for prevention pro-\ngramming.\nFinally, we expected that more complex pro-\ngrams would be re-invented, but there was no\nconsistent pattern supporting this assumption. All\nof the programs were heavily modified by schools\nand individual teachers. The exception was Life\nSkills Training, which was reported to be faithfully\nimplemented by most of the districts that confirmed\nthey actually used the program.\nGiven these results, what are the implications\nfor closing the gap between science and practice?\nOn balance, it appears that the policy, with its\nincentive, has mobilized school districts to recon-\nsider their process for selecting prevention pro-\ngrams. Districts lacked guidance, however, in\nchoosing research-based programs. The US Depart-\nment of Education convened an expert panel in\n1998 to review program effectiveness and provide\nsuch guidance, but their report was not released\nuntil January 2001, well after our survey. Change\nagent effort is known to be a predictor in the rate\nof diffusion (Rogers, 1995) and the lack of federal\nguidance may have been an important contributor\nto the poor implementation of Principle 3.\nOne example of how federal guidance and\nconsistent reinforcement can lead to the selection\nof research-based programs is Life Skills. Life\nSkills is a familiar name on every federal agency's\nmodel substance abuse programs list and it is also\nincluded on the US Department of Education's list\nof exemplary programs. Some agencies, such as\nthe Office of Juvenile Justice, have even provided\nsupport to school districts to implement the pro-\ngram faithfully (Mihalic and Elliott, 2001). This\neffort appears to have paid off with findings that\nLife Skills was the most commonly selected of the\nresearch-based programs and the best implemented.\nAnother diffusion problem may be the decentral-\nized decision making in school districts. Although\ncoordinators have great input in the selection of\nprevention programs, schools can often make their\nown decisions about the prevention programs that\nthey will implement. Rogers (Rogers, 1995) notes\nthat decentralized systems tend to lack quality\ncontrols for effective diffusion. A related problem\nis the relatively low funding for prevention.\nAlthough half of our sample districts were large,\ntelephone interviews indicated that a well-organ-\nized central infrastructure to select, disseminate and\nmonitor the quality of substance abuse prevention\nimplementation was rare.\nThe present study has several notable limitations.\nFirst, the sample is not representative of school\ndistricts in the US; large school districts are over-\nrepresented. Small and rural schools make up the\nmajority of districts in this country, and such\ndistricts were less likely to know about the Prin-\nciples of Effectiveness or have a dedicated (more\nthan 25% time) substance abuse prevention coord-\nDiffusion study\ninator (Hallfors et al., 2000). Small and rural\ndistricts also receive considerably less funding for\nsubstance abuse prevention. Large districts make\nup only 6% of districts, but they serve 50% of\nschool children in the US, giving some justification\nfor our over sample of large districts. Moreover,\nour findings on popular curricula are supported by\na similar study that did achieve a population-based\nsample of school districts (Ringwalt et al., 2000).\nAnother limitation was the difficulty in reaching\ncoordinators for telephone follow-up. Data were\nincomplete despite repeated contacts to coordina-\ntors by telephone and E-mail (average of six\ncontacts for each completed interview). Of the 84\ninterviews that we attempted to conduct across the\nfour research-based programs, only 58 (69%) were\nactually conducted. Finally, an important limitation\nwas the reliance on reports of the coordinator.\nIdeally, we would have made site visits to observe\nprevention programming in all district schools, but\nsuch methods were beyond the scope of the study.\nAlthough imperfect, district-level substance abuse\ncoordinators are the most knowledgeable informa-\ntion source about prevention curricula. Data collec-\ntion was strengthened by including interviews with\nstaff closer to the actual implementation of specific\nprograms (e.g. teachers), whenever possible. Given\nthe findings of previous investigators (Hansen and\nthat our 19% estimate of districts faithfully imple-\nmenting effective curricula is optimistic.\nDespite the limitations, this study is the first\nattempt to assess the quality of implementation of\nresearch-based programs by school districts, in\nrelation to the new Department of Education policy.\nStudy findings were assessed at an early stage of\npolicy diffusion. The next phase of the study will\nquery the same sample of coordinators almost\n3 years after policy implementation. If no further\nprogress is observed, then it is doubtful that the\npolicy will achieve the stated goals related to use\nof effective programs. Given the low levels of\nfunding to school districts, it is unlikely that\nthe needed infrastructure will be developed to\nsupport effective implementation of research-based\nprograms.\n"
}