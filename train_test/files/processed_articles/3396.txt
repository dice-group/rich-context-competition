{
    "abstract": "Abstract\nThis paper is based on ethnographic research of data practices in a public health project called Weather Health and Air\nPollution. (All names are pseudonyms.) I examine two different kinds of practices that make air pollution data, focusing\non how they relate to particular modes of sensing and articulating air pollution. I begin by describing the interstitial\nspaces involved in making measurements of air pollution at monitoring sites and in the running of a computer simulation.\nSpecifically, I attend to a shared dimension of these practices, the checking of a numerical reading for error. Checking a\nmeasurement for error is routine practice and a fundamental component of making data, yet these are also moments of\ninterpretation, where the form and meaning of numbers are ambiguous. Through two case studies of modelling and\nmonitoring data practices, I show that making a `good' (error free) measurement requires developing a feeling for the\ninstrument\u00adair pollution interaction in terms of the intended functionality of the measurements made. These affective\ndimensions of practice are useful analytically, making explicit the interaction of standardised ways of knowing and\nembodied skill in stabilising data. I suggest that environmental data practices can be studied through researchers'\nmaterialisation of error, which complicate normative accounts of Big Data and highlight the non-linear and entangled\nrelations that are at work in the making of stable, accurate data.\n",
    "reduced_content": "Original Research Article\nDeveloping a feeling for error:\nPractices of monitoring and\nmodelling air pollution data\nEmma Garnett\n Keywords\nEnvironmental data, air pollution, error, modelling, monitoring, data practices\nIntroduction\nAir is not a one, it does not offer fixity or community,\nbut it is no less substantial. The question is whether we\nChoy's description of air encompasses its materiality\nand immateriality, its multiplicity and fluidity through\nwhich he inquires: how do we feel this amorphous\nyet substantial thing? In his chapter entitled `Air's\nSubstantiations', Choy uses air as a heuristic to capture\nthe many atmospheric experiences air provides, among\nthem dust, oxygen, dioxin, smell, particulate matter,\nvisibility, humidity, heat, and various gases (2012:\n127). His subsequent abstraction of air into `atmos-\npheric experiences' involves an interweaving of the mul-\ntiple encounters air makes possible, producing what he\ncalls a `poetics of air'. This conceptualisation of air\nenables him to trace the particular and everyday experi-\nences of `honghei' (ambient air) in Hong Kong, along-\nside the scientific and technical practices which seek to\nmeasure and scale air as a universal category. These\nexperiences, he shows, are different ways of feeling air.\nI begin with Choy's descriptions of `airy matters'\nbecause he captures both human\u00admaterial\nFaculty of Public Health and Policy, London School of Hygiene and\nTropical Medicine, UK\nCorresponding author:\nEmma Garnett, Faculty of Public Health and Policy, London School of\nHygiene and Tropical Medicine, 15-17 Tavistock Place, London\nEmail: emma.garnett@lshtm.ac.uk\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://\nwww.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nentanglements with air, and the different sensory scales\nand registers at work within these. The story of air is\nincreasingly described in relation to human activities, as\nthe result of life-styles in modern industrial societies\nstood as the product of human and non-human rela-\ntions which have actively changed the material\nconstitution of air. Data generation has also actively\nshaped what constitutes air, and how air is experienced\nand engaged with. Occasions of measuring air are pro-\nliferating: from government-led monitoring devices to\nglobal scale computer models, and from participatory-\nmodes of citizen science (Gabrys, 2014) to the collecting\nof data through mobile phone apps (e.g., the London\nAir iPhone app by the London Air Quality Network),\nwhich make new kinds of relations of air sensible. As\nresearch on urban spaces and `smart cities' have\nemphasised, digital data's ability to record different\nkinds of sensations and states (Gabrys, 2014; Thrift,\nways in which data are now made and used.\nTechnical devices that sense air and make it measur-\nable are also prescriptive, configured by metrics\nand methods of measurement and compartmentalised\ninto different species of particles and gases. Yet, as\nsocial studies of informational practices have shown,\nstandards and classification have socio-material and\npolitical effects because they effect what will, or will\nnot, be made visible (Bowker and Star, 1999). The\nnotion of the `making up' of data (Boellstorff, 2013:\ncf. Hacking, 2006) has been used to delineate the mater-\nial and political dimensions of Big Data. Particular\nfocus has been on transactional and surveillance data\n(Beer and Burrows, 2013), personal health data and the\nNafus and Sherman, 2014) and what this means for\nempirical sociology (Savage and Burrows, 2009).\nThese studies pose new questions about the ethics and\npolitics of Big Data, particularly in terms of how data\nare rendered meaningful and functional through every-\nday practices of data production, use and analysis.\nThere have been fewer studies, however, critically\nexploring scientific data practices and how these differ-\nent kinds of Big Data are `made up' in order to carry\nmeaning and have political effects. Environmental data,\nin the form of air pollution data, are my specific focus\nin this paper. Scientific data of air pollution as part of\nother environmental Big Data raise particular kind of\nissues. The proliferation of methods to measure and\nmake data of air pollution may even shift the object\nof study and therefore the relationship between\nhuman bodies and their environments. As Mei at al.'s\n(2014) research on `sniffing social media' shows, by\nusing text content from social media posts with\nspatiotemporal correlations among cities and days it\nis possible to measure and predict air quality in very\ndifferent ways. This expansion of what should or can be\nmonitored is also raised by Ottinger and Zurer (2011),\nwho point out that introducing monitoring technolo-\ngies for communities to measure their exposure\n(rather than relying on data generated by local indus-\ntries) assumes a benchmark for good/bad air can be set.\nThe authors suggest that expanding who can measure\nair quality will not resolve the politics of air and data.\nIndeed, focusing only on the endpoint of data closes\ndown alternative probings into how standards are\nachieved, concerns about air pollution stabilised, and\ndecisions about what should and can be measured\nmade. Acknowledging the role of air pollution data as\npart of these Big Data practices is vital if we are to\nunderstand the intricate ways in which environmental\ndata gain scientific and political affordance.\nBased on ethnographic fieldwork as member of\na multi-disciplinary public health project called\nWeather, Health and Air Pollution (WHAP), in this\npaper I examine a key component of making environ-\nmental data: sensing error. I will examine the checking\nfor error in two different data practices of air pollution\n\u00ad modelling and monitoring \u00ad focusing on the devices\ninvolved in these practices, and the material processes\nof collecting, capturing and `making present' air pollu-\ntion. I focus on modelling and monitoring practices due\nto their very different ways of managing `error' (within\nWHAP these data were often opposed and contrasted).\nThe different properties and meanings of modelled and\nmonitored data meant they also caused tensions for\nresearchers trying to share and combine these data\nacross different situated practices. By focusing on how\nerror was managed, I was able to go on and explore\ninternal differences within data rather than only exter-\nnal differences between data (which researchers already\nacknowledged). Starting out with the performative\ndimensions and potential of data practices when mean-\ning and value remain uncertain, I pay attention to the\ncraftwork of articulating error, and therefore the stabi-\nlising of numerical measurements as mobile forms.\nEmbodiment and performance:\nDistinguishing data from error\nAs Gitelman and Jackson write, data are `evolving\nassemblages rather than discrete entities', which need\nto be understood as `framed and framing' (2013: 5).\nDrawing on ethnographic research of environmental\ndata practices, I found air pollution data were similarly\nmade through particular framings, both in terms\nof their considered geographic and environmental con-\ntext and their role in national and global modes of gov-\nernance. Error was a way to come to understand how\n2 Big Data & Society\ndata were framed because what counted as error was\nshaped by the anticipated material form and discursive\nuse. Indeed, to publish data on public platforms, in\nofficial documents or in academic journals requires\naccounting for `omissions' \u00ad how error was managed\nin data production.\nBowker et al. suggest that the invisible work and\n`quiet politics' of knowledge infrastructure are how\nvalues, policies and modes of practice become\nembedded in larger informational systems. If we con-\nsider air pollution data as part of a wider system of\nenvironmental health governance, then the work of\nmaking and maintaining this system need to be\nexplored. Accordingly, data infrastructures can also\nbe performative in enabling, or not, how data are clas-\nsified and therefore how air pollution is made visible.\nIn practice, classification is articulated through visual\nplatforms, which enable modellers, for example, to\nmanipulate and `play with' atmospheric structures\nand processes on the computer screen (Alac, 2008).\nSeeing and responding to changes in a measuring\ninstrument also result from the cultivating of an intu-\nition for the phenomena being studied. It is this invis-\nible craftwork of scientific practices tied up with the\ngeneration of environmental data which often get\nnegated through a focus on what data do and how to\nanalyse Big Data.\nError, I found, was a key part of crafting data and\nclassifying what counts as data of air pollution. This is\nnot an issue of determining right from wrong, because,\nas Bowker et al. have pointed out, part of what makes a\ngood classification scheme is the enabling of compar-\nability and prescription, an effective level of complexity.\nThus, it is the craft of the data technician and scientist\nto make a judgement about how differentiated to make\nmutual process of constructing and shaping differences\nthrough classification systems is crucial in our concep-\nceptualising classification as performative aligns with\ncontemporary interests in STS and philosophy of sci-\nence which foreground the material, relational and\nontological dimensions of scientific practice (Barad,\n2015b). Indeed, an interest in the situated and lively\nnature of data \u00ad their socio-material lives (Helgesson,\nresulted from a particular emphasis on the embodied\nand imaginative work that render data sense-able and\nIn terms of error, it is this sense that the measure-\nment is measuring the unintended that forces research-\ners to understand and materialise error as a form\n`other' to that being studied. Further, error is interest-\ning sociologically because it suggests a correction from\nnormative expectations about `the real'. As Tilly (1996)\nhas argued, error-correction is also a counter-factual\nexplanation crucial to understanding social relations\nand therefore the duration of, for example, socio-tech-\nnical assemblages. By focusing on error in air pollution\ndata practices alternative `theories of the possible' may\nemerge because responses to error are neither instru-\nmental nor random; they draw on historically accumu-\npoint has also been developed by Sennett: using the\ncoupling of resistance and ambiguity in craftwork\nunexpected outcomes are productive for material\nknowledge making, and it is at these moments that\nimaginations of and competence in coming to know\nan object can be expanded.\nThe different properties of error in modelling and\nmonitoring problematise the assumption that data are\ndirect measures of air pollution, and instead highlight\nthe active ways in which the modification of data and\nthereby what constitutes air pollution take place in\neveryday environmental knowledge practices.\nReflecting on the conceptual capacities of `error', I sug-\ngest, opens up avenues for thinking about and research-\ning the configuring of technical devices, bodily\nmovements and materials (and their relations) that\ning very much a part of these environmental data.\nDifferent data practices, shifting\narticulations of error\nThe first case study I am going to describe is an air\npollution monitoring station. The PI (Principal\nInvestigator) put me in touch with a contact involved\nin monitoring air pollution in City 1, and who was also\na member of WHAP's advisory committee. This led to\nme attending a routine monitoring site trip, during\nwhich I observed the process of checking monitors\nwere functioning correctly and recording the perfor-\nmance of the monitors as part of wider `quality assur-\nance' processes.\nThe second case study was a very different kind of\nmaterial setting, and focuses on the data practices of\nthe atmospheric chemists on WHAP who used large-\nscale computer models to simulate atmospheric pro-\ncesses of air pollution. This involved sharing an office\nwith two key modellers to observe and participate in\nmodel runs, and was followed up by emails and phone\nconversations to explicate these processes further,\nallowing me to ask questions and query particular\nmotions and interpretations of model outputs.\nSpending time with the modellers also enabled me to\nexperience the banality and the everyday-ness of mod-\nelling as particular kinds of data practices.\nAlthough these case studies took place in distinct\nlocations, both modelled and monitored data were cen-\ntral to weekly team discussions, during which their dif-\nference and comparability were considered and very\noften contested. Following traces and the formation\nis an Actor Network Theory inspired approach to the\nstudy of knowledge, which accounts for the agency of\nhuman and non-human forms and relations, and\nattends to how these are constituted in and articulated\nthrough socio-material practices. Indeed, it was\nthrough articulated differences by researchers on\nWHAP, that I came to appreciate the role error\nplayed in the making of air pollution data. By following\nthe material practices of data making, I was able to\nrender visible some of the ways in which particular\ndata practices were embedded in a wider network of\nrelations. Tracing these associations was a continual\nprocess because heterogeneous relations are always\nshifting, (re)producing and reshuffling all kinds of\nactors, including data, scientists and their institutional\nCase study 1: Monitoring instruments\nWe start by climbing up the outdoor stairway to the\nroof of the school. On top of the roof I see a grey porta-\ncabin [. . .] On entering, I am greeted by a set of four\nlarge rectangular boxes stacked on top of each other,\nsupported by a shelving unit. Inside the boxes are two\ntubes, one attached to an outlet in the roof and the\nother connecting to four stacked boxes. To the left of\nthe shelving unit are two gas canisters [which later\nI learnt host the different certified gases]. (Fieldnotes\nThe monitoring station I discuss here is a `back-\nground'2 monitoring station, which, I was told, had\nbeen used to collect measurement for seventeen years.\nThe area has four other sites, and this was known as\n`number one', which relates to its relatively long his-\ntory. Phil, an air monitoring expert, visits the site\nevery two weeks to test the calibration equipment.\nCalibration is a process whereby the measurement\nmade is compared with another `true' measurement in\norder to test it for error. This is one part of a much\nmore elaborate process of testing collected data for\nerror. Indeed, monitoring sites are also visited by engin-\neers and auditors, so the site check I attended was one\namong many others to ensure `quality data'.\nAir pollution is monitored across the UK, initiated\nby central government and often carried out by local\ncouncils. Measurements are collected at different moni-\ntoring sites, and these are organised into different\nnetworks according to the location of the site. For\nexample, London has one accredited `air quality net-\nwork' managed by government departments, local\ncouncils, university research groups and environmental\nagencies. The pollutants measured at the site I visited\nincluded: ozone (O3\n), particulate matter (PM10\n), oxides\nof nitrogen (NOx\n) and sulphur dioxide (SO2\n).3 The pur-\npose of these measurement data are stated as twofold,\nas providing the public and authorities `real-time'4\ninformation on current air pollution levels, and to\nenable short-term and long-term responses to air pol-\nlution as a public health concern (DEFRA, 2012a).\nUsing the data produced at monitoring stations, air\npollution levels are reported in `real time', on a scale\nthat informs the public of air quality in different areas,\ninciting recommendations and actions to protect\nhealth.\nZero air and the calibration test\n`Error' is a term used for a numerical reading not con-\nsidered as measuring air pollution accurately. However,\nerror is not always a mistake, although this is\naccounted for, but also a scale, an acceptable range of\nvariability. In this way, error seems to be a part of\nmeasuring, rather than an unanticipated outcome.\nThere are many different reasons for an erroneous\nmeasurement and the aim of a `calibration test' is to\ncontrol and account for some of these. In order to make\na non-erroneous measurement, it is essential that the\ninstrument used does not influence the measurement\nbeing taken. The main cause of error that calibration\ntests for is `the drift' of the instrument from `zero'. Zero\nis a term used to denote a baseline from which a meas-\nurement can be made, to construct an `unmediated set-\nting'. If the baseline is not zero, then the instrument is\ndrifting by the difference between the measurement\ntaken and zero. Drift is, then, a measurement too,\nand is a technique that accounts for the margin of\nerror in a measurement.\nZero air (also referred to as `pure air') is understood\nas air with no pollutants in it. Zero air does not exist in\nthe environment, but has to be actively made using a\nscrubber, a device that quite literally scrubs away the\nparts of the air that are not being measured. The notion\nof pure air was described by Phil, the technician and\nresearcher who I attended the site check with, as a\nstandardised external reference material, made on-site\nor in a laboratory, and which can be physically intro-\nduced into the measurement setting in order to test for\nerror. Without this fabricated reference point, the\nmeasurement made cannot be stabilised as data. Phil\nmediated the standard and the actual measurement in\nhis manual calibration test, where the standard became\na way to gain purchase on the authenticity of the\n4 Big Data & Society\nmeasurement. In this way, pure air was created to pro-\nduce a kind of `objective nature' through which error\nand measures of air could be made material and `real'.\nEach monitor takes an air sample and measures the\nconcentration of, in our case here, ozone, in the sample\nwith a sensor. The measurement is the concentration of\nozone in the sample with a metric of one-millionth of a\ngram per cubic meter air (mg/m3). The air sample is\ndrawn into tubes by a pump unit connecting the outside\nof the station with the indoor instruments (see\nFigure 1). A measurement is made with a UV light\nbeam that shines through the tube and reacts with the\ndifferent chemical components in the air sample. The\ntubes are called single reaction cells and are fitted with\npneumatic valves, which enable them to switch between\nthe zero measure and the sampled ambient air paths.\nThe measure of ozone is the measure of this reaction in\ncomparison to the measurement taken made with zero\nair. Without this comparative process no measurement\ncan be made.\nThe reading is the level of absorption of ozone in the\nUV beam, compared with the measurement made of\nabsorbance of the pollutant in the zero air sample.\nPhil explained to me that it is the switching between\nthese measurements which results in the making of\ndata of air pollution, where the monitor is:\n[. . .] alternately measuring the absorption of the air\npath with no ozone present (zero air) and the absorp-\ntion in the ambient sample. Gases pass through these\nUV beams and absorb some of the transmitted energy,\nwhich appear in the measured absorbance data.\nWhat this explanation shows is the multiple kinds of\nmeasurements being made in the process of working\nout the concentration of an air pollutant in an air\nsample (considered as ambient air). Indeed, there is\nno baseline, as I've detailed, so measurements of air\nwith the pollutant in and air with no pollutants in are\nmade and used in order to construct a measure of air\npollution. The purpose of testing this process is to\ncheck whether the monitoring device is measuring air\npollution concentrations accurately, so that the\ninaccuracy of the measures can be accounted for in\nthe final data. What is interesting here is the multiple\nmeasurements made in the process of configuring a\nfinal measure of air pollution.\nSeeing error, sensing data\nThe flux that results from the manual calibration test is\nvisible. As I sat next to Phil, a series of numbers\nappeared very quickly on the small screen at the front\nof the monitoring boxes. Indeed, numbers were con-\ntinuously shown on the front of the box as air was\nconstantly pumped through the tube and measured.\nZero air and the calibration gases are measured and\ncompared and, ideally, the readings on the front of\nthe monitor should be the same as the measure in the\ngas canisters. Looking for this `span and drift' in a\ncalibration test means waiting for the reaction to take\nplace and a stable measurement to be made\u00adboth of\nzero air and the calibration gases.5 Phil has to wait at\nleast ten minutes in order for the analyser (the name\nused to refer to the piece of equipment that makes the\nmeasurement) to stabilise:\nAs the numbers on the front of the monitor start to\nslow down, Phil tells me that the display on the front of\nthe monitor box is the `zero reading'. Phil types this\ninto the spreadsheet under the table `data acquisition\nOnce stabilised, the readings are formally recorded\non a shared spreadsheet, which also operational-\nises the numerical readings in a series of further trans-\nformations, `because Excel is also a calculating tool'\nthat of the calibration gas are compared by a mathem-\natical equation, which then provides a measure\nof `error'.\nThe process of reading and interpreting the sequence\nof numbers that stabilises as measurements are, as\nexplained Phil, contingent on the pollutant being mea-\nsured. So, for example, the calibration of ozone\nrequires checking another nearby monitoring station\nto see if it is similar, as ozone is stable over a regional\narea. However, particulate matter (pm2.5\n)\nFigure 1. Inside an air pollution monitor in City 1 (Photo\ncourtesy of author).\nwould not generate a stable reading, since particulates\nare unstable in space and time:\nI asked Phil how he knew what the numbers meant.\nHe responded by explaining that meaning comes\nfrom `experience [. . .] you need an eye to know\nwhat to look for'. Expanding on this notion of\nexperience, Phil suggests there is an embodied\naspect of doing this kind of work, through which\nsomeone can develop `a good eye'. (Fieldnotes, 25\nThe experience of carrying out calibration tests, then,\nenables one to `know what to look for', drawing upon\nthe age old distinction between seeing and knowing\n(Lynch and Woolgar, 1990), and exemplifies the sym-\nbiotic relationship and circulatory nature of seeing and\nknowing in practice (see also Latour, 1999a). As Myers\n(2014) develops in her account of protein modellers, the\nwork of seeing and knowing extends beyond vision to\nthe embodied, kinaesthetic and performative processes\nof coming to materialise scientific phenomena. Indeed,\nPhil went on to compare his own experience of seeing\nand thereby getting `good data' with `non-data ana-\nlysers' (specifically individuals employed by local\nauthorities and inexperienced technicians) as people\nwho `don't know what to look for', and who have\nnot developed the necessary craft skills to re-present\nphenomena in ways that make data of a high enough\nquality.\nCleaning and mobilising air pollution data\nWhilst running the calibration test, Phil balanced his\nlaptop on his knees and opened up the spreadsheet\nready to input the recordings he made. The mainten-\nance of the record of measurements taken and calibra-\ntion results was the second major task of visiting the\nsite, which Phil explicitly referred to as `a record keep-\ning exercise'. The spreadsheet is a table which structures\nthe measurements, with a list of variables including the\nname of the monitoring site, the date, time, temperature\nin the cabin, and the calibration results. These records\ngo straight into a database:\nConstant data is the aim and records of calibration\nresults are kept and put into the database for the time\nperiod [. . .] and you scale it [the data from the monitor]\nuntil the next time someone comes to the site [according\nto the calibration results of this visit]. (Fieldnotes,\nThe calibration results then become attached to the\nmeasurements made by the monitor, so that future\ndata analysis can draw upon the results to check and\nexplain the measurements made and make any adjust-\nments required.\nThe cleaning procedures for air pollution data are\ngoverned by standardised protocols and related thresh-\nolds of validity according to UK and EU legislation, so\nthat the data become further shaped and formatted by\ninfluences from outside the situation of initial capture\norded, maintained and sustained and become part of\nthe history of air pollution monitoring. Indeed, during\nthe site visit, Phil emphasised the importance of main-\ntaining the records for `data capture' and the proceed-\ning journey of these captures to their stabilisation as\n`ratified data'.\nContinuity is also a useful metaphor to think ana-\nlytically about this process of getting `good data'. Air\npollution was conceptualised by Phil as something that\nis always in emergence and therefore continuous.\nHowever, continuity is difficult to measure in practice\nand one of the ways in which continuity was con-\nstructed was through checking for errors and maintain-\ning the material context of measurement, which\nremained identifiable and attached to numerical read-\nings in their journey to becoming data.6 Constructing\ncontinuity by making lots of different kinds of meas-\nurements \u00ad of humidity, of instrument performance, of\ndate and time \u00ad enabled the data to become `more real'.\nThis is a logic which resonates with claims made in\ncontemporary discourse about Big Data and reality\ndata, in order to make data continuous and `big', a\nseries of interferences were required to construct this\nsense of `the real'. The way in which data were made\nreal, however, was specific to the particular pollutant\nmonitored.\nThe standardised procedures of reducing and\naccounting for error, through which monitored data\nbecame more stable, simultaneously mobilised data\ninto different practices. For example, the London Air\nQuality Network publishes data in `real time' on their\nwebsite and these are then classified according to low,\nmedium or high air pollution levels. In this movement\nfrom measurement practices to the practices that work\nwith and re-use these data, data become an objectified\nform, from which claims can be made and further epi-\nstemic inquiries initiated. By providing discrete meas-\nurements every 15 seconds, monitoring enables the\npotential extension of scientific relations and analytical\npatterns of air pollution. This process is a component\nof making Big Data and, although not defined as such\nby Phil, the checking of error and stabilising of accurate\ndata is informed by its functionality, as making up\nlarger data sets for controlling and responding to air\nquality and urban environments.\n6 Big Data & Society\nCase study 2: Running a simulation and\nchecking for error\nThe modeller states `see, there is an error' pointing to\nthe computer screen where, after several seconds, a\nseries of numbers appear. However, I can't see the\nerror. Following this apparent visualisation of error\nthe modeller describes how he is going to now seek to\nunderstand this error, explaining that the compilation\nof code is tricky because if it is compiled on one com-\nputer then it won't necessarily work on another, so by\nre-running the model you start to work out where the\nerror lies and therefore what counts as data.\nModelling involves a different kind of measurement\nsetting and data practice to monitoring. At the same\ntime, I found there were strong resonances between\nthese practices. In modelling, the measurement setting\nwas built with a computer, and the complexities that\nmake up environments, such as temperature, weather\nconditions and time, were constructed within the\nmodel structure. This approach contrasts with moni-\ntoring, where the complexities in taking a measure-\nment influence the setting in which a monitor is\ninitially located. In order, for example, to measure\ntraffic pollution monitors are placed on `the roadside';\nfor `ambient air'7 monitors are located away from\ntraffic to pick up `background air'. Furthermore, the\nprocess of deciding which interactions to study was\nthe subject of continuing debate among the modellers\nand other researchers on WHAP. So, the process of\nproducing air pollution data was worked out contin-\ngently in modelling \u00ad as a result of particular research\ninterests and in relation to the aims of the wider pro-\nject \u00ad rather than as the result of a standardised\nsystem of data collection.\nA simulation model is often considered a theoretical\nrepresentation of the atmosphere. The assumptions\nthat underpin the model are described through math-\nematical equations. The combined model, of the wea-\nther and atmosphere, simulates atmospheric relations\nin process by reducing chemical processes to a\nnumber of physical laws and by inputting other data\nfor specific variables that function as parameters for the\nrunning of a simulation. These equations represent an\n`exact determination of how the [environmental] system\nwill evolve through time' (Winsburg, 1999: 5), so that\nthe actual simulation process is internal to the com-\nputer model.\nThe modellers on WHAP talked about the model as\nthree dimensional, simulating the fluxes, flows and\ntransport of air pollution rather than measuring air\npollution at one point in time and space like monitors.8\nSo, even though the model can simulate a number of\ndifferent pollutants, including ozone and particulate\nmatter, these were considered to be relational and in\nprocess. The different pollutants were referred to by\nthe modellers as relations in the atmosphere, for exam-\nple, as `nitrogen and sulphur deposition' or `surface\nozone'. This is significant if we are to understand how\nmodelling transforms a measurement into data, because\nhow air pollution is configured and imagined in time\nand space shapes the stabilisation process.\nThe policy value of modelling is its ability to produce\ndata on past, present and future air pollution, which\ncan be used in environmental governance and policy\nmaking. At the same time, this feedback between data\nand use plays out in the making of data. As one senior\nmodeller explained, which simulations to run is depend-\nent on the pollutants considered as a health risk and\ntherefore of interest to the policy maker (Elizabeth,\nA simulation run\nI am sharing an office space with Craig and Tom and\nobserving their running a simulation. I am surprised to\nfind a simple and rather ordinary setting, an office very\nmuch like my own, considering the global remit of the\nCM-MW model. It seems to be time consuming. I learn\nthat modelling relies on access to external expertise and\ntechnical resources, specifically, the model interface of\nthe PC is connected to a super computer, which Craig\ncan communicate with at his office computer.9 On the\nscreen are lines of code, and below a box that Craig\nbegins to type commands for the model into.\nThe process of working out error in a simulation run\nwas a key component of the practice of running a simu-\nlation, where there was an alteration between the com-\nputer screen as interface and the mathematical model\naccessed through the computer code. The computer\nscreen became the material way in which the researcher\nengaged with air pollution as a digital abstraction. By\ntyping out particular commands in the box on the\nscreen, Craig manipulated the modelled atmosphere\nto produce a measurement of an air pollutant.\nCommunicating with the model through computer\ncode was an engagement that made the model do\nthings:\nThe core model code provided by the model developers\nis modified for the specific needs of the project [WHAP]\nby manually editing via the keyboard. This human\nreadable `source code' is then `compiled' via a standard\nsoftware tool (compiler) into a set of binary instruc-\ntions which can be understood and executed by the\ncomputer. (Craig, personal correspondence, 19\nThe next step, compiling, translated the line of code\ninto a series of actions. Compiling is an action con-\nsidered as potentially generating error because how\nthe code comes together and performs in practice is\nuncertain. The line of code was translated by the\nmodel into a series of actions, which then performed\na simulation according to the specific parameters deli-\nneated: `to execute the sequence of instructions created\nby the compile stage'. The instructions in the code\norder the variables of interest (e.g., which pollutant,\nmeteorological conditions, location) into output files\nthat are structured and stored under the details of the\nsimulation run (see Figure 2). The arrangements of the\noutput data into files results from a successful simula-\ntion run. This process of arranging output and input\nfiles and its effect on materialising some output over\nothers resonates with Bowker et al.'s (1995) arguments\naround knowledge infrastructures. It is this work of\nstructuring and storing data files, of encoding and clas-\nsifying modelled outputs, which are made visible in\npractice based accounts of data. These organising tech-\nniques ultimately influence the informational content\nand material form air pollution data take.\nChecking for error, configuring air pollution\nThe arrangements of the output data into files results\nfrom a successful simulation run. However, the major-\nity of runs involve error. Craig's demonstration of a\nsimulation run showed me what error means in\npractice, and how it is visualised on the computer\ninterface:\nHaving pressed `run' on the computer interface, we\nwait about ten seconds for a series of lines of code,\nsimilar visually to my untrained eye, as the code that\nwas input into the model. This is because the model\noutput is also in code. The model presents a result\nin the response box above the command box on the\ninterface that Craig sits in front of. (Fieldnotes,\nThere is error if the code does not produce a `legible\noutput', appearing as a line of script within which the\nerror lies and needs to be worked out. The line of script\nbecomes the object of interest. `De-bugging' is an\nexercise in trying to understand the sources of error\nby re-examining the different elements of a model\nrun. Primarily, this involves going back to the typed-\nin code commands. Indeed, there are a number of\nrecognised sources of error in the data. There may be\nerror in the performance of the code in a compilation;\nthere may be error in the assumptions within the code,\nfor example, the approximated measurement by those\nwho wrote the code not co-ordinating with the approxi-\nmation of measurement being produced in the simula-\ntion; or `human error' in the process of re-writing the\ncode for the particular compilation. These different\nkinds of sources of error were found, understood and\ncontrolled for through a series of interactions at the\nmodel interface.\nFigure 2. Organising the input and output data in a simulation run (Table from CM-MW User-manual).\n8 Big Data & Society\nIn Craig's account, checking for error seemed to be a\nprocess of getting a sense for the balance of the model\nas a good representation of the atmosphere, the code as\nthe means by which the computer and modeller can\ncommunicate, and the assumptions behind the data\nused as inputs into the model. Craig described this act\nof generating data an `art', of balancing the different\nelements comprising the modelled atmosphere, along-\nside an understanding of what kinds of air pollution\nrelations are of interest to those using the data.10 This\nempirical anecdote extends what counts as error\nbecause it is through achieving balance, and the feeling\nfor work that is required to do this, that `good data' is\nmade. Here, error free data is neither a classification\nnor a scale but an enactment, as Sennett suggests, a\nmodelling was not simply a process of getting a good\nrepresentation of the atmosphere, but an engagement\nthat plays with atmospheric relations in ways that\nshape how air pollution in the atmosphere came to be\nknown and performed.\nAt a different scale, then, the modelled generation of\nbig, national scale data of air pollution is not unprob-\nlematic either. Modelled data of air pollution was also\nsubject to the effects and affects of mundane, everyday\ndata practices. Once the modelled outputs were made\nlegible (but not stabilised data), they were read using\nopen source software, which is an aid for analysing data\nthrough visualisation as mapped concentrations (see\nFigure 3). The mapped outputs were then used to\nhone and develop the simulation run in ways which\nwould make data more accurate. Like in the calibration\ntest, this shift back to the technical arrangements of the\nmeasurement setting by Craig was where the `tinkering'\n(Knorr-Cetina, 1981) took place, so that Craig's\nengagement with the atmosphere in the building, run-\nning and re-running of model simulations intervened in\nthe articulation of air pollution by making particular\natmospheric relations `more visible'. Error was a fun-\ndamental part of making `good data', a process through\nwhich what counted as good/ bad data shaped how air\npollution was ultimately stabilised and made real.\nDiscussion: The role of error in\nthe making of air pollution data\nThe proliferation of environmental data poses a chan-\nging set of inquiries for those studying scientific prac-\ntices and knowledge making. If we are going to study\ndata practices as a particular way of doing science, then\nthe ways in which these practices articulate phenomena,\nand how researchers sense and enact data in different\nways, need to be explored at the multiple sites where\nthese transformations take place. The ethnographic\naccount of air pollution data offered in this paper is\nan attempt to consider how data emerge as a result of\none kind of transformation, that of a purification of\ndata through the working out of error. In doing so,\nthe affordances embedded in different kinds of data\nand how these relate to the scientific and policy\nworlds in which they are made and used were explored.\nFor Phil and Craig, making stable data was achieved\nthrough carefully balancing the context of measure-\nment, the phenomena under study and their ability to\neffect and affect air pollution as a research object.\nMaking data was not unmediated or discrete\n(Bowker, 2010), but rather a process that unfolded tem-\nporally through interaction between the phenomena in\nquestion, technical objects and other scientific values.\nChecking a measurement for error was routine practice,\nyet it was also an inherently uncertain and ambiguous\nprocess, which involved practices of feeling for error in\norder to both get a sense for, and make sense of, air\npollution as data; a practice that didn't simply repre-\nsent, but materialised air pollution as a tangible form.\nModel and monitor devices were engaged with in ways\nwhich made visible particular and contingent relations\nof air pollution as data.\nThis process of stabilising accurate and useable data\nof air pollution involved sensing for error, where error\nbecame the focus of investigation and the materialised\nrelation from which air pollution could emerge and\nFigure 3. The visualisation of error: A map of SO2\nconcen-\ntake form. I have shown that in monitoring and mod-\nelling, what counted as error was shaped by contingent\nlogics of functionality and framing. In monitoring,\nomissions were used to further fabricate and make\naccurate the final data. In modelling, error was used\nto re-configure the very measurement process, therefore\noperating differently to monitoring in its entanglement\nwithin data making rather than the final data.\nDeveloping a feeling for the relational inter-dependen-\ncies of making data was both a honing of professional\nvision (Goodwin, 1994) and the craftwork of articulat-\ning and managing error. Performance of error was also\ninfluenced by the social and political networks of mod-\nelling and monitoring air pollution. For example,\nmonitored data is used in everyday public health man-\nagement, and error is constantly made and taken away\nto make data instantly useable and verifiable. In mod-\nelling, error is checked for and then taken away by\nrestructuring the measurement process (a simulation).\nRather than a scale of an acceptable range of error in\ndata (how far the measurement errs from `the truth'), in\nmodelling error is an enactment that, as Sennett notes,\ninvolves working with error and thereby reconfiguring\nnew articulations of atmospheric relations and thereby\nallowing new kinds of questions to be posed.\nExploring these particular roles and artefacts of\nerror extends our understanding of the social and pol-\nitical lives of data. It is through measuring and account-\ning for error that data's validity was made and\nmobilised. I have demonstrated the ways in which\ndata are always shifting, demanding flexibility and\nIn doing so, I have also pointed out that an important\npart of this alignment process is the performance, man-\nagement and taking away of error. Thus, starting out\nwith the premise that data are always situated, material\nenactments is productive in coming to understand the\nsocial and political dimensions of data, and the ways in\nwhich data gain social and political validity as Big Data\nI've shown that the experienced multiplicity and het-\nerogeneity involved in stabilising what comes to count\nas `real data' are constantly negotiated by those scien-\ntists and technicians who craft and `make up' data.\nIndeed, it was researchers' articulation of their embo-\ndied work that also enabled me, as an ethnographer, to\nget a feeling for the multiple agencies mobilised in\nmeasurement practices. Like Myers, I have emphasised\nthe sensory dimensions of feeling for error and the sen-\nsibilities which configure these so that they become\nsensible and useable data. Error practices mobilise het-\nerogeneous elements which give air relations both their\nsensitivity \u00ad ability to respond to intervening practices \u00ad\nand their sensibility \u00ad to endow them with a kind of\nresponsivity that can be used to make sense of their\nworlds (Myers, 2015a). It is these different kinds of\nsensing practices and the attentiveness demonstrated\nby scientists in my research that are made active in\nthe accounts of error provided in this paper, and\nwhich highlight how valid data are made accountable\nto their relations, thereby becoming valid and `sensible'.\nIt is through error, then, that we can better understand\nthe multiple agencies which configure different versions\nof air pollution in practice.\naccounting for errors and ambiguity in everyday prac-\ntices is fundamental to the maintenance of social\n(and socio-technical) relations and to the extension of\nknowledge (embodied, material and informational).\nThis relationship between error and social and political\nnetworks is of particular significance if we are to under-\nstand and trace the expanse and extent of relations\nwhich form the social lives of scientific data, and\nindeed Big Data. Starting out with case studies of\ndata practices of air pollution, I've also problematised\nthe focus on `data' in discourses of Big Data by fore-\ngrounding the other kinds of relations which configure\nand contain data. I've demonstrated the ways in which\nperformative work of data making always involves\nmaterialising and accounting for error, a practice vital\nfor data to carry meaning, circulate freely and mobilise\nas informational forms.\n"
}