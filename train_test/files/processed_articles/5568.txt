{
    "abstract": "Abstract. Using vision for navigation is important for many animals and a common debate is the\nextent to which spatial performance can be explained by \"simple\" view-based matching strategies.\nWe discuss, in the context of recent work, how confusion between image-matching algorithms and\nthe broader class of view-based navigation strategies, is hindering the debate around the use of\nvision in spatial cognition. A proper consideration of view-based matching strategies requires an\nunderstanding of the visual information available to a given animal within a particular experiment.\n",
    "reduced_content": "a Pion publication\nAntoine Wystrach\nSchool of Life Sciences, University of Sussex, Brighton, UK; e-mail: a.wystrach@sussex.ac.uk\nPaul Graham\nSchool of Life Sciences, University of Sussex, Brighton, UK; e-mail: p.r.graham@sussex.ac.uk\n Keywords: navigation, spatial cognition, view-based matching, image matching, geometry.\n1 How might animals use vision for navigation?\nThe navigation of many animals relies on vision through learning about the appearance of the world\nfrom important locations. One interesting question concerns the processing and computation needed to\ngo from visual input to navigational memory and then to behaviour. One possibility is that vision can\nbe used in a rather direct way (sensu J. J. Gibson, 1979). For example, in a complex world, two photo-\ngraphs taken with the same camera can only be identical when the camera location and orientation are\nmatched. This is also true for natural visual systems. Thus, if the view from a location is memorized, it\ncan be used directly, by simple matching with the currently perceived view, to recover both the original\nlocation and orientation (Zeil, Hofmann, & Chahl, 2003). An alternative, indirect method, would be to\nprocess and interpret the egocentric visual input in order to construct a higher-order representation of\nspace with a different coordinate frame, such as an environmentally referenced or allocentric cogni-\ntive map. The spatial computations that produce navigational behaviour could then be performed on\nthe new construct.\nAn emerging debate that pits direct and indirect ideas against each other concerns whether animals\npossess and use a geometric module to represent the shape of environments (Cheng, 2008). Vertebrates\nhave been assumed to functionally extract the geometrical layout of an environment for reorientation.\nThe original result that inspired this idea came from rats rewarded in one corner of a rectangular arena\n(Cheng, 1986). Rats often made errors by confusing the rewarded corner and its geometrical equiva-\nlent (i.e., the diagonally opposite corner, which shares the same position relative to the rectangular\nshape of the arena), even when each corner is marked by a distinct visual feature. This suggested that\nthe geometry of the arena was constructed and represented independent of the features that compose it.\nThe alternative explanation involves simply storing raw views that are associated with the goal\ncorner. It has been shown that the shape of such arenas is implicitly contained in panoramic views\n(St\u00fcrzl, Cheung, Cheng, & Zeil, 2008) and that simple view-based matching strategies could explain\nmany experimental results (Cheung, St\u00fcrzl, Zeil, & Cheng, 2008). The analysis involved in these pa-\npers (Cheung et al., 2008; St\u00fcrzl et al., 2008) used a virtual reality model of experimental arenas, so\nthat animal's perspective views could be generated. Views from across the entire arena were compared\nwith a goal view from a position near the target corner. The comparison is performed by summing the\nintensity differences between location-matched pixels across the two views. Simple methods of this\ntype are often called image-matching strategies, as views are represented by images. We discuss here,\nin the context of a recent paper, how confusion between simple pixel-wise image-matching algorithms\ni-Comment\nView-based matching can be more than image matching:\nThe importance of considering an animal's perspective\nand the broader class of view-based navigation strategies is hindering the debate around the use of\nvision in spatial cognition.\n2 The difference between image matching and view-based matching\nAcross a series of papers, Lee and colleagues have tried to pick apart the use of geometry and im-\nage matching in reorientation tasks for infants (Lee & Spelke, 2011) and chicks (Lee, Spelke, &\nVallortigara, 2012). In a visual working memory task, individuals are shown a rewarded corner in a\nrectangular array and then disoriented. After their release, it is recorded whether subjects confuse the\ncorrect corner with its diagonally opposite corner (i.e., geometrical success) or with all corners (i.e.,\ngeometrical failure). Interestingly, chicks, like young children, \"succeeded\" when the surrounding\nrectangular shape was defined by a subtle three-dimensional (3D) perturbation of the floor but \"failed\"\nwhen the rectangular shape was defined by salient high-contrast 2D cues, such as a coloured surface\non the floor or conspicuous columns at the corners of the rectangle. As claimed by the authors, this in-\ndeed goes directly against the prediction of image matching, because a raw panoramic image oriented\ntoward the rewarded corner will generate a good match when facing the diagonally opposite corner in\nboth conditions with salient 2D cues. However, we would like to emphasize why those results--that\ndo refute image matching--do not similarly refute view-based matching.\nView-based matching refers not to the 2D or 3D nature of the cues used but to the fact that views\nare stored and matched in an egocentric frame of reference. A key question, for any given experimen-\ntal subject, is to ask what information would be present in such an egocentric view. That is, we need\nto understand an animal's umwelt or self-world (von Uexk\u00fcll, 1957). Walking insects appear to use\nmostly 2D cues, hence the relevance of using 2D images to model their views. But flying insects and\nvertebrates can generate effective depth information from parallax and/or binocularity. These depth\ncues can also be incorporated into view-based models of reorientation (bees: Dittmar, St\u00fcrzl, Baird,\nBoeddeker, & Egelhaaf, 2010; humans: Pickup, Fitzgibbon, Gilson, & Glennerster, 2011). View-based\nmodels should consider visual properties such as colour, regional acuity variations, binocularity, as\nwell as the influence of active sensing on the information perceived (e.g., self-generated parallax).\nThat is, we have to remember that navigating animals are embodied cognitive systems (Clark, 1997)\nwith particular sensors and particular ways of moving in the world.\nFor example, to fully understand the results in Lee et al. (2012), we need to understand the chick\nvisual system, two aspects of which may play a role in explaining Lee et al.'s pattern of results.\n(i) With the flat contrasted rectangular shape on the floor, perhaps chicks did not spontaneously dis-\ncriminate between correct and incorrect corners because, due to their fovea, high-contrast features\nmay have increased salience in the frontal visual field. Thus, chicks' perspective views facing all four\ncorners will be more similar to each other than raw images would be. (ii) Chicks may have been able to\nextract shape cues from horizontal walls rather than vertical columns because depth information was\ngenerated by vertical head-bobbing rather than horizontal swaying. The results of Lee et al. are sugges-\ntive about of the nature of the cues used by chicks for reorientation. However, further knowledge of the\nchicks' visual system (including any active components) is required before we can address questions\nabout the potential of view-based matching.\nWithin this and other experimental paradigms, the research program that might lead to a full\nevaluation of view-based matching would involve a systematic investigation of an animal's visual\nsystem and ability to discriminate different cues. This knowledge would allow the design of experi-\nments where simple manipulations of the environment or the subject's starting position will lead to\npredictions about different paths being taken if the subject is using a view-based matching strategy\n(e.g., Wystrach, Cheng, Sosa, & Beugnon, 2011). In contrast, such manipulations should not alter the\nstraightness of the path of a subject using higher-order representations, enabling us to distinguish be-\ntween the two hypotheses. Similarly, forcing the subjects to perceive a scene from different directions\nduring training should affect view-based matching and not allocentric navigation. With such an ap-\nproach, Pecchia and Vallortigara (2010) and Pecchia, Gagliardo, and Vallortigara (2011) demonstrated\nthe use of a view-based matching strategy for certain tasks in chicks and pigeons.\n3 Conclusion\nBecause of the parsimony of the idea, the use of views for navigation is often thought of as an insect\nsolution. However, view-based matching is a useful strategy for any navigator (Wystrach & Graham,\n2012). For animals with any type of visual system, view-based matching is an inexpensive process\nbecause information is perceived, stored, and used in the same egocentric frame of reference. The\nPublished under a Creative Commons Licence a Pion publication\nView-based matching can be more than image matching 549\nagent is therefore freed of any computations required to move information between different coor-\ndinate schemes (i.e., from egocentric to allocentric for storage and from allocentric to egocentric for\naction). This is true for navigation but does not necessarily mean that view-based matching is good\nfor other tasks. Object recognition, for instance, needs to be viewpoint invariant and therefore the\ndemands of the task may have driven different perceptual systems (Biederman & Gerhardstein, 1995).\nAlternatively, object recognition may depend on the integrated use of multiple egocentric views (Tarr\nWe have explained here that refuting 2D image matching and emphasizing the use of 3D cues in\nvisuospatial tasks (Lee et al., 2012) is interesting as it provides insight into which cues are extracted\nby the visual system of a given species. However, this approach does not fully test for view-based\nmatching. View-based matching refers not to which cues are used but how those cues are used. We\nhope this paper will help future studies to clearly disentangle between the nature of visual cues used\nby an animal and how they are processed and used: where view-based matching is often an alternative\nhypothesis to the use of allocentric representations.\nReferences\nBiederman, I., & Gerhardstein, P. C. (1995). Viewpoint-dependent mechanisms in visual object recognition:\nReply to Tarr and B\u00fclthoff (1995). Journal of Experimental Psychology: Human Perception and\nCheng, K. (2008). Whither geometry? Troubles of the geometric module. Trends in Cognitive Sciences, 12,\nCheung, A., St\u00fcrzl, W., Zeil, J., & Cheng, K. (2008). Information content of panoramic images: II. View-\nbased navigation in nonrectangular experimental arenas. Journal of Experimental Psychology: Animal\nClark, A. (1997). Being there: Putting brain, body and world together again. Cambridge, MA: MIT Press.\nDittmar, L., St\u00fcrzl, W., Baird, E., Boeddeker, N., & Egelhaaf, M. (2010). Goal seeking in honeybees: matching\nGibson, J. J. (1979). The ecological approach to visual perception. Boston, MA: Houghton Mifflin.\nLee, S. A., & Spelke, E. S. (2011). Young children reorient by computing layout geometry, not by matching\nLee, S. A., Spelke, E. S., & Vallortigara, G. (2012). Chicks, like children, spontaneously reorient by three-\nPecchia, T., Gagliardo, A., & Vallortigara, G. (2011). Stable panoramic views facilitate snap-shot like memories\nPecchia, T., & Vallortigara, G. (2010). View-based strategy for reorientation by geometry. Journal of\nPickup, L. C., Fitzgibbon, A. W., Gilson, S. J., & Glennerster, A. (2011). View-based modelling of human\nSt\u00fcrzl, W., Cheung, A., Cheng, K., & Zeil, J. (2008). Information content of panoramic images: I. Rotational\nerrors and the similarity of views in rectangular arenas. Journal of Experimental Psychology: Animal\nTarr, M. J., & B\u00fclthoff, H. H. (1998). Image-based object recognition in man, monkey and machine. Cognition,\nvon Uexk\u00fcll, J. (1957). A stroll through the worlds of animals and men: A picture book of invisible worlds. In\nC. H. Schiller (Ed.), Instinctive behavior: The development of a modern concept. New York:\nInternational Universities Press.\nWystrach, A., Cheng, K., Sosa, S., & Beugnon, G. (2011). Geometry, features, and panoramic views: Ants in\nrectangular arenas. Journal of Experimental Psychology: Animal Behavior Processes, 37, 420\u00ad435.\nWystrach, A., & Graham, P. (2012). What can we learn from studies of insect navigation? Animal Behaviour,\nZeil, J., Hofmann, M. I., & Chahl, J. S. (2003). Catchment areas of panoramic snapshots in outdoor scenes."
}