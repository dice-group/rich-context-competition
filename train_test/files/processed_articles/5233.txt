{
    "abstract": "Abstract\nAs algorithms pervade numerous facets of daily life, they are incorporated into systems for increasingly diverse purposes.\nThese systems' results are often interpreted differently by the designers who created them than by the lay persons who\ninteract with them. This paper offers a proposal for human-centered algorithm design, which incorporates human and\nsocial interpretations into the design process for algorithmically based systems. It articulates three specific strategies for\ndoing so: theoretical, participatory, and speculative. Drawing on the author's work designing and deploying multiple\nrelated systems, the paper provides a detailed example of using a theoretical approach. It also discusses findings pertinent\nto participatory and speculative design approaches. The paper addresses both strengths and challenges for each strategy\nin helping to center the process of designing algorithmically based systems around humans.\n",
    "reduced_content": "Original Research Article\nToward human-centered algorithm design\nEric PS Baumer\n Keywords\nHuman-centered design, interpretation of algorithms, performance metrics, theory, speculative and critical design\nPremature optimization is the root of all evil (or at least\nAlgorithms are designed to perform. A given algorithm\nis considered better when its results show an improve-\nment according to some agreed-upon performance\nmetric. These metrics assess aspects such as accuracy\n(e.g., which algorithm correctly identifies more spam\nemail, a Nai\u00a8ve Bayes classifier or a support vector\nmachine?), speed (e.g., which sorts faster, insertion\nsort or bubble sort?), or computational resources\n(e.g., which requires less bandwidth, a Skype conversa-\ntion or a Google hangout?). Such metrics establish\nagreed-upon goals against which algorithm designers\ncan compare their results.\nThese metrics' primacy, however, can also preclude\nother aspects of an algorithm's performance.\nSignificant work has documented disconnects between\nthe functioning of algorithmically based systems and\nthe social interpretations thereof (e.g., Ananny, 2011;\n2015). Among the many factors involved, the metrics\nused to assess algorithms' performance are based on\nthose aspects that are most readily computationally\nquantifiable. As a result, those metrics may or may\nnot closely align with human and lay interpretations\nof what said algorithms do and mean.\nThis paper suggests addressing these disconnects\nthrough a practice termed human-centered algorithm\ndesign (HCAD). This practice applies techniques from\nhuman-centered design to the technical components of\nalgorithmic systems. This paper presents the author's\nexperiences designing, implementing, and evaluating\nseveral algorithmically based systems around issues\nrelated to political framing (Chong and Druckman,\npaper distills three strategies by which algorithm\ndesign could become more human centered.\nFirst, theoretical approaches can incorporate a\nwealth of concepts and theories from behavioral and\nsocial sciences on topics pertinent to algorithmic sys-\ntems. While some work has used such an approach in\nresearch contexts (e.g., Hopkins and King, 2010;\nLehigh University, USA\nCorresponding author:\nEric PS Baumer, Lehigh University, Computer Science & Engineering, 379\nEmail: ericpsb@lehigh.edu\nCreative Commons Non Commercial CC BY-NC: This article is distributed under the terms of the Creative Commons Attribution-\nNonCommercial 4.0 License (http://www.creativecommons.org/licenses/by-nc/4.0/) which permits non-commercial use, reproduction\nand distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nBig Data & Society\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\njournals.sagepub.com/home/bds\nless often used for developing the algorithmic compo-\nnents of interactive systems. This paper will draw on\nthe authors' own work in the area of political framing\n(Baumer et al., 2015a) to demonstrate this approach,\ndescribing both successes and limitations (cf. Agre,\nSecond, current conceptions of what computation\nsystems can do or should do may limit the design of\nsuch systems. Thus, speculative approaches may pro-\nvide a means of overcoming how we currently conceive\nof algorithmically based systems. Drawing on insights\nfrom a field study led by the author (Baumer et al.,\n2014b), the paper will describe how speculative design\nalternatives for living with algorithms.\nFinally, both theoretical constructs and speculative\ndesign can at times be removed from human practice.\nThus, participatory approaches offer another avenue.\nParticipatory design has as established history, both in\ntechnology design and elsewhere (Ehn, 1988; Muller and\nthe same field study (Baumer et al., 2014b), this section\ndescribes both some potential challenges and unique\nopportunities with participatory approaches.\nThese three approaches do not exhaustively map the\nspace of possible strategies. Rather, they provide a\nsense for the range of possibilities. The discussion con-\nsiders the potential and limits of these approaches in\naddressing the relationship between the technical and\nsocial dimensions of algorithms. The paper concludes\nwith some considerations about the practicality of fol-\nlowing such approaches.\nRelated work: The interpretation of\nalgorithmic systems\nMisalignments often occur between the functioning of\nalgorithmic systems and how those systems are inter-\npreted. The term ``algorithm'' itself only recently entered\nthe public consciousness (Sandvig, 2014), due in part to\nincreasing attention in popular media. For instance,\nFacebook regularly tweaks the algorithm that curates\nusers' news feeds to test how different variants impact\nsubsequent behavior. This fact was pointedly high-\nlighted in a study of emotional contagion (Kramer\net al., 2014). Despite this practice having existed for\nquite some time previously, the Kramer et al. study\nresulted in significant backlash, including some calls to\ntry abandoning Facebook (Baumer et al., 2015b).\nNumerous other examples exist. At one time, the\nGoogle Play store page for Grindr, a dating app for\ngay men, recommended as similar an app to determine\nif the user lives near a sex offender (Ananny, 2011).\nFlickr's automatic photo tagging been observed sug-\ngesting that African-American faces be labeled as\n``apes'' (Hern, 2015). Google's search correction feature\nhas been noted to ask those who search for ``she\ninvented'' if they actually intended to search for ``he\nSearches for African-American names are more likely\nto show ads suggesting that that person has an arrest\nFrom a design perspective, these and other similar\nexamples would likely have been difficult to predict,\nperhaps even impossible. When confined to a labora-\ntory or scientific setting, results along these lines could\nbe seen as aberrant and interpreted as such. However,\nwhen algorithms become incorporated into interactive,\npublic-facing systems, their results become interpreted\nby people who likely have less knowledge of the tech-\nnical implementation details of these algorithms. In\nsuch situations, what becomes most striking is not the\nresults themselves but what they are interpreted to\nmean about, e.g. deviant behaviors (Ananny, 2011),\nsocial norms around gender and race (Barocas and\nFurthermore, these lay interpretations are rarely, if\never, accounted for during the development of such\nalgorithms. Lay interpretations often enter the picture\nonly after an algorithmically based system is publicly\nreleased. The process of algorithm design is driven pri-\nmarily by technical constraints, performance on bench-\nmarks, etc. One of the goals in the work proposed here\nis to link these lay conceptions of algorithmic analysis\ninto the design process for systems driven by\nalgorithms.\nThe case for humancentered\nThis paper intentionally avoids use of the term ``user\ncentered.'' No one would dispute the value that user-\ncentered design has brought to human\u00adcomputer inter-\naction (HCI) and related fields. However, the process of\ndesign should not hinge entirely on the construct of\n``the user'' (Redstro\nships to, with, and through technology. Those impacted\nin the above cases--gay men who are implicitly asso-\nciated with sex offenders (Ananny, 2011), African-\nAmericans who are suggested as having an arrest\nand women whose pregnancies are predicted based on\ntheir purchase histories (Baumer, 2015; Duhigg,\n2012)--would not be included among users of algorith-\nmically based systems. Although the examples\ndescribed in the case study below focus primarily on\nusers of the systems being designed, the strategies\n2 Big Data & Society\ndistilled from these experiences could apply to a\nwide variety of different subject positions in relation\nto algorithmically based systems (Baumer and\nComputational supports for frame\nreflection\nThe example systems in this paper come from a larger\nproject exploring computational approaches to iden-\ntifying the language of political framing (Chong and\nanalytic in nature; this project did not involve identify-\ning frames per se. Rather, the goal was reflective. We1\nwanted to develop algorithmically based interactive\ntechnologies that could draw attention to, and promote\ncritical thinking about, framing, i.e. frame reflection\n(Scho\nBackground: Political framing\n``Facts have no intrinsic meaning. They take on their\nmeaning by being embedded in a frame [. . .] that organ-\nizes them and gives them coherence'' (Gamson, 1989:\n157). Frames can be invoked by ``keywords, stock\nphrases, stereotype images, sources of information''\nphrases, depictions, and visual images'' (Gamson and\nlinguistic or rhetorical devices provide an interpretive\nlens or ``package'' (Gamson and Modigliani, 1989)\nthrough which to perceive and make sense of facts or\nevents.\nMost work on framing essentially compares the impact\nof different frames, exposing study participants to two (or\nmore) different frames and assessing the impact of those\nframes on participants' opinions (Brewer, 2002;\nA complementary line of work seeks to shift framing\nfrom a source of subconscious influence to the focus of\nconscious inquiry. In what Scho\nframe reflection: ``assumptions, views of the world, and\nvalues that have heretofore remained in the back-\nground, giving shape to foreground inquiry but keep-\ning, as it were to the shadows, become foreground\nissues, open to discussion and inquiry in their own\nright'' (Rein and Scho\nWhile it can result in more productive dialog, enga-\nging in frame reflection is no mean feat. Framing is so\neffective in part because it operates largely subcon-\nsciously (Gamson and Modigliani, 1989; Lakoff and\nTurner, 1989). It can be difficult to notice that a\nframe is even present, let alone interrogate it critically\nor explore alternatives.\nThe project described here asked: can we leverage\ndata-intensive techniques from natural language pro-\ncessing and computational linguistics to help bring\nthe language of framing to conscious attention?\nDoing so, we suggested, could help serve as a scaffold\nfor frame reflection. The remainder of this section\nrecounts experiences with two such systems, the strate-\ngies employed in their design, and those strategies'\nefficacies.\nFrame reflection in action: FrameCheck\nImagine that you are reading a news article online.\nWhile reading that article, a web browser plug-in high-\nlights a few key words and phrases most related to\nframing. This subsection describes work related to a\nnatural language classifier designed to perform just\nThe design process explicitly employed a theoretical\nstrategy. We began by inventorying the linguistic and\nrhetorical devices mentioned in sociological, political,\nor communication work related to framing (Brewer,\nstudy where we asked human participants to read an\narticle and highlight the terms most relevant to fram-\ning. In debriefing interviews, participants then\nexplained which terms they highlighted, which they\ndid not highlight, and their reasoning behind each\n(for details, see Baumer et al., 2015a).\nThe question became: how would we (or could we\neven) develop computational analogs for each of these\nlinguistic or rhetorical devices? Some were fairly\nstraightforward. For example, keywords and catch-\nphrases (Entman, 1993; Gamson and Modigliani,\n1989) can be operationalized as n-grams, i.e. one-,\ntwo-, or three-word phrases that consistently occurred\ntogether. Grammatical construction, e.g. active versus\npassive voice (Fairclough, 1999), can be identified using\nexisting parsing tools (De Marneffe et al., 2006). Some\ndevices required more sophisticated techniques, draw-\ning on computational work for identifying, e.g. meta-\nphorical language (Turney et al., 2011). Others had to\nbe abandoned entirely. For example, we could find no\nsuitable algorithmic technique for identifying stereo-\ntyped language.\nFrom a technical standpoint, this strategy proved\nrelatively successful. We were able to create a classifier\nthat, when tasked with identifying framing in political\nnews articles, agreed with human annotators about as\noften as the human annotators agreed with one\nanother. Specifically, the classifier's average F1 scores\n(using 10-fold cross validation) were statistically indis-\ntinguishable from average F1 scores comparing each\nhuman annotator against the others. The results war-\nranted publication in a top computational linguistics\nFrom a theoretical standpoint, the work also made\nsome interesting contributions. Prior work in political\ncommunication had noted that ``straightforward guide-\nlines on how to identify [. . .] a frame in communication\nresults of our experiments suggest that the presence of\nframing could be effectively identified using only lexical\nfeatures (i.e., n-grams). That is, word choice mattered\nmore than grammatical construction, figurative lan-\nguage, or other aspects suggested as important by the\ntheoretical literature. This system was trained on data\ncollected from lay annotators, though. Analysis of\nframing annotation by expert trained coders (e.g.,\nCard et al., 2015) could yield different results.\nThe theoretical strategy. These connections demonstrate\nthe potential of a theoretical approach to HCAD.\nBehavioral and social sciences provide numerous the-\nories to help understand how people work. The above\nexample shows how such theories can serve at least two\nvaluable functions within algorithm design.\nFirst, theories can be prescriptive. Most data sets\ninclude a potentially overwhelming number of possible\nfeatures. About a given individual, one might know\nage, gender identity, height, weight, personality, drug\nuse, annual income, charitable donations, frequency of\nphysical activity, cholesterol levels, purchasing his-\ntories, credit score, marital status, parental status, etc.\nOne could derive similarly myriad features from our\nannotated data about framing. When developing a\nmodel, how does one choose which features to include\nin the model? A nai\u00a8ve approach would simply test every\npossible feature. Not only might this prove computa-\ntionally intractable, but testing more features increases\nthe likelihood of identifying spurious relationships. A\ndata-centric approach would likely emphasize feature\nselection (Guyon and Elisseeff, 2003), which quantita-\ntively determines which features in a model are most\ninformative. Again, though, feature selection provides\nless guidance as to why a given feature should be\nincluded in, or excluded from, a model. Instead,\nFrameCheck explicitly and clearly drew on social sci-\nentific theories of framing, augmented by a human sub-\njects study, to inform feature selection. This example\ndemonstrates how theory can be used prescriptively to\nguide algorithm design.\nSecond, theories can be descriptive. Given the results\nproduced by an algorithm, what do they mean?\nTheories can help sort through potential interpretations\nfor the results of such systems. The evaluation of\nFrameCheck was specifically arranged to allow for test-\ning different theoretical hypotheses about which lin-\nguistic features of framing mattered most. By\ncomparing the algorithm's performance against data\ncollected from human annotators, the results were\nable to speak back to these open questions about how\nto identify framing (Chong and Druckman, 2007).\nWhile leveraging social and behavioral theories can\nbe beneficial, the nature of the transformations neces-\nsary for their incorporation in algorithmic analysis also\nposes serious challenges (Agre, 1997). For instance, the\ntechnique used here to identify metaphorical language\nfocuses on mismatches in levels of abstraction between\na term and its modifier. The phrase ``dark paint''\ninvolves a concrete modifier and a concrete object,\nwhile the phrase ``dark thoughts'' matches a concrete\nmodifier with an abstract object; the latter is thus more\nlikely metaphorical (Turney et al., 2011).\nSuch codifications happen with virtually any kind of\ncategorizing representational system, computational, or\notherwise (Bowker and Star, 1999). However, as those\nrepresentations become the subject of algorithmic\nmanipulation, they can become farther and farther\nremoved from an intuitive understanding of the phe-\nnomena they are meant to capture. Burrell (2016)\ndescribes the application of deep learning techniques\nto computer vision tasks, such as identifying handwrit-\nten digits 0 through 9. The resultant neural networks\ncue in on visual features that bear little apparent resem-\nblance to how humans perceive their own process of\ntelling the difference between, say, a 1 and a 7.\nA similar situation occurs in FrameCheck's use of\nlexical features. The most influential values for this fea-\nture show that words in close proximity to prepositions\nor conjunctions--``to,'' ``and,'' ``in,'' ``of,'' etc.--are\nmost likely to be classified as invoking framing. While\nthey resonate strongly with comments made during our\nformative human subjects study, such patterns differ\nsignificantly from the keywords and catchphrases\nthese lexical features were intended to identify. Burrell\n(2016) notes difficulties in interpreting the complex and\nsometimes inscrutable features identified by algorithms.\nThe example here shows how even seemingly simple\nfeatures, such as the specific words used in a sentence,\ncan be algorithmically transformed into something\nother than the theoretical attribute they are meant to\nrepresent.\nThis situation raises some serious difficulties for the-\noretical approaches. Social science's extant commitment\nto theory-driven work often requires that researchers\nformulate and test hypotheses within a theoretical\nframework, e.g. framing (Chong and Druckman, 2007;\nalgorithmic transformation of theoretical concept to\n4 Big Data & Society\ncomputationally identifiable feature problematizes, or\nperhaps even renders impossible, this kind of theoretical\nhypothesis testing. If the lexical features to which an\nalgorithm attends do not clearly map onto the keywords\nand catchphrases described in theory (Chong and\nModigliani, 1989), what can we say conclusively about\ntheir importance in identifying framing?\nCollectively, these points highlight differences\nbetween the manner in which human centering occurs\nduring the design process and during the evaluation.\nDespite drawing on social scientific concepts, the clas-\nsifier was still evaluated using traditional performance\nmeasures for machine learning algorithms, namely\naccuracy, precision, recall, and F1 score. Optimizing\nthese performance metrics, however, may elide the\nvery attributes on which the theoretical strategy\nhinges. Furthermore, while likely necessary for publica-\ntion in technical venues, such performance metrics may\nhighlight different kinds of issues than evaluation with\nhuman users. For instance, an overly aggressive classi-\nfier may achieve high recall, finding most of the words\nrelated to framing, but highlight so many terms that it\nbecomes overwhelming. Such examples suggest that,\nwhile valuable, metric-based performance evaluation\nshould not and cannot serve as the ultimate evaluation\nin human-centered approaches to algorithm design. As\nan alternative, the next subsection shows how field stu-\ndies with human users can elucidate such nuances\nabout the real-world functioning of algorithmic\nsystems.\nFrame reflection on action: Reflext\nThe design of a second system began with a similar\ntheoretical motivation: to identify and draw attention\nto patterns of language relevant to political framing\nand Modigliani, 1989) as a means of supporting frame\nreflection (Scho\n\u00a8 n and Rein, 1994). However, this design\nprocess followed a less prescriptive, more speculative\nif the system avoided technically codifying commit-\nments to particular linguistic or rhetorical devices?\nWhat if the implementation embodied fewer assump-\ntions about what text means, either denotatively or\nconnotatively? What if, instead, the algorithm was\ndesigned to present prevalent language patterns but to\nleave their exact meaning open to interpretation (Pinch\nlevel, we as system designers needed to choose which\npatterns of language the system would identify.\nHowever, we stopped short of ascribing any particular\nmeaning or interpretation to those patterns, leaving\nthat task in the hands of those using the system.\nThis approach resulted in an interactive text visual-\nization called Reflext (Baumer et al., 2014b). This sec-\ntion first presents a technical overview of how Reflext\nworks, then describes a subset of users' experiences that\narose during our roughly six-month field trial of the\ntool. This method complements the metric-oriented\nevaluation of FrameCheck. It then draws out several\nresonances and tensions between these experiences and\nour speculative design strategy. Finally, while this work\ndid not directly involve users in the design process,\nmany of the experiences that emerged provide useful\nguidance for future work on participatory approaches\nto HCAD.\nReflext implementation. This system's design hinged on a\nfundamental tension: how could we decide which pat-\nterns of language to identify without making overly\nprescriptive commitments about the constitution of\nframing or those patterns' meanings? Ultimately, we\nchose a technique called selectional preference learning\ntendency for co-occurrence between sets of linguistic\npredicates and arguments. For instance, the verb ``to\ndrink'' tends to prefer, i.e. to select for, animals as its\nsubject and potable liquids as its object. Reflext applied\nthis technique to political news articles and political\nblog posts. What kinds of terms would likely appear\nwith, i.e. be selected for by, such words as health care,\nCongress, privacy, war, etc.?\nThe visualization for Reflext allowed users to select a\ngiven term from a word cloud like interface (not\ndepicted here) and see how different sources discussed\nthat term. For instance, Figure 1 shows how the term\n``contraception'' was discussed (i.e., the term's selec-\ntional preferences) in content about health care (in\npurple across the top) and in content about abortion\n(in blue around the bottom). The image shows how the\nselectional preference algorithm was mapped onto a\nvisual representation, where strength of preference cor-\nresponds to size. This example demonstrates Reflext's\nsupport for interpretive flexibility (Pinch and Bijker,\nexposes linguistic patterns, e.g. in the context of\nhealth care, one is more likely to ``provide,'' ``pay,''\nor ``cover'' contraception, while in the context of abor-\ntion, one is more likely to ``ban,'' ``withdraw,'' or\n``deny'' contraception. However, in contrast to most\nnatural language processing tools, the user must inter-\npret what these patterns mean.\nWe deployed Reflext in a qualitative field between\nMay and November 2012, which was the main cam-\npaign season for the US 2012 national elections (presi-\ndent, congress, etc.). For full details about both\nReflext's functionality and the field study, please see\nOur initial analysis of interview data from this field\nstudy focused on how Reflext supported (or did not\nsupport) practices consistent with frame reflection\n(Scho\n\u00a8 n and Rein, 1994). However, during the inter-\nviews, we also noticed moments when participants\ndescribed specific expectations about what the system\nshould be showing them. These expectations and dis-\nconnects highlight the relationship between the actual\nfunctioning of algorithmically based systems and beliefs\nabout both what computers can do and what they\nshould do. Here, I refer to this construct as computa-\ntional imaginaries, which I suggest plays a key role in\nspeculative design.\nComputational imaginaries. Prior work has described the\nidea of social imaginaries (Anderson, 1983; Castoriadis,\ncannot know how every other individual in a society\nbehaves. Thus, people form social imaginaries, descrip-\ntions of how they believe others within a culture and/or\nour society behave. Here, I apply this same sensibility\nto beliefs about technology. Most people do not and\ncannot know everything about how a given technology\nfunctions. At some point, people use what they do\nknow to extrapolate imaginaries, both about how tech-\nnology does function and about how it might be able to\nfunction (Bucher, 2017). Such imaginaries--beliefs\nabout what is easy, difficult, or even possible--may\nnot align with current thinking, either technical or\nphilosophical.\nIn one such disconnect, many participants described\ntrying to use Reflext to identify bias. For instance, one\nrespondent wanted to know ``what percentage of the\ntalk is from one side and what percentage of the talk\nis from the other.'' Another wanted ``to see infographs\nat like a higher level. Like overall, just bias, not about\nspecific terms or specific stories, but just [. . .] There's\nstill more straight news that is biased.'' A third\nrespondent described how he opened the analysis of\nFox News, known for its fairly conservative viewpoint,\nand of the New York Times, which usually takes a rela-\ntively liberal stance. He then selected the term\n``Obama'' to see how each source discussed the then-\npresident running for reelection. ``I figured that there\nwould be some more unpleasanter [sic] words from the\nright leaning paper [Fox News] about Obama and more\nFigure 1. A screen shot from Reflext. This image shows selectional preferences for the term ``contraception'' in content about\nhealth care (in purple across top) and in content about abortion (in blue across bottom). The visualization shows groups of words that\nwere statistically likely to appear in similar grammatical relationships, e.g. verbs appearing with ``contraception'' as their direct object.\n6 Big Data & Society\nnice words from the NY Times.'' These and other\nresponses evince a normative desire for unbiased polit-\nical coverage. Failing that ideal, algorithmically based\nsystems should, according to these respondents, at least\ncall attention to the presence of bias.\nSuch expectations expose some of the tensions at\nwork in computational imaginaries. First, the detection\nof bias is an active and challenging area of computa-\ntional linguistic research, exacerbated by the fact that\nhumans often do not agree with one another as to what\nconstitutes bias (Recasens et al., 2013). Thus, partici-\npants' desire for a system that identified ``overall just\nbias'' does not align with what is currently algorithmic-\nally possible. Second, according to most theories of\nframing, a neutral, unbiased description does not exist\n(Entman, 1993). ``Facts have no intrinsic meaning''\ntion literature rules out the possibility of the kind of\nneutral, unbiased, ``straight news'' participants sought.\nThus, a disconnect occurs not only between partici-\npants' expectations and Reflext's functionality, but\nalso between participants' expectations and what is\neven possible, both theoretically and technically.\nThis concept applies quite broadly. Due to their\ndetails being obscured as trade secrets, computational\nimaginaries are necessary to interpret algorithmically\nbased systems ranging from the Facebook news feed\nGoogle Play's app recommender system (Ananny,\nIn many ways, the computational imaginaries I\ndescribe here resemble Bucher's (2017) notion of the\nalgorithmic imaginary. As Bucher (2017: 2) puts it:\n``the algorithmic imaginary is [. . .] the way in which\npeople imagine, perceive and experience algorithms\nand what these imaginations make possible.'' Thus,\n``the algorithmic imaginary does not merely describe\nthe mental models that people construct about algo-\nrithms but also the productive and affective power\nWhile they share much in common, two important\naspects distinguish computational imaginaries, as\ndescribed here, from Bucher's (2017) algorithmic\nimaginary. First, computational imaginaries include a\nnormative dimension of what computers should do.\nParticipants' exhortations of how Reflext in particular\nshould function stem largely from its political context,\nwhich implies specific democratic norms. In contrast,\nBucher's (2017) work on interpretations of the\nFacebook newsfeed algorithm does not identify a simi-\nlar normative component. Further work is required to\nascertain whether this normative dimension of compu-\ntational imaginaries arises more from the context and\npurpose of the system in question or from the notion of\nan algorithm per se. A second distinguishing aspect\ncomes from the current paper's focus on design and,\nin particular, the use of a speculative approach.\nThe speculative strategy. Imagination plays a prominent\nrole in critical and speculative design approaches\nis concerned not necessarily with producing objects that\nare useful so much as provocative. For instance,\nBiojewellery (Thompson et al., 2006) crafts engagement\nrings out of bone that is grown from donor couples'\nwisdom teeth. The ``Object for Lonely Men'' series\n(Toran, 2001) provides other examples, such as a\nrobot that throws plates as if having an argument, or\na small fan in one's bed that mimics the sensation of\nsleeping next to a heavy breather. To reiterate, rather\nthan produce functional products or prototypes, these\ndesign provoke dialog about the normative role of such\ntechnologies.\nSince they involve extrapolating from existing cir-\ncumstances to imagine possible futures, speculative\napproaches are less constrained by the realm of what\nis (currently) technically feasible. This freedom can\nfacilitate thinking through the ramifications of different\ndesign variants without them necessarily existing\nbecome more pronounced in the case of algorithm\ndesign, where the boundaries of what is possible can\nseem to change quite rapidly (although see Dreyfus,\n1992). Instead of technical feasibility, what comes to\nthe fore are the assumptions and values embedded in\ntechnology. Dunne and Raby (2001) describe critical\ndesign as a sort of value fiction. In the process of critical\ndesign, a designer identifies a value held by the domin-\nant societal mainstream; subverts, negates, or otherwise\nalters that value; and then uses the altered value as the\nbasis for a design.\nReflext applies this speculative approach to algo-\nrithm design. Traditional natural language processing\nand computational linguistics techniques often seek to\nprovide an answer about, e.g. expressions of sentiment\n(Pang and Lee, 2008), topics discussed (Blei et al.,\n2003), or the grammatical decomposition of a sentence\n(De Marneffe et al., 2006)? This orientation toward\nachieving the answer emphasizes measurable perform-\nance metrics--accuracy, F1 score, etc.--as noted\nabove. These metrics also align with values traditionally\nemphasized by HCI design, such as usability, speed,\nefficiency, responsiveness, a transparent interface, etc.\nIn contrast, Reflext can be described as a system that\nraises questions. Unlike sentiment analysis, topic mod-\neling, or syntactic parsing, Reflext offers no prescriptive\ninterpretation of the patterns it identifies. Instead, it\nasks users to determine what the differences might\nmean between, say, the discussion of contraception in\nthe context of health care versus in the context of abor-\ntion. This approach aligns with other speculative or\nludic designs, such as the Drift Table (Gaver et al.,\n2004). This coffee table features a small, round display\nin the center showing aerial photos. It can only be\n``controlled'' by placing heavy objects on the table to\nalter the direction that the photos slowly drift through\nthe display. The Drift Table exchanges the values of\nspeed, efficiency, and clear control for slowness, play-\nfulness, and curiosity. Similarly, Reflext exchanges the\nvalues of performance and efficiency for provocation\nand exploration.\nWhile this speculative approach successfully gener-\nated an artifact that deviated from the dominant\nnorms, it also led to disconnects between participants'\nexpectations and Reflext's functionality. Specifically,\nthe interpretive flexibility (Pinch and Bijker, 1987;\nSengers and Gaver, 2006) at the heart of our design\nbecame a point of tension and, at times, confusion.\nParticipants in our study regularly requested a more\nprescriptive analysis and representation. As one partici-\npant put it: ``Like if I pick out taxes, [I would want to\nsee] how are taxes talked about in a left-leaning paper\ncompared to a right-leaning paper [. . .] and I'll get it in\nlike an objective kind of quantitative way.'' Another\nparticipant ``would want to have frequency numbers,\ncharts and tables that shows me how these two terms\nwork together [. . .] Just anything to kind of spell out,\nyeah we can do this but what does it mean [emphasis\nadded]?'' These requests about prescribed meaning\necho participants' perceptions and experiences around\nbias, where they desired a system that would explicitly\nstate the degree and kind of bias present in an article.\nAgain, these computational imaginaries belie expect-\nations both about what a computational system could\ndo and about what it should do. With respect to tech-\nnical feasibility, natural language processing has devel-\noped numerous techniques well adapted for analyzing\ntext to identify important patterns. Ultimately, though,\na human must interpret what those patterns mean\n(Rhody, 2012). Thus, participants' requests for an algo-\nrithm to ``spell out [. . .] what does it mean'' diverge\nfrom current state of the art. Furthermore, the above\nstatements carry a normative implication that the\nsystem should perform these interpretive functions.\nThese participants imply that, in order to be valuable,\nan algorithmically based system not only must identify\npatterns of interest, it must also ascribe human-\ninterpretable meaning to those patterns. In other\nwords, while our design made a commitment to inter-\npretive flexibility, it did not necessarily convince our\nparticipants to adopt this same value.\nThis disconnect raises an important final point about\na speculative strategy for algorithm design. Speculative\napproaches do not always generate specific designs to\nbe widely adopted or even necessarily to be imple-\nmented at all. Although such designs often could be\nand sometimes are implemented, speculative\napproaches serve primarily as a conceptual device to\nhelp explicate and interrogate the values embedded in\nexisting technologies. That is, speculative designs are\njust as much a discursive intervention as a technical\nWhile Reflext offers one example, similar approaches\ncould be applied to more well-known algorithms. What\nif the Facebook news feed showed items it believed you\nwere least likely to click on or to like? What if\nAmazon's recommendations were designed not to\nencourage additional purchases but to make a user\nfeel a particularly way about their socioeconomic\nstatus (e.g., affluent, low-brow, middle-class) compared\nto other users? What if Twitter's trending topics\nselected those terms or hashtags most likely to be con-\nfusing or misinterpreted out of context? These variants\nmay not meet the economic goals of Facebook,\nAmazon, or Twitter. Making users feel variously low-\nbrow or affluent would not likely increase their spend-\ning. Highlighting confusing hashtags might not increase\nTwitter's monetizable user traffic.\nHowever, imagining such alternatives offers at least\ntwo important outcomes. First, it shows how the cur-\nrent state of computational systems both is contingent\nand is nonexclusive with other possibilities. Second, it\nhighlights how abstract concepts and values, such as\nlikability, socioeconomic status, or interpretability,\nare in some ways already algorithmically encoded into\nthese systems.\nWhile not employed here, a third strategy may be\nuseful in addressing disconnects between algorithm\ndesigners' intent and lay persons' expectations.\nThe participatory strategy. Although quite different from\none another, both the theoretical strategy and the\nspeculative strategy share at least one commonality.\nNeither incorporates people who might be impacted\nby a system as participants in the design process.\nParticipatory design emerged largely from\nScandinavian countries as a form of empowering\nlaborers, giving them some measure of control in the\ndesign of their workplace settings. Participatory tech-\nniques gained significant traction in information tech-\nnology design, especially for workplace technologies\nNot all instances, though, carry the original move-\nment's political orientation and weight (Beck, 2002).\nOne of the unique challenges in participatory design\ndeals with differential expertise. In his work developing\nsoftware for typographers, Ehn (1988) describes how he\nand his design team needed to gain some amount of\nproficiency in typographic practices. Although Ehn\n8 Big Data & Society\ndoes not discuss it at length, the same imbalance occurs\nin the other direction; typographers needed to under-\nstand something of software engineering and interface\ndesign. However, Ehn also acknowledges limits of this\ntechnique. One cannot simply grant a participant-\ndesigner the equivalent of a graduate-level training in\ncomputer science. Similarly, one cannot easily bestow\nupon a computer scientist an expert understanding of\nthe subtle nuances in journalistic news reporting.\nConsider, for instance, the way that participants rea-\nsoned about size in the visualization. Following a mass\nshooting in Aurora, CO, one participant recounted\nbeing surprised not to see the term ``Aurora'' more\nprominently in the visualization. When she ``finally\nfound a really tiny Aurora somewhere on the list, [she]\nthought `well it should've probably been bigger' because\nit was such a hot story.'' Another participant similarly\nasserted that ``the word that's biggest [. . .] is the word\nthat's the most important word of the discussion.''\nWe see here a conflation between frequency and\nimportance, a conflation on which most natural lan-\nguage processing algorithms hinge. However, numer-\nous words that occur very frequently--``the,'' ``is,''\n``to,'' ``this,'' ``and''--likely have little importance.\nComputational approaches often employ stopword\nlists (Leskovec et al., 2014), which screen out such\nhigh frequency terms, as a means of focusing on what\nare supposedly more content-oriented terms.\nEssentially, this point highlights differences between\nhow computers count and how people count. If a\nshooting occurs in Aurora, the location will likely be\nnamed once in a given news article, perhaps twice.\nOther terms, even nonstopwords, will likely occur far\nmore frequently, regardless of whether the article is\nabout such words.\nApplying a participatory approach from the outset\nmay not necessarily resolve such tensions. However, it\nmight identify them sooner and potentially incorporate\nthem into the design. For instance, the data-driven,\nprobabilistic selectional preference technique could\nhave been replaced or supplemented with sentiment\nanalysis, named entity recognition, or other methods\nmore in line with participants' intuitions about the\nimportant aspects of a given article. Moreover, a par-\nticipatory approach could facilitate a dialectic exchange\namong those who might interact with the system and\nthose who are designing and implementing it. Doing so\nwould help better understand the formation of, and\npotentially how to intervene in the process of develop-\ning, computational imaginaries.\nImplications and practicalities\nThis paper presents the author's experiences with\napplying human-centered strategies to the design of\nalgorithmically based systems. It discusses both\nstrengths and challenges in this approach. However,\nthe ``toward'' in the paper's title should be taken ser-\niously. The steps described here provide important\ngroundwork, but they initially move along a longer\nterm trajectory. To conclude, I discuss some important\nconsiderations of what will likely need to be done if we\nare to foster a more developed HCAD practice.\nOne major concern revolves around what is actually\nmeant by the term ``theory.'' As described above, even\nwhen social scientific theories are used to inform algo-\nrithm design, the specific features on which an algorith-\nmically based system focuses may vary in how closely\nthey resemble those theoretical concepts. Moreover,\nwhat actually constitutes a theory can vary across dis-\nciplines, sometimes drastically so. For instance, the the-\nories of framing applied above bear little similarity to\nthe concerns of theoretical computer science (complex-\nity, formal logic, automata, etc.). Thus, understanding,\nnot to mention grappling with, theoretical concerns\nacross multiple disciplines requires expertise across\nmultiple disciplines.\nThis point draws attention to a second major con-\ncern. Rhetoric around interdisciplinary research often\nlauds training that results in so-called T-shaped people,\nwho have a breadth of knowledge across a wide variety\nof subjects as well as deep expertise in one area. In\npractice, though, strict focus on a single depth of\nexpertise can become limiting. For instance, the com-\nputer scientist who has a surface understanding about\nsocial scientific theories of framing will have limited\nguidance in terms of feature selection. Instead, it may\nprove more effective to cultivate what one might call\n``\u00c5-shaped'' people, ones who have a wide breadth of\nknowledge but also deep expertise in multiple disparate\ndisciplines. Clearly, such an approach becomes difficult\ngiven traditionally disciplinary publication venues,\ngrant funding programs, departments, research insti-\ntutes, etc. Thorough discussion of such concerns, how-\never, far exceeds the scope of this paper.\nFinally, gaps in understanding exist not only for\ndesigners across disciplines but also for those impacted\nby algorithms. While an algorithm's designers do not\nnecessarily need or want others to understand how it\nworks, they likely want to reach consensus on what it\nmeans. This paper provides strategies to address mis-\nalignments or disconnects between algorithmically\nbased systems' technical functioning and their inter-\npreted meaning. Theoretical approaches can help\nensure from the outset that algorithm design is\ngrounded in a thorough understanding of behavioral\nand social phenomena. Speculative approaches can\nhelp highlight the values and assumptions on which\nalgorithmically based systems are predicated.\nParticipatory strategies can help identify and\nincorporate lay interpretations earlier in the design pro-\ncess. The crucial question then becomes: to what extent\nand in what ways are these various strategies effective\nor ineffective?\nThis question returns to the paper's central motiv-\nation about evaluation. The work presented above\nvariously used metric-driven evaluation and human\nsubject studies at different stages. While traditional\nalgorithmic performance metrics and human-centered\ndesign approaches are both necessary, neither is suffi-\ncient alone. In as much as evaluation is central to\ndesign, HCAD must be paired with human-centered\nevaluation methods. Doing so does not preclude, say,\nrunning two variants of an algorithm to compare per-\nformance metrics. It does, however, require combining\nsuch metrics with more time- and labor-intensive\nhuman subject studies. Whether via ``\u00c5-shaped''\npeople or multidisciplinary teams, a confluence of\nhuman-centered expertise and technical expertise is\nnecessary to assess exactly when (i.e., during which\nphases of the design process) and how human subject\nstudies should be deployed.\nIt is tempting to ask whether such design approaches\ncould have avoided algorithms that, e.g. demonstrate\nbias along the lines of race, gender, or sexual orienta-\nwe ask how we would know. What kinds of evaluation\ntechniques must we develop to identify such situations?\nWhile some algorithmic tools purport to identify such\nbiases (Bolukbasi et al., 2016), questions of bias are\nultimately questions of interpretation, of meaning\nmaking. It is these questions of meaning making that\nmost clearly elucidate the limitations of traditional\nalgorithm design and evaluation methods. Similarly,\nalgorithmic support for the processes of meaning\nmaking is where human-centered approaches stand to\ncontribute the most.\n"
}