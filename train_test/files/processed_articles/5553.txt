{
    "abstract": "Abstract\nPolitical scientists interested in studying the political implications of citizens' cognitive abilities often turn to readily\navailable \"intelligence ratings\" in the American National Election Studies (ANES). These ratings are generally thought\nto represent respondents' cognitive abilities, albeit imperfectly. I hypothesize that these ratings do not reflect more-\nobjective tests of cognitive ability, and instead better capture considerations related to the political subject of the\ninterviews and other contextual factors. In the 2012 ANES, which contained a cognitive ability test (\"Wordsum\"), I\nfind that political engagement and demographic factors but not actual measurements of cognitive ability are associated with\ninterviewer intelligence rating scores. In bivariate analyses, verbal intelligence and interviewer ratings are moderately\ncorrelated with one another, consistent with conventional wisdom. But this correlation is a spurious one, as the same\nis also true for political engagement, education, and household income. Further, in multivariate models, Wordsum\nscores were neither statistically nor substantively predictive of interviewer ratings. The results suggest that interviewer\nratings are better understood as proxies for political engagement, not cognitive ability. I conclude by arguing that the\ngrowing importance of studying cognitive ability in political science makes it necessary to include more-objective verbal\nintelligence tests more frequently in public opinion surveys.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons\nAttribution-NonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use,\nreproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open\nAccess pages (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nIntroduction\nPolitical scientists have become increasingly interested in\nthe relationship between voters' cognitive abilities and\nmany political outcomes (e.g. Hillygus, 2005; Berinsky and\nInterviewer ratings of respondent intelligence are arguably\nthe most accessible measure of cognitive ability in political\nscience research (Luskin, 1990), and have been employed\nas proxies for intelligence in both the recent past (e.g.\nResearchers are often careful to point out that these meas-\nures are imperfect. Yet, because they are at least thought to\ncorrelate moderately highly with more formal measure-\nments of verbal intelligence (Gibson and Wenzel, 1988;\nLuskin, 1990), scholars nevertheless assume that the rat-\nings are more-or-less valid indicators of a general intelli-\ngence construct.\nWhether or not this assumption is a good one is an\nimportant question in the study of cognitive ability, as\nscholars continue to investigate its role in shaping citizens'\nattitudes and behaviors. If interviewer ratings do not accu-\nrately reflect the construct they purport to measure,\nresearchers will need to consider regularly including other\nitems (such as more-formal tests of cognitive ability) on\nsurveys such as the American National Election Studies\n(ANES) in the future.\nIn this research article, I cast doubt on the premise that\ninterviewer ratings are even generally valid indicators of\ncognitive ability. Verbal intelligence tests, one form of cog-\nnitive ability that is highly-correlated with general intelli-\ngence (see Malhotra et al., 2007: for a comprehensive\nWhat do interviewer intelligence\nratings actually measure?\nMatthew P. Motta\n Keywords\ninterviewer ratings, cognitive ability, Wordsum, political sophistication\nUniversity of Minnesota - Twin Cities, USA\nCorresponding author:\nMatthew P. Motta, University of Minnesota - Twin Cities, Political\nResearch Article\n2 Research and Politics \noverview), show moderately high bivariate correlations\nwith interviewer ratings. However, so do a wide range of\nother political and demographic variables. Multivariate\nmodels reveal that interviewer ratings are substantively and\nstatistically unrelated to verbal intelligence. Instead, and in\nline with theoretical expectations, the results suggest that\ninterviewer ratings of intelligence more strongly reflect\nindividuals' interest and knowledge about politics, as well\nas their income and educational background.\nA growing scholarly interest in cognitive ability\nScholars have been interested in voters' cognitive abilities\nand their relevance to politics for decades, and research\nusing intelligence to predict various political outcomes\ncontinues to be well-studied. For example, cognitive\nability is thought to play a role in shaping whether or not\nvoters are knowledgable about politics and make use of\nabstract ideological concepts to guide their issue prefer-\n1996). It has also been hypothesized to increase citizens'\nwillingness to participate in politics (Brady et al., 1995;\nance toward social out-groups (Bobo and Licari, 1989;\ntude extremity (Makowsky and Miller, 2014). Scholars\nhave also taken a recent interest in investigating whether or\nnot highly intelligent individuals hold different symbolic\nandoperationalideologicalorientations(EllisandStimson's\n(2012) terminology) than those who are less intelligent\nif they are less likely to engage in partisan-motivated rea-\nMany of the above studies make use of ten-item\nWordsum measurements (Thorndike, 1942; Thorndike and\nGallup, 1944) available in surveys such as the General\nSocial Survey (GSS) and 2012 ANES. Others rely on\nnumeracy (quantitative reasoning) measurements or verbal\nand/or mathematical standardized test scores. Still others\nmake use of interviewer intelligence ratings, whereby sur-\nvey administrators score each respondent they interview\nface-to-face in terms of how intelligent they seem.\nInterviewer ratings have been used, often with some reser-\nvations, to show that cognitive ability is associated with\nhigher levels of political sophistication (Luskin, 1990), the\ntendency to acquire political information over the course\nof campaigns (Holbrook, 1999), and (to some extent) par-\nticipation in electoral politics (Hauser, 2000). The meas-\nurement has also been shown to be negatively related to\nquantitative \"heaping\" (i.e. rounding) response strategies\non questions asking respondents to offer answers that span\nlarge numeric ranges (Holbrook et al., 2014).\nSimply put, scholars employ measurements of cogni-\ntive ability in making sense of a wide range of political\nphenomena. Interviewer ratings of respondent intelligence\nare one strategy by which scholars can do this, and, unlike\nother measures of cognitive ability, have the benefit of\nbeing readily available in many cross-sections of the\nInterviewer intelligence ratings\nIn the ANES, interviewer intelligence ratings are five-point\nscales based on survey administrators' responses to the fol-\nlowing question:\n\"How intelligent would you say the respondent is? Much\nabove average, a little above average, average, a little blow\naverage, or much below average?\"\nIn practice, interviewer ratings are typically recorded at\nthe conclusion of the survey by two different interviewers\n(one administering a pre-election interview, and another\ndoing the same after the election). Scholars have long\nknown that this method boasts high inter-rater and intra-\nrater reliability levels (Campbell et al., 1976). The measure\nis also thought to have some amount of construct validity,\ncorrelating moderately highly with Wordsum measures of\nverbal intelligence (Gibson and Wenzel, 1988). It is also\nthought to have some amount of predictive validity, as the\nmeasure tends to positively correlate with political knowl-\nHowever, scholars have also held reservations about the\nvalidity of the rating measurement for some time. As\nLuskin (1990) points out, interviewers are \"hardly expert\njudges\" and their ratings may reflect \"seepage from other\nvariables:\" such as respondents' levels of political sophisti-\ncation, income, or education. Demonstrative of this point,\nLeal and Hess (1999) found that interviewers across three\nnationally representative studies were more likely to rate\nhigh-income and highly educated individuals are more\nintelligent, informed, and competent with respect to mak-\ning sense of the survey's instructions.\nRecognizing these imperfections, Luskin nevertheless\nlaments that interviewer ratings are \"the only real option\"\nfor measuring intelligence in \"a national probability sample\nextensively questioned about politics\". While internally\nand conceptually valid measurements such as Wordsum are\nadministered regularly on the GSS, the words tested are\nprotected by the National Opinion Research Center\n(Malhotra et al., 2007). Wordsum, for example, has been\nincluded only once in the ANES (in 2012), lending cre-\ndence to Luskin's concerns.\nThus, while scholars are certainly aware of methodo-\nlogical imperfections in the interviewer rating measures,\nthey sometimes (perhaps because of the scarcity of viable\nalternatives) make use of them as a generally acceptable\nproxy for respondents' cognitive abilities. Whether or not\nthis is the case is an open empirical question, with impor-\ntant implications for the study of cognitive ability.\nTheory and expectations: how\nsurvey-context explains interviewer\nintelligence ratings\nI believe that there are at least two aspects of the ANES\nface-to-face survey-taking environment that cast doubt\non the premise that interviewer ratings of intelligence\nare generally acceptable proxies for respondents' cogni-\ntive abilities. The first pertains to the subjective context\nof the survey. The ANES, by and large, is a survey about\npolitics. Thus, the considerations administrators have\navailable for rendering a judgment about each respond-\nent's cognitive ability should be predominately political.\nAs others have speculated (e.g. Luskin, 1990; Hauser,\n2000), highly politically knowledgable and interested\nindividuals should be better than less-knowledgable and\nless-interested individuals in transforming their abstract\npolitical attitudes and beliefs into coherent responses on\nthe survey.\nThus, because the politically articulate are not\nobserved in a non-political context, interviewers might\nreasonably infer that these individuals are generally\narticulate, and therefore more intelligent. From this the-\noretical expectation, I propose the \"Subjective Survey\nContext Hypothesis\":\nH1: Respondents' levels of interest in, and knowledge\nabout, politics will be more strongly associated with\ninterviewer ratings of intelligence than actual measures\nof verbal cognitive ability.\nA second aspect of the ANES survey-taking environ-\nment that might influence interviewer ratings is the phys-\nical context of the survey. Face-to-face ANES samples\nare typically conducted in individuals' homes,1 implying\nthat the spaces and other physical contexts (e.g. respond-\nents' appearance) in which interviewers rate cognitive\nabilities vary at the level of individual respondents.\nWealthier or more educated individuals might live in\nhomes that provide interviewers with potentially mis-\nleading cues, suggestive of higher cognitive ability.\nInterviewers might associate voluminous bookshelves,\nnice clothing, and other status symbols (e.g. a luxury car)\nwith literacy and financial success, and assume that their\nattainment is conditional upon being highly intelligent\n(potentially inaccurately, of course).2 These findings\nwould be consistent with the interviewer biases docu-\nThis leads me to propose the \"Physical Survey Context\nHypothesis\":\nH2: Respondents' levels of education and income are\nmore strongly associated with interviewer ratings of\nintelligence than actual measures of verbal cognitive\nability.\nData and measures\nData\nTo test these hypotheses, I make use of data from the 2012\nANES, which included tests of individuals' verbal cogni-\ntive skills (Wordsum), interviewer intelligence ratings,\npolitical knowledge and interest items, and a wide range of\ndemographic controls. In total, the study interviewed over\nrecruited via two different sampling procedures.3 Because\ninterviewer intelligence ratings can only be ascertained\nface-to-face, I make use of only that sample. Face-to-face\ninterviews for the pre-election wave were conducted\nbetween 8 September and 5 November, and post-election\ninterviews were conducted after 7 November; with a re-\nMeasures\nInterviewer intelligence ratings.As described previously,\ninterviewers were asked to rate respondents on five-point\nscales with respect to their perceived levels of intelligence:\nranging from \"much below average\" to \"much above aver-\nage\". These ratings were recorded by interviewers in both\nthe pre- and post-interview surveys, both of which I scaled\nto range from 0 to 1 (such that an increase reflects higher\nlevels of intelligence). More often than not (59% of the\ntime), raters agreed on these assessments ( = 0.39). More-\nover, their independent ratings formed an internally reliable\nscale ( = 0.81).5 In all analyses, I make use of both pre/\npost-interviewer ratings and the combined measure.\nVerbal intelligence.I measure respondents' cognitive abili-\nties using the ten-item vocabulary test known as Wordsum\nfirst presents respondents with a word, and then asks them\nto select one of five other words that is closest in meaning\nto it. Scores are coded to range from 0 to 1, such that higher\nscores reflect higher intelligence.\nWhile the Wordsum measure was originally conceptual-\nized as a test of individuals' verbal cognitive abilities, and\nis therefore not a direct measurement of general intelli-\ngence per se (Caplan and Miller, 2010), researchers often\nrely on it as a proxy of intelligence more broadly.\nEmpirically, this decision appears well-founded. Scores on\nthis test have been shown to be highly correlated with more\ngeneral measures of intelligence on the Army General\nscores, much like IQ scores (Rosenbaum, 2000), are not\ndistributed uniformly throughout the mass public, and tend\nto be higher amongst older, wealthier, and more educated\n4 Research and Politics \nOf course, the Wordsum test is not without limitations.\nFor example, an important body of literature finds that aggre-\ngate Wordsum scores have been decreasing over time despite\ngains in formal education, perhaps because the words used\non the test have fallen out of popular usage (Dorius et al.,\n2016). Moreover, although the test has existed for more than\nhalf a century, scholars still debate how to properly use\nWordsum, and the conditions under which it is a valid\nsubstitute for lengthier and more-direct intelligence tests\nPolitical knowledge and interest. I incorporate one measure of\nrespondents' knowledge and two measures of respondents'\ninterest in politics in my analysis. Political knowledge was\nmeasured using a standard five-item civic knowledge test\n1996).7 One political interest item was measured using a\nfive-point scale reflecting the extent to which respondents\n\"pay attention to politics and elections\". A second measure\nwas a five-item scale reflecting the extent to which individu-\nals reported following the 2012 election making use of sev-\neral different types of media (=0.64). For brevity (and\nbecause all items are publicly available), please consult the\nappendix for specific question wording. All variables are\nrecoded to range from 0 to 1, such that a score of one reflects\nthe highest possible levels of interest and knowledge.\nDemographic controls. All multivariate models control for a\nwide range of demographic, political, and psychological\ncontrols; all coded to range from 0 to 1.8\nResults\nBivariate analysis\nAs others have noted in the past (e.g. Luskin, 1990), I find\nthat interviewer ratings of intelligence in the 2012 ANES\nare moderately correlated with actual verbal intelligence\nratings. As the right-hand pane in Figure 1 demonstrates,\nhigher scores on Wordsum are generally associated with\nhigher scores on the (combined) intelligence ratings\n(weighted r = 0.38 ). Perhaps this is unsurprising, as the\ndistribution of the two variables were similar across data-\nsets (see left-hand pane of Figure 1).9 This bivariate\nFigure 1. The bivariate relationship between interviewer ratings and verbal intelligence in the 2012 ANES. Left-hand panels display\nthe weighted frequency distributions of the interviewer rating and Wordsum measures of intelligence, respectively. The right-hand\npanel is a scatterplot of the two, with the line best fitting the data displayed in black. Values of zero represent the lowest possible\nscore on each variable, while values of one represent the highest possible score. Correlation estimate (lower right-hand corner) is\nbased on weighted survey data. For display purposes, five random shocks are added to each point in the scatterplot.\nanalysis offers some support for the conventional wisdom\nthat interviewer ratings are generally acceptable (though by\nno means perfect) proxies for intelligence.\nHowever, a more-thorough look into the data suggest\nthat this correlation might be a spurious one. As Figure 2\ndemonstrates, several other variables hypothesized to play\na role in shaping interviewer ratings of intelligence are also\nmoderately correlated with the interviewer ratings. In fact,\nrespondents' levels of knowledge about politics, interest\nin campaigns, attentiveness to campaigns in the media,\nincome, and education, are all more highly correlated with\nthe ratings than are verbal intelligence. This finding offers\npreliminary support for the Subjective Survey Context\nHypothesis, suggesting that interviewer ratings are influ-\nenced by a wide range of political and demographic factors.\nIn order to determine which factors are most strongly asso-\nciated with the ratings, it is necessary to pit these variables\nagainst one another in a multivariate framework.\nMultivariate analysis\nIn Table 1, I provide a stricter test of my expectations by\nconstructing six ordinary least-squares (OLS) models (see\nLeal and Hess, 1999) estimating the impact of a wide range\nof factors on interviewer intelligence ratings. The models\nvary whether or not the outcome variable is a single-wave\n(pre, post; columns 1\u00ad4) or combined rating (i.e. pooled\nacross pre and post election surveys; columns 5 and 6).10\nThey also alternate whether education is operationalized as\na dichotomous indicator of whether or not individuals com-\npleted college, or the broader degree-attainment scale.\nBecause all variables are coded to range from zero to\none, the coefficients can be interpreted as percent change\nin intelligence ratings, moving from the minimum to maxi-\nmum value on that covariate (i.e. a \"first difference\"). The\nresults show that political knowledge, political media use,\ninterest in politics, education (both specifications), and\nincome are consistently strong and statistically significant\npredictors of intelligence ratings. Often, first differences in\nthese covariates approach or exceed effect sizes of 10%.\nThe media use index, for example, plays a particularly\nstrong substantive role in predicting interviewer ratings, as\nit is associated with increases in intelligence ratings of at\nStrikingly, though, the actual measure of verbal intel-\nligence was never statistically associated with increased\nratings on the intelligence scale. Moreover, the sub-\nstantive effect sizes associated with the intelligence\nFigure 2. The bivariate relationship between interviewer ratings and other factors in the 2012 ANES. Each panel is a scatterplot of\nthe relationship between interviewer intelligence ratings and some other political or demographic variable, with the line best fitting\nthe data displayed in black. Correlation estimates (lower right-hand corner of each figure) are based on weighted survey data. For\ndisplay purposes, five random shocks are added to each point in the scatterplot.\n6 Research and Politics \nparameters were uniformly small once accounting for\nthese other variables.12\nFigure 3 offers an additional level of detail to these\nsubstantive effects, showing the percentage changes in\nTable 1. Predicting interviewer ratings of intelligence. OLS coefficients with standard errors presented in parentheses (weighted\nsurvey data). Outcomes variables are pre-election (columns 1 and 2), post-election (columns 3 and 4), and combined (columns 5\nand 6) interviewer ratings. Because outcomes in columns 1\u00ad4 are five-point scales, I re-estimate each model using ordered probit\nregression in Table A1 in the online appendix. The results are substantively and statistically robust to this specification.\nPre (College) Pre (Full) Post (College) Post (Full) Combined (College) Combined (Full)\nlinear predictions on the intelligence rating outcome\nacross the lowest and highest observed values of each of\nthe covariates described above (with 95% confidence\nintervals extending outward from each one). All covari-\nates depicted, except for actual levels of verbal intelli-\ngence, are associated with large substantive gains on\nintelligence ratings.13\nThe results in Table 1 and Figure 3 are strongly consist-\nent with both sets of theoretical expectations. Individuals\nwho are highly interested and knowledgable about politics\nare rated as more intelligent in the ANES than are individu-\nals who less interested and knowledgable (the Subjective\nSurvey Context Hypothesis). Moreover, wealthier and\nmore educated individuals are also rated as being more\nintelligent than those less well-off financially and with\nlower levels of education (the Physical Survey Context\nHypothesis).\nClearly, scholars making use of these ratings in the past\nwere correct to point out that \"seepage\" (Luskin, 1990)\nfrom other variables colors interviewers' ratings of\nrespondent intelligence. What scholars perhaps did not\nanticipate, though, is that this seepage completely explains\naway the modest relationship between actual measures of\nverbal intelligence and interviewer ratings. Interviewer\nintelligence ratings are less a summary of individuals'cog-\nnitive abilities, and more accurately a depiction of their\ninterest in politics and financial status.\nRobustness check: political\ninformation ratings\nIn addition to rating individuals' intelligence, interviewers\nwere also asked to rate respondents on a number of other\ncharacteristics, including their levels of information about\nIf intelligence ratings are truly a better indicator of polit-\nical knowledge and interest than cognitive ability, we\nshould expect these two ratings to be highly correlated.\nRelying on two pooled (pre/post) rating scales, I find that\nthis is indeed the case (weighted r = 0.80). Moreover, in\nmultivariate analyses presented in the Appendix (Table\nA2), I find that political knowledge and interest, but not\ncognitive ability, were strongly associated with increased\nratings on the information scale (more so than in Table 1).\nWhen interviewer ratings of intelligence are added to this\nmodel (e.g., the penultimate column of Table A2, with a\npooled pre/post information rating outcome, a dichotomous\ncollege indicator, and all other controls present in Table 1),\nFigure 3. The effect of political and demographic factors on predicted interviewer ratings. Linear predictions derived from the\nfifth OLS model displayed in Table 1 (with a combined intelligence rating outcome, and dichotomous measure of college degree\nattainment). The 95% confidence intervals extend outward from each estimate. All differences are statistically significant, with the\nexception of intelligence scores on the Wordsum measure.\n8 Research and Politics \nit is a statistically significant ( p < 0.05 ) and substantively\nlarge ( ^\n=0.81) predictor of increased information ratings.\nThis means that a first difference in intelligence ratings\nleads to more than an 80% increase in information ratings.\nThe strong link between interviewer ratings of intelligence\nand political information provide additional evidence that\ninterviewer ratings of intelligence can be thought about as a\nproxy of knowledge and interest in politics.\nConclusion\nAs hypothesized, survey context plays a key role in explain-\ning the intelligence ratings that interviewers assign to\nANES respondents. This research has provided evidence in\nsupport of the following four claims.\n1. Tests of verbal cognitive ability are somewhat asso-\nciated with interviewer ratings of intelligence in\nbivariate analysis.\n2. Several other factors (e.g. political knowledge and\ninterest, education, income) are at least as highly cor-\nrelated with interviewer ratings in bivariate analyses.\n3. In multivariate analyses, verbal cognitive ability\nplays no statistical or substantive role in explaining\ninterviewer ratings of intelligence. Interviewer rat-\nings of intelligence are more strongly shaped by\nrespondents' levels of political engagement, educa-\ntion, and financial status.\n4. Interviewer information ratings are very highly\ncorrelated with intelligence ratings, suggesting that\nthe two are likely measuring substantively similar\nrespondent traits.\nThe key results also appear to be robust to a number of\nalternate estimation strategies and model specifications, all\nof which can be found in the online supplementary materi-\nals (Tables A1\u00adA4 and Figures A1 and A2).\nDiscussion\nPolitical scientists have been interested in the link between\ncognitive ability and political attitudes for decades, and\ncontinue to be interested in the relevance of intelligence in\npolitics. Cognitive ability is an important predictor of voter\nbehavior and competency (e.g. Hillygus, 2005; Hodson and\nbe studied in more detail with respect to motivated reason-\ning and other aspects of political judgment formation. This\nresearch cautions that scholars hoping to make use of\ninterviewer ratings of intelligence in the ANES for these\npurposes (or any others) ought to reconsider whether or\nnot these measures are valid measures of cognitive ability.\nThese ratings, in my view, are probably better indicators of\npolitical engagement and financial/educational resources.\nYet, while the study of cognitive ability is certainly of\ninterest to political scientists, intelligence ratings remain the\nmost accessible way to measure it in public opinion surveys\nfocusing predominantly on political matters. More-objective\nmeasures such as Wordsum have been administered only\nonce in the ANES. Because they make use of protected con-\ntent, they are not easily administered amongst researchers\nhoping to study cognitive ability in their own studies and\ndatasets. In order to facilitate this research, large publicly\navailable surveys about politics (such as theANES) ought to\nconsider offering (or continuing to offer) cognitive ability\ntests such as Wordsum (perhaps with some modifications, as\nconstructed in such a way that specific words need not be\nkept secret) more frequently.\n"
}