{
    "abstract": "Abstract\nBoundary extension is a common false memory error, in which people confidently remember\nseeing a wider angle view of the scene than was viewed. Previous research found that boundary\nextension is scene-specific and did not examine this phenomenon in nonscenes. The present\nresearch explored boundary extension in cropped face images. Participants completed either a\nshort-term or a long-term condition of the task. During the encoding, they observed photographs\nof faces, cropped either in a forehead or in a chin area, and subsequently performed face\nrecognition through a forced-choice selection. The recognition options represented different\ndegrees of boundary extension and boundary restriction errors. Eye-tracking and performance\ndata were collected. The results demonstrated boundary extension in both memory conditions.\nFurthermore, previous literature reported the asymmetry in amounts of expansion at different\nsides of an image. The present work provides the evidence of asymmetry in boundary extension.\nIn the short-term condition, boundary extension errors were more pronounced for forehead,\nthan for chin face areas. Finally, this research examined the relationships between the measures of\nboundary extension, imagery, and emotion. The results suggest that individual differences in\nemotional ability and object, but not spatial, imagery could be associated with boundary\nextension in face processing.\n",
    "reduced_content": "Article\nBoundary Extension in Face\nProcessing\nOlesya Blazhenkova\nFaculty of Arts and Social Sciences, Sabanci University, Istanbul, Turkey\n Keywords\nboundary extension, false memory errors, face processing, object and spatial imagery, individual\ndifferences, emotion\nIntroduction\nBoundary extension (BE) is a common false memory error, in which people confidently\nremember seeing a wider angle view of the scene than was actually viewed. They recall the\nsurrounding regions of the scene, which were not visible during the encoding. BE is a\nconstrained phenomenon, and scene extrapolation occurs just beyond the edges of a view\n(Gottesman & Intraub, 1999; Intraub & Richardson, 1989). This phenomenon is thought to\ncomprise a two-stage process. First, a spontaneous extrapolation beyond the visible\nCorresponding author:\nOlesya Blazhenkova, Faculty of Arts and Social Sciences, Sabanci University, Orta Mahalle, U\n\u00a8 niversite Caddesi No: 27,\nEmail: olesya@sabanciuniv.edu\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License\n(http://www.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sage-\npub.com/en-us/nam/open-access-at-sage).\ni-Perception\njournals.sagepub.com/home/ipe\nboundaries occurs during the initial encoding of an encountered scene. This happens due to a\nconstructive nature of perception. It was suggested that the limited view of a scene activates a\nperceptual schema or a constructed visual-spatial representation of the expected layout\noutside the view (Chadwick, Mullally, & Maguire, 2013; Intraub, Bender, & Mangels,\n1992; Intraub, Gottesman, & Bills, 1998). Subsequently, during the retrieval, BE manifests\nas a false memory error. In contrast to the traditional model of scene perception, which\npostulates that visual input is a single source of information in scene representation, the\nmultisource model of scene perception proposes that scene perception involves processing\nin multiple modalities, one of which is vision (Intraub, 2010, 2012). According to the\nmultisource model, scene perception is based on spatial egocentric framework. Multiple\nsources of input (e.g., visual sensory, amodal, conceptual, and contextual) `fill-in' this\nframework with expectations about the surrounding visual field, which is only partially\nrevealed in a picture. Thus, mental scene extrapolation does not happen after the stimulus\nis gone but already occurs during the scene presentation. In this view, BE is a source\nmonitoring error (Johnson, Hashtroudi, & Lindsay, 1993) that arises due to the confusion\nbetween original sensory input and internally generated detail (Seamon, Schlegel, Hiester,\nLandau, & Blumenthal, 2002) and the attempts to determine which portion of the scene\nmatches to the visual source. BE is considered to be a highly adaptive process that\ncontributes to the representation of continuity and coherence of the world that exists\nbeyond the limited visual input (Chadwick et al., 2013; Mullally, Intraub, & Maguire, 2012).\nThis phenomenon is remarkably robust. A variety of different methods, such as drawing\nfrom memory, recognition/rating of images that contain more or less of the original scene, or\nborder-adjustment, consistently demonstrated BE effect (Hubbard, Hutchison, & Courtney,\nfound in people of various ages, including 3- to 7-month infants and adults up to 84 years of\nage (Quinn & Intraub, 2007; Seamon et al., 2002). BE was observed for a wide range of target\nRichardson, 1989; Safer, Christianson, Autry, & O\nrectangular images but occurs for different shapes such as circular or irregular (Daniels &\nIntraub, 2006). BE is so robust that it is not eliminated even by the awareness of this\nphenomenon, though, test-informed instruction may lead to a reduction in BE (Gagnier,\nHowever, despite its robustness, BE does not occur for all kinds of image contents. It was\nfound primarily for images containing a background surface, but not for images with\ndepictions on blank backgrounds (Intraub et al., 1998). This was explained by the role of\nmental schema anticipating the continuous layout beyond the frame and not activating for\nimages that do not depict partial view of a scene (Intraub et al., 1998). The hypothesis that\nBE occurs due to object completion was ruled out because this phenomenon was observed\neven for scenes that did not contain incomplete cropped objects (Intraub et al., 1992; Intraub\n& Bodamer, 1993). Besides, BE did not differ for scenes containing whole objects and\ncropped objects (Gagnier et al., 2013). BE was suggested to occur only for limited\nboundaries of the view, but not in response to isolated objects removed from the context\nbackground (Gottesman & Intraub, 2002, 2003). Though, there is contrary evidence that BE\nmay occur for abstract scenes containing figures on a white blank background (McDunn,\nSiddiqui, & Brown, 2014). BE was suggested to be specific to representation of scenes or\nimages that convey scene structure (Intraub et al., 1998). A scene is generally defined as a\ncontinuous spatial layout, which extends beyond the visible boundaries, while a nonscene\nrefers to an isolated object, which is not embedded in a spatial context (Hubbard et al., 2010).\n2 i-Perception\nMore generally, a scene is defined as a depiction of a truncated view of a continuous world\n(Gottesman & Intraub, 2002). In this sense, cropped faces may be perceived, similarly to\nscenes, as truncated views of a full body extending beyond the edges of the perceived view\nIndeed, BE phenomenon was observed for a variety scene contents, including scenes with\nsingle or multiple objects, scenes with cropped and noncropped objects, and even abstract\nrepresentations (Intraub & Bodamer, 1993; Intraub & Richardson, 1989; McDunn et al.,\n2014). However, the majority of studies did not focus on examination of BE in nonscenes.\nPossibly, BE may occur in response to some categories of nonscene objects, and in particular,\nto a very special kind of images, such as faces (Farah, Wilson, Drain, & Tanaka, 1998).\nAlthough human bodies and faces were sometimes included in the studied scenes (Gallagher,\n\u00b4 ne\nMunger & Multhaup, 2016), no research specifically focused on BE in face representations.\nThere is some evidence that presence of humans and human faces may affect BE. For\nexample, Gallagher et al. (2005) found an intriguing interaction between the background\ncomplexity and the presence of a person. When scenes contained a human, the amount of BE\nlinearly increased with the complexity of the background, whereas for scenes without a\nhuman, the simplest and the most complex scenes yielded the highest amounts of BE.\nOverall, investigation of faces seems to be an overlooked dimension in the research on BE.\nThus, the main purpose of the present study was to extend the existing knowledge on BE for\nnonscene images by examining this phenomenon in face processing.\nAlthough a large body of evidence supports cognitive and neurological dissociation in\nprocessing faces and scenes, there is also research suggesting that processing of faces and\nscenes shares a lot of similarities. Functional magnetic resonance imaging research (Haxby\nindicated that face processing is underpinned by the inferior occipital gyrus and lateral\nfusiform gyrus (also known as ``Fusiform Face Area'' or FFA). Lateral Occipital Cortex\nand FFA were found to have selective response to images of faces, but not those of houses or\nplaces (Kanwisher et al., 1997; Levy, Hasson, Avidan, Hendler, & Malach, 2001). In contrast,\nso-called Parahippocampal Place Area (PPA) was found to respond strongly to depictions of\nplaces, including indoor and outdoor scenes, and to respond weaker to buildings, but not to\nfaces (Epstein & Kanwisher, 1998). Previously published research indicated the crucial role of\nhippocampus in scene viewing and processing spatial locations (Bird & Burgess, 2008).\nNeuropsychological literature documented cases of hippocampal damage followed by a\nselective impairment of scene recognition and intact face memory (Carlesimo, Fadda,\naddition, it was found that face processing relied on a detailed central scrutiny and was\nmore strongly associated with processing of central information, whereas representations\nof buildings or scenes involved more peripheral information (Kanwisher, 2001; Levy et al.,\n2001). Neuroimaging research confirmed that BE is underpinned by the scene-selective\nregions of the brain.\nChadwick et al. (2013) indicated the crucial role of hippocampus in anticipation and\nconstruction of scenes, as well as in extrapolation of scenes beyond their physical borders.\nResearch emphasized the role of hippocampus in construction of scenes both in memory\nand the imagination (Zeidman, Mullally, & Maguire, 2014). Mullally et al. (2012) found\nthat patients with selective bilateral damage to hippocampus demonstrated less BE errors\nthan control participants, supporting the role of hippocampus in BE. Inconsistent with\nthis finding, Kim, Dede, Hopkins, and Squire (2015) found that amnestic patients\nwith hippocampal damage exhibited BE similarly to healthy controls. Furthermore,\nBlazhenkova 3\nPark, Intraub, Yi, Widders, and Chun (2007) demonstrated that BE task causes selective\nactivation in the PPA, a region associated with processing scenes such as landscapes or\nbuildings (Epstein & Kanwisher, 1998), but not in the Lateral Occipital Cortex, typically\nassociated with object recognition (Grill-Spector et al., 1999).\nAlthough many studies found the dissociation between face and place processing (Epstein\nevidence that faces and places share neural underpinnings, for example, involvement of the\nventral temporal cortex (Haxby et al., 2001). In addition, prosopagnosia (inability to\nrecognize the faces) often co-occurs with topographical prosopagnosia (Landis, Cummings,\nBenson, & Palmer, 1986). Overall, given the earlier distinction between face and scene\nprocessing as well as the evidence of BE underpinnings by scene-selective brain areas, it\ncould be possible that BE may not occur in response to faces. However, a cropped face\nimage may elicit the sense of continuation beyond the picture boundaries (Intraub et al.,\n1992), similar to how a close-up portrait (disembodied head) may be interpreted as a\ncontinuous scene that includes the lower parts of the body (Intraub & Richardson, 1989).\nPossibly, a close-up face image may perform as a scene, and when being cropped, it may\nenforce the perception of continuity and mental reconstruction of a coherent representation\nin the same way as the truncated scene does. The present study aimed to examine whether BE\ncan be observed for cropped face images. Particularly, to examine BE for faces rather than\nscenes, face images were presented on black backgrounds.\nFurthermore, the present work examined whether there is an asymmetry of BE in face\nprocessing. Although BE typically occurs for all boundaries of an image, for example, four\nedges of a photograph (Intraub & Richardson, 1989), the asymmetry in BE, for example,\ndifferent amounts of expansion at different sides of an image, was demonstrated by previous\nstudies. In particular, BE was found to enlarge in the direction of the implied/anticipated\nmotion, but not in the opposite direction (Courtney & Hubbard, 2004). Furthermore, a\ngreater BE was found for the objects that typically move faster than for the objects that\ntypically move slower (e.g., airplane vs. automobile). These findings may be explained by the\nrole of BE in facilitation of the subsequent scene recognition (Dickinson & Intraub, 2008).\nFor instance, anticipating a movement may increase BE in the expected direction and may\naid spatial integration of the successive views (Hubbard et al., 2010). Similarly, implied\nmotion in frozen-motion pictures and abrupt disappearance of a moving target cause\ndisplacement in memory toward the direction of motion (Freyd, 1983; Freyd & Finke,\nand representational momentum, that is, displacement in memory of a moving target\nbeyond the true final location. However, Munger, Owens, and Conway (2005) showed that\nBE and representational momentum are separate processes. They found that establishing the\nspatial layout occurs before the continuation of movement within a scene and concluded that\nBE cannot be due to a displacement in depth. Another evidence of asymmetry in BE comes\nfrom experiments using attentional cueing. As it was demonstrated by Intraub et al. (2006),\nBE can be affected by planned eye fixations: while BE appeared on the cued (to-be-fixated)\nside of the image, it was inhibited on the uncued side. Inconsistent with this finding, research\ndemonstrated that focal and increased attention may constrain BE error (Dickinson &\nIntraub, 2009; Intraub, Daniel, Horowitz, & Wolfe, 2008). Overall, these findings implied\nthat asymmetry in scene representation may be caused by the anticipatory processing in a\ncertain direction.\nThe present work tested the hypothesis that asymmetry in BE may exist for different facial\nparts. This prediction was made based on the evidence for asymmetry in attention to different\nregions of a face. In particular, previous research demonstrated that upper face regions\n4 i-Perception\nattract more attention than lower face parts (James, Huh, & Kim, 2010; McKelvie, 1976).\nGoldstein and Mackenberg (1966) examined the recognition of isolated portions of faces and\ndemonstrated that upper portions of the face are more important for identification than\nlower portions. Most probably, this asymmetry in attention occurs because the eyes are\nthe most salient and socially important part of the face (Janik, Wellens, Goldberg, &\nDell'Osso, 1978). Our gaze naturally focuses on eyes and mouth face regions (Janik et al.,\n1967), which are crucial for face identification and memorization and play important role in\nsocial communication (Ellis, 1975). Mouth region is also important but less salient than\n``eyes'' region (McKelvie, 1976; Pellicano, Rhodes, & Peters, 2006). Markedly, the\nconsistent looking preference for the upper part versus lower face part was not found for\nscenes (James et al., 2010). It was proposed that common configuration for faces determines\ntop-focused pattern of exploration, whereas no such stereotypic patterns exist for scene\nexploration. Thus, due to asymmetry in attention in processing top versus lower facial\nparts, it was expected in the present research to find the corresponding top versus bottom\nasymmetry in BE of cropped face images. In addition, the present hypothesis about\nasymmetry of BE in face processing was inspired by the portrait composition ``rules'' from\nphotography and visual arts, which put certain limitations on framing faces. There are\ncommon recommendations suggesting ``good'' places to crop and those to avoid. In\nparticular, a crop is acceptable in a forehead area; however, it is advised to be avoided in\na chin area. For example, according to Peterson (2016), the creator of Digital Photo Secrets\nwebsite, ``We are used to seeing pictures of people with the top of their head cropped off. It\ntypically looks fine. The same cannot be said for the bottom of the face \u00ad do not remove\nsomeone's chin!'' The present study aimed to explore whether the location of a crop in face\nrepresentation (forehead vs. chin) affects BE. Based on Intraub et al. (2006) findings, which\nindicate that BE can be amplified on the attended part of an image, it can be predicted that\nBE would be greater for the top (forehead) than for the lower (chin) part of the head.\nAlternatively, consistent with Intraub et al. (2008) study, which observed that focal\nattention may constrain BE, a greater attention to the upper part of the head may result\nin a reduction of BE. Gagnier et al. (2013) recorded oculomotor activity to study the\nmechanisms underlying the reduction in BE and to examine whether BE reflects a lack of\neye fixations near the edges of a picture. They contrasted cropped and whole-objects stimuli\nand assumed that the cropped area creates a salient marker of boundary placement. The\nreduction in BE was expected at the side where the object was cropped with a picture\nboundary due to increased attention to the crop area. However, Gagnier et al. found that\nthere was no difference in fixations to the boundary and to the cropped region. BE occurred\nin spite of multiple fixations to the boundary region and regardless whether the objects were\ncropped or not. In the present study, the oculomotor behavior was recorded to examine the\ndistribution of attention. Consistent with findings of greater attention to upper face regions\n(James et al., 2010; McKelvie, 1976), it was expected to observe a higher oculomotor activity\nin the top part of the face, including eyes and cropped boundary regions. In addition, the\npresent study explored the possible asymmetry in BE for lower versus upper parts of the face.\nFinally, the present research intended to explore face BE in relation to imagery. Mental\nimagery seems to have a special importance in understanding mechanisms of BE. It was\nsuggested that scene continuity activates schematic expectations representing the visual\nworld beyond the picture boundaries, and such mental schema underlies not only\nperception and memory but also imagination of scenes (Intraub, 1997). Intraub et al.\n(1998) proposed that BE may occur regardless whether perception or imagination\nactivated the perceptual schema. Indeed, a great body of evidence demonstrated the\nBlazhenkova 5\nsimilarity between perception and imagery, which share common representations, cognitive\nmechanisms, and neural underpinnings (Farah, 1988; Ganis, Thompson, & Kosslyn, 2004;\nKosslyn et al., 1999; Shepard & Metzler, 1971). In particular, O'Craven and Kanwisher\n(2000) found that imagery of faces and places activated the same stimulus-specific brain\nregions (FFA and PPA) as in perception, but the magnitude of activation was lower for\nimagery than for perception. Although visual imagery and perception showed significant\noverlap, research highlighted that imagery involves more top-down processing and more\nprefrontal cortex involvement than perception (Ganis et al., 2004; Mechelli, Price, Friston,\n& Ishai, 2004). Hubbard et al. (2010) showed that early visual processing areas were not\ninvolved in BE and proposed that BE is underpinned by high-level processes. These findings\nhighlight the possible role of mental imagery in BE. However, previous studies provided only\nlimited evidence about the relationship between imagery and BE. In particular, research\nexamined how imagery instructions may influence BE. Although BE was found primarily\nfor images containing a background surface but not for those with blank backgrounds\n(Intraub et al., 1998), imagery instructions altered these findings. In particular, BE was\nobserved when participants were deliberately imagining extended backgrounds around\nobjects on blank backgrounds (Gottesman & Intraub, 2002). Imagery may make people\nbelieve that they have experienced some events that they have not experienced (Loftus,\n2003). Yet, it was proposed that the act of imagination per se does not always increase\nsource memory errors. Foley, Foy, Schlemmer, and Belser-Ehrlich (2010) demonstrated\nthat, in contrast to spontaneous imagery instructions, deliberate imagery instructions may\neven decrease source memory errors. Foley et al. claimed that the awareness about the act of\ngenerating the images might distinguish these imagined items from the actually seen items,\nthus leading to reduced source memory errors. Using a variety of imagery instructions,\nMunger and Multhaup (2016) found that explicit imagination of sensory details in the\nscene does not result in increased BE. They concluded that there is no imagination effect\non BE.\nWhile the majority of studies on imagery and BE primarily focused on the effect of\nimagery instructions on BE, there were only few attempts to explore the relationship\nbetween BE and individual differences in imagery. Previous literature reported individual\ndifferences in visual-object (visualizing pictorial appearances in terms of shape, color, and\ntexture) and visual-spatial (visualizing spatial relations and transformations) imagery\nparticular, Kozhevnikov et al. distinguished between two types of individuals: object and\nspatial visualizers. Object visualizers tend to experience vivid and colorful mental images and\nto excel in object visualization tasks (e.g., recognizing degraded objects), whereas spatial\nvisualizers tend to use imagery for representing spatial relations and transformations and\nto excel in tasks that require spatial visualization (e.g., mental rotation). This distinction in\nindividual differences in imagery was based on evidence of the dissociation between ventral\n``visual-object'' and dorsal ``visual-spatial'' pathways in the brain, underpinning processing\nof different aspects of visual information (Farah, Hammond, Levine, & Calvanio, 1988;\nKosslyn & Koenig, 1992; Mazard, Tzourio-Mazoyer, Crivello, Mazoyer, & Mellet, 2004;\nUngerleider & Haxby, 1994). The relationship between BE and individual differences in\nobject versus spatial visual imagery was examined by Munger and Multhaup (2016). Using\nthe Object-Spatial Imagery Questionnaire (OSIQ), assessing object and spatial imagery\n(Blajenkova, Kozhevnikov, & Motes, 2006), Munger and Multhaup found a significant\npositive correlation between spatial imagery and BE, but not between object imagery and\nBE. The authors concluded that BE might be related to a superior visual-spatial rather than\nvisual-object imagery ability. In addition, there is evidence that individual differences in\n6 i-Perception\nspatial but not object imagery are related to spatial dispersion of eye movements during the\nrecall of scenes (Johansson, Holsanova, & Holmqvist, 2011). Furthermore, Mullally et al.\n(2012) asked participants to visualize in their imagination the scenes extending beyond the\ncurrent view and to rate the vividness of their subjective imagery. Researchers found that\npatients with selective bilateral hippocampal lesions had significant impairments in the ability\nto visually imagine spatially coherent scenes (e.g., spatial relationships and locations).\nMarkedly, the same patients demonstrated attenuated BE (thus, better memory) than\ncontrol participants. These findings indicate that the subjective vividness of scene imagery\nmay be associated with the increased BE. It is important to note that Mullally et al. (2012)\nmeasured vividness associated with imagination of spatially coherent scenes. As reported in\nBlazhenkova (2016), vividness that refers to imagery of spatial properties (locations, spatial\nstructure, and relationships) versus pictorial object properties (color, texture, and shape)\nconstitute separate, spatial and object, vividness dimensions. Thus, the results of Mullally\net al. (2012) may be interpreted as a finding of the relationship between spatial imagery and\nBE for scenes.\nAt the same time, the existing evidence suggests the possibility of positive association\nbetween object imagery and BE. While some studies demonstrated that vivid and pictorial\nobject imagery may facilitate memory (Marks, 1973; Vannucci, Pelagatti, Chiorri, &\nMazzoni, 2016), other literature showed that the elaboration of sensory details during the\nencoding leads to the increase in source memory errors (Thomas, Bulevich, & Loftus, 2003).\nResearch has suggested that vivid and rich in sensory detail imagery experiences may lead to\nlater confusion between real and imagined experiences, causing false memory errors\nrelationship between individual vividness of imagery and memory errors in a task that,\nduring the encoding, required to imagine half-shapes as complete symmetrical geometric\nforms. The participants were divided in ``high-imagery'' and ``low-imagery'' groups,\naccording to their scores on Vividness of Visual Imagery Questionnaire (VVIQ; Marks,\n1973). During the recall of shapes, high-imagery participants made more reality\nmonitoring errors, confusing half-shapes with complete shapes, than low-imagery\nparticipants. Notably, in contrast to Mullally et al. (2012) assessing spatial vividness, the\nVVIQ instrument (Marks, 1973) measures object vividness (Blajenkova et al., 2006;\nBlazhenkova, 2016). Therefore, the results of Markham and Hynes may be interpreted as\na finding of the relationship between object imagery vividness and mental extrapolation\nerrors for objects (symmetrical shapes), but not for scenes. Besides, numerous reports\nprovided evidence that higher VVIQ scores were associated with higher reality monitoring\nerrors and false memory (Dobson & Markham, 1993; Gonsalves et al., 2004; Hyman &\nPentland, 1996; Mazzoni & Memon, 2003). Overall, these findings suggest that vividness\nand strength of individual imagery may lead to increased BE errors. However, it is not yet\nknown how BE may be affected by the type of imagery (object vs. spatial) and the content of\nthe image (a single object on a blank background vs. scene). The present work examined the\nrelationship between BE and individual differences in the two types of imagery: object versus\nspatial. The present investigation implemented nonspatial stimuli: faces on blank\nbackgrounds. On the basis of the previous literature, it was expected that larger BE in face\nprocessing would be associated with superior object, but not spatial, visual imagery.\nIn addition, because the present research used face stimuli conveying emotions, the\nmeasures of emotional processing were also included along with the imagery assessments.\nPrevious studies indicated that, similar to vividness, emotional content might induce the\ncreation of false memories (Hyman & Pentland, 1996; Porter, Spencer, & Birt, 2003).\nHowever, the evidence regarding the relationship between emotional processing and BE is\nBlazhenkova 7\nquite controversial. Several studies suggested that BE effect may depend on the emotional\ncontent of scenes. For example, Me\n\u00b4 ne\n\u00b4 trier et al. (2013) demonstrated that positively valenced\nstimuli (i.e., actors showing happiness and pleasure through facial and postural expressions)\nled to BE effect, whereas negatively valenced stimuli (i.e., expressing anger and irritation) did\nnot produce directional memory distortion. Safer et al. (1998) showed that negative\nemotional content of an image may lead to ``tunnel memory effect'' or boundary\nrestriction (BR), opposite to BE effect. However, other research found no difference in\nmagnitude of BE for pictures with emotionally neutral and emotionally charged content\n(Candel, Merckelbach, Houben, & Vandyck, 2004; Candel, Merckelbach, & Zandbergen,\n2003). Mathews and Mackintosh (2004) found that scene extrapolation interacts with\nindividual differences in emotionality: BE for very negative scenes was reduced in high-\ntrait-anxious individuals. Blazhenkova and Kozhevnikov (2010) indicated positive\nassociation between individual differences in emotion and object, but not spatial, imagery.\nTherefore, in the present study, it was expected that both, individual differences in emotional\nability and object imagery, similar to each other, would be positively related to BE in face\nprocessing.\nTo summarize, there is an increasing body of research on BE in scene perception and\nmemory. However, there are gaps in the knowledge regarding BE for nonscene\nrepresentations, and in particular, face images. The present work examined BE errors in\nface processing using cropped face images on black backgrounds. Study 1 explored BE in\ndifferent facial parts. Using short-term and long-term memory conditions, it examined\nwhether the location of a crop (forehead vs. chin) affects BE. Study 2 examined the\nrelationships between the measures of BE in face images and different assessments of\nindividual differences in object/spatial imagery and emotion.\nThe goal of Study 1 was to examine BE using face stimuli, cropped either in a forehead or\nchin. The task was administered in two memory conditions to explore the robustness and the\npossible differences in BE effect. On the basis of previous research that showed BE for both\nlong-term and short-term retention (Intraub & Dickinson, 2008; Safer et al., 1998), it was\nexpected to find this effect in both memory conditions. The early onset of BE indicates that\nthis phenomenon occurs at the border between perceiving and remembering (Intraub &\nDickinson, 2008; Roediger, 1996). The comparison between the long-term and short-term\nmemory conditions could elucidate the role of perception and memory in BE. Furthermore, it\nwas hypothesized that participants would make BE errors for both crop locations. However,\nbased on the attentional asymmetry in face processing, forehead extension was expected to be\ndifferent from chin extension. In addition, this study explored the effect of the expressed\nemotion on scene extrapolation. Eye-tracking data were collected alongside with the task\nperformance data.\nMethod\nParticipants. Eighty-three1 participants were Sabanci University students (19\u00ad26 years old,\nM age \u00bc 22). Participants were run either in the short-term memory (22 males, 19 females)\nor in the long-term (19 males, 23 females) condition. They were reimbursed with course\ncredits for their participation. The research was approved by the Sabanci University\nResearch Ethics Council. All participants provided written informed consent.\n8 i-Perception\nMaterials and procedure. All participants were tested individually. They completed either a\nshort-term or a long-term condition of the Faces Task. This task included Encoding &\nEmotional Identification and Recognition parts. During the Encoding, participants\nobserved photographs of faces, cropped either in a forehead or in a chin area. The stimuli\nwere created based on Karolinska Directed Emotional Faces picture set (Lundqvist, Flykt, &\nO\n\u00a8 hman, 1998). They comprised color pictures of female faces displaying six different\nemotional states (anger, disgust, happiness, neutrality, sadness, and surprise) photographed\nfrom the front. Each emotional state was presented in four different faces (two cropped in the\nforehead and two in the chin area), thus making 24 pictures in total. The same Encoding face\nstimuli were used in both conditions. Forehead- and chin-cropped faces had fixed width, but\nvaried in height. The height of full uncropped original Karolinska Directed Emotional Faces\nimages corresponded to the full height of the screen. The cropped stimuli were shifted so that\nthe bottom of forehead-cropped faces touched the bottom of the screen, and the top of chin-\ncropped faces touched the top of the screen, thus leaving some space for a mental\ncontinuation of a cropped part. Each face appeared for 4 s, and it was followed by\nEmotional Identification.\nThe instruction included two parts. The Encoding & Emotional Identification instruction\nwas as follows: You will see 24 pictures of people with different facial expressions. Try to\nidentify the emotions conveyed in each of these photos. After each face presentation, you will\nbe automatically taken to the page with a question about this face emotion. During the\nEmotional Identification, participants were asked, ``What was the emotion expressed by\nthis face?'' They had to select among the seven answer options: ``fear,'' ``anger,'' ``disgust,''\n``happiness,'' ``neutral,'' ``sadness,'' and ``surprise.'' The answer time was not limited, and\nprogram proceeded to the next trial after the response. The recognition of the emotional\nexpressions was included in the Faces Task to encourage participants focusing on the other\nface properties rather than local areas of crops and to separate the presentation of the\nEncoding and Recognition face stimuli.\nThe second part of the instruction was as follows: You will see the same picture again\namong the other options. Try to recognize the picture that you saw before. Select this picture\nby clicking your mouse on it. During the Recognition, participants had to recognize the\npreviously seen images and to make a forced-choice selection among the four options.\nDuring the first trial of the Emotional Identification task, to ensure that a participant\nunderstands the task, the experimenter noted and showed that, though, all the choice\nimages represent the same face, but the crop area is different. These selection options\nincluded the same faces with two extended and two reduced crops, representing different\ndegrees of possible BE and BR errors. To increase the sensitivity of the test, none of the\nanswer options were the same as an image presented during the Encoding (so there was no\ncorrect answer option). Some other BE studies also did not include the selection option\nidentical to the studied view (Mathews & Mackintosh, 2004; Quinn & Intraub, 2007). For\nexample, Mathews and Mackintosh (2004) used a forced-choice recognition task that\npresented four alternatives with different degrees of BE and restriction but did not present\nthe originally memorized scene. Remarkably, research on eyewitness memories demonstrated\nthat participants confidently and falsely identified someone in the lineup of potential suspects\nas being the perpetrator, when the actual perpetrator was not in the lineup (Wells et al.,\nIn the short-term memory condition, both parts of the instruction were presented prior\nto start of the test. In the long-term condition, the second part of the instruction came after\neach block of Encoding & Emotional Identification. In the short-term memory condition,\neach Encoding image was immediately followed by the Emotional Identification and then\nBlazhenkova 9\nRecognition (Figure 1). The response time for Emotional Identification was not limited and\nnot recorded. Pilot participants testing showed that it took approximately 3 to 10 s. Because\nthe time interval between the stimuli Encoding and Recognition did not exceed 30 s, this\ncondition was labeled as ``short-term'' memory (Baddeley & Hitch, 1974). In the long-term\nmemory condition, the encoding images were presented in four blocks, each containing six\ntrials. Each Encoding block was followed by a Recognition block. This condition required\nlong-term memory because the time interval between the Encoding and Recognition stimuli\nThe same Recognition alternatives were used in both conditions. However, in the short-\nmemory condition, half of the Recognition options increased in size from right to left,\nreflecting the magnitude of BE/BR in the solution options (i.e., Large BR, Small BR,\nSmall BE, Large BE), and another half had the reversed order. In the long-memory\ncondition, all choice options increased in size from right to left. To have a visual diversity\nin the selection options, the size of the face crops varied, and it was balanced between the chin\nand forehead areas. The face width was constant between the four alternative options.\nHowever, because faces were cropped differently in either forehead or chin, their heights\nwere different. The order of face images with varying crop locations was intermixed. It\nwas fixed for the Encoding and Recognition, which allowed to keep the time between the\ncorresponding encoding and Recognition stimuli for the same face relatively constant.\nBehavioral responses (mouse clicks on the selected pictures) were recorded.2 The response\ntime for Recognition was not limited. Eye-tracking data were collected using Tobii TX300\nEye Tracker (data rate 120, framerate 5, fixation filter I-VT) and Tobbi Studio software.\nResults - Behavioral Data\nTypes of errors during the Recognition. For the Recognition data analysis, Regions of Interest\n(ROIs) were created around each answer option (Figure 2). Because none of the answer\noptions represented a correct answer, the Recognition was analyzed in terms of different\ntypes of errors (i.e., Large BE, Small BE, Small BR, and Large BR), corresponding to the\nFigure 1. Schematic of trial and block sequences in the short-term and long-term conditions of the\nFaces Task.\nfour answer options. Error Frequency was the number of mouse clicks on different\nRecognition options. The two types of BE answers and the two types of BR answers were\nobtained in the forced-choice task, leading to two categories of responses: Extension and\nRestriction Errors. The mean frequencies of responses falling in these two categories were\nfurther compared for faces cropped in the forehead or chin areas.\nRecognition analysis. A mixed 2 \u00c2 2 \u00c2 2 three-way (two within- and one between-subjects factors)\nanalysis of variance (ANOVA) was conducted to assess the impact of two repeated measures\nvariables, Error Type (BE, BR) and cropping Location (Forehead, Chin), on participants'\nrecognition (Error Frequency), across the two memory Conditions (Short-Term, Long-\nTerm).3 The adjustment for multiple comparisons was done with Bonferroni correction. The\nrestriction (M \u00bc 4.383, SE \u00bc 0.191) answer options. There was a significant but weak\nsuggesting that Error Type effect differed depending on the cropping Location. While the\nextension effect was present for both locations of crop, it was greater for forehead-cropped\nimages (MDBE-BR\nSE \u00bc 0.232) than for chin-cropped (MDBE-BR\nError Type effect between the short-term and long-term conditions. There was a weak but\nZ2 \u00bc .124. A separate analysis for two conditions revealed that forehead versus chin asymmetry\nmean Error Frequencies of BE and BR responses for both conditions.\nThe effect of Emotion. The possible effect of Emotion on boundary extrapolation in cropped face\nimages was additionally explored. A mixed 6 \u00c2 2 \u00c2 2 three-way ANOVA was conducted to\nassess the impact of Emotion (Anger, Disgust, Happiness, Neural, Sadness, Surprise) and\nError Type on Error Frequency, across the two memory Conditions.4 The adjustment for\nmultiple comparisons was done with Bonferroni correction. The analysis revealed a main\nZ2 \u00bc .012. There was a weak but significant interaction between Emotion and Error Type, F(5,\nFigure 2. ROIs around different types of Recognition errors in the Faces Task.\nROIs \u00bc Regions of Interest.\nEmotion. This interaction effect was analyzed using a simple main effects analysis. Pairwise\ncomparisons of the Emotion \u00c2 Error Type interaction revealed that Mean Difference (BE-BR)\nwas significant only for ``Angry'' (MDBE-BR\nBR\n``Surprise'' (MDBE-BR\nsize of BE effect for different types of emotions, the additional repeated measures analysis with\nBE-BR as a dependent measure and Emotion as a within-subjects variable. Pairwise\ncomparisons revealed that BE-BR difference for ``Happy'' emotion was significantly larger\nfor ``Neutral'' emotion compared with BE-BR for ``Angry'' (MD \u00bc \u00c01.084, SE \u00bc 0.262,\nSE \u00bc 0.273, p \u00bc .002) emotions. Figure 3 shows mean Error Frequencies of BE and BR\nanswers for different types of emotions. In addition, for a clearer demonstration of Error\nType effect for each emotion, this figure displays mean difference between BE and BR\nmeasures (BE-BR). There was no interaction between Emotion, Error Type, and Condition,\nResults - Eye-Tracking Data\nEncoding analysis. To analyze the visual processing during the Encoding, five ROIs were\ncreated for each cropped face image: ``cropped'' area, ``border'' area, ``eyes'' area,\n``mouth'' area, and ``noncropped'' face area. As illustrated on the Figure 4, the ``cropped''\nROI was defined as a missing part of a face, whereas ``noncropped'' ROI was the area above\nthe ``eyes'' in chin-cropped images or the area below the ``mouth'' the forehead-cropped\nFigure 3. Recognition data: Error Frequencies for different types of errors in the Faces Task.\nROI \u00bc Region of Interest; BE \u00bc Boundary Extension; BR \u00bc Boundary Restriction.\nfaces. The ``eyes'' ROI was created around the eyes, between the top of the alar nasal sulcus\nand just above the superciliary arch. The ``mouth'' ROI included the area below the ``eyes''\nROI and mentolabial sulcus. The ``border'' ROI included the area between the ``eyes'' and\n``cropped'' ROIs in the forehead-cropped faces or area between the ``mouth'' and ``cropped''\nROIs in the in chin-cropped images. Two different eye-tracking metrics, based on the ROIs\ndefined in Figure 4, were used in the analysis. Visit Duration was the total time in seconds\nspent within a particular ROI. Visit Count was the total number of visits to an ROI, where\nvisit is defined as a time interval between the first fixation inside the ROI and the next fixation\noutside the ROI.\nA mixed 5 \u00c2 2 \u00c2 2 three-way (two within- and one between-subjects factors) ANOVA was\nconducted to assess the impact of two repeated measures variables, Face Area (``cropped,''\n``border,'' ``eyes,'' ``mouth,'' ``noncropped'') and cropping Location (Forehead, Chin), on\noculomotor variables (Visit Duration, Visit Count) across the two Conditions (Short-Term,\nLong-Term). The adjustment for multiple comparisons was done with Bonferroni correction.\nThe data were analyzed separately for Visit Duration and Visit Count. Eye-tracking heat\nmaps representing relative visit durations during the Encoding are presented in Figure 5.\nThe analysis of Visit Duration revealed a substantial main effect for Face Area,\n``eyes'' (M \u00bc 19.965, SE \u00bc 0.694) attracted significantly more attention than any other face\nareas all p's < .001). The ``mouth'' was the second most salient area; mean Visit Duration was\nsignificantly greater for the ``mouth'' ROIs (M \u00bc 12.407, SE \u00bc 0.582) than for other ROIs (all\nand for ``noncropped'' (M \u00bc 2.736, SE \u00bc 0.226) areas were significantly smaller than all other\nROIs (all p's < .001), but not different from each other (p \u00bc .087). Mean Visit Duration for\nthe ``border'' areas (M \u00bc 4.065, SE \u00bc 0.318) was greater than for the ``noncropped'' ROIs\npartial Z2 \u00bc .021; there were no difference in Visit Duration between forehead- and chin-\ncropped images. The effect of Condition was not significant, F(1, 81) \u00bc .094, p \u00bc .760, partial\nZ2 \u00bc .001. There was a significant interaction between Face Area and cropping Location, F(4,\nFigure 4. ROIs around different parts of the face viewed during the Encoding of the Faces Task.\nROIs \u00bc Regions of Interest.\nZ2 \u00bc .188. Mean Visit Duration for the ``cropped,'' ``border,'' and ``uncropped'' areas was\ngreater in the short-term condition than in the long-term condition, but it was greater for the\n``eyes'' area in the long-term condition than in the short-term condition (all p's < .001). Face\npartial Z2 \u00bc .114, indicating that the discrepancy in Face Area effect between the forehead-\nand chin-cropped images was different in short and long-term conditions. In both conditions,\nthere were no differences in mean Visit Duration between the forehead versus chin areas for\nthe ``cropped'' and the ``mouth'' ROIs. For the ``border'' and the ``eyes'' ROIs', Visit\nDuration was greater for forehead- than for chin-cropped faces in the short-term\ncondition (p's < .001), but not in the long-term condition. Mean Visit Duration for the\n``noncropped'' ROIs' was greater for the chin- than for the forehead-cropped faces in the\nshort-term condition (p < .001), but not in the long-term condition. Figure 6 demonstrates\nthe results of comparisons between mean Visit Duration for different ROIs in the forehead-\nand the chin-cropped faces, separately for the two conditions.\nFigure 5. Encoding eye-tracking heatmaps representing relative Visit Duration in the short-term and long-\nterm conditions of the Faces Task.\nConsistent with Visit Duration, the analysis of Visit Count revealed a substantial main effect\nattention than any other face areas all p's < .001. The ``Mouth'' was the second most salient\narea; mean Visit Count was significantly greater for the ``mouth'' ROIs (M \u00bc 27.132,\nSE \u00bc 0.758) than for the other ROIs (all p's < .001), except for the ``eyes'' areas. Mean Visit\nCount for the ``cropped'' areas (M \u00bc 3.481, SE \u00bc 0.325) was the smallest compared with all\nother ROIs (all p's < .001). There was no difference between mean Visit Count for the ``border''\nMean Visit Count was greater for the short-term (M \u00bc 16.568, SE \u00bc 0.479) than for the long-\nZ2 \u00bc .155. Visit Count for the ``cropped,'' ``border,'' and ``uncropped'' areas was greater in\nthe short-term condition than in the long-term condition (all p's < .001), but it was greater for\nthe ``mouth'' area in the long-term condition than in the short-term condition (p \u00bc .041). There\npartial Z2 \u00bc .001. Face Area \u00c2 Location \u00c2 Condition interaction was significant but weak,\nin mean Visit Count between the forehead versus chin areas for the ``cropped'' and the\n``mouth'' ROIs. For the ``border'' and the ``eyes'' ROIs', mean Visit Count was greater for\nthe forehead- than for the chin-cropped faces in the short-term condition (p's < .001), but not\nin the long-term condition. Visit Duration for the ``noncropped'' ROIs' was greater for the\nchin- than for the forehead-cropped faces in both, the short-term (p < .001) and the long-term\nFigure 6. Faces Task Encoding data: Visit Duration and Visit Count for the different ROIs.\nROIs \u00bc Regions of Interest.\n(p \u00bc .002), conditions. Figure 6 demonstrates the results of comparisons between mean Visit\nCount for different ROIs in forehead- and chin-cropped faces, separately for the two\nconditions.\nRecognition analysis. Consistent with the behavioral data analysis, a mixed 2 \u00c2 2 \u00c2 2 ANOVA\nwas conducted to assess the impact Error Type and Cropping Location on participants'\noculomotor variables across the two memory Conditions. The data were analyzed\nseparately for Visit Duration and Visit Count. The adjustment for multiple comparisons\nwas done with Bonferroni correction. The eye-tracking data were examined using the same\nROIs as the behavioral Recognition analysis (Figure 2). The eye-tracking heatmap\nvisualizations are presented in Figure 7.\nThe analysis of Visit Duration revealed a substantial main effect for Error Type,\nbetween the mean Visit Duration for BE and BR answer options for the forehead-cropped\nimages (MDBE-BR\nSE \u00bc 1. 080) was greater than for the chin-cropped images (MDBE-BR\nsignificant Error Type \u00c2 Location \u00c2 Condition interaction between, F(1, 81) \u00bc 26.185,\np < .001, partial Z2 \u00bc .244, indicating that the Error Type \u00c2 Location interaction varied\nbetween the long-term and the short-term conditions. A separate analysis for the two\nconditions revealed that forehead versus chin asymmetry of BE effect (Error\nType \u00c2 Location interaction) was present in the short-term condition, F(1, 40) \u00bc 37.554,\npartial Z2 \u00bc .007. Figure 8 presents recognition results for the different Error Types,\nConditions and cropping Locations, based on eye-tracking data.\nThe analysis of Visit Count revealed a substantial main effect for Error Type,\nbetween the mean Visit Count for BE and BR answer options for the forehead-cropped\nimages (MDBE-BR\nSE \u00bc 1.518) was greater than for the chin-cropped images (MDBE-BR\nError Type effect was less pronounced in the short-term (MDBE-BR\nFigure 7. Recognition eye-tracking heatmaps representing relative Visit Duration and mouse clicks for the short-term and long-term conditions of the Faces\nTask.\np \u00bc .153, partial Z2 \u00bc .025. There was a significant Error Type \u00c2 Location \u00c2 Condition\nconditions revealed that the forehead versus chin asymmetry of BE effect (Error\nType \u00c2 Location interaction) was present in the short-term condition, F(1, 40) \u00bc 54.347,\nFigure 8. Recognition data: Visit Duration and Visit Count for the different types of errors in the Faces Task.\nROI \u00bc Region of Interest; BE \u00bc Boundary Extension; BR \u00bc Boundary Restriction.\nZ2 \u00bc .000. Figure 8 presents the recognition results for the different Error Types, Conditions and\ncropping Locations, based on the eye-tracking data.\nThe effect of Emotion. Consistent with the behavioral data analysis, a mixed 6 \u00c2 2 \u00c2 2 ANOVA\nwas conducted to assess the impact of Emotion and Error Type on the eye-tracking measures,\nacross the two memory Conditions. The data were analyzed separately for Visit Duration and\nVisit Count. The adjustment for the multiple comparisons was done with Bonferroni\ncorrection.\nThe analysis of Visit Duration revealed a substantial main effect of Error Type,\nthe BE answer options longer than at the BR answer options. It was a significant, though\ndemonstrated that the faces representing ``Angry'' emotion evoked longer Visit Duration\nthan all the other emotions (all p's < .001, but for the ``Sad'' emotion p \u00bc .054). There was\na significant but weak interaction between Emotion and Error Type, F(5, 405) \u00bc 4.216,\np < .001, partial Z2 \u00bc .049, suggesting that the Error Type effect differed depending on the\nEmotion. Pairwise comparisons of the Emotion and Error Type interaction revealed that\nMean Difference (BE-BR) was significant for all emotions: ``Angry'' (MDBE-BR\nBR\nfor different types of emotions, the additional repeated measures analysis with BE-BR as a\ndependent measure and Emotion as a within-subjects variable. Pairwise comparisons\ndemonstrated that mean BE-BR for the ``Angry'' emotion was significantly greater\np \u00bc .909, partial Z2 < .001. There was a weak but significant interaction between Emotion and\ndiffered depending on the Condition. Figure 8 presents mean Visit Duration of BE and BR\nanswers for the different types of Emotions in both Conditions. In addition, for a clearer\ndemonstration of Error Type effect for each emotion, this figure displays the difference\nbetween BE and BR measures (BE-BR). The interaction between Emotion, Error Type,\nSimilarly, the analysis of Visit Count revealed a substantial main effect for Error Type,\nthe BE answer options more often than at the BR answer options. There was a significant\ndemonstrated that the faces representing ``Angry'' emotion evoked longer Visit Count than\nall the other emotions (all MDBE-BR\nwith the ``Surprise'' emotion was lower than the ``Neutral'' (MDBE-BR\np \u00bc .030) and ``Sad'' (MDBE-BR\nsignificant but weak interaction between Emotion and Error Type, F(5, 405) \u00bc 4.827,\np \u00bc .811, partial Z2 \u00bc .001. There was a weak but significant interaction between Type and\nPairwise comparisons of the Emotion and Error Type interaction revealed that Mean\nDifference (BE-BR) was significant for all emotions: ``Angry'' (MDBE-BR\nBR\np < .001) emotions. Furthermore, to compare the size of BE effect for different types of\nemotions, the additional repeated measures analysis with BE-BR as a dependent measure\nand Emotion as a within-subjects variable. Pairwise comparisons demonstrated that BE-BR\nfor the ``Angry'' emotion was significantly greater compared with the ``Disgust'' (MD \u00bc 3.084,\nthe ``Happy'' emotion was also greater compared with the ``Disgust'' (MD \u00bc 2.843,\nDiscussion\nThe analyses of behavioral (Error Frequency) and eye-tracking (Visit Duration and Visit\nCounts) Recognition data yielded highly consistent results. The main effect of Error Type\nwas found: The BE answer options were selected significantly more frequently and attracted\nmore attention than the BR answer options. Thus, Study 1 demonstrated the evidence of BE\neffect in face processing. The analysis revealed no interaction between Error Type and\nCondition for Error Frequency and Visit Duration measures; however, there was a weak\nbut significant interaction for Visit Count measure. These results provide the robust evidence\nof the BE effect for the different retention intervals, though, Visit Count results indicate that\nthis effect may be somewhat more conspicuous in the long-term condition. Furthermore, the\nBE effect was more pronounced for the forehead-cropped images. This demonstrates the\nevidence of the asymmetry of BE effect in face images. However, the forehead-biased\nasymmetry of BE effect manifested only in the short-term Condition. Markedly, during the\nEncoding, the forehead-biased asymmetry in Face Area effect was also found in the short-\nterm, but not in the long-term Condition.\nThe additional analyses revealed the effect of Emotion on Visit Duration and Count: The\n``angry'' emotion evoked more attention than all the other emotions. Furthermore, the found\ninteraction between Emotion and Error type suggests that the strength of BE effect may differ\ndepending on the type of Emotion. The analysis of the eye-tracking measures revealed that\nthe BE effect was greater for the ``Angry'' and the ``Happy'' emotions and smaller for the\n``Disgust'' and the ``Sad'' emotions. Similarly, but not fully consistent, the analysis of mouse\nclicks revealed that the BE effect was greater for the ``Happy'' emotion but smaller for the\n``Disgust'' and the ``Neutral'' emotions.\nAfter Study 1 has established BE in processing of cropped face images, Study 2 aimed to\nexamine the relationship between the strength of BE and individual differences in\nimagery and emotion. Thus, the same participants as in the Study 1 were asked to\nperform additional tests on individual differences in object and spatial imagery and\nemotional processing.\nMethod\nParticipants. Thirty-nine participants from the short-term memory condition and 38\nparticipants from the long-term memory condition completed the full set of assessments.\nMaterials and procedure. The participants received four different questionnaires and tasks:\nOSIQ, Emotion Vividness task, Geneva Emotion Recognition Test (GERT), and Range\nand Differentiation of Emotional Experience Scale (RDEES). In addition, Emotion\nRecognition performance data from Faces Task (Study 1) were analyzed.\nOSIQ. This is a self-report measure assessing individual differences in visual-object and\nvisual-spatial imagery (Blajenkova et al., 2006). The OSIQ consists of 15 statements assessing\nobject mental visualization and 15 statements assessing spatial mental visualization.\nParticipants had to rate these items on a 5-point scale from total agreement to total\ndisagreement. The scores for object and spatial imagery are calculated by averaging the\ncorresponding 15 ratings per subscale. The internal reliabilities (Cronbach's alpha) of the\nobject and spatial imagery subscales are .83 and .79, respectively (Blajenkova et al., 2006).\nEmotion Vividness task. In this task, participants had to imagine six basic emotions (anger,\nsurprise, happiness, disgust, sadness, fear; see Ekman, 1992) as well as neutral facial\nexpression inside the empty face outline, presented for 5 s on a computer screen. After\nimagining each emotion, participants rated the vividness of their subjective mental image\nusing the 5-point scale adopted from the VVIQ (Marks, 1973): 5 perfectly clear and as vivid\nas normal vision), 4 (clear and reasonably vivid), 3 (moderately clear and vivid), 2 (vague and\ndim), 1 (no image at all, you only ``know'' that you are thinking of the object). This task was\ndeveloped by the author of this study, and it is not a validated task of imagined emotion\nvividness.\nGERT. This test is assessing the ability to accurately recognize emotional states (Schlegel,\nGrandjean, & Scherer, 2014). Participants watched 83 short video clips with sound, in which\n5 male and 5 female actors expressed different emotions conveyed both by facial expressions\nand voice (using pseudolinguistic sentences). Participants had to select the emotion word\n(from 14 emotions), which best describes the emotion expresses in the video.\nRDEES. This is self-report assessing individual differences in emotional complexity, which\nis defined as having emotional experiences that are broad in range and well-differentiated\n(Kang & Shaver, 2004). Seven items tap Range (e.g., ``I have experienced a wide range of\nemotions throughout my life''), and seven Differentiation of Emotional Experiences (e.g.,\n``I am aware of the subtle differences in the feelings that I have''). Participants rated each item\non a 5-point scale (1 \u00bc does not describe me very well and 5 \u00bc describes me very well). The\nscores for Range (RDEES-r) and Differentiation (RDEES-d) subscales were computed by\naveraging the corresponding ratings. The internal reliability (Cronbach's alpha) of this\nquestionnaire is .85 (.82 for the RDEES-r subscale and .79 for RDEES-d subscale; Kang\nEmotion Recognition. Based on the responses given in a Faces Task (Study 1), the accuracy\nof emotional recognition was computed as a number of correctly identified emotions. Note,\nthis is not a validated task of emotional recognition, and also it was done simultaneously with\nmemorization. The reliability was quite low (Cronbach's alpha \u00bc .210). Thus, another\nvariable, Emotion Recognition\u00c1 accuracy was computed by excluding the most inconsistent\n4 items with accuracy below 50% (Cronbach's alpha was still quite low \u00bc .259).\nResults\nThe relationships between different memory errors, imagery, and emotional measures\nwere analyzed using Pearson's Correlational Analysis. In the short-term memory condition\n(Table 1), the analysis revealed no relationship between the frequency of BE/BR errors and\nany of imagery measures. Emotion Recognition\u00c1 accuracy in the Faces Task tended to be\npositively correlated with BE and negatively with BR errors (p's \u00bc .091). Furthermore, longer\nVisit Duration and Count (both for BE/BR images) were positively associated with higher\nscores on GERT (all p's .015). Visit Count was negatively associated with Vividness of\nEmotional Imagery (p \u00bc .054 for Extension and p \u00bc .015 for Restriction Visit Count).\nIn the long-term memory condition (Table 2), the analysis revealed significant positive\nrelationship between the frequency of BE errors and object imagery (p \u00bc .038). Inversely,\nthe relationship between the frequency of BR errors and object imagery was negative\n(p \u00bc .020). In addition, attention to BR images was significantly and negatively associated\nwith object imagery (p \u00bc .039 for Visit Duration and p \u00bc .014 for Visit Count). Furthermore,\nRDEES-d was positively associated with frequency of BE Errors (p \u00bc .042), and negatively\nwith frequency of BR Errors (p \u00bc .036), as well as restriction Visit Count (p \u00bc .025). The\nsimilar, but nonsignificant, trends were observed between the BE Errors and Emotion\nVividness as well as GERT. Emotion Recognition accuracy in the Faces Task tended to be\nnegatively associated with visual attention, both for BE/BR images (p's .033 for Emotion\nRecognition\u00c1 and Visit Count, p \u00bc .032 for Visit Duration).\nIn addition, the relationship between imagery and emotional measures was examined\nusing the combined data from the long-term and short-term conditions (N \u00bc 79). Object\nimagery scale of the OSIQ was positively correlated with Emotion Vividness (r \u00bc .289,\nEmotion Recognition accuracy in the Faces Task (r \u00bc .223, p \u00bc .049)/Emotion\np \u00bc .018), but negatively and marginally significantly with spatial imagery scale of the OSIQ\nDiscussion\nStudy 2 demonstrated positive relationship between BE measures and individual differences\nin object but not spatial imagery. This indicates that individuals with higher object imagery\nshowed greater BE and lesser BR effect. Similarly, individuals who reported higher emotional\ndifferentiation tended to have more pronounced BE errors. However, the relation between\nBE and imagery or emotional measures was observed primarily in the long-term condition.\nConsistent with the previous research (Blazhenkova & Kozhevnikov, 2010), object, but not\nspatial imagery, tended to be positively associated with different emotional measures, which\ntended to be interrelated. Overall, object imagery and emotional measures' relationships with\nBE measures were similar.\nGeneral Discussion\nThe present work examined BE phenomenon in face processing. The results of the Study 1\ndemonstrated significantly more BE than restriction recognition errors, as revealed by both\nperformance and eye-tracking measures. Thus, consistent with previous literature (Hubbard\nTable 1. Correlations Between all the Measures in the Short-Term Condition of the Faces Task.\nNote. OSIQ \u00bc Object-Spatial Imagery Questionnaire; GERT \u00bc Geneva Emotion Recognition Test; RDEES \u00bc Range and Differentiation of Emotional Experience Scale.\nTable 2. Correlations Between all the Measures in the Long-Term Condition of the Faces Task.\nNote. OSIQ \u00bc Object-Spatial Imagery Questionnaire; GERT \u00bc Geneva Emotion Recognition Test; RDEES \u00bc Range and Differentiation of Emotional Experience Scale.\net al., 2010; Intraub & Bodamer, 1993), the present findings suggest that people are prompted\nto perceptual and memory errors, in which they see or remember seeing parts of images that\nwere not visible during the presentation but were possible to be inferred from the visible\ninformation. The present study demonstrated BE effect in both short-term and long-\nterm memory conditions. This is consistent with other research that found BE for a wide\nrange of retention intervals (Intraub & Dickinson, 2008; Intraub & Richardson, 1989; Safer\nThe present results indicate that BE effect could be somewhat more pronounced in the long-\nterm condition. The found difference in BE effect between the two memory conditions was\nrather small and appeared only in Visit Count analysis. From the multisource model\nperspective (Intraub, 2012), this finding may suggest that the increased retention time\ncould lead to a greater difficulty in distinguishing between amodally generated and\nremembered visual sensory information, thus resulting in a greater BE in the long-term\ncondition. In addition, the difference between the two memory conditions may be\nexplained by the difference in the awareness about the nature of the memory task. In the\nshort-term condition, after the very first trial, participants became aware of the subsequent\nmemory test that required boundary knowledge, whereas in the long-term condition,\nparticipants became aware about the memory test only after the first block of trials.\nMoreover, due to a significant delay between the Encoding and Recognition in the long-\nterm condition, the awareness about memorization demand during the Encoding could\ndecrease. It is important to note that during the Encoding, participants had to\nsimultaneously identify the emotion and to memorize the face, which could be considered\nas a dual task (Pashler, 1994). Emotional identification was a primary task, and Recognition\ncame afterward. Eye-tracking data indicated that the participants, first, focused on the\nemotional identification. The majority of the first fixations were in the ``eyes'' area,\nwhereas other facial parts were examined later. Because the differences between the\nRecognition answer options appeared solely in the place of a crop, the ``border'' area\nprovided important cues for face memorization, critical for the subsequent Recognition.\nAs it can be seen from the eye-tracking visualizations, with more trials, participants\nincreased their attention toward the ``border'' areas, most probably because they were\nrealizing the importance of this area for the Recognition performance. Indeed, several\nparticipants spontaneously mentioned that they were focused on border areas in demand\nto the expected subsequent Recognition task. Similarly, Gagnier et al. (2013) suggested that\nthe cropped region is a salient marker of boundary placement and found multiple fixations to\nthis area. In the long-term condition, the Recognition was delayed, so, possibly, the\nparticipants were more focused on emotional identification and less focused on the\nmemorization for further Recognition task. Indeed, in the short-term condition, the\nattention to the ``border'' and its neighboring ``cropped'' areas was greater than in the\nlong-term condition, while the attention to the ``eyes'' and ``mouth'' areas was greater in\nthe long-term condition. These results are similar to Gagnier et al. (2013) finding that the\nparticipants who were told that the boundary memory was tested had higher oculomotor\nactivity near the boundaries of pictures than the participants who were not informed about\nthe nature of the test. Thus, consistent with previous research that showed the reduction of\nBE error when participants were informed about the nature of the task (Gagnier et al., 2013;\nIntraub & Bodamer, 1993), the short-term memory condition in the present research yielded\nsomewhat reduced BE error.\nThe present research demonstrated BE for isolated face images on blank backgrounds. This\nis unlike the majority of the studies, which found BE primarily for images containing scenes\nwith background texture but not in response to isolated objects removed from the\n1998; Mullally et al., 2012). It should be noted that stimuli used in the present work contained\ncropped faces, which are likely to elicit the sense of continuation beyond the picture\nboundaries. Thus, the perception of close-up cropped faces yields BE, as the perception of\nscenes does. Probably, cropped faces, similar to truncated scenes, elicit BE, which may be not\ntrue for other types of nonface objects. Further research is needed to examine whether this\neffect could be found for different nonface objects. The present findings advocate some\ncommon mechanisms in face and scene perception. As suggested by previous literature, the\nperception of faces involves configural processing, for example, a perception of the relations\nbetween the separate features rather than their processing in piecemeal or analytical way\n(Tanaka & Farah, 1993; Tanaka & Sengco, 1997). Configural processing of faces implies\nholistic processing (building a gestalt from separate individual features), as well as relational\narrangement of features (two eyes above the nose and a mouth below) and distances between\nthese features (Maurer, Grand, & Mondloch, 2002). Similarly, scene perception involves\nconfigural and relational processing (Chun, 2003). Furthermore, Gallagher et al. (2005)\nfound that the salience of the scene background increases the magnitude of BE. Indeed, a\nlarge body of evidence advocates that faces are extremely salient visual stimuli (Farah et al.,\n1998). Besides, it may be the case that face images elicited BE in the current study because the\nstimuli represented close-up views. As indicated by previous research (Intraub et al., 1992),\nthe degree of BE grows as increasingly close-up views are presented.\nFurthermore, the present findings provide a new evidence on asymmetry of BE. Consistent\nwith predictions, the results of the Study 1 demonstrated significantly more pronounced BE\nerrors for forehead- than for chin-cropped face area. The Recognition analysis showed that,\noverall, the measures of BE in the forehead-cropped faces exceeded the measures of BE in the\nchin-cropped faces. In support of the attentional explanation of the asymmetry in BE,\nthe present work found the asymmetry in attention in processing different facial parts. The\npresent findings are consistent with the literature on face processing, showing that greater\nattention is paid to the upper part of the face, and in particular, to eyes area (Janik et al.,\nYarbus, 1967). The eye-tracking analysis of the Encoding demonstrated that, in both memory\nconditions, for both forehead- and chin-cropped faces, most attention was paid to the ``eyes''\nand then ``mouth'' regions. Possibly, the presence of eyes may cue the attention to the\nneighboring ``border'' areas. The attentional cueing by the ``eyes'' may elicit BE in the\nupper part of the face and suppress it on the uncued side (see also Intraub et al., 2006).\nThe present findings that BE was greater for the upper than the lower part the face may be\nexplained by the increased attention to the ``eyes'' area. Surprisingly, while the ``eyes'' was the\nmost salient area in both conditions, the asymmetry of BE effect was evident only in the\nshort-term, but not in the long-term condition. It is possible that with a more delayed\nrecognition testing, the memory representation may include less visual information and\nmore information suggested by the mental schema. The enhanced visual attention in the\nupper part of a face may trigger the forehead-biased asymmetry of BE. However, visual\ndetail may be not remembered after a significant delay, thus the asymmetry of BE could be\nobserved only in the short-term, but not in the long-term condition. Also, the intriguing\ndifference in asymmetry of BE between the two memory conditions could be related to the\ndifference in attentional distribution during the Encoding. The analysis of oculomotor\nbehavior during the Encoding demonstrated that in the short-term condition, the ``eyes''\nareas in the forehead-cropped faces attracted more attention than in the chin-cropped faces.\nEven when excluding ``eyes,'' the upper ``border'' areas attracted more attention than the lower\n``border'' face areas. Besides, ``noncropped'' upper face areas attracted more attention than\n``noncropped'' lower face areas. Markedly, this increased attention to the upper face parts in\nthe forehead-cropped compared with chin-cropped faces was found only in the short-term\ncondition. This indicates that the differences in processing forehead- and chin-cropped faces\nbetween the two memory conditions already appeared during the Encoding. Overall, the earlier\ncomparisons between the two memory conditions suggest that the specific patterns of BE\nerrors, that is, forehead-biased asymmetry, revealed in memory (during the Recognition)\nwere consistent with the specific patterns of attention revealed during the perception\n(Encoding).\nBesides the attentional asymmetry in face processing, there are other possible explanations\nwhy such an asymmetry in BE in face processing may exist. One possibility could be that in\nour everyday life, we are more used to see forehead- than chin-cropped faces (e.g., people\nwearing hats); due to this experience, we may be better in mental continuation of the upper\nparts rather than lower parts of faces. This possibility could be explored by examining BE in\nface images of human-like animals (e.g., monkeys), who do not typically cover their heads.\nFurthermore, because previous research found that asymmetry in scene representation may\nbe caused by the anticipatory processing in a certain direction, and in particular, anticipated\nmotion (Courtney & Hubbard, 2004), it is possible that ``eyes'' may convey some sort of\nmotion information, causing BE. There is evidence that gaze direction may indicate\nanticipated action and intention (Klin, Jones, Schultz, & Volkmar, 2003). Additional\nresearch is needed to explore the possibility whether gaze direction (e.g., looking to the left\nor to the right side, looking up or down, closed or opened eyes) may influence the asymmetry\nof BE. Alternatively, a greater BE of the upper face part could be because the upper face area\nis greater in size than the lower one. Experiments, using artificially modified faces with\nequalized forehead and chin area, can address this possibility. Another possibility, which\nalso can be tested by using artificially modified face stimuli, is that the asymmetry of BE in\nfaces may be caused by the perceptual differences between the top of the head, which does not\ncontinue, and the chin area, which extends to the neck and below body parts. Further\nresearch is necessary to determine which explanation is best supported. Notably, because\nboth conditions used the same face stimuli, the found difference in the asymmetry of BE\nbetween the two conditions indicates that this asymmetry could not fully be due to a bias\ncaused by the characteristics of the stimuli. Irrespective of why such an asymmetry may\noccur, the present finding of asymmetry of BE in face images expands the existing\nevidence that the content of an image may influence BE. Furthermore, it contributes to\nunderstanding the roles of perception and memory in BE phenomenon.\nThe results of Study 2 showed that individual differences in imagery relate to BE. In\nparticular, object, but not spatial, imagery was associated with BE. These findings support\nour hypothesis that, depending on the content of the image (scene vs. isolated object or face),\nBE may relate not only to the strength of spatial imagery, as in Munger and Multhaup (2016)\nstudy, which used scenes, but also to the strength of object imagery, as in the present study,\nwhich used faces. Besides, the present results demonstrated that object imagery was positively\nassociated with vividness of imagined emotional expressions, which requires visualizing\npictorial perceptual details inside the empty face contour.\nEmotion vividness, similar to object imagery, tended to be positively associated with\nextension and negatively with restriction errors. The present results extend the research of\nMullally et al. (2012), who found that individual differences in spatial scene imagery vividness\nwas associated with BE for scenes, whereas the present findings show the association between\nobject imagery and BE for faces. In addition to measures of individual differences in imagery,\nthe current research included measures of emotional processing. Consistent with expectations\nand previous research (Blazhenkova & Kozhevnikov, 2010), emotional measures were\npositively related with object but not with spatial imagery. Similar to object imagery, the\nemotional measures tended to have positive relationship with BE errors. Intriguingly, the\nrelationships between the BE measures and individual differences in imagery were evident\nmostly in the long-term condition. Thus, the individuals with greater imagery abilities tended\nto have a greater BE. Probably, because imagery involves a top-down mental constructive\nprocessing (Ganis et al., 2004; Mechelli et al., 2004), these individuals were more likely to\nmake commission memory errors. The relationship between BE and imagery could manifest\nonly in the long-term condition because a delayed recognition testing task required less\nsensory information and more mental schema suggestions. Previous explanations of BE\nmechanisms indicated the involvement of mental imagery or mental visualization of\nnonvisible image parts, extending beyond the borders. Foley, Foley, Scheye, and Bonacci\n(2007) indicated the resemblance of BE errors to object completion errors, resulting from\nmental filling-in. Perceptual filling-in involves mental completion of visual image and the\nperception of visual features that are physically present only the surroundings (Komatsu,\n2006; Walls, 1954). It is phenomenologically experienced as a perception of the completed\ntextured and colorful objects (Kanizsa, 1979; Ramachandran & Gregory, 1991) and having\na coherent object representation (Shipley & Kellman, 1992; Yantis, 1995). However, previous\nresearch that showed that BE for scenes occurred regardless of whether a boundary cropped\nan object or not, ruled out the object completion explanation of BE (Intraub et al., 1992;\nindicated that imagination of background outward of central objects (visual-spatial\ninformation) resulted in BE, whereas imagination of the color within the central object\n(visual-object information) did not. In contrast to scene images, possibly, in case of face\ncropped images, the anticipatory representation of the cropped face part may cause the\nexpectation of a continuation of the face itself (a central cropped object), rather than a\nscene background outward the face, thus it may involve mental completion processes.\nResearch showed that eye scanpaths during visual imagery may reenact those of\nperception and reflect visual scene content (Brandt, & Stark, 1997; Laeng, & Teodorescu,\n2002; Mast, & Kosslyn, 2002). Thus, it is probable that the eye fixations in the cropped areas\nindicate that participants mentally visualized the missing facial part. As can be seen from the\neye-tracking encoding heat maps (Study 1), in the cropped areas, participants were mostly\nfocused inside the anticipated missing face part rather in the background of a scene outward\nthe face. As suggested by Gottesman and Intraub (2003), image boundaries can be considered a\npartial occluder of a larger scene that extends behind it. In case of amodal completion, the\ninformation about the occluded parts of an object is not supported by sensory modalities.\nHubbard et al. (2010) proposed that, although object completion is unlikely to be the cause of\nBE, BE might induce amodal continuation/completion of a cropped object, as in case of a\npartially occluded object (Hubbard et al., 2010).\nGottesman and Intraub (2002) proposed that amodal continuation of surfaces plays\nfundamental role in scene perception, especially for surfaces truncated by boundaries.\nThey claimed that spatial extrapolation does not require detailed and rich background but\nis just the suggestion of a continuity beyond the image boundaries. However, it is possible\nthat in case of close-up face images, BE may involve some sort of modal completion rather\nthan (or in addition to) amodal continuation. Nonspatial object imagery, which refers to\nvisualization of sensory surface properties such as color and texture, may be more involved in\nextrapolation of a cropped face image beyond the boundaries. Future efforts to establish the\nnature of mental extrapolation in processing of cropped face images could provide insight\nabout the characteristics of imagery involved in BE. The current study brings a question\nwhether the scene construction schema activated by a limited view is purely spatial and how it\ndepends on the content of the scene. While the existing neuroimaging research indicates that\nBE is supported by the scene-selective brain regions but not by those specific to object\nrecognition (Park et al., 2007), the present results raise a question whether BE of cropped\nface images may be underpinned by brain areas involved in visual-object processing. Further\nresearch is needed to examine the cognitive mechanisms and neural underpinnings of BE\nduring cropped face processing. The current literature indicates the existence of both visual-\nspatial and visual-object aspects in scene representations. A dual-path model of scene\nprocessing, based on the neurological ``what'' versus ``where'' distinction (Ungerleider &\nHaxby, 1994), dissociate between processing object and spatial information (Chun, 2003).\nSimilarly, Aguirre and D'Esposito (1997) distinguished between processing of large-scale\nenvironmental scene pictorial appearance (e.g., landmark knowledge) and spatial locations\nwithin a scene (e.g., survey knowledge), subserved by distinct ventral ``what'' and dorsal\n``where'' neural pathway. Although visual-spatial properties such as position were shown\nto be better encoded in scene representations than visual-object surface properties such as\ncolor (Aginsky & Tarr, 2000), object visualization may play a significant role in scene\nprocessing and BE, depending on scene content. The present work adds a new dimension\nto the research on BE and encourages the future investigation on individual differences in\nobject versus spatial imagery and emotion in relation to BE depending on stimuli type.\nFurthermore, the exploratory analyses of the effect of different emotions on BE revealed\nmixed findings. Faces with ``Angry'' and ``Happy'' emotions tended to evoke a greater\nboundary extrapolation. These results are inconsistent with previous research that showed\nno difference in BE for images with different emotional content (Candel et al., 2003, 2004).\nThey are only partially consistent with the research that demonstrated BE only for scenes\nwith positive but not negative emotional content (Me\n\u00b4 ne\nSuch a discrepancy in findings could be due to the nature of the stimuli that may mobilize\ndifferent cognitive mechanisms. In particular, Candel et al. as well as Safer et al. used neutral\nand aversive scenes that evoked highly unpleasant and arousing emotions, whereas the\npresent study used faces and the task that involved emotional recognition. Me\n\u00b4 ne\n\u00b4 trier et al.\nused dynamic stimuli representing human bodies cropped in the midthigh or knee level,\nwhereas the present study used static pictures representing cropped faces. The present\nfindings suggest that both positive and negative emotions may induce BE. Notably, both\nAngry and Happy emotions are categorized as ``aroused,'' while disgust and sad are ``not\naroused'' (Russel, 1980). Possibly, the level or arousal may affect the strength of BE.\nContrary to the suggestion that BE may be ``immune to emotional content because it\ninvolves early perceptual processes, rather than relatively late reconstructive memory\nconditions produced similar effects of emotion on BE. However, present results should be\ntreated with caution due to a small number of stimuli per Emotion. Future studies are needed\nto examine the effect of different emotions on spatial extrapolation in face stimuli.\nOne of the limitations of the present study was using only female faces as stimuli. Unlikely,\nbut possibly, the found results may not be observed for stimuli using faces of males. The\ninvestigation of this possibility requires an additional investigation. For the present data, the\ncomparison between males and females on BE and BR measures (mean Error Frequencies,\nVisit Duration, and Count) did not reveal any significant sex differences. Another limitation\nwas that presentation of Recognition answer options was not exactly the same for the short-\nterm (half increased from right to left, half increased from left to right) and the long-term\ncondition (all increased from right to left). This discrepancy was due to the fact that the long-\nterm condition was developed first. After the consideration of the long-term memory\nlimitations, the research design was improved in the short-term condition. Despite this\ndifference, the results indicate the evidence of BE for both orders of the Recognition choices.\nFurthermore, the present study did not use any deliberate imagery instructions, and during\nthe encoding, participants were prompted to focus on recognition of facial expressions and\nface memorization. As suggested by previous research, spontaneous imagery, in contrast to\ndeliberately induced imagery, may encourage source memory errors (Foley et al., 2010).\nPossibly, such an instruction amplified the found BE effect. Further research is needed to\nexamine the effects of imagery instructions on BE, manipulating not only deliberate versus\nspontaneous imagery but also object versus spatial imagery instructions.\nIn sum, the current work presented new evidence on BE in face processing. Furthermore, it\ndemonstrated that for short retention intervals, BE errors were more pronounced for\nforehead, than for chin face areas, which provided a new evidence of asymmetry in BE.\nMoreover, individual differences in emotional ability and object, but not spatial, imagery\nwere found to be positively associated with BE in face processing. This investigation raises a\nnumber of research questions and highlights future research directions, which could elucidate\nthe nature of BE phenomenon.\n"
}