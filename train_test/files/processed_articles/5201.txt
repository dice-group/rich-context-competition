{
    "abstract": "Abstract\nQualitative content analysis is commonly used for analyzing qualitative data. However, few articles have examined the\ntrustworthiness of its use in nursing science studies. The trustworthiness of qualitative content analysis is often presented by\nusing terms such as credibility, dependability, conformability, transferability, and authenticity. This article focuses on trustworthiness\nbased on a review of previous studies, our own experiences, and methodological textbooks. Trustworthiness was described\nfor the main qualitative content analysis phases from data collection to reporting of the results. We concluded that it is\nimportant to scrutinize the trustworthiness of every phase of the analysis process, including the preparation, organization,\nand reporting of results. Together, these phases should give a reader a clear indication of the overall trustworthiness of\nthe study. Based on our findings, we compiled a checklist for researchers attempting to improve the trustworthiness of a\ncontent analysis study. The discussion in this article helps to clarify how content analysis should be reported in a valid and\nunderstandable manner, which would be of particular benefit to reviewers of scientific articles. Furthermore, we discuss that\nit is often difficult to evaluate the trustworthiness of qualitative content analysis studies because of defective data collection\nmethod description and/or analysis description.\n",
    "reduced_content": "sgo.sagepub.com\nArticle\nAlthough qualitative content analysis is commonly used in\nnursing science research, the trustworthiness of its use has\nnot yet been systematically evaluated. There is an ongoing\ndemand for effective and straightforward strategies for eval-\nuating content analysis studies. A more focused discussion\nabout the quality of qualitative content analysis findings is\nalso needed, particularly as several articles have been pub-\nlished on the validity and reliability of quantitative content\nanalysis (Neuendorf, 2011; Potter & Levine-Donnerstein,\nanalysis. Whereas many standardized procedures are avail-\nable for performing quantitative content analysis (Baxter,\n2009), this is not the case for qualitative content analysis.\nQualitative content analysis is one of the several qualita-\ntive methods currently available for analyzing data and inter-\npreting its meaning (Schreier, 2012). As a research method,\nit represents a systematic and objective means of describing\nand quantifying phenomena (Downe-Wamboldt, 1992;\nSchreier, 2012). A prerequisite for successful content analy-\nsis is that data can be reduced to concepts that describe the\nresearch phenomenon (Cavanagh, 1997; Elo & Kyng\u00e4s,\ncepts, a model, conceptual system, or conceptual map (Elo &\nquestion specifies what to analyze and what to create (Elo &\nsis, the abstraction process is the stage during which con-\ncepts are created. Usually, some aspects of the process can be\nreadily described, but it also partially depends on the\nresearcher's insight or intuitive action, which may be very\ndifficult to describe to others (Elo & Kyng\u00e4s, 2008;\nGraneheim & Lundman, 2004). From the perspective of\nvalidity, it is important to report how the results were cre-\nated. Readers should be able to clearly follow the analysis\nand resulting conclusions (Schreier, 2012).\nQualitative content analysis can be used in either an\ninductive or a deductive way. Both inductive and deductive\ncontent analysis processes involve three main phases: prepa-\nration, organization, and reporting of results. The preparation\n1University of Oulu, Finland\n2Medical Research Center, Oulu University Hospital, Finland\n3National Institute of Health and Welfare, Oulu, Finland\nCorresponding Author:\nSatu Elo, Senior University Lecturer, Institute of Health Sciences, Medical\nResearch Center Oulu, Oulu University Hospital and University of Oulu,\nEmail: satu.elo@oulu.fi\nQualitative Content Analysis: A Focus on\nTrustworthiness\nSatu Elo1, Maria K\u00e4\u00e4ri\u00e4inen1,2, Outi Kanste3, Tarja P\u00f6lkki1, Kati\nUtriainen1, and Helvi Kyng\u00e4s1,2\n Keywords\nanalysis method, nursing methodology research, qualitative content analysis, qualitative research, rigor, trustworthiness,\nvalidity\n2 SAGE Open\nphase consists of collecting suitable data for content analy-\nsis, making sense of the data, and selecting the unit of analy-\nsis. In the inductive approach, the organization phase\nincludes open coding, creating categories, and abstraction\n(Elo & Kyng\u00e4s, 2008). In deductive content analysis, the\norganization phase involves categorization matrix develop-\nment, whereby all the data are reviewed for content and\ncoded for correspondence to or exemplification of the identi-\nfied categories (Polit & Beck, 2012). The categorization\nmatrix can be regarded as valid if the categories adequately\nrepresent the concepts, and from the viewpoint of validity,\nthe categorization matrix accurately captures what was\nintended (Schreier, 2012). In the reporting phase, results are\ndescribed by the content of the categories describing the phe-\nnomenon using a selected approach (either deductive or\ninductive).\nThere has been much debate about the most appropriate\nterms (rigor, validity, reliability, trustworthiness) for assess-\ning qualitative research validity (Koch & Harrington, 1998).\nCriteria for reliability and validity are used in both quantita-\ntive and qualitative studies when assessing the credibility\nRyan-Nicholls & Will, 2009). Such terms are mainly rooted\nin a positivist conception of research. According to Schreier\n(2012), there is no clear dividing line between qualitative\nand quantitative content analysis, and similar terms and cri-\nteria for reliability and validity are often used. Researchers\nhave mainly used qualitative criteria when evaluating aspects\nof validity in content analysis (Kyng\u00e4s et al., 2011). The\nmost widely used criteria for evaluating qualitative content\nanalysis are those developed by Lincoln and Guba (1985).\nThey used the term trustworthiness. The aim of trustworthi-\nness in a qualitative inquiry is to support the argument that\nthe inquiry's findings are \"worth paying attention to\"\n(Lincoln & Guba, 1985). This is especially important when\nusing inductive content analysis as categories are created\nfrom the raw data without a theory-based categorization\nmatrix. Thus, we decided to use such traditional qualitative\nresearch terms when identifying factors affecting the trust-\nworthiness of data collection, analysis, and presentation of\nthe results of content analysis.\nSeveral other trustworthiness evaluation criteria have\nbeen proposed for qualitative studies (Emden, Hancock,\nHowever, a common feature of these criteria is that they\naspire to support the trustworthiness by reporting the pro-\ncess of content analysis accurately. Lincoln and Guba (1985)\nhave proposed four alternatives for assessing the trustwor-\nthiness of qualitative research, that is, credibility, depend-\nability, conformability, and transferability. In 1994, the\nauthors added a fifth criterion referred to as authenticity.\nFrom the perspective of establishing credibility, researchers\nmust ensure that those participating in research are\nidentified and described accurately. Dependability refers to\nthe stability of data over time and under different condi-\ntions. Conformability refers to the objectivity, that is, the\npotential for congruence between two or more independent\npeople about the data's accuracy, relevance, or meaning.\nTransferability refers to the potential for extrapolation. It\nrelies on the reasoning that findings can be generalized or\ntransferred to other settings or groups. The last criterion,\nauthenticity, refers to the extent to which researchers, fairly\nand faithfully, show a range of realities (Lincoln & Guba,\nResearchers often struggle with problems that compro-\nmise the trustworthiness of qualitative research findings (de\nCasterl\u00e9, Gastmans, Bryon, & Denier, 2012). The aim of the\nstudy described in this article was to describe trustworthiness\nbased on the main qualitative content analysis phases, and to\ncompile a checklist for evaluating trustworthiness of content\nanalysis study. The primary research question was, \"What is\nessential for researchers attempting to improve the trustwor-\nthiness of a content analysis study in each phase?\" The\nknowledge presented was identified from a narrative litera-\nture review of earlier studies, our own experiences, and\nmethodological textbooks. A combined search of Medline\n(Ovid) and CINAHL (EBSCO) was conducted, using the fol-\nlowing key words: trustworthiness, rigor OR validity, AND\nqualitative content analysis. The following were used as\ninclusion criteria: methodological articles focused on quali-\ntative content analysis in the area of health sciences pub-\nlished in English and with no restrictions on year. The search\nidentified 12 methodological content analysis articles from\ndatabases and reference list checks (Cavanagh, 1997;\nRourke & Anderson, 2004; Vaismoradi, Bondas, & Turunen,\n2013). The reference list of selected papers was also checked,\nand qualitative research methodology textbooks were used\nwhen writing the synthesis of the review. The discussion in\nthis article helps to clarify how content analysis should be\nreported in a valid and understandable manner, which, we\nexpect, will be of particular benefit to reviewers of scientific\narticles.\nTrustworthiness in the Preparation\nPhase in Content Analysis Study\nBased on the results of the literature search, the main trust-\nworthiness issues in the preparation phases were identified\nas trustworthiness of the data collection method, sampling\nstrategy, and the selection of a suitable unit of analysis.\nBased on the findings, we have compiled a checklist for\nresearchers attempting to improve the trustworthiness of a\ncontent analysis study in each phase (Table 1).\nElo et al. 3\nData Collection Method\nDemonstration of the trustworthiness of data collection is\none aspect that supports a researcher's ultimate argument\nconcerning the trustworthiness of a study (Rourke &\nAnderson, 2004). Selection of the most appropriate method\nof data collection is essential for ensuring the credibility of\ncontent analysis (Graneheim & Lundman, 2004). Credibility\ndeals with the focus of the research and refers to the confi-\ndence in how well the data address the intended focus (Polit\n& Beck, 2012). Thus, the researcher should put a lot of\nthought into how to collect the most suitable data for content\nanalysis. The strategy to ensure trustworthiness of content\nanalysis starts by choosing the best data collection method to\nanswer the research questions of interest. In most studies\nwhere content analysis is used, the collected data are unstruc-\n1995b), gathered by methods such as interviews, observa-\ntions, diaries, other written documents, or a combination of\ndifferent methods. However, depending on the aim of the\nstudy, the collected data may be open and semi-structured. If\ninductive content analysis is used, it is important that the data\nare as unstructured as possible (Dey, 1993; Neuendorf,\nFrom the perspective of trustworthiness, a key question is,\n\"What is the relationship between prefiguration and the data\ncollection method, that is, should the researcher use descrip-\ntive or semi-structured questions?\" Nowadays, qualitative\ncontent analysis is most often applied to verbal data such as\ninterview transcripts (Schreier, 2012). With descriptive data\nTable 1. Checklist for Researchers Attempting to Improve the Trustworthiness of a Content Analysis Study.\nPhase of the content analysis\nstudy Questions to check\nPreparation phase Data collection method\n How do I collect the most suitable data for my content analysis?\n Is this method the best available to answer the target research question?\n Should I use either descriptive or semi-structured questions?\n Self-awareness: what are my skills as a researcher?\n How do I pre-test my data collection method?\nSampling strategy\n What is the best sampling method for my study?\n Who are the best informants for my study?\n What criteria should be used to select the participants?\n Is my sample appropriate?\n Is my data well saturated?\nSelecting the unit of analysis\n What is the unit of analysis?\n Is the unit of analysis too narrow or too broad?\nOrganization phase Categorization and abstraction\n How should the concepts or categories be created?\n Is there still too many concepts?\n Is there any overlap between categories?\nInterpretation\n What is the degree of interpretation in the analysis?\n How do I ensure that the data accurately represent the information that the participants provided?\nRepresentativeness\n How to I check the trustworthiness of the analysis process?\n How do I check the representativeness of the data as a whole?\nReporting phase Reporting results\n Are the results reported systematically and logically?\n How are connections between the data and results reported?\n Is the content and structure of concepts presented in a clear and understandable way?\n Can the reader evaluate the transferability of the results (are the data, sampling method, and\nparticipants described in a detailed manner)?\n Are quotations used systematically?\n How well do the categories cover the data?\n Are there similarities within and differences between categories?\n Is scientific language used to convey the results?\nReporting analysis process\n Is there a full description of the analysis process?\n Is the trustworthiness of the content analysis discussed based on some criteria?\n4 SAGE Open\ncollection, it can often be challenging to control the diversity of\nexperiences and prevent interviewer bias and the privileging of\none type of information or analytical perspective (Warr &\nPyett, 1999). For example, when using a descriptive question\nsuch as \"Could you please tell me, how do you take care of\nyourself?\" the researcher has to consider the aim of data collec-\ntion and try to extract data for that purpose. However, if the\nresearcher opts for a semi-structured data collection method,\nthey should be careful not to steer the participant's answers too\nmuch to obtain inductive data. It may be useful for the inter-\nview questions to be developed in association with a \"critical\nreference group\" (Pyett, 2003). Critical reference groups are\nused in participatory action research and is a generic term for\nthose the research and evaluation is intended primarily to ben-\nefit (Wadsworth, 1998). Subjecting the interview questions to\nevaluation by this kind of group may help to construct under-\nstandable questions that make better sense of the studied phe-\nnomenon by asking the \"right questions in the right way.\"\nFrom the viewpoint of credibility, self-awareness of the\nresearcher is essential (Koch, 1994). Pre-interviews may\nhelp to determine whether the interview questions are suit-\nable for obtaining rich data that answer the proposed research\nquestions. Interview tapes, videos, and transcribed text\nshould be examined carefully to critically assess the research-\ner's own actions For instance, questions should be asked\nsuch as \"Did I manipulate or lead the participant?\" and \"Did\nI ask too broad or structured questions?\" Such evaluation\nshould not only begin at the start of the study but also be sup-\nported by continuous reflection to ensure the trustworthiness\nof content analysis.\nTo manage the data, pre-testing of the analysis method is\nas important in qualitative as in quantitative research. When\nusing a deductive content analysis approach, the categoriza-\ntion matrix also needs to be pretested in a pilot phase\n(Schreier, 2012). This is essential, especially when two or\nmore researchers are involved in the coding. In trial coding,\nresearchers independently try out the coding of the newly\ndeveloped matrix (Schreier, 2012) and then discuss any\napparent difficulties in using the matrix (Kyng\u00e4s et. al.,\n2011) and the units of coding they have interpreted differ-\nently (Schreier, 2012). Based on their discussion, the catego-\nrization matrix is modified, if needed.\nSampling Strategy\nFrom the viewpoint of sampling strategy, it is essential to ask\nquestions such as the following: What is the best sampling\nmethod for my study? Who are the best informants for my\nstudy and what criteria to use for selecting the participants?\nIs my sample appropriate? Are my data well saturated?\nThoroughness as a criterion of validity refers to the adequacy\nof the data and also depends on sound sampling and satura-\ntion (Whittemore, Chase, & Mandle, 2001). It is important to\nconsider the sampling method used in qualitative studies\n(Creswell, 2013). Based on our research, the sampling\nmethod is rarely mentioned in qualitative content analysis\nstudies (Kyng\u00e4s et. al., 2011). In qualitative research, the\nsampling strategy is usually chosen based on the methodol-\nogy and topic, and not by the need for generalizability of the\nfindings (Higginbottom, 2004). Types of qualitative sam-\npling include convenience, purposive, theoretical, selective,\nwithin-case and snowball sampling (Creswell, 2013;\nsample must be appropriate and comprise participants who\nbest represent or have knowledge of the research topic.\nThe most commonly used method in content analysis\nstudies is purposive sampling (Kyng\u00e4s, Elo, P\u00f6lkki,\nK\u00e4\u00e4ri\u00e4inen, & Kanste, 2011): purposive sampling is suitable\nfor qualitative studies where the researcher is interested in\ninformants who have the best knowledge concerning the\nresearch topic. When using purposeful sampling, decisions\nneed to be made about who or what is sampled, what form\nthe sampling should take, and how many people or sites need\nto be sampled (Creswell, 2013). However, a disadvantage of\npurposive sampling is that it can be difficult for the reader to\njudge the trustworthiness of sampling if full details are not\nprovided. The researcher needs to determine which type of\npurposeful sampling would be best to use (Creswell, 2013),\nand a brief description of the sampling method should be\nprovided.\nDependability refers to the stability of data over time and\nunder different conditions. Therefore, it is important to state\nthe principles and criteria used to select participants and\ndetail the participants' main characteristics so that the trans-\nferability of the results to other contexts can be assessed\n(e.g., see Moretti et al., 2011). The main question is then,\n\"Would the findings of an inquiry be repeated if it were rep-\nlicated with the same or similar participants in the same con-\nAccording to Lincoln and Guba's (1985) criteria for estab-\nlishing credibility, researchers must ensure that those partici-\npating in research are identified and described accurately. To\ngather credible data, different sampling methods may be\nrequired in different studies.\nSelection of the most appropriate sample size is important\nfor ensuring the credibility of content analysis study\n(Graneheim & Lundman, 2004). Information on the sample\nsize is essential when evaluating whether the sample is\nappropriate. There is no commonly accepted sample size for\nqualitative studies because the optimal sample depends on\nthe purpose of the study, research questions, and richness of\nthe data. In qualitative content analysis, the homogeneity of\nthe study participants or differences expected between\ngroups are evaluated (Burmeister, 2012; Sandelowski,\n1995a). For example, a study on the well-being and the sup-\nportive physical environment characteristics of home-dwell-\ning elderly is likely to generate fairly heterogeneous data and\nmay need more participants than if restrictions are applied,\nfor example, studying only elderly aged above 85 years or\nthose living in rural areas.\nIt has been suggested that saturation of data may indicate\nthe optimal sample size (Guthrie et al., 2004; Sandelowski,\nElo et al. 5\n1995a). By definition, saturated data ensure replication in\ncategories, which in turn verifies and ensures comprehension\nand completeness (Morse, Barrett, Mayan, Olson, & Spiers,\n2002). If the saturation of data is incomplete, it may cause\nproblems in data analysis and prevent items being linked\ntogether (Cavanagh, 1997). Well-saturated data facilitates its\ncategorization and abstraction. It is easier to recognize when\nsaturation is achieved if data are at least preliminarily col-\nlected and analyzed at the same time (Guthrie et al., 2004;\nfirst collected and then analyzed later. We recommend that\npreliminary analysis should start, for example, after a few\ninterviews. When saturation is not achieved, it is often diffi-\ncult to group the data and create concepts (Elo & Kyng\u00e4s,\nventing a complete analysis and generating simplified results\nSelection of a Suitable Unit of Analysis\nThe success of data collection should be assessed in relation\nto the specific research questions and study aim. The prepa-\nration phase also involves the selection of a suitable unit of\nanalysis, which is also important for ensuring the credibility\nof content analysis. The meaning unit can, for example, be a\nletter, word, sentence portion of pages, or words (Robson,\n1993). Too broad a unit of analysis will be difficult to man-\nage and may have various meanings. Too narrow a meaning\nunit may result in fragmentation. The most suitable unit of\nanalysis will be sufficiently large to be considered as a whole\nbut small enough to be a relevant meaning unit during the\nanalysis process. It is important to fully describe the meaning\nunit when reporting the analysis process so that readers can\nevaluate the trustworthiness of the analysis (Graneheim &\nLundman, 2004). However, in previous scientific articles,\nthe unit of analysis has often been inadequately described,\nmaking it difficult to evaluate how successful was the mean-\nTrustworthiness of Organization Phase\nin Content Analysis Study\nAccording to Moretti et al. (2011), the advantage of qualita-\ntive research is the richness of the collected data and such\ndata need to be interpreted and coded in a valid and reliable\nway. In the following sections, we discuss trustworthiness\nissues associated with the organization phase. In this phase,\nit is essential to consider whether the categories are well cre-\nated, what the level of interpretation is, and how to check the\ntrustworthiness of the analysis.\nAs part of the organization phase, an explanation of how\nthe concepts or categories are created should be provided to\nindicate the trustworthiness of study. Describing the con-\ncepts and how they have been created can often be challeng-\ning, which may hinder a complete analysis, particularly if the\nresearcher has not abstracted the data, or too many different\ntypes of items have been grouped together (Dey, 1993;\nHickey & Kipping, 1996). In addition, a large number of\nconcepts usually indicates that the researcher has been unable\nto group the data, that is, the abstraction process is incom-\nplete, and categories may also overlap (Kyng\u00e4s et al., 2011).\nIn this case, the researcher must continue the grouping to\nidentify any similarities within and differences between\ncategories.\nAccording to Graneheim and Lundman (2004), an essen-\ntial consideration when discussing the trustworthiness of\nfindings from a qualitative content analysis is that there is\nalways some degree of interpretation when approaching a\ntext. All researchers have to consider how to confirm the\ncredibility and conformability of the organization phase.\nConformability of findings means that the data accurately\nrepresent the information that the participants provided and\nthe interpretations of those data are not invented by the\ninquirer (Polit & Beck, 2012). This is particularly important\nif the researcher decides to analyze the latent content (notic-\ning silence, sighs, laughter, posture etc.) in addition to mani-\nin over interpretation (Elo & Kyng\u00e4s, 2008). It is recom-\nmended that the analysis be performed by more than one per-\nson to increase the comprehensivity and provide sound\nHowever, high intercoder reliability (ICR) is required when\nmore than one coder is involved in deductive data analysis\n(Vaismoradi et al., 2013). Burla, Knierim, Barth, Duetz, and\nAbel (2008) have demonstrated how ICR assessment can be\nused to improve coding in qualitative content analysis. This\nis useful when using deductive content analysis, which is\nbased on a categorization matrix or coding scheme.\nHowever, there are no published recommendations on\nhow the trustworthiness should be checked if the inductive\ncontent analysis is conducted by two or more researcher. Our\nsuggestion is that one researcher is responsible for the analy-\nsis and others carefully follow-up on the whole analysis pro-\ncess and categorization. All the researchers should\nsubsequently get together and discuss any divergent opinions\nconcerning the categorization, like in the pilot phase men-\ntioned earlier. For example, in one of our studies, two\nresearch team members checked the adequacy of the analysis\nand asked for possible complements (Kyng\u00e4s et al., 2011).\nOne study (Kyng\u00e4s et al., 2011) has suggested that data\nare most often analyzed by one researcher, especially when\nusing inductive content analysis. In such a case, the credibil-\nity of the analysis can be confirmed by checking for the rep-\nresentativeness of the data as a whole (Thomas & Magilvy,\nresearcher cannot avoid the time-consuming work of return-\ning again and again to the data, to check whether the inter-\npretation is true to the data and the features identified are\ncorroborated by other interviews. Face validity has also been\nused to estimate the trustworthiness of studies (Cavanagh,\n6 SAGE Open\nIn this case, the results are presented to people familiar with\nthe research topic, who then evaluate whether the results\nmatch reality. If the deductive approach is used, double-cod-\ning often helps to assess the quality of categorization matrix.\nAccording to Schreier (2012), if the code definitions are\nclear and subcategories do not overlap, then two rounds of\nindependence coding should produce approximately the\nsame results.\nThe value of dialogue among co-researchers has often\nbeen highlighted and it has been suggested that the partici-\npant's recognition of the findings can also be used to indicate\nthe credibility or conformability (Graneheim & Lundman,\nthat this be undertaken with caution (Ryan-Nicholls & Will,\n2009). Some studies have used member checks, whereby\nparticipants check the research findings to make sure that\nthey are true to their experiences (Holloway & Wheeler,\nmember checks as a continuous process during data analysis\n(e.g., by asking participants about hypothetical situations), it\nhas largely been interpreted and used by researchers for veri-\nfication of the overall results with participants. Although it\nmay seem attractive to return the results to the original par-\nticipants for verification, it is not an established verification\nstrategy. Several methodologists have warned against basing\nverification on whether readers, participants, or potential\nusers of the research judge the analysis to be correct, stating\nthat it is actually more often a threat to validity (Morse et al.,\nnot always understand their own actions and motives,\nwhereas researchers have more capacity and academic obli-\ngation to apply critical understanding to accounts.\nReporting Phase From the Viewpoint of\nContent Analysis Trustworthiness\nWriting makes something disappear and then reappear in\nwords. This is not always easy to achieve with rich data sets,\nas encountered in nursing science. The problem with writing\nis that phenomena that may escape all representation need to\nbe accurately represented in words (van Manen, 2006)\nAccording to Holdford (2008), the analysis and reporting\ncomponent of content analysis should aim to make sense of\nthe findings for readers in a meaningful and useful way.\nHowever, little attention has been paid to the most important\nelement of qualitative studies: the presentation of findings in\nthe reports (Sandelowski & Leeman, 2011). In the next sec-\ntions, we discuss trustworthiness issues associated with the\nreporting results, methods, and analysis process.\nReporting Results\nReporting results of content analysis is particularly linked to\ntransferability, conformability, and credibility. Results\nshould be reported systematically and carefully, with particu-\nlar attention paid to how connections between the data and\nresults are reported. However, the reporting of results sys-\ntematically can often be challenging (Kyng\u00e4s et al., 2011).\nProblems with reporting results can be a consequence of\ndifficulties in describing the process of abstraction because it\nin part depends on the researcher's insight or intuitive action,\nwhich may be difficult to describe to others (Elo & Kyng\u00e4s,\nThe content and structure of concepts created by content\nanalysis should be presented in a clear and understandable\nway. It is often useful to provide a figure to give an overview\nof the whole result. The aim of the study dictates what\nresearch phenomena are conceptualized through the analysis\nprocess. However, conception may have different objectives.\nFor example, the aim of the study may be merely to identify\nconcepts. In contrast, if the aim is to construct a model, the\nresults should be presented as a model outlining the con-\ncepts, their hierarchy, and possible connections. Content\nanalysis per se does not include a technique to connect con-\nmain consideration is to ensure that the structure of results is\nequivalent and answers the aim and research questions.\nFrom the perspective of trustworthiness, the main ques-\ntion is, \"How can the reader evaluate the transferability of\nthe results?\" Transferability refers to the extent to which the\nfindings can be transferred to other settings or groups. (Koch,\nabout transferability, but it is ultimately down to the reader's\njudgment as to whether or not the reported results are trans-\nferable to another context (Graneheim & Lundman, 2004).\nAgain, this highlights the importance of ensuring high qual-\nity results and reporting of the analysis process. It is also\nvaluable to give clear descriptions of the culture, context,\nselection, and characteristics of participants. Trustworthiness\nis increased if the results are presented in a way that allows\nthe reader to look for alternative interpretations (Graneheim\nthat qualitative methods require sensitive interpretive skills\nand creative talents from the researcher. Thus, scientific writ-\ning is a skill that needs to be enhanced by writing and com-\nparing others' analysis results.\nIt has been argued that the use of quotations is necessary\nto indicate the trustworthiness of results (Polit & Beck, 2012;\nSandelowski, 1995a). Conformability refers to objectivity\nand implies that the data accurately represent the information\nthat the participants provided and interpretations of those\ndata are not invented by the inquirer. The findings must\nreflect the participants' voice and conditions of the inquiry,\nand not the researcher's biases, motivations, or perspectives\nson why authors often present representative quotations from\ntranscribed text (Graneheim & Lundman, 2004), particularly\nto show a connection between the data and results. For exam-\nple, each main concept should be linked to the data by a\nElo et al. 7\nquotation. Examples of quotations from as many participants\nas possible help confirm the connection between the results\nand data as well as the richness of data. However, the sys-\ntematic use of quotations needs careful attention. Ideally,\nquotations should be selected that are at least connected to all\nmain concepts and widely representative of the sample.\nHowever, there is a risk that quotations may be overused,\nthus weakening the analysis (Downe-Wamboldt, 1992;\nexample, if quotations are overused in the Results section,\nthe results of the analysis may be unclear.\nAccording to Hsieh and Shannon (2005), an important\nproblem is failure to develop a complete understanding of\nthe context, resulting in failure to identify the key categories.\nIn such a case, findings do not accurately represent the data.\nTo ensure the trustworthiness and especially credibility of\nthe results, it is important to evaluate how well categories\ncover the data and identify whether there are similarities\nwithin and differences between categories. In addition, fail-\nure to complete the analysis abstraction process may mean\nthat concepts are presented as results that are not mutually\nexclusive, leading to oversimplistic conclusions (Harwood\ninvolve the use of everyday expressions or repetition of\nrespondents' statements and/or their opinions rather than\nreporting the results of the analysis (Kyng\u00e4s et al., 2011).\nReporting the Analysis Process\nWithout a full description of the analysis and logical use of\nconcepts, it is impossible to evaluate how the results have\nbeen created and their trustworthiness (Guthrie et al., 2004).\nAn accurate description of the analysis and the relationship\nbetween the results and original data allow readers to draw\ntheir own conclusions regarding the trustworthiness of the\nresults. In nursing science, the number of methods concern-\ning content analysis published in books and scientific articles\nhas increased considerably over the last decade (Elo &\nto improvements in the quality of reports on the process of\ncontent analysis. More attention is now paid to descriptions\nof the analysis, results, and how to evaluate the trustworthi-\nness of studies. Consequently, this makes it easier for readers\nto evaluate the trustworthiness of studies.\nThe dependability of a study is high if another researcher\ncan readily follow the decision trail used by the initial\nresearcher (Thomas & Magilvy, 2011). Whittemore et al.\n(2001) have argued that vividness involves the presentation\nof rich, vivid, faithful, and artful descriptions that highlight\nthe salient themes in the data. The analysis process should be\nreported in an adequate manner regardless of the methods\nused to present the findings (see Moretti et al., 2011). Steps\nshould be taken to demonstrate credibility in research reports\nto ensure the trustworthiness of the content analysis.\nMonograph research reports facilitate detailed descriptions\nof the analysis process and the use of figures, tables, and\nattachments to explain the categorization process. Based on\nour experiences, evaluation of the trustworthiness of results\nas a reader can often be difficult because of insufficient\ndescription of the analysis process (Kyng\u00e4s et. al., 2011).\nJournal articles generally focus on the results rather than\ndescribing the content analysis process. All too often, the use\nof qualitative content analysis is only briefly mentioned in\nthe methodology section, making it hard for readers to evalu-\nate the process. A key question is, \"In what detail should\ntrustworthiness be presented in scientific articles?\"--partic-\nularly as word limits often apply.\nThe fact that pictures may convey results more clearly\nthan words should be borne in mind when reporting content\nanalysis findings. The use of figures can be highly effective\nwhen reporting content analysis findings, especially when\nexplaining the purpose and process of the analysis and struc-\nture of concepts. Very often, these aspects can be shown in\nthe same figure, for example, a diagram that illustrates the\nhierarchy of concepts or categories may also give an insight\ninto the analysis process (see, for example, Timlin, Riala, &\nKyng\u00e4s, 2013). After reporting the results, a discussion of\nthe trustworthiness of the analysis should be provided. It\nshould be based on a defined set of criteria that are followed\nlogically for each qualitative content analysis phase.\nDiscussion\nThe main purpose of this article was to discuss and highlight\nfactors affecting trustworthiness of qualitative content analy-\nsis studies. The literature review used here was not a system-\natic review, so there are some limitations. First, we recognize\nthat this is not a full description of trustworthiness and some\npoints may be missing. For example, the language restric-\ntions may have influenced the findings; research studies in\nother languages might have added new information to our\ndescription. Further studies are needed to systematically\nevaluate the reporting of content analysis in scientific jour-\nnals, that is, to examine what researchers have emphasized\nwhen reporting the trustworthiness of their qualitative con-\ntent analysis study, and how criteria of trustworthiness have\nbeen interpreted by those studies. This may help to develop a\nmore complete description of trustworthiness in qualitative\ncontent analysis. However, the present methodological arti-\ncle was written by several authors who have extensive expe-\nrience in using the content analysis method. In addition, the\nauthors' experience as researchers, teachers, and supervisors\nof master's and doctoral students lends weight to our\ndiscussion.\nHolloway and Wheeler (2010) have stated that research-\ners often have difficulty in agreeing on how to judge the\ntrustworthiness of their qualitative study. The aim of this\narticle was to identify factors affecting qualitative content\nanalysis trustworthiness from the viewpoint of data collec-\ntion and reporting of results. Qualitative researchers are\nadvised to be systematic and well organized to enhance the\n8 SAGE Open\ntrustworthiness of their study (Salda\u00f1a, 2011). According to\nSchreier (2012), content analysis is systematic because all\nrelevant material is taken into account, a sequence of steps is\nfollowed during the analysis, and the researcher has to check\nthe coding for consistency. The information presented here\nraises important issues about the use and development of\ncontent analysis. If the method is thoroughly documented for\nall phases of the process (preparation, organization, and\nreporting), all aspects of the trustworthiness criteria are\nincreased.\nBefore choosing an analysis method, the researcher\nshould select the most suitable method for answering the tar-\nget research question and consider whether the data richness\nis sufficient for using content analysis. Prior to using the\nmethod, the researcher should ask the question, \"Is this\nmethod the best available to answer the target research ques-\ntion?\" No analysis method is without drawbacks, but each\nmay be good for a certain purpose. It is essential for research-\ners to delineate the approach they are going to use to perform\ncontent analysis before beginning the data analysis because\nthe use of a robust analytic procedure will increase the trust-\nworthiness of the study (Hsieh & Shannon, 2005).\nQualitative content analysis is a popular method for ana-\nlyzing written material. This means that results spanning a\nwide range of qualities have been obtained using the method.\nContent analysis is a methodology that requires researchers\nwho use it to make a strong case for the trustworthiness of\ntheir data (Potter & Levine-Donnerstein, 1999; Sandelowski,\n1995a). Every finding should be as trustworthy as possible,\nand the study must be evaluated in relation to the procedures\nused to generate the findings (Graneheim & Lundman,\n2004). In many studies, content analysis has been used to\nanalyze answers to open-ended questions in questionnaires\n(Kyng\u00e4s et al., 2011). However, such answers are often so\nbrief that it is difficult to use content analysis effectively;\nreduction, grouping, and abstraction require rich data. In\naddition, trustworthiness has often been difficult to evaluate\nbecause articles have mainly focused on reporting the analy-\nsis of quantitative rather than qualitative data obtained in the\nstudy. Whether this affects the trustworthiness of the results\ncan only be speculated upon. However, if researchers use\ncontent analysis to analyze answers to open-ended questions,\nthey should provide an adequate description so that readers\nare able to readily evaluate its trustworthiness. Content anal-\nysis has also been commonly used in quantitative studies to\nanalyze answers to open-ended questions.\nThere is a need for a self-criticism and good analysis\nskills when conducting qualitative content analysis. Any\nqualitative analysis should include continuous reflection\nand self-criticism by the researcher (Pyett, 2003; Thomas\n& Magilvy, 2011) from the beginning of the study. The\nresearcher's individual attributes and perspectives can\nhave an important influence on the analysis process\n(Whittemore et al., 2001). It is possible to obtain simplistic\nresults using any method even when analysis skills are\nthe content analysis method can be as easy or as difficult as\nthe researcher allows. Many researchers still perceive it as\na simple method, and hence, it is widely used. However,\ninexperienced researchers may be unable to perform an\naccurate analysis because they do not have the knowledge\nand skills required. This can affect the authenticity (Lincoln\nrefers to the extent to which researchers fairly and faith-\nfully show a range of realities. A simplified result may be\nobtained if the researcher is unable to use and report the\nresults correctly.\nFurthermore, the reporting of the content analysis process\nshould be based on self-critical thinking at each phase of the\nanalysis. Whittemore et al. (2001) have argued that integrity\nis demonstrated by ongoing self-reflection and self-scrutiny\nto ensure that interpretations are valid and grounded in the\ndata. Not only should a sufficient description of the analysis\nbe provided to help validate the data, but the researcher\nshould also openly discuss the limitations of the study. We\nagree with Creswell's (2013) comment that validation in a\nqualitative study is an attempt to assess the accuracy of the\nfindings, as best described by the researcher and the partici-\npants. This means that any report of research is a representa-\ntion by the author. Discussion of the trustworthiness of a\nstudy should be based on a defined set of criteria that are\nfollowed logically. Although many criteria have been pro-\nposed to evaluate the trustworthiness of qualitative studies,\nthey have rarely been followed. It is recommended that\nauthors clearly define their validation terms (see example\nfrom Tucker, van Zandvoort, Burke, & Irwin, 2011) because\nthere are many types of qualitative validation terms in use,\nfor example, trustworthiness, verification, and authenticity\nConclusion\nThe trustworthiness of content analysis results depends on\nthe availability of rich, appropriate, and well-saturated data.\nTherefore, data collection, analysis, and result reporting go\nhand in hand. Improving the trustworthiness of content anal-\nysis begins with thorough preparation prior to the study and\nrequires advanced skills in data gathering, content analysis,\ntrustworthiness discussion, and result reporting. The trust-\nworthiness of data collection can be verified by providing\nprecise details of the sampling method and participants'\ndescriptions. Here, we showed how content analysis can be\nreported in a valid and understandable manner, which we\nanticipate will be of benefit to both writers and reviewers of\nscientific articles. As important qualitative research results\nare often reported as monograph reports, there is a need for\nfurther study to analyze published articles where content\nanalysis is used. This may produce further information that\nhelps content analysis writers present their studies in a more\neffective way.\nElo et al. 9\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nBaxter, J. (2009). Content analysis. In R. Kitchin & N. Thrift (Eds.),\nInternational encyclopedia of human geography (Vol. 1, pp.\nBurla, L., Knierim, B., Barth, K. L., Duetz, M., & Abel, T. (2008).\nFrom the text to coding: Intercoder reliability assessment in\nBurmeister, E. (2012). Sample size: How many is enough?\nCatanzaro, M. (1988). Using qualitative analytical techniques. In P.\nWoods & M. Catanzaro (Eds.), Nursing research: Theory and\nCavanagh, S. (1997). Content analysis: Concepts, methods and\napplications. Nurse Researcher, 4, 5-16.\nCreswell, J. W. (2013). Qualitative inquiry and research design:\nChoosing among five approaches. Thousand Oaks, CA: Sage.\nde Casterl\u00e9, B. D., Gastmans, C., Bryon, E., & Denier, Y. (2012).\nQUAGOL: A guide for qualitative data analysis. International\nDey, I. (1993). Qualitative data analysis: A user-friendly guide for\nsocial scientists. London, England: Routledge.\nDowne-Wamboldt, B. (1992). Content analysis: Method, applica-\ntions and issues. Health Care for Women International, 13,\nElo, S., & Kyng\u00e4s, H. (2008). The qualitative content analysis pro-\nEmden, C., Hancock, H., Schubert, S., & Darbyshire, P. (2001). A\nweb of intrigue: The search for quality in qualitative research.\nEmden, C., & Sandelowski, M. (1999). The good, the bad and the\nrelative, part two: Goodness and the criterion problem in quali-\ntative research. International Journal of Nursing Practice, 5,\nGraneheim, U. H., & Lundman, B. (2004). Qualitative content anal-\nysis in nursing research: Concepts, procedures and measures to\nGuthrie, J., Yongvanich, K., & Ricceri, F. (2004). Using content\nanalysis as a research method to inquire into intellectual capital\nHarwood, T. G., & Garry, T. (2003). An overview of content analy-\nHickey, G., & Kipping, E. (1996). A multi-stage approach to the\ncoding of data from open-ended questions. Nurse Researcher,\nHigginbottom, G. M. (2004). Sampling issues in qualitative\nHoldford, D. (2008). Content analysis methods for conducting\nresearch in social and administrative pharmacy. Research in\nHolloway, I., & Wheeler, S. (2010). Qualitative research in nursing\nand healthcare. Oxford, UK: Blackwell.\nHsieh, H.-F., & Shannon, S. (2005). Three approaches to qualitative\nKoch, T. (1994). Establishing rigour in qualitative research: The\nKoch, T., & Harrington, A. (1998). Reconceptualizing rigour: The\nKyng\u00e4s, H., Elo, S., P\u00f6lkki, T., K\u00e4\u00e4ri\u00e4inen, M., & Kanste, O.\n(2011). Sis\u00e4ll\u00f6nanalyysi suomalaisessa hoitotieteellisess\u00e4 tut-\nkimuksessa [The use of content analysis in Finnish nursing sci-\nLincoln, S. Y., & Guba, E. G. (1985). Naturalistic inquiry.\nThousand Oaks, CA: Sage.\nMoretti, F., van Vliet, L., Bensing, J., Deledda, G., Mazzi, M.,\nRimondini, M., . . . Fletcher, I. (2011). A standardized approach\nto qualitative content analysis of focus group discussions from\ndifferent countries. Patient Education & Counseling, 82, 420-\nMorgan, D. L. (1993). Qualitative content analysis: A guide to\nMorse, J. M., Barrett, M., Mayan, M., Olson, K., & Spiers, J. (2002).\nVerification strategies for establishing reliability and validity\nin qualitative research. International Journal of Qualitative\nNeuendorf, K. (2002). The content analysis guidebook. Thousand\nOaks, CA: Sage.\nNeuendorf, K. (2011). Content analysis--A methodological primer\nPolit, D. F., & Beck, C. T. (2012). Nursing research: Principles and\nmethods. Philadelphia, PA: Lippincott Williams & Wilkins.\nPotter, J. W., & Levine-Donnerstein, D. (1999). Rethinking valid-\nity and reliability in content analysis. Journal of Applied\nPyett, P. M. (2003). Validation of qualitative research in the \"real\nRobson, C. (1993). Real world research: A resource for social sci-\nentists and practitioner-researchers. Oxford, UK: Blackwell.\nRourke, L., & Anderson, T. (2004). Validity in quantitative content\nanalysis. Educational Technology Research & Development,\nRyan-Nicholls, K., & Will, C. (2009). Rigour in qualitative research:\nSalda\u00f1a, J. (2011). The coding manual for qualitative researchers.\nThousand Oaks, CA: Sage.\nSandelowski, M. (1995a). Qualitative analysis: What it is and how\nSandelowski, M. (1995b). Sample size in qualitative research.\nSandelowski, M. (2001). Real qualitative researchers do not count:\nThe use of numbers in qualitative research. Research in\nSandelowski, M., & Leeman, J. (2011). Writing usable qualita-\ntive health research findings. Qualitative Health Research, 22,\nSchreier, M. (2012). Qualitative content analysis in practice.\nThousand Oaks, CA: Sage.\nThomas, E., & Magilvy, J. K. (2011). Qualitative rigour or research\nvalidity in qualitative research. Journal for Specialists in\nTimlin, U., Riala, K., & Kyng\u00e4s, H. (2013). Adherence to treatment\namong adolescents in a psychiatric ward. Journal of Clinical\nTucker, P., van Zandvoort, M. M., Burke, S. M., & Irwin, J. D.\n(2011). The influence of parents and the home environment\non preschoolers' physical activity behaviours: A qualitative\ninvestigation of childcare providers' perspectives. BMC Public\nVaismoradi, M., Bondas, T., & Turunen, H. (2013). Content analy-\nsis and thematic analysis: Implications for conducting a quali-\ntative descriptive study. Journal of Nursing & Health Sciences,\nvan Manen, M. (2006). Writing qualitatively, or the demands of\nWadsworth, Y. (1998). What is participatory action research?\nAction research international (Paper 2). Retrieved from http://\nwww.aral.com.au/ari/p-ywadsworth98.html\nWarr, D., & Pyett, P. (1999). Difficult relations: Sex work, love and\nWeber, R. P. (1990). Basic content analysis. Newbury Park, CA:\nSage.\nWhittemore, R., Chase, S. K., & Mandle, C. L. (2001). Validity in\nAuthor Biographies\nSatu Elo, PhD, is a Senior university lecturer in University of Oulu,\nInstitute of Health Sciences. She is the second chairman of Finnish\nResearch Society of Nursing Science. Her research and teaching\narea are both focusing on elderly care environments, and research\nmethods especially from the viewpoint of theory development.\nMaria K\u00e4\u00e4ri\u00e4inen is Professor in University of Oulu, Institute of\nHealth Sciences. She is in charge of the Teacher Education Program\nin Health Sciences. Her research work has focused on two fields: 1)\nHealth promotive counselling of chronically ill patients, and people\nwith overweight, and 2) Effectiveness of education on the compe-\ntence of nursing staff, students and teachers.\nOuti Kanste (PhD) is a senior researcher at the National Institute\nfor Health and Welfare in Finland. She has also worked at the\nUniversity of Oulu and development projects of social and health\nservices in municipalities. Her research interests are nursing leader-\nship and management as well as service system and integration par-\nticularly in services for children, youth and families.\nTarja P\u00f6lkki, PhD, is an Adjunct Professor, senior researcher and\nlecturer in the Institute of Health Sciences, University of Oulu. Her\nresearch interests concern the methodological issues in nursing sci-\nence, and the well-being of children and their families focusing on\nthe aspects of pain assessment and non-pharmacological interven-\ntions, and promoting of child-and family-centeredness in nursing.\nKati Utriainen, PhD, is a Coordinator in University of Oulu,\nInstitute of Health Sciences, Finland. She is active in conducting\nand developing web-based learning of physicians specializing in\noccupational health care and education of their trainer doctors. She\nalso works as an occupational health nurse in the occupational\nhealth centre of the City of Oulu.\nHelvi Kyng\u00e4s is Professor in University of Oulu, Institute of Health\nSciences. She is also head on Nursing Science studies and head of\nPhD studies in Health Sciences. She is also working a Part-time\nChief Nursing Officer at Northern Ostrobothnia Hospital."
}