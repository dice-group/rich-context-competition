{
    "abstract": "Abstract\nCognitive interviewing is a common method used to evaluate survey questions. This study compares traditional cognitive\ninterviewing methods with crowdsourcing, or \"tapping into the collective intelligence of the public to complete a task.\"\nCrowdsourcing may provide researchers with access to a diverse pool of potential participants in a very timely and cost-\nefficient way. Exploratory work found that crowdsourcing participants, with self-administered data collection, may be a viable\nalternative, or addition, to traditional pretesting methods. Using three crowdsourcing designs (TryMyUI, Amazon Mechanical\nTurk, and Facebook), we compared the participant characteristics, costs, and quantity and quality of data with traditional\nlaboratory-based cognitive interviews. Results suggest that crowdsourcing and self-administered protocols may be a viable\nway to collect survey pretesting information, as participants were able to complete the tasks and provide useful information;\nhowever, complex tasks may require the skills of an interviewer to administer unscripted probes.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nIntroduction\nCognitive interviews are a qualitative method often used to\nunderstand cognitive processes in the pretesting of survey\nquestions (Willis, 1999). Interviews are typically conducted\nin a laboratory setting with members of a survey's target\npopulation. Participants are recruited via posted flyers, word\nof mouth, newspaper ads, and, more recently, classified ads\non online forums such as Craigslist (Murphy et al., 2007).\nCognitive interview participants travel to the laboratory to\ncomplete face-to-face interviews and respond to prompts\nabout their thought processes and reactions in answering\nquestions. Although cognitive interviewing has been\nemployed in different settings, such as with embedded prob-\ning in field interviews (Converse & Presser, 1986) and web\nprobing has been used to capture open-ended items to assess\nitem comprehension (Behr, Kaczmirek, et al., 2012), we\ncharacterize the traditional approach as involving the follow-\ning combination of recruitment, setting, and administration:\n(a) recruitment via traditional media source (listed above),\n(b) travel by the participant to a cognitive interviewing labo-\nratory or similar setting, and (c) interviewer-administered\nquestions with scripted and unscripted probes.\nAlthough results from cognitive interviews are not\nintended to be generalized, to best understand the appropri-\nateness of survey questions, it is important to ensure that the\nsample for a cognitive interviewing study reflects, to the\nextent possible, the characteristics of the target population\n(Miller, Chepp, Wilson, & Padilla, 2014). However, cogni-\ntive interviews usually rely on a variety of convenience sam-\npling methods (Baker et al., 2013). A researcher can only use\ncognitive interviews to better understand how questions may\nbe interpreted, whether they are interpreted as intended, or\nhow well they are suited for the intended target population.\nIdeally, cognitive interview study samples represent the\nkey subgroups of interest, that is, smokers or those who have\nmade specific types of purchases (Miller et al., 2014), but\ncommonly researchers must rely on participants who find the\nrecruiting advertisements and are willing and able to partici-\npate in the given location at the given time. Almost always,\ncognitive interview studies are limited to one or two geo-\ngraphic areas, making the issue of geographic differences in\nlanguage or how a given concept is experienced (e.g., par-\nticipants who live in a moderate climate will think about\n\"seasons\" in a different way than someone who experiences\nfour distinct seasons each year) of significant concern.\n1U.S. Bureau of Labor Statistics, Washington, DC, USA\n2RTI International, Chicago, IL, USA\n3RTI International, Research Triangle Park, NC, USA\nCorresponding Author:\nEmail: jmurphy@rti.org\nComparing Traditional and\nCrowdsourcing Methods for\nPretesting Survey Questions\nJennifer Edgar1, Joe Murphy2, and Michael Keating3\n Keywords\ncognitive interviewing, pretesting, crowdsourcing, survey methodology\n2 SAGE Open\nIn addition to the limitations noted above, resources can\noften limit the number of cognitive interviews that can be\nconducted in advance of a survey. Although not often\nacknowledged in cognitive interviewing reports, sample\nsizes are often dictated by the amount of funding available,\nrather than the ideal number of participants. Working with a\nsmaller than desired pool of participants can increase the\nchance that an important perspective is missed (Blair &\nConrad, 2011). Time and resource limitations can also restrict\nthe extent to which multiple rounds of testing can be con-\nducted. Ideally, when pretesting new survey questions or\nforms, cognitive interviewing should be done using an itera-\ntive design, with each round building on the findings of the\nAnother limitation of cognitive interviewing concerns\nconsistency of the approach used among those conducting\nthe cognitive interviews. Cognitive interviewers can use\nboth scripted and unscripted probes to delve into aspects of\nthe response process to better understand issues with the\nsurvey questions. Especially with unscripted probing,\ninterviewers can probe inconsistently across participants,\nand this has the potential to reduce the quantity of the data\ncollected, if not all incomplete responses are probed for\nexample. In addition to reducing the amount of informa-\ntion collected, thoughtful probing is required to ensure that\nhigh quality data are collected from all participants. If an\ninterviewer probes inconsistently, or does not probe suffi-\nciently to capture the true underlying response process, the\nquality of the data can be compromised. This impact on the\nquantity and quality of the data collected can be magnified\nin studies with more than one interviewer, when interview-\ners are probing inconsistently within and across their\ninterviews.\nAlthough cognitive interviewing has been a popular pre-\ntesting method for decades, another research method termed\n\"crowdsourcing\" has gained popularity in recent years due\nto the speed and low cost with which information can be\ncollected. Crowdsourcing has been described as the act of\n\"tapping into the collective intelligence of the public to\ncomplete a task\" (King, 2009) and involves recruiting a pool\nof willing individuals (the \"crowd\") through convenient\nInternet platforms to complete discrete tasks, typically in\ntheir own homes, at a low cost, and in a short amount of time\n(Hill, Dean, & Murphy, 2013). The vast reach, quick deploy-\nment, and inexpensiveness of crowdsourcing address sev-\neral of the limitations of traditional laboratory-based\ncognitive interviewing research. Using crowdsourcing, a\nresearcher has low-cost access to a geographically and\ndemographically diverse group of participants from whom\ninformation can be collected in a short time frame in a sys-\ntematic way. Specifically, crowdsourcing may be a way to\ncollect more information than what is traditionally collected\nin a cognitive laboratory setting, while overcoming some of\nthe associated challenges of obtaining a diverse sample with\nlimited resources.\nIn research, crowdsourcing has been used to generate\nideas, collect data, and analyze large volumes of unstruc-\ntured information (Keating, Rhodes, & Richards, 2013). In\nrecent years, crowdsourcing has been applied to parts of the\nsurvey process, including brainstorming methods used dur-\ning a survey design phase to drive a research agenda (Keating\n& Furberg, 2014), ad hoc field data collection for quality\ncontrol (Kim, Lieberman, & Dench, 2013), supplemental\ndata collection to add depth to survey data sets (Keating\net al., 2013), and data analysis to predict study outcomes\nBehrend, Sharek, Meade, and Weibe (2011) compared the\nquality of responses from a sample compiled from volun-\nteers recruited on the crowdsourcing platform Amazon\nMechanical Turk (MTurk) with those from a traditionally\nrecruited sample of university students. They found few dif-\nferences between the data collected from Mechanical Turk\nand students in terms of completion time or word count of\nresponses. Quite importantly, they found Mechanical Turk to\nbe much more efficient than the student sample, with hun-\ndreds of participants recruited in less than 48 hr and each\nrequiring less than US$1 a piece in reimbursement for com-\npletion of a 30-min survey. Researchers (Behrend et al.,\nTurk offers a more diverse sampling pool than college cam-\npuses, suggesting that, in some situations, crowdsourced\nsamples may be more representative than those obtained\nusing traditional methods.\nGiven its low cost, speed, and geographic reach, we view\ncrowdsourcing as a potentially viable option for recruiting\nparticipants and conducting survey pretesting activities.\nHere, we investigate the potential for crowdsourcing to\nrecruit participants and conduct cognitive interviews. With\ncrowdsourcing, there is typically no direct interaction\nbetween the person who designed the task (the researcher)\nand the person completing it (the participant). This means\nthat all pretesting activities must be self-administered.\nExploratory research using an online panel and self-\nadministered cognitive interviews found that the same types\nof information from a cognitive interview could be collected\nResponding to a series of questions on a web survey, partici-\npants were able to complete tasks commonly used in cogni-\ntive interviews: defining key terms and explaining their\nresponse process. Efficiencies were realized in the low cost\nand speed of data collection, and the demographic character-\nistics of the crowdsourced participants were generally com-\nparable with the laboratory participants. These preliminary\nstudies showed promise in the quantity and quality of the\ndata collected online, but there were mixed results in the\ncomparability of substantive findings when compared with\ntraditional cognitive interviews for some questions tested\n(Edgar, 2012). A recent comparison of cognitive interview\nand online probing response quality found that although tra-\nditional cognitive interviews were higher in quality, the large\nEdgar et al. 3\nnumber of online responses compensated for their overall\nlower quality by uncovering a variety of error types and\nIf a self-administered methodology can be used to collect\ninformation useful for the survey pretesting process, crowd-\nsourcing participants could address some of the limitations\nfound in traditional laboratory research. As noted above,\ncrowdsourcing is a cost-effective way to collect information\nfrom a large, geographically diverse group of participants\nquickly. This would allow researchers to test in a more dis-\npersed area and with more people than traditional methods\nwould allow. In addition, some crowdsourcing platforms\nallow for sampling quotas to be used, letting researchers\nspecify the key characteristics in advance of recruitment.\nFinally, using a self-administered methodology ensures that\nall participants are asked the same questions, and follow-up\nprobes, in the same order. Potential interviewer effects are\ncompletely eliminated.\nFor this application, we define web probing\u00adbased crowd-\nsourcing as involving the following combination of recruit-\nment, setting, and administration: (a) recruitment via an\nautomated Internet-based system, (b) no travel required or\ndirect interaction between the participant and researcher, and\n(c) self-administered questions with scripted follow-up\nprobes. Although crowdsourcing appears at face value to\noffer some advantages to traditional cognitive interviewing,\nthere is some limitation in using it for research purposes.\nFirst, all participant demographic information is self-\nreported, so the accuracy of sample composition depends on\nthe honesty of participants. To the extent that participants are\nmotivated to misrepresent themselves to meet the require-\nments for a given project, and that the inaccurate characteris-\ntics relate to the information being collected, the conclusions\nmay be unreliable.\nOther crowdsourcing limitations overlap with those\nfaced in traditional cognitive laboratory recruitment.\nParticipants for both methods must volunteer and agree to\nparticipate to receive monetary compensation in the form of\nan incentive, and their motivation to expend effort when\ncompleting the tasks or interview may be low. Whereas in\nthe laboratory, interviewers are able to gauge the level of\neffort a participant is extending for the interview and can\nchoose to exclude the data if they believe there are issues\nwith the data, there is no direct way to gauge the quality of\ncrowdsourced information.\nBuilding on past work, we consider three alternative\ncrowdsourcing platforms for cognitive interview recruit-\nment. We compare results from each platform with data col-\nlected using traditional laboratory methods in terms of\nparticipant characteristics, and cost. We also explore the\neffectiveness of collecting pretesting information using a\nself-administered form, evaluate the quantity and quality of\ndata collected, and consider the effects on the substantive\nconclusions that would be drawn using each method.\nMethod\nRecruitment\nFor this study, we recruited participants in four different\nways. The first approach (traditional) recruited local partici-\npants from four different areas across the United States. The\nprimary methods of recruitment were newspaper ads, flyers,\nword of mouth, and Craigslist ads. A total of 71 participants\nwere recruited. The researcher defined demographic targets,\nor quotas, for recruitment. Researchers aimed to fill each\nquota, but when not enough participants were available to fill\na quota, researchers generally accepted the next available\nparticipants. In addition to meeting the demographic require-\nments, participants had to be available to come to the labora-\ntory or interview site during the times specified by the\nresearcher--generally business hours on a weekday.\nInterviews were conducted in three states in the spring of\nWisconsin) and with 22 participants in Washington, D.C., in\ntions and follow-up probes, as well as additional unscripted\nprobes based on their answers. Interviews ranged in duration,\nbut averaged 45 min.\nThe second method of recruitment was an online, remote\nusability platform called TryMyUI. The service coordinates\nrecruitment of and payment to participants, administration of\ntasks, collection and storage of audio recording of the respon-\ndent's voice responses, and video recording of the respon-\ndent's computer screen during the task. TryMyUI has a\nvolunteer panel of participants with a range of known demo-\ngraphic characteristics and backgrounds, allowing for quotas\nto be set. TryMyUI typically asks panel members to evaluate\nwebsites and provide verbal feedback. Each participant\nreceives a rating from the researcher after completing the task,\nand only panel members with high ratings are eligible to com-\nplete future tasks. Because the members typically evaluate\nwebsites and other online materials, they are used to reacting\nto stimuli and expressing their opinions out loud, experience\nwhichhasthepotentialtohelpthemprovidein-depthresponses\nto cognitive interviewing questions asking about their thought\nprocesses or response strategy. Researchers are able to specify\nquotas (e.g., five males with high school education), and as\neligible participants come to the site, they may complete the\ntask, which is offered as a \"first come, first served\" process\nuntil the quotas have been filled. Depending on the specifica-\ntions of the quotas, TryMyUI may not be able to fill each quota\n(e.g., if there are not enough males aged 65 or older with a\ncollege degree), at which point the researcher is able to rede-\nfine the target quotas or increase the size of another group.\nAfter accepting the task, participants were directed to a\n43-question web survey, designed to mirror the interview pro-\ntocol used in the traditional cognitive interviews. TryMyUI\nlimits tasks to 20 min, and most participants were able to com-\nplete the web survey in that time. We collected data from 44\n4 SAGE Open\nself-administered interviews with participants recruited from\nThe third method used Amazon's Mechanical Turk, one\nof the most popular current crowdsourcing platforms.\nMechanical Turk allows for requesters to post human intel-\nligence tasks (HITs) and for workers to complete these HITs\nfor payment. At any given point, hundreds of thousands of\nHITs are available on the website, and there are currently\nare \"micro-tasks,\" taking only a few seconds or minutes to\ncomplete with a very small incentive, often less than 5 cents.\nMechanical Turk is increasingly being used by researchers to\nrecruit participants and to collect survey response data\nMechanical Turk HITs are typically very short, we included\nonly one section of the full web survey in each HIT; each\nMechanical Turk participant only answered 10 to 15 ques-\ntions, estimated to take 5 min to complete. We collected data\nfrom 1,019 self-administered interviews with participants\nrecruited from Mechanical Turk in September 2013.\nThe fourth method of recruitment used Facebook, a popu-\nlar social networking site with more than a billion active\nusers worldwide, including the majority of U.S. adults (Pew\nResearch Center, 2015). We purchased advertising space tar-\ngeted to specific demographic groups or those with particu-\nlar interests or \"likes.\" For this study, we used targeted ads to\nlet participants know they could receive a US$5 electronic\ngift card for completing a 10-min survey. Given that we were\ninterested in a general population for this cognitive interview\nproject, we presented the ads only to English-speaking\nFacebook users 18 years or older and living in the United\nStates. Participants who clicked on the ad were directed to an\nabbreviated version1 of the web survey, estimated to take 10\nmin to complete. We collected data from 60 self-adminis-\ntered interviews with participants recruited from Facebook\nTable 1 provides a summary of the study design.\nCollecting the Data\nA variety of questions were selected to test using the four\ndifferent methods. Questions were selected based on their\nlikely relevance to a variety of participants. They had all\nbeen used in prior pretesting studies, with relevant informa-\ntion about response processes and reactions able to be\ncollected.\nWe followed traditional procedures for the laboratory\ncognitive interviews (Willis, 2005). A single interviewer\nconducted all interviews, using a scripted protocol. In gen-\neral, participants first answered the survey question and then\nwere asked follow-up probes aimed at understanding their\nresponse process. After the scripted probes, the interviewer\nasked unscripted probes as necessary to obtain as complete\nan understanding as possible. We used both open-ended\nprobes (e.g., tell me what you thought about when you heard\nthe question) and closed-ended probes (e.g., are the follow-\ning items included in the category?). We audio recorded the\ninterviews and took notes during each interview.\nFor the three crowdsourcing designs, we developed web\nsurvey versions of the cognitive protocol for self-administra-\ntion. Follow-up probes, mirroring those used in traditional\ncognitive interviews, followed each survey question. For\nTryMyUI, a full-length web survey was administered. Two\nof the topics in this study (flu and smoking) were not included\nfor TryMyUI, but additional topics from the laboratory inter-\nviews were. We administered abbreviated versions of the\nweb survey to the Mechanical Turk and Facebook partici-\npants to align the estimated completion time with partici-\npants'expectations for a short task. For Mechanical Turk, we\ndeveloped three sub-surveys, each focusing on a single topic.\nFor Facebook, we only included the topics discussed in this\narticle in the web survey. Table 2 includes a list of the ques-\ntions administered by data collection mode.\nWithin the protocol, we included typical cognitive inter-\nviewing tasks to evaluate survey questions of different sub-\nstantive topics. There is variation in what a participant must\ndo for each type of task, with each expected to have a differ-\nent level of cognitive complexity. Reporting examples of a\ntype of item is likely to be less complex than articulating the\nmental calculations used to arrive at an estimate. In Table 3,\nfrom top to bottom, we list the tasks in order of complexity,\nranging from straightforward (e.g., provide examples of\nsportswear) to complex (e.g., explain response process in\ndetermining clothing expenditures).\nTable 1. Recruitment Method and Interview Mode Summary.\nTraditional (classified ads,\nword of mouth, etc.) TryMyUI Mechanical Turk Facebook\nInterview mode Face-to-face in\ncognitive laboratory\nWeb-based self-\ninterview with typed\nand audio responses\nWeb-based self-\ninterview with typed\nresponses\nWeb-based self-\ninterview with typed\nresponses\nAverage questions answered\nper respondent\nAverage length of interview\n(minutes)\naNot including unscripted probes.\nEdgar et al. 5\nTable 2. Questions Administered by Method.\nTraditional laboratory TryMyUI Facebook Mechanical Turk\nSportswear module\nQuestion Since the first of [reference month] have you or any\nmember of your household purchased any swimsuits or\nwarm-up or ski suits?\n Yes \n No \nProbe What types of items did you think of when you saw that\nquestion about swimsuits, or warm-up or ski suits?\nFlu shot module\nQuestion Have you had a flu shot this season? X X X\n Yes \n No \nProbe In your own words, how do you define the word\n\"season\" when you read the question above?\nSmoking module\nQuestion Have you smoked at least 100 cigarettes in your entire\nlife? [SELECT ONE]\n Yes \n No \nProbe How did you mentally calculate the answer to the last\nquestion?\nClothing module\nQuestion How much have you or any member of your household\nspent on clothing since the first of May?\n Probe 1 What did you think about when deciding how much\nyou've spent on clothing?\n Probe 2 What clothing items have you or any member of your\nhousehold bought since the first of October and how\nmuch did each cost? Please fill in up to 6 items (Example:\nDemographics module\nQ1 What is your gender? X X X X\n Male \n Female \nQ2 What is your age? X X X X\nQ3 What is the highest level of education you have\ncompleted?\n  Less than high school \n  High school degree \n  Some college \n  College degree \n  Some graduate school \n  Completed graduate school \nQ4 What is your approximate average annual household\nincome?\n6 SAGE Open\nTryMyUI participants answered the follow-up probes\n(e.g., what did you think of . . . ?) as verbal think-alouds\nrecorded by the computer and then transcribed. Mechanical\nTurk and Facebook participants provided typed answers to\nall questions via a web form.\nIn analyzing the data, we calculated the number of words\nprovided in each response by participants and the number of\nexamples provided in their answers. We also subjectively\nrated the relevance (yes or no) and usefulness of the response\n(on a 4-point scale, from completely unusable to complete).\nThe subjective metrics were blind coded by two researchers,\nand inter-rater reliability was calculated using Cohen's\nkappa. We made one round of revisions to the coding instruc-\ntions based on preliminary reliability results, and researchers\nrevised their codes as applicable. Each final kappa exceeded\n.70 for both subjective measures, suggesting a good level of\nagreement. To illustrate the coding process, we present sam-\nple responses and how we coded them in Table 4.\nResults\nRecruitment\nTo evaluate the extent to which the different methods resulted\nin samples mirroring the general population (the target popu-\nlation for the tested survey questions), we compared partici-\npant demographics with various benchmark data sources.\nAlthough the interviews were conducted over a wide time\nspan (laboratory participants as far back as 2006), a review\nof the selected benchmarks does not suggest substantial\nchanges over the course of our study period. Demographic\nTable 3. Tasks, Survey Questions, and Follow-Up Probes.\nTask Survey question Follow-up probe\nProvide examples\n(sportswear)\nSince the first of [reference month], have\nyou or any member of your household\npurchased any swimsuits or warm-up or ski\nsuits?\nWhat types of items did you think of when you saw\nthat question about swimsuits, or warm-up or ski\nsuits? Please list up to six items.\nDefine concept (flu\nseason)\nHave you had a flu shot this season? In your own words, how do you define the word\n\"season\" when you read the last question?\nExplain answer (smoking) Have you smoked at least 100 cigarettes in\nyour entire life?\nHow did you mentally calculate the answer to the last\nquestion?\nExplain response process\n(clothing)\nHow much have you or any member of your\nhousehold spent on clothing since the first\nof [reference month]?\nWhat did you think about when deciding how much\nyou've spent on clothing?\nTable 4. Sample Responses to Follow-Up Probes.\nTask\nFollow-up\nprobe\nSample\nresponse\nNumber of\nwords\nRelevant\nresponse?\nCompleteness\nrating\nProvide\nexamples\n(sportswear)\nWhat types of items did\nyou think of when you\nsaw that question about\nswimsuits, or warm-up\nor ski suits?\n\"One piece, bikinis, sweatpants\" 4 Yes Complete\n\"Comfort\" 1 No Completely\nunusable\nDefine concept\n(flu season)\nIn your own words, how\ndo you define the word\n\"season\" when you\nread the last question?\n\"The period between October\nand March\"\n6 Yes Complete\n\"It is fall, and this is the time of\nyear when we discuss such\nthings\"\ninformation\nExplain answer\n(smoking)\nHow did you mentally\ncalculate the answer to\nthe last question?\n\"I've never smoked in my life, so\nit was easy\"\nunusable\n\"Wouldn't need to\" 3 No Completely\nunusable\nExplain response\nprocess\n(clothing)\nWhat did you think about\nwhen deciding how\nmuch you've spent on\nclothing?\n\"I thought about what I bought,\nhow many of them, and the\nquality of them to estimate the\ncost\"\n19 Yes Mostly complete\n\"How much clothing was\nabsolutely needed\"\n6 No Completely\nunusable\nEdgar et al. 7\nquotas were requested from both TryMyUI and the lab meth-\nods, but given the nature of the convenience sampling (e.g.,\nsampling from the online panel), they were not always met.\nFor Mechanical Turk and Facebook recruitment, it was not\npossible to screen participants based on demographic charac-\nteristics and so we did not use quotas.\nIn Table 5, we present participant demographics and\nanswers to substantive questions by method. We did not con-\nduct tests for statistical significance given the nonprobability\nnature of the recruitment. By gender, all methods were com-\nparable with the benchmark, with the exception of the\nTryMyUI sample, which was 75% female. TryMyUI over-\nrepresented the 25- to 44-year-old age group compared with\nthe benchmark, and underrepresented the 63+ group.\nFacebook participants were overrepresented by 18- to\ntive to the benchmark. The laboratory and TryMyUI cases\nwere more concentrated in the Western United States com-\npared with the benchmark and included fewer states than\nMechanical Turk and Facebook. All methods provided sam-\nples that were more highly educated than the benchmark--\nthose with a high school degree or less education were\nunderrepresented compared with the benchmark. Participants\nby method generally matched the benchmark income\ndistribution.\nWe also compared participants with external benchmarks\nto ensure that we were not recruiting a biased sample for the\ncognitive interviews and that participants did not differ from\nnational averages on the substantive questions being studied\n(Table 5). For instance, one survey question asked whether\nthe participant had smoked at least 100 cigarettes in his or\nher life. Ideally, the percentage of participants who report yes\nwould be consistent across method, and similar to a national\nbenchmark. The laboratory participants were more likely to\nTable 5. Participant Demographic Characteristics by Method (and Ratio of Method to Benchmark).\nBenchmark Traditional Laboratory TryMyUI Mechanical Turk Facebook\nGendera\nAgea\nRegionb\nEducationb\nAnnual household incomeb\nPercentage who have smoked at least 100\ncigarettes in entire lifef\nAverage amount spent on clothing in last\naBenchmark: 2013 Estimates of the Resident Population by Single Year of Age (18+), U.S. Census Bureau, Population Division.\nbBenchmark: Adult civilian persons, 2013 Current Population Survey, Annual Social and Economic Supplement.\ncIncluding Washington, D.C.\ndCollected from 21 participants.\neCollected from 30 participants.\nfBenchmark: 2012 National Health Interview Survey.\ngCollected from 255 participants.\nhBenchmark: 2013 Consumer Expenditure Survey.\n8 SAGE Open\nhave not smoked 100 cigarettes compared with the bench-\nmark. Mechanical Turk and Facebook respondents were\ncomparable with the benchmark on this item. For clothing\nexpenditures, laboratory responses resulted in a higher\namount than the benchmark, and the crowdsourcing methods\nresulted in lower mean expenditures than the benchmark.\nAlthough differences may be a result of the small sample\nsize, they may also indicate that research samples are some-\nhow substantively different than the survey population of\ninterest.\nIn terms of cost, the primary and most directly compara-\nble cost is participant incentives. Across all methods, partici-\npant incentives were contingent on a completed interview or\nsurvey. For TryMyUI and Mechanical Turk, the incentives\nwere normed to payment expectations on the respective plat-\nforms. We provided laboratory participants with a US$40\nincentive for a 45-min average interview, TryMyUI partici-\npants received US$10 for a 20-min task, Mechanical Turk\nparticipants received US$0.75 for a 5-min task, and Facebook\nparticipants received US$5 for a 10-min task. Additional\nlaboratory costs included interviewer and recruiter labor and\ncheck processing fees for the laboratory. It is estimated that\nthe recruiter spent about 1 hr per interview to screen, sched-\nule, and confirm appointments. Each laboratory interview\nalso required about 1 hr of interviewer labor. The time spent\ndeveloping the web survey and setting up the study was rela-\ntively consistent across the three crowdsourcing modes.\nAdditional costs for the crowdsourcing designs included a\nUS$20 per-participant fee in addition to incentives for\nTryMyUI, a 10% fee on HITs in Mechanical Turk, and\nUS$300 for Facebook advertising, which was based on the\nnumber of clicks on each ad (i.e., pay-per-click). Only about\none person in 10 who clicked on a Facebook ad completed an\ninterview. Researcher time spent compiling and analyzing\nthe web survey data was consistent for each question regard-\nless of mode, with the exception of TryMyUI, which required\nindividual participant audio files to be transcribed before\nthey were analyzed. Table 6 provides a summary of incen-\ntives and costs by design.\nQuantity of Data\nCognitive interviews aim to elicit information from partici-\npants about their response processes to evaluate the ques-\ntions being tested. Generally, researchers find more reliance\nin the conclusions as the amount of information collected\nincreases. In the laboratory, interviewers are able to use vari-\nous methods, such as follow-up probes or encouraging facial\nexpressions, to elicit additional information from partici-\npants. When using self-administered modes, however, we\nrely on the participant to provide enough information to be\nuseful without the benefit of interviewer intervention. One\nway to evaluate the successfulness of self-administered\nmodes in eliciting information is to consider the number of\nwords that participants used in their responses.\nExamining the number of words in participants'responses,\nwe see in Table 7 that the number of words used by labora-\ntory participants increases with the complexity of the task,\nfrom a median of four for providing examples of sportswear\nto 32 for explaining their response process in reporting cloth-\ning expenditures. For the simpler tasks (providing examples\nof sportswear and defining flu season), word counts for the\ncrowdsourcing designs were comparable or even higher than\nin the laboratory, suggesting that the presence of the inter-\nviewer and ability to probe may not be as strong a benefit for\nthe simpler tasks as it is for the more complex. Because inter-\nviewers have the ability to follow up with additional probes\nin the laboratory, the success in obtaining a higher median\nTable 6. Participant Incentives and Other Costs by Design.\nTraditional (classified ads,\nword of mouth, etc.) TryMyUI Mechanical Turk Facebook\nIncentives per hour of\nparticipant time\nAdditional costs Interviewer labor,\nper incentive\nAdditional US$20 per-\nparticipant service\nfee\nservice fee\nadvertisements\nTable 7. Median Number of Words in Response to Follow-Up Probe by Design and Task.\nTask Laboratory TryMyUI Mechanical Turk Facebook\nDefine concept (flu season) 6 Not collected 10 4\naData captured after all probes.\nEdgar et al. 9\ncount of words for the more complex tasks is not surprising\nand illustrates the potential downside of self-administered\ncognitive interviews. In general, additional, unscripted\nprobes were only used for the response process explanation\n(clothing) tasks, as most laboratory participants completed\nthe other tasks without the need for additional probing, again\nsupporting the idea that interviewers may not be needed to\ncollect information on straightforward tasks. It should be\nnoted that our count included all words, not only relevant\nwords. A review of the data however suggests that the pro-\nportion of relevant words does not vary by total length of the\nresponses. Furthermore, we examine quality of the data in\nthe next section.\nQuality of Data\nBeyond the quantity of data obtained by the different methods,\nwe sought to compare the quality of responses obtained. The\nfirst dimension of quality we examined was relevance--that\nis, whether the response was appropriate given the topic of the\nfollow-up probe. As shown in Table 8, there was not a consis-\ntent pattern of relevant responses by task or design. For the\nmost straightforward task, providing examples of sportswear,\na majority of participants provided relevant responses, from a\nlow of 68% for TryMyUI to a high of 98% for Mechanical\nTurk. Although it may be surprising that 32% of TryMyUI\nparticipants provided irrelevant responses, it could be that the\ntask was in fact too straightforward, especially compared with\nthe website usability tasks they commonly complete. The\nirrelevant responses indicated a misunderstanding of the\ntask--it may be that participants assumed that because the\nquestion itself included examples (swimsuits, warm-up or ski\nsuits), the follow-up probe was asking for something else.\nIrrelevant responses to the sportswear item tended to focus on\nsituations when people used sportswear, or times when they\nhad bought sportswear, rather than example items.\nThe task of explaining \"flu season\" appears to have been\nconfusing for some participants. In the laboratory, 68% pro-\nvided a relevant response. Mechanical Turk's rate of relevant\nresponses was 86% and Facebook's was 80%. Irrelevant\nresponses to this item may have resulted from the fact that\npeople are generally not used to explaining what commonly\nused terms mean. The fact that laboratory participants\nprovided the lowest percentage of relevant responses may\nsuggest an interviewer effect; it is possible that participants\ndid not expect the interviewer to violate conversational\nnorms in asking something so straightforward. When asked\non a self-administered survey, however, participants appeared\nto take the task at face value and a higher percentage were\nable to provide a relevant response.\nAs opposed to the flu question, most participants provided\nrelevant responses for the smoking question--93% for the\nlaboratory and Mechanical Turk and 97% for Facebook. This\ntask asked participants a probe that likely made sense, espe-\ncially in the context of having just answered the survey ques-\ntion. The task of explaining the response process in\nresponding to the clothing item was most complicated,\nthough nearly all participants in the laboratory and TryMyUI\nprovided relevant responses (100% and 96%, respectively).\nEven when self-administered, a majority of participants were\nable to provide a relevant response--81% for Mechanical\nTurk and 80% for Facebook. In all, there was no clear evi-\ndence that the crowdsourcing designs or self-administration\nresulted in substantially lower levels of relevant answers col-\nlected than the laboratory; the quality of the information col-\nlected, in terms of relevance, was generally consistent across\nmodes.\nFrom Tables 7 and 8, we see that quantity in terms of\nword count and quality in terms of response relevance were\ngenerally comparable across designs, especially for the more\nstraightforward tasks included in the study. Differences\nbegin to emerge, however, as participants are asked to com-\nplete more complicated tasks such as explaining their\nresponse process, with quantity and quality being higher for\nthe laboratory. It may be that these more complicated tasks\nare where the presence of an interviewer, who can probe for\nmore information and provide clarification of the tasks, is\ncritical.\nTo delve further into the quality differences for the most\ncomplex task in this study, we considered the quality of\nresponses in terms of completeness. That is, beyond simply\nproviding a relevant response, was the response complete\nenough to be able to assess a participant's response strategy?\nWe coded the responses from the explaining response pro-\ncess task in terms of completeness using four categories, as\ndescribed in Table 9.\nTable 8. Percentage of Relevant Answers to Probes by Design and Task.\nTask\nPercentage\nLaboratory, interviewer administered TryMyUI Mechanical Turk Facebook\naData captured after all probes.\nUsing the completeness ratings, we assigned each\nresponse a \"grade,\" to allow for comparison of the results\nacross modes. An A indicates a high quality, complete\nanswer, and an F indicate a poor incomplete answer as\ndescribed in Table 9. Table 10 provides the distribution of the\nusefulness ratings to the clothing expenditure follow-up\nprobe. We focus on this single item as an example, because it\nwas included in all treatments with a very similar presenta-\ntion. We present the laboratory distribution both with just the\ninitial probe (\"how did you arrive at your answer?\") as well\nas the total response after any unscripted probing the inter-\nviewer added to clarify the response (average of 2.4\nunscripted probes per participant). For each design, we also\ncalculated an average \"grade\" indicating the overall quality\nof the responses in terms of completeness. As with the other\nquality ratings, this measure is subjective, but gives a sense\nof the overall success of each design in obtaining usable\ninformation for this complex measure. We calculated\n\"grades\" as follows: complete--no additional information\nneeded = 100/A, mostly complete = 85/B, some usable infor-\nmation = 75/C, completely unusable = 60/F.\nWe find that based on just a single, scripted, follow-up\nprobe, none of the methods was able to obtain useful\ninformation from a majority of participants. Examining the\nunusable responses, we see that many participants did not\nunderstand the task at hand even if their responses were rel-\nevant or on topic. Many discussed the reasons they buy\nclothing, personal situations, or the types of clothing they\nlike to buy rather than their thought process in determining\ntheir answer. Such misinterpretations of the task probably\ncould have been easily redirected by an interviewer with\nsome feedback and a follow-up probe. In fact, when the addi-\ntional, unscripted follow-up probes are factored in, the labo-\nratory interviewers are able to gain useful feedback from\nalmost all respondents; with all probes, the laboratory design\nreceives an A grade. With the single scripted probe, the labo-\nratory design receives a C grade.\nAmong crowdsourcing designs, we see TryMyUI as the\nmost effective (with a grade of C), followed by Mechanical\nTurk (D), and then Facebook (F). TryMyUI participants pro-\nvided the \"best\" answers to this probe among the crowdsourc-\ningmethods,perhapsaidedbyverbalratherthantypedresponse\nor their experience thinking out loud and explaining their reac-\ntions in past usability tasks. In essence, they are already trained\nfor this type of task. TryMyUI and Mechanical Turk partici-\npants are motivated to successfully complete the task because\nTable 9. Assessed Usefulness for Clothing Expenditures Probe Responses.\nAssessed usefulness Description Sample response\nComplete (A) Enough information to be able to code\nresponse strategy without probing\n\"How much have I spent on footwear? I bought two\npairs of shoes, and they were $50 a pair, so I came up\nMostly complete (B) Only a little probing would be required to be\nable to code response strategy\n\"I remember we went clothes shopping because we\nneeded some shorts. There were some good sales so\nwe bought a few more things. We are very happy with\nour purchases, it was out of need, so that is why we\nmade the purchases.\"\nSome usable information (C) Some information to identify a response\nstrategy, but considerable probing would be\nneeded to code a response strategy\n\"The amount of individual clothing items I bought\"\nCompletely unusable (F) No information that could be used to code a\nresponse strategy\n\"Quality, if I need it\"\n\"Our income\"\nTable 10. Usefulness of Clothing Expenditures Probe Responses by Design.\nLevel of usefulness Usefulness \"grade\"\nPercentage\nLaboratory, interviewer\nadministered\nMechanical\nTurk Facebook\nAll probes\nScripted\nprobe only\nAverage \"grade\" A C C D F\nthey have volunteered for the panel and know they will be\nevaluated by the researcher. Facebook participants, by com-\nparison, likely had the least motivation to provide useful infor-\nmation in the interviews, as they were not actively seeking\n\"work\" when they were approached to complete the task and\nhave nothing to gain personally beyond the incentive for\nparticipation.\nRegardless of the method, a majority of crowdsourced\nparticipants were not able to complete this complex task\nwithout the aid of an interviewer, whereas in the laboratory,\nthe interviewer was able to use probes to collect usable infor-\nmation from almost 90% of participants. This suggests that\ncrowdsourced, self-administered web surveys are better\nsuited for the simpler comprehension tasks but that the more\ncomplex cognitive interviewing tasks require an interviewer\nto appropriately guide the participant through the process.\nFinally, we were interested to learn whether crowdsourcing\nwould result in the same conclusions as the laboratory-based\ncognitive interviews. That is, would each design lead us to the\nsame understanding of respondent reactions or need for item\nrevision? For the four tasks included in this study, we assessed\nhow conclusions from the crowdsourcing designs compared\nwith the traditional design. For providing examples of sports-\nwear and defining flu season, we compared the percentage\naccurately comprehending the question based on their response\nto the probe. For the explanation of the answer to the smoking\nquestion probe and the type of response process for expendi-\ntures on clothing, we compared the percentage using a reason-\nable response strategy.2 Next, we assessed whether the results\nsuggest each crowdsourcing design supplied the same conclu-\nsion as was gained from the laboratory cognitive interviews. We\nconsidered the conclusions the same if the crowdsourcing met-\nric evaluated was 10 or fewer percentage points from the tradi-\ntional metric.Although this threshold is somewhat arbitrary, it is\na reasonable approach to compare the conclusions by design.\nTable 11 provides the results of these comparisons by task\nand design. If the laboratory design, with its known limita-\ntions, is used as the basis for comparison, all three crowd-\nsourced designs lead to the same conclusions for the simplest\nof the tasks, providing examples (sportswear). The same\nconclusion was also reached across methods for the explana-\ntion of answer (smoking). For concept definition (flu sea-\nson), Mechanical Turk led to a different conclusion than the\nlaboratory or Facebook results. As the most complicated\ntask, the conclusions reached for response process explana-\ntion (clothing) differed across all three modes. This inconsis-\ntency is critical because it suggests that researchers would\nreach a different conclusion if they used only one of these\ndesigns, and likely take a different action in terms of revising\nthe question, depending on the designs.\nDiscussion\nIn this study, we compared traditional laboratory-based cogni-\ntive interviewing with web-based self-administered interviews\nwith respondents recruited from three crowdsourcing plat-\nforms. These results suggest that crowdsourcing offers the\npotential to recruit a larger number of participants who are\nmore diverse geographically faster and at a lower cost than\ntraditional methods. Even for the crowdsourcing platforms\nthat did not allow the setting of quotas, similar representation\namong demographic subgroups was obtained, and all crowd-\nsourced samples were more geographically and demographi-\ncally diverse. The ability to collect information from a wider\nrange of participants may help researchers achieve more rep-\nresentative samples, and strengthen the results and recommen-\ndations. The inability to set--and achieve--quotas in some\ncrowdsourcing platforms is a limitation when the researcher\nneeds to control the distribution of participants along one or\nmore dimensions. In our case, strict quotas were not sought\nand we were able to achieve a fairly balanced sample without\nthe use of quotas.\nThe cost and time required to collect information required\nto evaluate questions is a common limitation for traditional\ncognitive interviewing. Crowdsourcing provides a promising\nalternative to overcome this obstacle by providing research-\ners a low-cost approach for quickly collecting data from a\nlarge number of participants. Eliminating the need for a\nrecruiter to screen, schedule, and remind participants saves\nconsiderable time, and in general participant incentives are\ngenerally lower when crowdsourcing than when using tradi-\ntional methods. There may be additional time on the back-\nend however, with self-administered interviews requiring a\nreview of the recording and/or open-ended answers to ana-\nlyze the data because there are no interviewer notes or\nsummaries.\nIn terms of data quantity and quality, for more straight-\nforward cognitive tasks, such as asking participants to\nprovide examples of sportswear, the crowdsourcing alter-\nnatives provided similar quality results. However, for\nmore complicated tasks, such as understanding a partici-\npant's response strategy, the quality of the information\ncollected using the self-administered crowdsourcing alter-\nnatives was inferior to the traditional methods. When a\npurpose of the interview is to capture a nuanced cognitive\nprocess such as response strategy, the presence of an inter-\nviewer administering unscripted probes can be crucial.\nFor instance, when using a single, scripted probe \"how did\nyou arrive at your answer?\" to follow up a question on\nclothing expenditures, most participants were unable to\nprovide obtain useful information, regardless of the\nmethod of recruitment or mode of administration, but\nadditional unscripted interviewer probes were successful.\nAlthough more specific scripted probes may be more\neffective than general ones, it may also be some tasks\n(e.g., straightforward comprehension tasks) are better\nsuited for crowdsourced, self-administered testing; more\ncomplex cognitive interviewing tasks require an inter-\nviewer to appropriately guide the participant through the\nprocess.\nOur research suggests that crowdsourcing can be a viable\n\"fit for purpose\" supplement to traditional cognitive inter-\nviewing, particularly when there may be regional differ-\nences or input from a large sample is needed quickly. It\nshould be noted that the Mechanical Turk platform, in par-\nticular, does not allow for the selection of specific sub-pop-\nulations for recruitment, which means it is most efficient for\ngeneral population studies. However, some services now\noffer, at an additional cost, Mechanical Turk samples pre-\nscreened for a variety of demographic characteristics\n(Fowler, Willis, Moser, Ferrer, & Berrigan, 2015).\nComparative analyses of the recruitment costs and quality\nwith specific sub-populations is a topic well suited for future\nstudy.\nFrom our findings, web self-administration appears to be\na viable cognitive interviewing mode for simple tasks.\nIncorporating crowdsourcing during the preliminary stages\nof cognitive interview studies may leverage the advantages\nof the approach, allowing researchers to gain insight into\nconcept comprehension by collecting information from large\nnumbers of participants. By collecting information from\nlarger number of participants, we are able to collect informa-\ntion that might be missed with traditional sample sizes.\nAlthough we found that lower quality data were collected\nfrom the crowdsourced platforms, it might be possible to\ncompensate for the quality by increasing the quantity or sup-\nplementing with traditional, in-depth, exploratory cognitive\ninterviews to provide deeper insight into the more compli-\ncated aspects of the question evaluation. Researchers should\ncarefully consider demographic characteristics and experi-\nences of participants and use traditional and crowdsourcing\nmethods in concert to optimize data quality and quantity.\nAlthough self-administered cognitive interviewing meth-\nodology has not, thus far, incorporated unscripted probes,\nweb survey technology makes adding a full spectrum of tai-\nlored probes based on a response possible. For example,\nresearchers could develop a web instrument that displays a\nprobe when the respondent answers a question too quickly or\nuses only a few words. Past research suggests that, although\nthe wording and format of probes must be carefully devel-\noped, asking such follow-up questions on web surveys can\nbe an effective method to collect in-depth information about\nthe respondent's response process (Behr, Bandilla,\nKaczmirek, & Braun, 2012; Behr, Kaczmirek, Bandilla, &\nBraun, 2012). The use of automated probes for web self-\nadministered cognitive interview is another topic worthy of\nfuture investigation.\nAdvantages of crowdsourcing are that it can be fast, inex-\npensive, and allow for greater geographic dispersion; can\nsometimes result in a more experienced audience (e.g.,\nTryMyUI); and can be used to target-specific groups (e.g.,\nFacebook). Disadvantages include the lack of unscripted or\nfollow-up probes and absence of an interviewer. Another dis-\nadvantage is potential panel bias. If panel members are dif-\nferent from the population in a systematic way, this could\nlead to bias. In particular, researchers have raised concerns\nwith \"professional respondents\" in Mechanical Turk who\nmay differ from the general population because of the large\nnumber of studies in which they participate (Marder, 2015).\nThere may be similar concerns with laboratory participants,\nwith some participating in many studies. Quantifying and\ndetermining the effect of \"professional respondents\" on the\noutcomes of cognitive interview research is another area ripe\nfor future investigation.\nConsidering these advantages and disadvantages, crowd-\nsourcing seems to be a promising addition to traditional pretest-\ning methodologies. Figure 1 highlights three potential project\nexamples. For projects that have the time and budget for tradi-\ntional cognitive interviews, crowdsourcing can be used as a\nfirst step in a pretesting process. Researchers can integrate a\nvariety of comprehension, paraphrasing, confidence, recall,\nspecific, and general probes in a web survey instrument (Willis,\nTable 11. Comparison of Conclusions From Crowdsourcing and Laboratory Cognitive Interviewing.\nPercentage\n\nLaboratory, interviewer\nadministered TryMyUI Mechanical Turk Facebook\nProvide examples (sportswear)\n Same conclusion as the laboratory? Yes Yes Yes\nDefine concept (flu season) Not collected \n Same conclusion as the laboratory? No Yes\nExplain answer (smoking) Not collected \n Same conclusion as the laboratory? Yes Yes\nExplain response process (clothing)\n Same conclusion as the laboratory? No No No\n2005). The breadth of respondents in a crowdsourcing process,\nnumbering in the hundreds or even thousands, allows research-\ners to identify potentially glaring issues with their question-\nnaire before they move into a cognitive interview that focuses\non depth. If the project has additional time and budget, then a\nsecond round of crowdsourcing could be implemented after the\ncognitive interviews, to serve as a final pretest. For projects that\ndo not have the time or the budget for traditional cognitive\ninterviews, crowdsourcing may serve as a quick pretesting\nmethod, allowing researchers to identify and resolve some of\nthe issues surrounding their questionnaire that otherwise might\nnot be found.\nMoving forward, further evaluation of the use of crowd-\nsourcing as a recruitment method and the effectiveness of\nself-administered surveys to collect cognitive interviewing\ntype information is needed. In addition, more research is\nneeded to investigate other crowdsourcing alternatives\n(e.g., Google Consumer Surveys, Promoted Tweets, online\nsurvey panels) and to determine more precisely where and\nwhen the need for an interviewer is necessary.\nDeclaration of Conflicting Interests\nThe author(s) declared the following potential conflicts of inter-\nest with respect to the research, authorship, and/or publication of\nthis article: Opinions expressed in this paper are those of the\nauthors and do not reflect official policy of the U.S. Bureau of\nLabor Statistics.\nFunding\nThe author(s) disclosed receipt of the following financial support for\nthe research, authorship, and/or publication of this article: Cognitive\nlaboratory and TryMyUI interviews were funded by the U.S. Bureau\nof Labor Statistics. Mechanical Turk and Facebook interviews were\nfunded by RTI International The authors received no additional\nfinancial support for the research and/or authorship of this article.\nNotes\n1. Laboratory and TryMyUI participants completed additional\nsubstantive topics not discussed in this study. Mechanical Turk\nand Facebook participants only completed the topics described\nhere.\n2. For each of these questions, researchers identified the response\nstrategy as most likely leading to an accurate response. Other\nstrategies were likely to lead to an inconsistent response if it\nwere used by all respondents.\nReferences\nBaker, R., Brick, J. M., Bates, N. A., Battaglia, M., Couper, M. P.,\nDever, J. A., . . . Tourangeau, R. (2013). Summary report of\nthe AAPOR task force on non-probability sampling. Journal of\nBehr, D., Bandilla, W., Kaczmirek, L., & Braun, M. (2012).\nCognitive probes in Web surveys: On the effect of different\ntext box size and probing exposure on response quality. Social\nBehr, D., Kaczmirek, L., Bandilla, W., & Braun, M. (2012). Asking\nprobing questions in web surveys: Which factors have an\nimpact on the quality of responses? Social Science Computer\nBehrend, T. S., Sharek, D. J., Meade, A. W., & Weibe, E. N. (2011).\nThe viability of Crowdsourcing for survey research. Behavior\nBlair, J., & Conrad, F. G. (2011). Sample size for cognitive inter-\nChristenson, D. P., & Glick, D. M. (2013). Crowdsourcing panel\nstudies and real-time experiments in MTurk. The Political\nChunara, R., Chhaya, V., Bane, S., Mekaru, S. R., Chan, E. H.,\nFreifeld, C. C., & Brownstein, J. S. (2012). Online reporting\nfor malaria surveillance using micro-monetary incentives, in\nConverse, J. M., & Presser, S. (1986). Survey questions:\nHandcrafting the standardized survey questionnaire. Newbury\nPark, CA: Sage.\nFigure 1. Pretesting flows that include crowdsourcing for two example projects.\nEdgar, J. (2012, May). Cognitive interviews without the cogni-\ntive interviewer? Paper read at 67th Annual Conference of the\nAmerican Association for Public Opinion Research, Orlando, FL.\nEdgar, J. (2013, May). Self-administered cognitive interview-\ning. Paper read at 68th Annual Conference of the American\nAssociation for Public Opinion Research, Boston, MA.\nErdman, C., & Bates, N. A. (2013, November). The U.S. Census\nBureau Mail Return Rate Challenge: Crowdsourcing to develop\na hard-to-count score. Paper read at Federal Committee on\nStatistical Methodology Research Conference, Washington, DC.\nFowler, S., Willis, G., Moser, R., Ferrer, R., & Berrigan, D. (2015).\nUse of Amazon MTurk online marketplace for questionnaire\ntesting and experimental analysis of survey features. Proceedings\nof the 2015 Federal Committee on Statistical Methodology\nResearch Conference, Washington, DC. Retrieved from http://\nHill, C. A., Dean, E. F., & Murphy, J. J. (2013). Social media, soci-\nality, and survey research. Hoboken, NJ: John Wiley.\nKeating, M. D., & Furberg, R. D. (2014). A methodological frame-\nwork for Crowdsourcing in research. Proceedings of the 2013\nFederal Committee on Statistical Methodology Research\nConference, Washington, DC. Retrieved from https://fcsm.\nKeating, M. D., Rhodes, B. B., & Richards, A. K. (2013, November).\nApplying Crowdsourcing methods in social science research.\nPaper read at Federal CASIC Workshops, Washington, DC.\nKim, A. E., Lieberman, A. J., & Dench, D. L. (2013). Case study\ncomparing data collected via Crowdsourcing vs. Trained Data\nCollectors. Proceedings of the 2013 Federal Committee on\nStatistical Methodology Research Conference, Washington,\nDC. Retrieved from https://fcsm.sites.usa.gov/files/2014/05/\nKing, S. (2009). Using social media and crowd-sourcing for\nquick and simple market research. Retrieved from http://\nmoney.usnews.com/money/blogs/outside-voices-small-busi-\nquick-and-simple-market-research\nMarder, J. (2015). The Internet's hidden science factory. Retrieved\nfrom http://www.pbs.org/newshour/updates/inside-amazons-\nhidden-science-factory\nMeitinger, K., & Behr, D. (2016). Comparing cognitive interview-\ning and online probing: Do they find similar results? Field\nMiller, K., Chepp, V., Wilson, S., & Padilla, J. L. (2014). Cognitive\ninterviewing methodology. Hoboken, NJ: John Wiley.\nMurphy, J. J., Sha, M., Flanigan, T. S., Dean, E. F., Morton, J. E.,\nSnodgrass, J. A., & Ruppenkamp, J. W. (2007, November).\nUsing craigslist to recruit cognitive interview respondents.\nPaper read at Midwest Association for Public Opinion\nResearch, Chicago, IL.\nPaolacci, G., & Chandler, J. (2014). Inside the Turk: Understanding\nMechanical Turk as a participant pool. Current Directions in\nPew Research Center. (2015). Demographics of key social net-\nworking platforms. Retrieved from http://www.pewinternet.\nWillis, G. B. (1999, August). Cognitive interviewing: A \"how\nto\" guide. Paper read at Annual Meeting of the American\nStatistical Association, at Research Triangle Park, Research\nTriangle Institute, NC.\nWillis, G. B. (2005). Cognitive interviewing: A tool for improving\nquestionnaire design. London, England: Sage.\nAuthor Biographies\nJennifer Edgar is director of the Behavioral Science Research\nCenter in the Office of Survey Methods Research at the U.S.\nBureau of Labor Statistics. Her research focuses on context\neffects, qualitative research methods, cognitive sources of mea-\nsurement error, questionnaire design, and interviewer training\nand evaluation.\nJoe Murphy is a senior survey methodologist at RTI International.\nHis research focuses on new technologies to improve data quality,\ntimeliness, and cost efficiency.\nMichael Keating is a survey research scientist at RTI International.\nHis research focuses on new technologies for data collection and\ninnovation management."
}