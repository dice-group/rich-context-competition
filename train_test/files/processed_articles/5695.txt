{
    "abstract": "Abstract\nThis article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemol-\nogies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering para-\ndigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare `the end of\ntheory', the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and\ncomputational social sciences that propose radically different ways to make sense of culture, history, economy and\nsociety. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many\ninstances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on\nthe epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the\nrapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it\nis contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually\nnuanced epistemology.\n",
    "reduced_content": "Original Research Article\nBig Data, new epistemologies and\nparadigm shifts\nRob Kitchin\n Keywords\nBig Data, data analytics, epistemology, paradigms, end of theory, data-driven science, digital humanities, computational\nsocial sciences\nIntroduction\nRevolutions in science have often been preceded by\nrevolutions in measurement. Sinan Aral (cited in\nBig Data creates a radical shift in how we think about\nresearch . . .. [It offers] a profound change at the levels\nof epistemology and ethics. Big Data reframes key\nquestions about the constitution of knowledge, the pro-\ncesses of research, how we should engage with informa-\ntion, and the nature and the categorization of\nreality . . . Big Data stakes out new terrains of objects,\nmethods of knowing, and definitions of social life.\nAs with many rapidly emerging concepts, Big Data has\nbeen variously defined and operationalized, ranging\nfrom trite proclamations that Big Data consists of data-\nsets too large to fit in an Excel spreadsheet or be stored\non a single machine (Strom, 2012) to more\nsophisticated ontological assessments that tease out its\ninherent characteristics (boyd and Crawford, 2012;\nMayer-Schonberger and Cukier, 2013). Drawing on\nan extensive engagement with the literature, Kitchin\n. huge in volume, consisting of terabytes or petabytes\nof data;\n. high in velocity, being created in or near real-time;\n. diverse in variety, being structured and unstructured\nin nature;\n. exhaustive in scope, striving to capture entire popu-\nlations or systems (n \u00bc all);\nNational Institute for Regional and Spatial Analysis, National University of\nIreland Maynooth, County Kildare, Ireland\nCorresponding author:\nRob Kitchin, National Institute for Regional and Spatial Analysis, National\nUniversity of Ireland Maynooth, County Kildare, Ireland.\nEmail: Rob.Kitchin@nuim.ie\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY-NC: This article is distributed under the terms of the Creative Commons Attribution-NonCommercial\n3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use, reproduction and distribution\nof the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages (http://\nwww.uk.sagepub.com/aboutus/openaccess.htm).\n. fine-grained in resolution and uniquely indexical in\nidentification;\n. relational in nature, containing common fields that\nenable the conjoining of different data sets;\n. flexible, holding the traits of extensionality (can add\nnew fields easily) and scaleability (can expand in size\nrapidly). (see boyd and Crawford, 2012; Dodge and\nMayer-Schonberger and Cukier, 2013; Zikopoulos\nIn other words, Big Data is not simply denoted by\nvolume. Indeed, industry, government and academia\nhave long produced massive data sets \u00ad for example,\nnational censuses. However, given the costs and diffi-\nculties of generating, processing, analysing and storing\nsuch datasets, these data have been produced in tightly\ncontrolled ways using sampling techniques that limit\ntheir scope, temporality and size (Miller, 2010). To\nmake the exercise of compiling census data manageable\nthey have been produced once every five or 10 years,\nasking just 30 to 40 questions, and their outputs are\nusually quite coarse in resolution (e.g. local areas or\ncounties rather than individuals and households).\nMoreover, the methods used to generate them are\nquite inflexible (for example, once a census is set and\nis being administered it is impossible to tweak or add/\nremove questions). Whereas the census seeks to be\nexhaustive, enumerating all people living in a country,\nmost surveys and other forms of data generation are\nsamples, seeking to be representative of a population.\nIn contrast, Big Data is characterized by being\ngenerated continuously, seeking to be exhaustive and\nfine-grained in scope, and flexible and scalable in its\nproduction. Examples of the production of such data\ninclude: digital CCTV; the recording of retail pur-\nchases; digital devices that record and communicate\nthe history of their own use (e.g. mobile phones); the\nlogging of transactions and interactions across digital\nnetworks (e.g. email or online banking); clickstream\ndata that record navigation through a website or app;\nmeasurements from sensors embedded into objects or\nenvironments; the scanning of machine-readable\nobjects such as travel passes or barcodes; and social\nmedia postings (Kitchin, 2014). These are producing\nmassive, dynamic flows of diverse, fine-grained,\nrelational data. For example, in 2012 Wal-Mart was\ngenerating more than 2.5 petabytes (250 bytes) of data\nrelating to more than 1 million customer transactions\nevery hour (Open Data Center Alliance, 2012) and\nFacebook reported that it was processing 2.5 billion\npieces of content (links, comments, etc.), 2.7 billion\n`Like' actions and 300 million photo uploads\nper day (Constine, 2012). Handling and analysing\nsuch data is a very different proposition to dealing\nwith a census every 10 years or a survey of a few hun-\ndred respondents.\nWhilst the production of such Big Data has existed\nin some domains, such as remote sensing, weather pre-\ndiction, and financial markets, for some time, a number\nof technological developments, such as ubiquitous com-\nputing, widespread internet working, and new database\ndesigns and storage solutions, have created a tipping\npoint for their routine generation and analysis, not\nleast of which are new forms of data analytics designed\nto cope with data abundance (Kitchin, 2014).\nTraditionally, data analysis techniques have been\ndesigned to extract insights from scarce, static, clean\nand poorly relational data sets, scientifically sampled\nand adhering to strict assumptions (such as independ-\nence, stationarity, and normality), and generated and\nanalysed with a specific question in mind (Miller, 2010).\nThe challenge of analysing Big Data is coping with\nabundance, exhaustivity and variety, timeliness and\ndynamism, messiness and uncertainty, high relational-\nity, and the fact that much of what is generated has no\nspecific question in mind or is a by-product of another\nactivity. Such a challenge was until recently too com-\nplex and difficult to implement, but has become pos-\nsible due to high-powered computation and new\nanalytical techniques. These new techniques are\nrooted in research concerning artificial intelligence\nand expert systems that have sought to produce\nmachine learning that can computationally and auto-\nmatically mine and detect patterns and build predictive\nmodels and optimize outcomes (Han et al., 2011; Hastie\net al., 2009). Moreover, since different models have\ntheir strengths and weaknesses, and it is often difficult\nto prejudge which type of model and its various ver-\nsions will perform best on any given data set, an ensem-\nble approach can be employed to build multiple\nsolutions (Seni and Elder, 2010). Here, literally hun-\ndreds of different algorithms can be applied to a dataset\nto determine the best or a composite model or explan-\nation (Siegel, 2013), a radically different approach to\nthat traditionally used wherein the analyst selects an\nappropriate method based on their knowledge of tech-\nniques and the data. In other words, Big Data analytics\nenables an entirely new epistemological approach for\nmaking sense of the world; rather than testing a\ntheory by analysing relevant data, new data analytics\nseek to gain insights `born from the data'.\nThe explosion in the production of Big Data, along\nwith the development of new epistemologies, is leading\nmany to argue that a data revolution is under way that\nhas far-reaching consequences to how knowledge is\nproduced, business conducted, and governance enacted\nSchonberger and Cukier, 2013). With respect to know-\nledge production, it is contended that Big Data presents\n2 Big Data & Society\nthe possibility of a new research paradigm across mul-\ntiple disciplines. As set out by Kuhn (1962), a paradigm\nconstitutes an accepted way of interrogating the world\nand synthesizing knowledge common to a substantial\nproportion of researchers in a discipline at any one\nmoment in time. Periodically, Kuhn argues, a new\nway of thinking emerges that challenges accepted the-\nories and approaches. For example, Darwin's theory of\nevolution radically altered conceptual thought within\nthe biological sciences, as well as challenging the reli-\ngious doctrine of creationism. Jim Gray (as detailed in\nHey et al., 2009) charts the evolution of science through\nfour broad paradigms (see Table 1). Unlike Kuhn's\nproposition that paradigm shifts occur because the\ndominant mode of science cannot account for particu-\nlar phenomena or answer key questions, thus demand-\ning the formulation of new ideas, Gray's transitions are\nfounded on advances in forms of data and the develop-\nment of new analytical methods. He thus proposes that\nscience is entering a fourth paradigm based on the\ngrowing availability of Big Data and new analytics.\nKuhn's argument has been subject to much critique,\nnot least because within some academic domains there\nis little evidence of paradigms operating, notably in\nsome social sciences where there is a diverse set of\nphilosophical approaches employed (e.g. human geog-\nraphy, sociology), although in other domains, such as\nthe sciences, there has been more epistemological unity\naround how science is conducted, using a well defined\nscientific method, underpinned by hypothesis testing to\nverify or falsify theories. Moreover, paradigmatic\naccounts produce overly sanitized and linear stories of\nhow disciplines evolve, smoothing over the messy, con-\ntested and plural ways in which science unfolds in prac-\ntice. Nevertheless, whilst the notion of paradigms is\nproblematic, it has utility in framing the current debates\nconcerning the development of Big Data and their con-\nsequences because many of the claims being made with\nrespect to knowledge production contend that a funda-\nmentally different epistemology is being created; that a\ntransition to a new paradigm is under way. However,\nthe form that this new epistemology is taking is con-\ntested. The rest of this paper critically examines the\ndevelopment of an emerging fourth paradigm in science\nand its form, and explores to what extent the data\nrevolution is leading to alternative epistemologies in\nthe humanities and social sciences and changing\nresearch practices.\nA fourth paradigm in science?\nWhilst Jim Gray envisages the fourth paradigm of science\nto be data-intensive and a radically new extension of the\nestablished scientific method, others suggest that Big\nData ushers in a new era of empiricism, wherein the\nvolume of data, accompanied by techniques that can\nreveal their inherent truth, enables data to speak for\nthemselves free of theory. The empiricist view has\ngained credence outside of the academy, especially\nwithin business circles, but its ideas have also taken root\nin the new field of data science and other sciences. In\ncontrast, a new mode of data-driven science is emerging\nwithin traditional disciplines in the academy. In this sec-\ntion, the epistemological claims of both approaches are\ncritically examined, mindful of the different drivers and\naspirations of business and the academy, with the former\npreoccupied with employing data analytics to identify\nnew products, markets and opportunities rather than\nadvance knowledge per se, and the latter focused on\nhow best to make sense of the world and to determine\nexplanations as to phenomena and processes.\nThe end of theory: Empiricism reborn\nFor commentators such as Chris Anderson, former\neditor-in-chief at Wired magazine, Big Data, new data\nanalytics and ensemble approaches signal a new era of\nknowledge production characterized by `the end of\ntheory'. In a provocative piece, Anderson (2008)\nargues that `the data deluge makes the scientific\nmethod obsolete'; that the patterns and relationships\ncontained within Big Data inherently produce meaning-\nful and insightful knowledge about complex phenom-\nena. Essentially arguing that Big Data enables an\nempiricist mode of knowledge production, he contends:\nThere is now a better way. Petabytes allow us to say:\n`Correlation is enough.' . . . We can analyze the data\nwithout hypotheses about what it might show. We\ncan throw the numbers into the biggest computing\nTable 1. Four paradigms of science.\nParadigm Nature Form When\nFirst Experimental science Empiricism; describing natural phenomena pre-Renaissance\nSecond Theoretical science Modelling and generalization pre-computers\nThird Computational science Simulation of complex phenomena pre-Big Data\nFourth Exploratory science Data-intensive; statistical exploration and data mining Now\nclusters the world has ever seen and let statistical algo-\nrithms find patterns where science cannot . . .\nCorrelation supersedes causation, and science can\nadvance even without coherent models, unified the-\nories, or really any mechanistic explanation at all.\nThere's no reason to cling to our old ways.\nscientists no longer have to make educated guesses,\nconstruct hypotheses and models, and test them with\ndata-based experiments and examples. Instead, they\ncan mine the complete set of data for patterns that\nreveal effects, producing scientific conclusions without\nfurther experimentation.\nDyche (2012) thus argues that `mining Big Data\nreveals relationships and patterns that we didn't\neven know to look for.' Likewise, Steadman (2013)\nargues:\nThe Big Data approach to intelligence gathering allows\nan analyst to get the full resolution on worldwide\naffairs. Nothing is lost from looking too closely at\none particular section of data; nothing is lost from\ntrying to get too wide a perspective on a situation\nthat the fine detail is lost . . . . The analyst doesn't\neven have to bother proposing a hypothesis anymore.\nThe examples used to illustrate such a position usually\nstem from marketing and retail. For example, Dyche\nyears' worth of purchase transactions for possible\nunnoticed relationships between products that ended\nup in shoppers' baskets. Discovering correlations\nbetween certain items led to new product placements\nand a 16% increase in revenue per shopping cart in\nthe first month's trial. There was no hypothesis that\nProduct A was often bought with Product H that was\nthen tested. The data were simply queried to discover\nwhat relationships existed that might have previously\nbeen unnoticed. Similarly, Amazon's recommendation\nsystem produces suggestions for other items a shopper\nmight be interested in without knowing anything about\nthe culture and conventions of books and reading; it\nsimply identifies patterns of purchasing across cus-\ntomers in order to determine if Person A likes Book\nX they are also likely to like Book Y given their own\nand others' consumption patterns. Whilst it might be\ndesirable to explain why associations exist within the\ndata and why they might be meaningful, such explan-\nthus argues with respect to predictive analytics: `We\nusually don't know about causation, and we often\ndon't necessarily care . . . the objective is more to predict\nthan it is to understand the world . . . It just needs to\nwork; prediction trumps explanation'.\nSome data analytics software is sold on precisely this\nnotion. For example, the data mining and visualization\nsoftware Ayasdi claims to be able to\nautomatically discover insights \u00ad regardless of complex-\nity \u00ad without asking questions. Ayasdi's customers can\nfinally learn the answers to questions that they didn't\nknow to ask in the first place. Simply stated, Ayasdi is\nFurther, it purports to have totally removed\nthe human element that goes into data mining \u00ad and, as\nsuch, all the human bias that goes with it. Instead of\nwaiting to be asked a question or be directed to specific\nexisting data links, the system will \u00ad undirected \u00ad deli-\nver patterns a human controller might not have\nThere is a powerful and attractive set of ideas at work\nin the empiricist epistemology that runs counter to the\ndeductive approach that is hegemonic within modern\nscience:\n. Big Data can capture a whole domain and provide\nfull resolution;\n. there is no need for a priori theory, models or\nhypotheses;\n. through the application of agnostic data analytics\nthe data can speak for themselves free of human\nbias or framing, and any patterns and relationships\nwithin Big Data are inherently meaningful and\ntruthful;\n. meaning transcends context or domain-specific\nknowledge, thus can be interpreted by anyone who\ncan decode a statistic or data visualization.\nThese work together to suggest that a new mode of\nscience is being created, one in which the modus oper-\nandi is purely inductive in nature.\nWhilst this empiricist epistemology is attractive, it is\nbased on fallacious thinking with respect to the four\nideas that underpin its formulation. First, though Big\nData may seek to be exhaustive, capturing a whole\ndomain and providing full resolution, it is both a rep-\nresentation and a sample, shaped by the technology and\nplatform used, the data ontology employed and the\nregulatory environment, and it is subject to sampling\nprovide oligoptic views of the world: views from certain\nvantage points, using particular tools, rather than an\nall-seeing, infallible God's eye view (Amin and Thrift,\n4 Big Data & Society\nnatural and essential elements that are abstracted from\nthe world in neutral and objective ways and can be\naccepted at face value; data are created within a com-\nplex assemblage that actively shapes its constitution\nSecond, Big Data does not arise from nowhere, free\nfrom the `the regulating force of philosophy' (Berry,\n2011: 8). Contra, systems are designed to capture cer-\ntain kinds of data and the analytics and algorithms\nused are based on scientific reasoning and have been\nrefined through scientific testing. As such, an inductive\nstrategy of identifying patterns within data does not\noccur in a scientific vacuum and is discursively\nframed by previous findings, theories, and training; by\nspeculation that is grounded in experience and know-\nledge (Leonelli, 2012). New analytics might present the\nillusion of automatically discovering insights without\nasking questions, but the algorithms used most cer-\ntainly did arise and were tested scientifically for validity\nand veracity.\nThird, just as data are not generated free from\ntheory, neither can they simply speak for themselves\nnotes, `inanimate data can never speak for themselves,\nand we always bring to bear some conceptual frame-\nwork, either intuitive and ill-formed, or tightly and for-\nmally structured, to the task of investigation, analysis,\nand interpretation'. Making sense of data is always\nframed \u00ad data are examined through a particular lens\nthat influences how they are interpreted. Even if the\nprocess is automated, the algorithms used to process\nthe data are imbued with particular values and contex-\ntualized within a particular scientific approach.\nFurther, patterns found within a data set are not inher-\nently meaningful. Correlations between variables\nwithin a data set can be random in nature and have\nno or little causal association, and interpreting them\nas such can produce serious ecological fallacies. This\ncan be exacerbated in the case of Big Data as the\nempiricist position appears to promote the practice of\ndata dredging \u00ad hunting for every association or model.\nFourth, the idea that data can speak for themselves\nsuggests that anyone with a reasonable understanding\nof statistics should be able to interpret them without\ncontext or domain-specific knowledge. This is a conceit\nvoiced by some data and computer scientists and other\nscientists, such as physicists, all of whom have become\nactive in practising social science and humanities\nresearch. For example, a number of physicists have\nturned their attention to cities, employing Big Data\nanalytics to model social and spatial processes and to\nidentify supposed laws that underpin their formation\nThese studies often wilfully ignore a couple of centuries\nof social science scholarship, including nearly a century\nof quantitative analysis and model building. The result\nis an analysis of cities that is reductionist, functionalist\nand ignores the effects of culture, politics, policy, gov-\nernance and capital (reproducing the same kinds of\nlimitations generated by the quantitative/positivist\nsocial sciences in the mid-20th century). A similar set\nof concerns is shared by those in the sciences. Strasser\n(2012), for example, notes that within the biological\nsciences, bioinformaticians who have a very narrow\nand particular way of understanding biology are claim-\ning ground once occupied by the clinician and the\nexperimental and molecular biologist. These scientists\nare undoubtedly ignoring the observations of Porway\nWithout subject matter experts available to articulate\nproblems in advance, you get [poor] results . . . . Subject\nmatter experts are doubly needed to assess the results of\nthe work, especially when you're dealing with sensitive\ndata about human behavior. As data scientists, we are\nwell equipped to explain the `what' of data, but rarely\nshould we touch the question of `why' on matters we\nare not experts in.\nPut simply, whilst data can be interpreted free of con-\ntext and domain-specific expertise, such an epistemo-\nlogical interpretation is likely to be anaemic or\nunhelpful as it lacks embedding in wider debates and\nknowledge.\nThese fallacious notions have gained some traction,\nespecially within business circles, because they possess a\nconvenient narrative for the aspirations of knowledge-\norientated businesses (e.g. data brokers, data analytic\nproviders, software vendors, consultancies) in selling\ntheir services. Within the empiricist frame, data ana-\nlytics offer the possibility of insightful, objective and\nprofitable knowledge without science or scientists, and\ntheir associated overheads of cost, contingencies, and\nsearch for explanation and truth. In this sense, whilst\nthe data science techniques employed might hold genu-\nine salience for practioners, the articulation of a new\nempiricism operates as a discursive rhetorical device\ndesigned to simplify a more complex epistemological\napproach and to convince vendors of the utility and\nvalue of Big Data analytics.\nData-driven science\nIn contrast to new forms of empiricism, data-driven\nscience seeks to hold to the tenets of the scientific\nmethod, but is more open to using a hybrid combin-\nation of abductive, inductive and deductive approaches\nto advance the understanding of a phenomenon. It dif-\nfers from the traditional, experimental deductive design\nin that it seeks to generate hypotheses and insights\n`born from the data' rather than `born from the theory'\nincorporate a mode of induction into the research\ndesign, though explanation through induction is not\nthe intended end-point (as with empiricist approaches).\nInstead, it forms a new mode of hypothesis generation\nbefore a deductive approach is employed. Nor does the\nprocess of induction arise from nowhere, but is situated\nand contextualized within a highly evolved theoretical\ndomain. As such, the epistemological strategy adopted\nwithin data-driven science is to use guided knowledge\ndiscovery techniques to identify potential questions\n(hypotheses) worthy of further examination and\ntesting.\nThe process is guided in the sense that existing\ntheory is used to direct the process of knowledge dis-\ncovery, rather than simply hoping to identify all rela-\ntionships within a dataset and assuming they are\nmeaningful in some way. As such, how data are gener-\nated or repurposed is directed by certain assumptions,\nunderpinned by theoretical and practical knowledge\nand experience as to whether technologies and their\nconfigurations will capture or produce appropriate\nand useful research material. Data are not generated\nby every means possible, using every kind of available\ntechnology or every kind of sampling framework;\nrather, strategies of data generation and repurposing\nare carefully thought out, with strategic decisions\nmade to harvest certain kinds of data and not others.\nSimilarly, how these data are processed, managed and\nanalysed is guided by assumptions as to which tech-\nniques might provide meaningful insights. The data\nare not subject to every ontological framing possible,\nor every form of data-mining technique in the hope that\nthey reveal some hidden truth. Rather, theoretically\ninformed decisions are made as to how best to tackle\na data set such that it will reveal information which will\nbe of potential interest and is worthy of further\nresearch. And instead of testing whether every relation-\nship revealed has veracity, attention is focused on those\n\u00ad based on some criteria \u00ad that seemingly offer the most\nlikely or valid way forward. Indeed, many supposed\nrelationships within data sets can quickly be dismissed\nas trivial or absurd by domain specialists, with others\nflagged as deserving more attention (Miller, 2010).\nSuch decision-making with respect to methods of\ndata generation and analysis are based on abductive\nreasoning. Abduction is a mode of logical inference\n(Miller, 2010). It seeks a conclusion that makes reason-\nable and logical sense, but is not definitive in its claim.\nFor example, there is no attempt to deduce what is the\nbest way to generate data, but rather to identify an\napproach that makes logical sense given what is already\nknown about such data production. Abduction is very\ncommonly used in science, especially in the formulation\nof hypotheses, though such use is not widely acknowl-\nedged. Any relationships revealed within the data do\nnot then arise from nowhere and nor do they simply\nspeak for themselves. The process of induction \u00ad of\ninsights emerging from the data \u00ad is contextually\nframed. And those insights are not the end-point of\nan investigation, arranged and reasoned into a theory.\nRather, the insights provide the basis for the formula-\ntion of hypotheses and the deductive testing of their\nvalidity. In other words, data-driven science is a recon-\nfigured version of the traditional scientific method, pro-\nviding a new way in which to build theory. Nonetheless,\nthe epistemological change is significant.\nRather than empiricism and the end of theory, it is\nargued by some that data-driven science will become\nthe new paradigm of scientific method in an age of\nBig Data because the epistemology favoured is suited\nto extracting additional, valuable insights that trad-\nitional `knowledge-driven science' would fail to gener-\nKnowledge-driven science, using a straight deductive\napproach, has particular utility in understanding and\nexplaining the world under the conditions of scarce\ndata and weak computation. Continuing to use such\nan approach, however, when technological and meth-\nodological advances mean that it is possible to under-\ntake much richer analysis of data \u00ad applying new data\nanalytics and being able to connect together large, dis-\nparate data together in ways that were hitherto impos-\nsible, and which produce new valuable data and\nidentify and tackle questions in new and exciting ways\n\u00ad makes little sense. Moreover, the advocates of data-\ndriven science argue that it is much more suited to\nexploring, extracting value and making sense of mas-\nsive, interconnected data sets, fostering interdisciplin-\nary research that conjoins domain expertise (as it is less\nlimited by the starting theoretical frame), and that it\nwill lead to more holistic and extensive models and\ntheories of entire complex systems rather than elements\nFor example, it is contended that data-driven science\nwill transform our understanding of environmental sys-\nenable high-resolution data being generated from a var-\niety of sources, often in real-time (such as conventional\nand mobile weather stations, satellite and aerial ima-\ngery, weather radar, stream observations and gauge sta-\ntions, citizen observations, ground and aerial LIDAR,\nwater-quality sampling, gas measures, soil cores, and\ndistributed sensors that measure selected domains\nsuch as air temperature and moisture) to be integrated\ntogether to provide very detailed models of environ-\nments in flux (as opposed to at freeze-points in time\nand space) and to identify specific relationships between\n6 Big Data & Society\nphenomena and processes that generate new hypoth-\neses and theories that can then be tested further to\nestablish their veracity. It will also help to identify\nand further understand connection points between dif-\nferent environmental spheres \u00ad such as the atmosphere\n(air), biosphere (ecosystems), hydrosphere (water sys-\ntems), lithosphere (rocky shell of the Earth) and pedo-\nsphere (soils) \u00ad and aid in the integration of theories\ninto a more holistic theoretical assemblage. This will\nprovide a better comprehension of the diverse, inter-\nrelated processes at work and the interconnections\nwith human systems, and can be used to guide models\nand simulations for predicting long-term trends and\npossible adaptive strategies.\nComputational social sciences and\ndigital humanities\nWhilst the epistemologies of Big Data empiricism and\ndata-driven science seem set to transform the approach\nto research taken in the natural, life, physical and\nengineering sciences, their trajectory in the humanities\nand social sciences is less certain. These areas of schol-\narship are highly diverse in their philosophical under-\npinnings, with only some scholars employing the\nepistemology common in the sciences. Those using\nthe scientific method in order to explain and model\nsocial phenomena, in general terms, draw on the ideas\nof positivism (though they might not adopt such a\nlabel; Kitchin, 2006). Such work tends to focus on fac-\ntual, quantified information \u00ad empirically observable\nphenomena that can be robustly measured (such as\ncounts, distance, cost, and time), as opposed to more\nintangible aspects of human life such as beliefs or ideol-\nogy \u00ad using statistical testing to establish causal rela-\ntionships and to build theories and predictive models\nand simulations. Positivistic approaches are well estab-\nlished in economics, political science, human geography\nand sociology, but are rare in the humanities. However,\nwithin those disciplines mentioned, there has been a\nstrong move over the past half century towards post-\npositivist approaches, especially in human geography\nand sociology.\nFor positivistic scholars in the social sciences, Big\nData offers a significant opportunity to develop more\nsophisticated, wider-scale, finer-grained models of\nhuman life. Notwithstanding concerns over access to\nsocial and economic Big Data (much of which is gen-\nerated by private interests) and issues such as data qual-\nity, Big Data offers the possibility of shifting `from\ndata-scarce to data-rich studies of societies; from\nstatic snapshots to dynamic unfoldings; from coarse\naggregations to high resolutions; from relatively\nsimple models to more complex, sophisticated simula-\ntions' (Kitchin, 2014: 3). The potential exists for a new\nera of computational social science that produces stu-\ndies with much greater breadth, depth, scale, and time-\nliness, and that are inherently longitudinal, in contrast\nto existing social sciences research (Lazer et al., 2009;\nBatty et al., 2012). Moreover, the variety, exhaustivity,\nresolution, and relationality of data, plus the growing\npower of computation and new data analytics, address\nsome of the critiques of positivistic scholarship to date,\nespecially those of reductionism and universalism, by\nproviding more finely-grained, sensitive, and nuanced\nanalysis that can take account of context and contin-\ngency, and can be used to refine and extend theoretical\nunderstandings of the social and spatial world (Kitchin,\n2013). Further, given the extensiveness of data, it is\npossible to test the veracity of such theory across a\nvariety of settings and situations. In such circum-\nstances, it is argued that knowledge about individuals,\ncommunities, societies and environments will become\nmore insightful and useful with respect to formulating\npolicy and addressing the various issues facing\nhumankind.\nFor post-positivist scholars, Big Data offers both\nopportunities and challenges. The opportunities are a\nproliferation, digitization and interlinking of a diverse\nset of analogue and unstructured data, much of it new\n(e.g. social media) and much of which has heretofore\nbeen difficult to access (e.g. millions of books, docu-\nments, newspapers, photographs, art works, material\nobjects, etc., from across history that have been ren-\ndered into digital form over the past couple of decades\nby a range of organizations; Cohen, 2008), and also the\nprovision of new tools of data curation, management\nand analysis that can handle massive numbers of data\nobjects. Consequently, rather than concentrating on a\nhandful of novels or photographs, or a couple of artists\nand their work, it becomes possible to search and con-\nnect across a large number of related works; rather than\nfocus on a handful of websites or chat rooms or\nvideos or online newspapers, it becomes possible to\nexamine hundreds of thousands of such media\n(Manovich, 2011). These opportunities are most\nwidely being examined through the emerging field of\ndigital humanities.\nInitially, the digital humanities consisted of the cur-\nation and analysis of data that are born digital and the\ndigitization and archiving projects that sought to\nrender analogue texts and material objects into digital\nforms that could be organized and searched and be\nsubjected to basic forms of overarching, automated or\nguided analysis such as summary visualizations of con-\ntent (Schnapp and Presner, 2009). Subsequently, its\nadvocates have been divided into two camps. The\nfirst group believes that new digital humanities tech-\nniques \u00ad counting, graphing, mapping and distant\nreading \u00ad bring methodological rigour and objectivity\nto disciplines that heretofore have been unsystematic\nand random in their focus and approach (Moretti,\nargues that, rather than replacing traditional methods\nor providing an empiricist or positivistic approach to\nhumanities scholarship, new techniques complement\nand augment existing humanities methods and facilitate\ntraditional forms of interpretation and theory-building,\nenabling studies of much wider scope to answer ques-\ntions that would be all but unanswerable without com-\nThe digital humanities has not been universally wel-\ncomed, with detractors contending that using com-\nputers as `reading machines' (Ramsay, 2010) to\nundertake `distant reading' (Moretti, 2005) runs coun-\nter to and undermines traditional methods of close\ninvolves paying `attention to how meaning is produced\nor conveyed, to what sorts of literary and rhetorical\nstrategies and techniques are deployed to achieve\nwhat the reader takes to be the effects of the work or\npassage' \u00ad something that a distant reading is unable to\nperform. His worry is that a digital humanities\napproach promotes literary scholarship that involves\nargues that a `statistically driven model of literary his-\ntory . . . seems to necessitate an impersonal invisible\nhand', continuing: `any attempt to see the big picture\nneeds to be informed by broad knowledge, an astute,\nhistoricized sense of how genres and literary institutions\nLikewise, Marche (2012) contends that cultural arte-\nfacts, such as literature, cannot be treated as mere\ndata. A piece of writing is not simply an order of letters\nand words; it is contextual and conveys meaning and\nhas qualities that are ineffable. Algorithms are very\npoor at capturing and deciphering meaning or context\nand, Marche argues, treat `all literature as if it were the\nsame'. He continues:\n[t]he algorithmic analysis of novels and of newspaper\narticles is necessarily at the limit of reductivism. The\nprocess of turning literature into data removes distinc-\ntion itself. It removes taste. It removes all the refine-\nment from criticism. It removes the history of the\nreception of works.\nthe value of the arts, the quality of a play or a painting,\nis not measurable. You could put all sorts of data into a\nmachine: dates, colours, images, box office receipts, and\nnone of it could explain what the artwork is, what it\nmeans, and why it is powerful. That requires man [sic],\nnot machine.\nFor many, then, the digital humanities is fostering\nweak, surface analysis, rather than deep, penetrating\ninsight. It is overly reductionist and crude in its tech-\nniques, sacrificing complexity, specificity, context,\ndepth and critique for scale, breadth, automation,\ndescriptive patterns and the impression that interpret-\nation does not require deep contextual knowledge.\nThe same kinds of argument can be levelled at com-\nputational social science. For example, a map of the\nlanguage of tweets in a city might reveal patterns of\ngeographic concentration of different ethnic commu-\nnities (Rogers, 2013), but the important questions are\nwho constitutes such concentrations, why do they exist,\nwhat were the processes of formation and reproduc-\ntion, and what are their social and economic conse-\nquences? It is one thing to identify patterns; it is\nanother to explain them. This requires social theory\nand deep contextual knowledge. As such, the pattern\nis not the end-point but rather a starting point for\nadditional analysis, which almost certainly is going to\nrequire other data sets.\nAs with earlier critiques of quantitative and positiv-\nist social sciences, computational social sciences are\ntaken to task by post-positivists as being mechanistic,\natomizing, and parochial, reducing diverse individuals\nand complex, multidimensional social structures to\nmere data points (Wyly, in press). Moreover, the ana-\nlysis is riddled with assumptions of social determinism,\nas exemplified by Pentland (2012): `the sort of person\nyou are is largely determined by your social context, so\nif I can see some of your behaviors, I can infer the rest,\njust by comparing you to the people in your crowd'. In\ncontrast, human societies, it is argued, are too complex,\ncontingent and messy to be reduced to formulae and\nlaws, with quantitative models providing little insight\ninto phenomena such as wars, genocide, domestic vio-\nlence and racism, and only circumscribed insight into\nother human systems such as the economy, inad-\nequately accounting for the role of politics, ideology,\nsocial structures, and culture (Harvey, 1972). People do\nnot act in rational, pre-determined ways, but rather live\nlives full of contradictions, paradoxes, and unpredict-\nable occurrences. How societies are organized and\noperate varies across time and space and there is no\noptimal or ideal form, or universal traits. Indeed,\nthere is an incredible diversity of individuals, cultures\nand modes of living across the planet. Reducing this\ncomplexity to the abstract subjects that populate uni-\nversal models does symbolic violence to how we create\nknowledge. Further, positivistic approaches wilfully\nignore the metaphysical aspects of human life (con-\ncerned with meanings, beliefs, experiences) and norma-\ntive questions (ethical and moral dilemmas about how\nthings should be as opposed to how they are) (Kitchin,\n2006). In other words, positivistic approaches only\n8 Big Data & Society\nfocus on certain kinds of questions, which they seek to\nanswer in a reductionist way that seemingly ignores\nwhat it means to be human and to live in richly diverse\nsocieties and places. This is not to say that quantitative\napproaches are not useful \u00ad they quite patently are \u00ad\nbut that their limitations in understanding human life\nshould be recognized and complemented with other\napproaches.\nBrooks (2013) thus contends that Big Data ana-\nlytics struggles with the social (people are not rationale\nand do not behave in predictable ways; human systems\nare incredibly complex, having contradictory and para-\ndoxical relation); struggles with context (data are lar-\ngely shorn of the social, political and economic and\nhistorical context); creates bigger haystacks (consisting\nof many more spurious correlations, making it difficult\nto identify needles); has trouble addressing big prob-\nlems (especially social and economic ones); favours\nmemes over masterpieces (identifies trends but not\nnecessarily significant features that may become a\ntrend); and obscures values (of the data producers\nand those that analyse them and their objectives). In\nother words, whilst Big Data analytics might provide\nsome insights, it needs to be recognized that they are\nlimited in scope, produce particular kinds of know-\nledge, and still need contextualization with respect to\nother information, whether that be existing theory,\npolicy documents, small data studies, or historical rec-\nords, that can help to make sense of the patterns evi-\nBeyond the epistemological and methodological\napproach, part of the issue is that much Big Data and\nanalysis seem to be generated with no specific questions\nin mind, or the focus is driven by the application of a\nmethod or the content of the data set rather than a\nparticular question, or the data set is being used to\nseek an answer to a question that it was never designed\nto answer in the first place. With respect to the latter,\ngeotagged Twitter data has not been produced to pro-\nvide answers with respect to the geographical concen-\ntration of language groups in a city and the processes\ndriving such spatial autocorrelation. We should per-\nhaps not be surprised then that it only provides a sur-\nface snapshot, albeit an interesting snapshot, rather\nthan deep penetrating insights into the geographies of\nrace, language, agglomeration and segregation in par-\nticular locales.\nWhereas most digital humanists recognize the value\nof close readings, and stress how distant readings com-\nplement them by providing depth and contextualiza-\ntion, positivistic forms of social science are\noppositional to post-positivist approaches. The differ-\nence between the humanities and social sciences in this\nrespect is because the statistics used in the digital huma-\nnities are largely descriptive \u00ad identifying and plotting\npatterns. In contrast, the computational social sciences\nemploy the scientific method, complementing descrip-\ntive statistics with inferential statistics that seek to iden-\ntify associations and causality. In other words, they are\nunderpinned by an epistemology wherein the aim is to\nproduce sophisticated statistical models that explain,\nsimulate and predict human life. This is much more\ndifficult to reconcile with post-positivist approaches.\nAdvocacy then rests on the utility and value of the\nmethod and models, not on providing complementary\nanalysis of a more expansive set of data.\nThere is a potentially fruitful alternative to this pos-\nition that adopts and extends the epistemologies\nemployed in critical GIS and radical statistics. These\napproaches employ quantitative techniques, inferential\nstatistics, modelling and simulation whilst being mind-\nful and open with respect to their epistemological short-\ncomings, drawing on critical social theory to frame how\nthe research is conducted, how sense is made of the\nfindings, and the knowledge employed. Here, there is\nrecognition that research is not a neutral, objective\nactivity that produces a view from nowhere, and that\nthere is an inherent politics pervading the datasets ana-\nlysed, the research conducted, and the interpretations\nresearcher is acknowledged to possess a certain posi-\ntionality (with respect to their knowledge, experience,\nbeliefs, aspirations, etc.), that the research is situated\n(within disciplinary debates, the funding landscape,\nwider societal politics, etc.), the data are reflective of\nthe technique used to generate them and hold certain\ncharacteristics (relating to sampling and ontological\nframes, data cleanliness, completeness, consistency, ver-\nacity and fidelity), and the methods of analysis utilized\nproduce particular effects with respect to the results\nproduced and interpretations made. Moreover, it is\nrecognized that how the research is employed is not\nideologically-neutral but is framed in subtle and explicit\nways by the aspirations and intentions of the research-\ners and funders/sponsors, and those that translate such\nresearch into various forms of policy, instruments, and\naction. In other words, within such an epistemology the\nresearch conducted is reflexive and open with respect\nto the research process, acknowledging the contingen-\ncies and relationalities of the approach employed, thus\nproducing nuanced and contextualized accounts\nand conclusions. Such an epistemology also does not\nforeclose complementing situated computational\nsocial science with small data studies that provide\nadditional and amplifying insights (Crampton et al.,\n2012). In other words, it is possible to think of\nnew epistemologies that do not dismiss or reject\nBig Data analytics, but rather employ the meth-\nodological approach of data-driven science within a\ndifferent epistemological framing that enables social\nscientists to draw valuable insights from Big Data that\nare situated and reflexive.\nConclusion\nThere is little doubt that the development of Big Data\nand new data analytics offers the possibility of refram-\ning the epistemology of science, social science and\nhumanities, and such a reframing is already actively\ntaking place across disciplines. Big Data and new\ndata analytics enable new approaches to data gener-\nation and analyses to be implemented that make it pos-\nsible to ask and answer questions in new ways. Rather\nthan seeking to extract insights from datasets limited by\nscope, temporality and size, Big Data provides the\ncounter problem of handling and analysing enormous,\ndynamic, and varied datasets. The solution has been the\ndevelopment of new forms of data management and\nanalytical techniques that rely on machine learning\nand new modes of visualization.\nWith respect to the sciences, access to Big Data and\nnew research praxes has led some to proclaim the emer-\ngence of a new fourth paradigm, one rooted in data-\nintensive exploration that challenges the established sci-\nentific deductive approach. At present, whilst it is clear\nthat Big Data is a disruptive innovation, presenting the\npossibility of a new approach to science, the form of\nthis approach is not set, with two potential paths pro-\nposed that have divergent epistemologies \u00ad empiricism,\nwherein the data can speak for themselves free of\ntheory, and data-driven science that radically modifies\nthe existing scientific method by blending aspects of\nabduction, induction and deduction. Given the weak-\nnesses in the empiricist arguments it seems likely that\nthe data-driven approach will eventually win out and\nover time, as Big Data becomes more common and new\ndata analytics are advanced, will present a strong chal-\nlenge to the established knowledge-driven scientific\nmethod. To accompany such a transformation the\nphilosophical underpinnings of data-driven science,\nwith respect to its epistemological tenets, principles\nand methodology, need to be worked through and\ndebated to provide a robust theoretical framework for\nthe new paradigm.\nThe situation in the humanities and social sciences is\nsomewhat more complex given the diversity of their\nphilosophical underpinnings, with Big Data and new\nanalytics being unlikely to lead to the establishment\nof new disciplinary paradigms. Instead, Big Data will\nenhance the suite of data available for analysis and\nenable new approaches and techniques, but will not\nfully replace traditional small data studies. This is\npartly due to philosophical positions, but also because\nit is unlikely that suitable Big Data will be produced\nthat can be utilized to answer particular questions, thus\nnecessitating more targeted studies. Nonetheless, as\npresents a number of opportunities for social scientists\nand humanities scholars, not least of which are massive\nquantities of very rich social, cultural, economic, polit-\nical and historical data. It also poses a number of chal-\nlenges, including a skills deficit for analysing and\nmaking sense of such data, and the creation of an epis-\ntemological approach that enables post-positivist forms\nof computational social science. One potential path for-\nward is an epistemology that draws inspiration from\ncritical GIS and radical statistics in which quantitative\nmethods and models are employed within a framework\nthat is reflexive and acknowledges the situatedness,\npositionality and politics of the social science being\nconducted, rather than rejecting such an approach out\nof hand. Such an epistemology also has potential utility\nin the sciences for recognizing and accounting for the\nuse of abduction and creating a more reflexive data-\ndriven science. As this tentative discussion illustrates,\nthere is an urgent need for wider critical reflection on\nthe epistemological implications of Big Data and data\nanalytics, a task that has barely begun despite the speed\nof change in the data landscape.\n"
}