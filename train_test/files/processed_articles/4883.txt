{
    "abstract": "Abstract\nTo what extent do frequently cited determinants of military spending allow us to predict and forecast future levels of\nexpenditure? The authors draw on the data and specifications of a recent model on military expenditure and assess the\npredictive power of its variables using in-sample predictions, out-of-sample forecasts and Bayesian model averaging.\nTo this end, this paper provides guidelines for prediction exercises in general using these three techniques. More\nsubstantially, however, the findings emphasize that previous levels of military spending as well as a country's institutional\nand economic characteristics particularly improve our ability to predict future levels of investment in the military.\nVariables pertaining to the international security environment also matter, but seem less important. In addition, the\nresults highlight that the updated model, which drops weak predictors, is not only more parsimonious, but also slightly\nmore accurate than the original specification.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons CC BY-NC-ND: This article is distributed under the terms of the Creative Commons Attribution-\nNonCommercial-NoDerivs 3.0 License (http://www.creativecommons.org/licenses/by-nc-nd/3.0/) which permits non-commercial\nuse, reproduction and distribution of the work as published without adaptation or alteration, without further permission provided the original work\nis attributed as specified on the SAGE and Open Access pages (http://www.uk.sagepub.com/aboutus/openaccess.htm).\nIntroduction\nEach year, states spend substantive monetary resources on\nthe military in terms of troops, arms, other equipment and\nso forth; according to the Stockholm International Peace\nResearch Institute (SIPRI), countries invested ca. 2.5% of\nthe world's GDP (gross domestic product) in 2012, which\ncomprised about US$ 1.753 trillion in that year. This cor-\nresponds to the GDP of Canada, the 11th largest economy\nin the world. Despite a decrease by 0.5% in real terms in\n1998), the world's military expenditure remains at histori-\ncally high levels and is still larger than the peak figures we\nobserved towards the end of the Cold War.\nThe amount of money allocated to the military has\nimportant implications for national, regional and global sta-\nbility, and has sparked an intense debate on the military\nbuild-up in post-conflict societies, and on whether and to\nwhat extent military spending affects a state's economic\ngrowth (e.g., Aizenman and Glick, 2006; Alptekin and\nconsiderably in the amount of resources they devote to their\narmed forces. For example, the military burden varies from\n0% (e.g., Costa Rica) to more than 14% of GDP in times of\npeace (e.g., Saudi Arabia), while even a nation's entire\nGDP may be used for the military in times of war (e.g.,\nKuwait).\nIn light of these patterns, another key issue pertains to\nthe determinants of military spending. In general, the litera-\nture identified a series of statistically significant results for\na range of economic, political and security-related varia-\nbles (see, e.g., Albalate et al., 2012; Dunne and Perlo-\nNordhaus et al., 2012). To the best of our knowledge, how-\never, the existent work has paid less attention to assessing\nvariables' ability to predict and forecast military spending.\nAs long as the predictive power of these factors and the\nunderlying theoretical model on the demand for military\nspending (see, e.g., Smith, 1995) remains ambiguous, little\nguidance is given to forecast the defence budget of indi-\nvidual countries. In fact, most existent research on this\ntopic does not explicitly address the question of whether we\nhave reliable models for predicting and forecasting levels\nForecasting military expenditure\nTobias B\u00f6hmelt and Vincenzo Bove\n Keywords\nBayesian model averaging, forecasting, in-sample prediction, military spending, out-of-sample prediction\nUniversity of Essex, UK\nCorresponding author:\nTobias B\u00f6hmelt, University of Essex, Wivenhoe Park, Colchester, CO4\nEmail: tbohmelt@essex.ac.uk\nResearch Article\n2 Research and Politics \nof military spending (see Ward et al., 2010). All this is even\nmore remarkable as the validity of policies based on empir-\nical models on states' behaviour has been the subject of\nseveral recent debates in other fields of international rela-\ntions (see, e.g., Bueno de Mesquita, 2011; Clayton and\nHypothesis testing that ignores out-of-sample heuristics\nfaces the inherent risk of fitting to a specific sample's idio-\nsyncrasies, rather than identifying stable structural relation-\nships between military spending and its determinants (see\nWard et al., 2010). In fact, if a model explains the relation-\nship between defence spending and some explanatory fac-\ntors fairly well in-sample, we merely assume that it also\nperforms well when presented with new data (i.e., out-of-\nsample). Yet, if the model only gives a description of this\nrelationship in the original data set without capturing\nunderlying causal relations, the chances to make correct\nand useful predictions with new data are likely to be under-\nIn order to demonstrate how predictions of military\nspending can be derived from a theoretical model, we use\none of the most recent models on military spending by\nNordhaus et al. (2012) and examine the predictive power of\nits main explanatory variables via in-sample predictions,\nout-of-sample forecasts and Bayesian model averaging.\nThis approach allows us to compare several complemen-\ntary ways for assessing the predictive power of variables.\nTo this end, the paper provides guidelines for prediction\nexercises in general using three different approaches that\njointly acknowledge in-sample and out-of-sample heuris-\ntics. More substantially, however, we study whether ex-\nante information about several explanatory variables can\nimprove our ability to predict future levels of military\nspending. The findings emphasize that previous levels of\nmilitary spending as well as a country's institutional and\neconomic characteristics particularly improve our ability to\npredict future levels of investment in the military. Variables\npertaining to the international security environment also\nmatter, but \u00ad perhaps surprisingly \u00ad seem less important.\nThe results additionally show that the new model, which\ndrops weak predictors, is not only more parsimonious, but\nalso slightly more accurate than the model's original speci-\nfication. We conclude by identifying those countries that\nperform best/worst in this forecasting research and by dis-\ncussing what suggested explanatory factors can be consid-\nered ex-ante by policymakers as opposed to features that\nare only available to the research ex-post.\nData and empirical strategy\nWe rely on one baseline model that was originally pre-\nsented by Nordhaus et al. (2012). These scholars use panel\nexpenditures are supplied by the SIPRI and the Correlates\nof War (COW) project. Since the SIPRI does not provide\nverted into constant US$ measured with purchasing power\nparity and log-transformed.The baseline model in Nordhaus\nMilitary Spending ln P GDP ln\nSpending Foes ln\nit\nMID\n( ) = ( )\n( )\n \n\n\n\n+\n+\n+ S\nSpending Friends ln\nDemocracy\nLagged Dependent Variable\n( )\n+\n+ +\n\n\u00b5 \nwhere Military Spending (ln) is the log of military\nspending and Pit\nMID\n pertains to the ex-ante probability of a\ncountry being involved in a fatal militarized interstate dis-\npute (MID). This latter variable is estimated using a stand-\nard liberal-realist model of interstate conflict, which\nincludes information on a country's political, economic and\nmilitary characteristics, as well as dyadic features such as\ndistance or the level of bilateral trade.2 GDP (ln) is the log\nof the real GDP, and is expected to be positive as larger\ncountries usually require larger defence forces. Spending\nFoes (ln) and Spending Friends (ln) belong to the log of the\nweighed defence spending of enemies and allies, respec-\ntively; these two items are meant to capture the effect of\narms races with enemies (i.e., the action\u00adreaction explana-\ntion of military expenditure) and the spillover benefits\naccruing from the expenditure of allies. Democracy is the\nPolity score (Marshall and Jaggers, 2004), which reflects\nthat autocratic systems invest more in the military appara-\ntus than democracies. Finally, there is also the lagged\ndependent variable, while  is the error term.\nWe implement two changes in this estimation strategy.\nFirstly, using Pit\nMID\n in a model ultimately meant for pre-\ndictions and forecasts would imply that we rely on an ex-\nante prediction to generate further ex-ante predictions.\nHence, something uncertain is used to produce other uncer-\ntain point estimates, and this item discards much of the\nvariation in the sub-indicators. In order to address this\nshortcoming, we decided to disaggregate Pit\nMID\n\nand use the\nsub-components of this indicator in our models. This also\nallows us to assess which of these components are more\naccurate in predicting military spending and, hence, are\nmore important. As indicated above (Nordhaus et al., 2012:\n492f), these sub-components comprise (1) a dyadic item on\nthe time elapsed in years since the last involvement of both\nstates in a fatal MID, (2) two variables on both states'\nregime type as measured by the Polity score (Marshall and\nJaggers, 2004), (3) a dyadic trade-to-GDP ratio, (4) a dyadic\ncontiguity variable, (5) the distance between two states in a\ndyad, (6) a dyadic GDP ratio used as a dyadic\nB\u00f6hmelt and Bove 3\npower measure, (7) a dyadic variable on joint alliance\nmembership, (8) a monadic measure on a state's GDP rela-\ntive to the world's GDP and (9) the number of states in the\ninternational system. Except for the distance and GDP ratio\nvariables, which are truly dyadic, we transformed all these\nitems into monadic measures, leading to the final baseline\nmodel specification:\nMilitary Spending ln Peace Years Democracy\nTrade GDP\nCo\n( ) =\n/\n \n\n\n+\n+\n+ n\nntiguity Allies\nNumber of States in System\nG\n+\n+\n+\n+\n\n\n\n\n/\nD\nDP ln Spending Foes ln\nSpending Friends ln\nLagged Depe\n( ) ( )\n( )\n+\n+\n+\n\n\n\u00b5 n\nndent Variable +\nwith Military Spending (ln) is the log of military spend-\ning, Peace Years counts the number of years since a state\nwas involved in any fatal MID with any state, Democracy\nis the monadic Polity score introduced above, Trade/GDP\nis a country's trade openness (i.e., the sum of imports and\nexports divided by GDP), Contiguity counts the number of\nland- or sea-based contiguous states of a country as defined\nby the COW project, Allies counts the number of alliances\na state has in a given year, GDP/World captures a state's\nGDP in relation to the world's GDP in a given year and\nNumber of States in System simply counts the number of\nexisting countries in a given year, while the last variables\nhave been discussed for Equation (1) above.\nThe second change we implement pertains to the states\nincluded in the sample. In the context of the following pre-\ndiction and forecasting exercises, we also will be assessing\nthe prediction/forecasting accuracy for individual states.\nFor this, however, we need to estimate country-individual\nmodels and, thus, we drop all countries for our final data\nsample that have fewer than 10 observations.3 The coun-\ntries covered by our data drop to 141 as a result.\nTable 1 reports the estimates of three different specifica-\ntions of Equation (2): we first run the model without the\nlagged dependent variable and with robust standard errors.\nAfterwards, we include the lagged dependent variable in\nModel 2 and, finally, we use standard errors clustered on\nstates in Model 3 to take into account intra-group depend-\nencies. To facilitate the interpretation of the main results in\nTable 1, we also plot the variables' coefficient estimates\nand their 90% confidence intervals in Figure 1.\nThe results in Table 1 and Figure 1 essentially mirror the\nfindings in Nordhaus et al. (2012) and are in line with the\ntheoretical expectations developed in the literature. Firstly,\npast military spending is a major determinant of current\nmilitary investments. Secondly, the log of real GDP, the\nproxy for the economic size and power of a state, displays\nthe expected positive sign and has a comparatively large\ncoefficient. Thirdly, a country's investment in defense does\nnot appear to be responsive to the expenditure of friendly\nnations, but seems to be affected by potential adversaries'\nspending. Finally, the higher the GDP/World GDP ratio for\na country, the higher its investment in the military. All other\nvariables, while being mostly statistically significant\naccording to conventional levels, display coefficients that\nare rather small in substance.\nNote, however, that Ward et al. (2010) forcefully remind\nus that empirical results in the form of regression coeffi-\ncients may not tell us much about the actual influence of\nspecific explanatory variables on military spending: policy\nprescriptions cannot be based on statistical summaries of\nprobabilistic models. Thus, we now proceed with in-sample\npredictions, out-of-sample forecasts and Bayesian model\naveraging. Moving from empirical analyses based on statis-\ntical significance to prediction/forecasting serves two pur-\nposes. Firstly, it allows us to discriminate among\nexplanatory factors more accurately according to their pre-\ndictive power. Secondly, it offers a more solid scientific\nbasis for assessing future levels of military spending, which\nis highly relevant from a policy perspective. In the follow-\ning, we focus on the third model specification in Table 1 as\nit appears more conservative than Models 1 and 2.\nIn-sample prediction\nHow effective is the model in predicting military spending\nin-sample? Put differently, how accurate are the \"condi-\ntional statements about a phenomenon for which the\nresearcher actually has data, i.e., the outcome variable has\nassess this, we compare the predicted yearly median levels\nof military spending using the estimated parameters from\nModel 3 with the truly observed levels. The results are\ndepicted in Figure 2: the model fails to predict the military\nbuild-up shortly after the Korean War (1954), underpredicts\nvalues toward the end of the 1990s, and overpredicts a peak\nnear the end of the Cold War. Still, this figure demonstrates\nthat the predicted values fit the time points of the actually\nobserved data reasonably well.\nTo assess the accuracy of this prediction more thor-\noughly, we use two goodness-of-fit measures: the mean\nsquared prediction error (MSPE) and Theil's U (Theil et al.,\n1966), which (unlike the MSPE) does not depend on the\nscale of the data (see also Bechtel and Leuffen, 2010).\nTheil's U is the square root of the ratio between the sum of\nsquared prediction errors of the baseline model (i.e., Model\n3) and the sum of squared prediction errors of a naive\nmodel, that is, a \"no-change prediction\" where the level of\nmilitary spending in t\u00ad1 fully corresponds to the level of\nmilitary spending in t. The closer the MPSE is to 0, the\nmore accurate is the model in making predictions.\nMoreover, if Theil's U is larger than 1, the model actually\nperforms worse than the naive model; values for Theil's U\n4 Research and Politics \nsmaller than 1 indicate that the \"theoretically informed\nmodel\" performs better than the naive specification. For\nour baseline model, the MPSE is 0.0879, while Theil's U is\nat 0.9643. Ultimately, therefore, the specifications used in\nModel 3 perform well in predicting military spending.\nFirstly, however, it remains to be seen how accurately\nthis model predicts military spending when moving to the\n\"harder\" test of an out-of-sample forecast. Put another way,\nwhat is the model's predictive power when trying to cor-\nrectly predict military spending that is not \"within the very\nsame set of data that was used to generate the models in the\nfirst place\" (Ward et al., 2010: 8)? Secondly, do some pre-\ndictors of Model 3 not contribute to the overall predictive\npower of this model \u00ad perhaps despite their statistical sig-\nnificance and the theoretical importance assigned to the\nsub-indicators of Pit\nMID\nmay therefore be dropped from the estimation? In order to\naddress both questions, we use out-of-sample forecasting\nand Bayesian model averaging, the issues considered next.\nOut-of-sample forecast\nFor the out-of-sample forecast, we use a four-fold cross-\nvalidation quasi-experimental setup that was repeated 10\ntimes (Ward et al., 2010) \u00ad either for the baseline model or\na model that omits an explanatory variable from the esti-\nmore detail than we can possibly do here due to space limi-\ntations. In short, however, this cross-validation randomly\ndivides our 5684 observations we used for Model 3 into\nfour segments. We then use three segments to estimate the\nparameters, while the fourth segment, also called the \"test\npredictive power of the the baseline model or a model that\nomits one of the predictors at each time on the pooled sub-\nsets. We drop one independent variable from the model at\na time in order to estimate the effect that this specific vari-\nable has on the model's ability to make out-of-sample fore-\ncasts. Again, we calculated the MPSE and Theil's U for\nmeasuring the predictive power, for which we then present\nthe average values over the 10 repetitions.\nTable 2 gives an overview of the baseline model's out-of-\nsample forecasting power and the individual contribution\nPeace Years\nDemocracy\nTrade/GDP\nContiguity\nAllies\nNumber of States in System\nGDP (ln)\nSpending Foes (ln)\nSpending Friends (ln)\nLagged Dependent Variable\nFigure 1. Coefficient plot of Table 1. Note: horizontal bars\npertain to 90% confidence interval; the vertical dashed line\nsignifies a coefficient value of 0.\nMilitary Spending (ln)\nFigure 2. Median levels of military spending: predicted (dashed\nline) and actual (solid line) values.\nTable 1. The determinants of national military spending \u00ad\nbaseline model (see also Nordhaus et al., 2012).\nStandard errors in parentheses; standard errors clustered on country in\nB\u00f6hmelt and Bove 5\neach of the variables makes. These contributions are meas-\nured in terms of the difference between the average value of\nthe baseline model's MSPE or Theil's U values on one hand\nand, on the other hand, the corresponding average goodness-\nof-fit measure's value calculated for a model that discards\nthat particular item. For example, excluding Peace Years\nfrom the baseline model leads to an increase in Theil's U\ntribute to the model's overall prediction and forecasting\npower by 0.0003 units according to Theil's U. Similarly,\nleaving out this variable induces an increase of 0.0001 in\nterms of the MSPE. The contribution of Peace Years to the\nmodel's forecasting power is therefore given, yet is small in\nsubstance, and this mirrors the findings for most other pre-\ndictors. Five variables constitute an exception to this,\nthough, as these seem to be major contributors to the mod-\nel's forecasting power: Democracy, Number of States in\nSystem, GDP (ln), Spending Foes (ln) and the lagged\ndependent variable. Overall, the four-fold cross-validation\nsuggests that these five contribute the most to the overall\npredictive power of the baseline model taken from Nordhaus\nTwo additional conclusions can be derived from these\nfindings. Firstly, none of the included predictors in Model 3\nactually worsens the forecasting power; that is, neither\nTheil's U nor the MSPE decrease when leaving out an item\nfrom the model specification and running the four-fold\ncross-validation. While this may constitute \"good news\",\nnote that there are several variables that are unlikely to have\nany impact on the forecasting power at all. Secondly, those\nfive variables that our out-of-sample analysis highlights as\nthe most important factors for predicting and forecasting\nfuture national military expenditure largely pertain to the\neconomic size of a country and its regime type. Most of the\nproxies for the international security environment, which is\ntreated as one of the core factors in, for example, Nordhaus\net al. (2012), are unlikely to matter \u00ad the military spending\nof foes and the total number of states in the international\nsystem are the only exceptions. This highlights (again) not\nonly the importance of going beyond statistically signifi-\ncant coefficients, but also that it was crucial to actually dis-\naggregate the MID involvement indicator used by Nordhaus\nBayesian model averaging\nThe four-fold cross-validation approach in the previous\nsection does not necessarily suggest that one should drop\nany variable from our baseline model in order to maximize\naccuracy in predictions and forecasts; however, some vari-\nables hardly make any contribution at all to the forecasting\npower and may therefore be dropped, even if only for effi-\nciency reasons. To further examine this issue, we imple-\nment a final methodological approach that similarly\naddresses both model and parameter uncertainty: Bayesian\nmodel averaging (Amini and Parmeter, 2011; Fern\u00e1ndez\ndetailed and formal overview of this method is given in the\nprovided references. In brief, however, Bayesian model\naveraging deals with the uncertainty about one model\nspecification \u00ad one specification that may be only one out\nof many. Inferences based on one model only, however,\nmight be limited and, thus, they should rather reflect the\nambiguity about the model. Bayesian model averaging\naddresses this by considering all possible combinations of\nvariables (in our case, there is a model space of 2048 differ-\nent models, as we have 11 predictors) in order to increase\nmodel fit, that is, \"all inference is averaged over models,\nusing the corresponding posterior model probabilities as\nmeasures such as the Akaike Information Criterion (AIC)\nor the Bayesian Information Criterion (BIC), while taking\nthe entire predictive distribution into account (Raftery,\nTable 2. Out-of-sample forecasting power.\nExcluded variable Mean U Mean MSPE U MSPE\n6 Research and Politics \nmodel space, while the data will then lead to a posterior\nprobability, which can be used to identify the \"best\" model\nthat is usually the one with the highest posterior probability.\nBayesian model averaging then uses these posterior model\nprobabilities as weights in order to mix over models (Steel,\n2014: 4). Ultimately, we are thus able to calculate posterior\ninclusion probabilities (PIPs), that is, the sum of posterior\nmodel probabilities for all models where a covariate was\nincluded (Amini and Parmeter, 2011), for entire models or\nsingle predictors in order to determine, in turn, which vari-\nables should be incorporated in a model and which ones can\nsafely be ignored.\nFigure 3 summarizes the predictors'PIPs for five differ-\nent specifications, that is, uniform, Bayesian (or) Risk\nInflation Criterion (BRIC), fixed, PIP and random, of the\nBayesian model averaging using the BMS package in R\n(Zeugner, 2012). These five specifications essentially per-\ntain to alternative settings of the (unit information) priors\nthat we assign to the 2048 models or the different predic-\ntors ex-ante. This is important, as the use of different prior\nassumptions can lead to very different results (Steel, 2014:\n2). In more detail, apart from the BRIC specification, all\nsetups rely on Zellner's g-prior as the unit information\nprior, while uniform has a uniform model prior, fixed has a\nbinomial model prior, PIP assigns a prior inclusion prob-\nability of 10% for the lagged dependent variable and 50%\nfor all other covariates4 and random assigns a beta-bino-\nmial model prior. Finally, BRIC has a uniform model prior,\nbut relies on a Markov Chain Monte Carlo (MCMC) sam-\npling technique, that is, \"the birth-death sampler\" in our\nmodel probabilities asymptotically either behave like the\nThe results in Figure 3 are highly robust across prior\nspecifications and shed more light on the findings we\nobtained from the four-fold cross-validation. Specifically,\nDemocracy, Number of States in System, GDP (ln) and the\nlagged dependent variable are characterized by PIPs that\nare close to 1.00, meaning that the sum of posterior model\nprobabilities for all models wherein these predictors were\nincluded is consistently at or close to 100%. Moreover,\nSpending Foes (ln) and Peace Years have PIPs that are on\naverage higher than 0.50, that is, the sum of posterior\nmodel probabilities for all models wherein these predictors\nwere included is consistently at or close to 50%. All other\npredictors display PIPs that are, sometimes quite substan-\ntially well, below 0.50 and we thus drop these for the final\nmodel. Interestingly, apart from Spending Friends (ln),\ndyadic versions of all variables we drop for the final model\nare actually included in Pit\nMID\n\n, the ex-ante probability of a\ncountry being involved in a MID (Nordhaus et al., 2012).\nThis highlights one more time the importance of disaggre-\ngating this variable and going beyond Nordhaus et al.\nDiscussion and conclusion\nIn light of the previous sections, we re-estimated the base-\nline model while leaving out the weakest predictors, that is,\nContiguity, GDP/World GDP, Trade/GDP, Spending\nFriends (ln) and Allies. Afterwards, we performed another\nround of four-fold cross-validations that we repeated again\n10 times (Ward et al., 2010) \u00ad either for the original base-\nline model or the new model that now omits the five weak-\nest predictors. When assessing the predictive power of the\nlatter specification, both the MSPE and Theil's U should\nhave lower values than the original model if the new, and\narguably more parsimonious model, is more powerful in\npredicting and forecasting future values of military spend-\ning. Table 3 summarizes our findings. This table clearly\nshows that the newly specified, alternative baseline model,\nwhich omits five predictors of the original baseline model,\nis not only more parsimonious, but also slightly more accu-\nrate than the model's original specification: the MSPE is\nthan the original baseline model's value.\nTo conclude, we assessed whether important drivers of\nmilitary expenditure put forward by the existent literature\nallow us to predict and forecast future levels of national\ninvestment in the military. We focused on a recent model by\nvariables perform relatively well in the out-of-sample pre-\ndiction and the Bayesian model averaging.\nTesting the validity of existing theoretical accounts has\nimportant implications for theory development and can also\noffer significant benefits for policymakers in terms of\nPIP\nLagged dependent variable\nDemocracy\nGDP (ln)\nNumber of States in System\nSpending Foes (ln)\nPeace Years\nContiguity\nTrade/GDP\nSpending Friends (ln)\nAllies\nUniform\nBRIC\nFixed\nPIP\nRandom\nFigure 3. Bayesian model averaging \u00ad posterior inclusion\nprobabilities.\nB\u00f6hmelt and Bove 7\neffectively allocating scarce resources to the many areas of\ngovernment spending. Specifically, our research finds that\nprevious levels of military spending as well as a country's\ninstitutional and economic characteristics particularly\nimprove our ability to predict future levels of investment in\nthe military. Variables pertaining to the international secu-\nrity environment also matter, but seem less important. Given\nthat most, if not all, of the best predictors we identified are\neither (largely) time-invariant or known ex-ante, we strongly\nbelieve that our work helps scholars and policymakers alike\nto foresee states' military expenditures more accurately.\nIn light of this, using the new model, we also assess the\nprediction and forecasting power for individual countries.\nThis gives us a more accurate idea of where the \"good\"\npredictions come from and whether the fairly high levels of\nprediction/forecasting accuracy are driven by particular\nstates. It is our hope that this also increases the policy rele-\nvance of our research. To this end, we use the new model\nspecifications for a sample that comprises one out of all 141\nstates only, assessed the prediction power via Theil's U and\nthe MSPE, and repeated this exercise for all countries in\nour data. Tables 4 and 5 display our findings as we sum-\nmarize the \"top-five\" and \"worst-five\" cases, respectively,\nin terms of prediction accuracy.\nWhile several conclusions can be derived from these final\ntables, we would like to highlight two of them. Firstly, despite\nsome exceptions, most of the \"top-five\" countries are fairly\nwell developed, have a relatively high economic power, and\nare democracies. Apparently, these characteristics line up\nwell with our strongest predictors identified above and, hence,\nfacilitate accurate forecasts. Similarly, most of the \"worst-\nfive\" states lack these characteristics, although exceptions do\nexist here as well. Secondly, somewhat surpirisingly, the\nUnited Kingdom is one of the best predictive cases according\nto the MSPE, but belongs to the worst cases in terms ofTheil's\nU. While this is arguably driven by some differences for cal-\nculating these statistics, also note that Theil's U for the United\nKingdom still is well below 1.00.\nUltimately, we believe that future work should ensure that\nforecasting becomes a more systematic empirical tool in the\ncurrent research on countries' strategic decisions. Similarly,\nand against this background, next to directly assessing the\npredictive power of some of the determinants of military\nspending, our research also sought to further develop the\nmodel by Nordhaus et al. (2012). The model that we identi-\nfied as the new, alternative specification might consequently\nbe used in future research as a baseline model against which\nto assess new variables of theoretical interest.\n"
}