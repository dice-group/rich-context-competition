{
    "abstract": "Abstract\nDNA sequencers, Twitter, MRIs, Facebook, particle accelerators, Google Books, radio telescopes, Tumblr: what do these\nthings have in common? According to the evangelists of ``data science,'' all of these are instruments for observing reality\nat unprecedentedly large scales and fine granularities. This perspective ignores the social reality of these very different\ntechnological systems, ignoring how they are made, how they work, and what they mean in favor of an exclusive focus on\nwhat they generate: Big Data. But no data, big or small, can be interpreted without an understanding of the process that\ngenerated them. Statistical data science is applicable to systems that have been designed as scientific instruments, but is\nlikely to lead to confusion when applied to systems that have not. In those cases, a historical inquiry is preferable.\n",
    "reduced_content": "Commentary\nBig Data and reality\nRyan Shaw\n Keywords\nBig Data, software architecture, data modeling, design, historical methods, ontology\nYahoo Research Berkeley (YRB), part of a group of\ngraduate students, UC Berkeley faculty, and profes-\nsional researchers with a remit to study the vast stores\nof data generated by Yahoo's social media ``properties''\nlike Flickr and Delicious, and to generate and experi-\nment with ideas for new properties (Qi, 2005; Shamma\net al., 2007). I recall one afternoon listening to another\nresearcher present some slides describing the current\nstate of the Flickr dataset. He was projecting on the\nscreen a plot of the number of photographs uploaded\nper user account. The plot had the typical power law\nshape characteristic of many variables in social media\ndatasets (a few people uploading many photos, many\npeople uploading a few or no photos) but it also had\nspikes at regular intervals--every multiple of six. After\nletting the room speculate for a while about why Flickr\nusers seemed to prefer uploading in batches of six, the\npresenter showed the next slide: a screenshot of Flickr's\nupload page, a grid of six forms, each of which could be\nused to select a JPEG from the user's file system and\nadd some textual description. Flickr users hadn't been\nfreely choosing to upload in batches of six, but neither\nhad they been forced to; it simply had been strongly\nsuggested that they should.\nSoftware interfaces are suggestive, sometimes liter-\nally so. One of the phenomena we were particularly\ninterested in at YRB was ``tagging.'' Sites like Flickr\nand Delicious allowed users to add uncontrolled key-\nwords or ``tags'' to items such as the photos they\nuploaded or the URLs they saved, and we were inter-\nested in understanding how these tags were chosen and\nused. One complication of studying datasets of tag\nassignments was that some sites had interfaces that,\nafter a user had typed a few letters of a tag, would\nsuggest possible completions, one of which could then\nbe chosen with a keystroke. How this autocompletion\ninfluenced users' tag assignments would depend on how\nautocompletion was implemented. One implementation\nmight suggest tags most often assigned by other users\nto the same item, while a different implementation\nmight suggest tags most often assigned by that user to\nany item in the past. The choice between these two\nalternatives (which are far from exhaustive) is con-\nstrained by the underlying data model. The first imple-\nmentation assumes that it is meaningful for two\nSchool of Information and Library Science, University of North Carolina\nat Chapel Hill, Chapel Hill, NC, USA\nCorresponding author:\nRyan Shaw, School of Information and Library Science, University of\nEmail: ryanshaw@unc.edu\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/\nopen-access-at-sage).\ndifferent user accounts to assign the same tag to the\nsame item (otherwise why bother suggesting a tag\nthat has already been assigned to that item by another\naccount?). This assumption in turn requires a data\nmodel in which a tag is modeled as a relation between\na user account and an item. If instead a tag was mod-\neled as a property of an item, then a given tag could\nonly be assigned to a particular item once.\nDecisions about how to model user accounts, items,\ntags, and the relations among them influence, and are\ninfluenced by, decisions about how the tagging inter-\nface should look and work and decisions about the\nalgorithm used to implement autocompletion. These\ndecisions in turn reflect decisions about the nature\nand purpose of the system. An image-sharing system\nintended to be used primarily for re-sharing images\noriginating elsewhere (for example news photos,\nmemes, or screenshots) might choose to model tags as\nrelations between user accounts and images, implying\nthat different accounts' tags for an image express\ndifferent interpretations of it. On the other hand an\nimage-sharing system intended for sharing one's own\nphotographs might choose to model tags as properties\nof an image, implying that the tags express an author-\nized description or categorization. Of course tools are\nnot always used as intended, but an understanding of\nhow a system was intended to be used, and how those\nintentions were hypostatized in the system's design, can\nhelp one interpret users' conduct: to what extent are\nthey working ``with'' or ``against'' the system? Taken\nas a whole, the data model and other design decisions\nmade during the development of a software system con-\nstitute the system's architecture (Taylor et al., 2010: 1).\nUnderstanding a software system's architecture is\npart of recognizing what Kirschenbaum (2008) has\ncalled the formal materiality of the system, the effects\nof a set of choices that determine what will be easy\nto do--``frictionless,'' to use the industry lingo\n(Wikipedia, 2014)--and what will not. Even the sim-\nplest piece of software has embedded within it a series\nof architectural decisions about what ``works'' with\nrespect to the purposes for which it was created. This\narchitecture is layered and constantly evolving.\nSoftware engineer Jean-Baptiste Que\nelegantly of the ``dizzying but invisible depth'' of\nlayered complexity that characterizes contemporary\nsoftware architecture. Engineers try to manage this\ncomplexity by treating systems as consisting of layers,\nabstractions which help one avoid having to think too\nmuch about the dizzying complexity of the overall\nsystem. In theory this means that the architectural deci-\nsions made at a given layer only have ramifications for\nadjoining layers. The ability to make new decisions at a\ngiven layer without re-engineering the entire stack is\nwhat allows software systems to evolve. Evolvability\nis a critical requirement for any real-world system,\nbut it further complicates the problem of mapping the\nontological terrain of software, as that terrain is con-\nstantly shifting: interfaces are redesigned, algorithms\ntweaked, data remodeled. Sometimes these changes\nreflect deeper changes in the designers' conception of\nthe system, which in turn can result from their obser-\nvations of how the system is being used. Flickr, for\nexample, started life as a chat network. Only later,\nafter a critical mass of users began using it for sharing\ntheir own original images, did the Flickr designers re-\nconceive it as a photo-sharing site (Hoopes, 2004).\nTo study Big Data is to study the traces left behind\nby the use of a large, complex, and constantly evolving\nsoftware system. These traces excite many social scien-\ntists, as they seem to provide fine-grained documenta-\ntion of social or cultural ``transactions.'' The dominant\nform of studying these traces is ``data science,'' which\ntreats the large software systems that generated them as\nmeasuring instruments (Loukides, 2010). Wired editor\nChris Anderson (2008) famously proclaimed that the\nsheer quantity of data produced by such systems\nmade them scientific instruments, even if they lacked\nany coherent model informing their design.\nBut it is not heaps of transactional data that make\nan inquiry scientific. Being scientific is an effect of work\ndone to establish stable, quantifiable concepts, and\nthe aim of science is to establish resilient statistical rela-\ntionships among those concepts (Oakeshott, 2002:\nthe data, but the stable, measurable concepts do not:\nthe concepts are a prerequisite for the existence of the\ndata. Thus data scientists must design and engineer\nmeasuring instruments that will produce data usable\nwithin their conceptual framework; the architecture of\nthose instruments must cohere with that framework.\nResearchers at Facebook and the other corporate\nowners of Big Data-generating systems recognize this,\nand in these organizations data scientists work with the\nengineers designing the systems to establish their use-\nfulness as measuring instruments (Fiore, 2015). This is\none possible path for social and cultural inquiry: scien-\ntists closely cooperating with engineers to simultan-\neously build massive software systems and study the\nbehavior of people using them. It is a path that leads\nsocial research outside the academy, into a few mas-\nsively resourced private or government-run research\nIn 1978 a research programmer at IBM named\nWilliam Kent wrote Data and Reality, a meditation\nupon the complexities of data modeling. The book sur-\nveyed the problem of how the data models that form\npart of the architectures of information systems relate\nto our shared reality. Kent's examples were mundane:\nbooks in a library, parts in warehouses, and players on\n2 Big Data & Society\nsports teams. He demonstrated that, even for such\nsimple applications, deciding how to store data involves\nanswering a number of interrelated questions. What is\none thing? How many things are there? What kinds of\nthings are there? How real are they? How long do they\nlast? Kent emphasized that there are no right answers\nto such questions: different people in different contexts\nwith different goals will choose different answers as they\nconstruct their data models. Data models are practical\ntools; like maps, they are ``correct'' to the extent that\nthey get you where you want to go. Furthermore tools\nare evaluated according to a host of criteria that have\nmuch do they cost? How fast do they run? How often\ndo they break? How often do they need to be updated?\nHow much training do they require to use? How\nquickly do they become obsolete? What guarantees do\ntheir makers provide? As a result, the data stored and\nemitted by software do not reflect a coherent theory of\nthe world, but ``an amalgam of fragments of theories''\nand loosely joined.\nKent's reflections cast doubt on the possibility of\ndata science. A scientific community requires measur-\ning instruments that make manifest a conceptual frame-\nwork widely shared by that community. Kent, after 200\npages of reflecting on his experience designing systems\nfor data processing, concluded by highlighting how dif-\nficult it is to achieve such a shared view through the\nmediation of an information system. If the scope of\nsuch a system is sufficiently local and limited, he wrote:\nwe can share a common enough view of [reality] for\nmost of our working purposes, so that reality does\nappear to be objective and stable. But the chances of\nachieving such a shared view become poorer when we\ntry to encompass broader purposes, and to involve\nmore people. This is precisely why the question is\nbecoming more relevant today: the thrust of technology\nis to foster interaction among greater numbers of\npeople, and to integrate processes into monoliths ser-\nving wider and wider purposes. It is in this environment\nthat discrepancies in fundamental assumptions will\nThe broader the array of uses of a system, the less con-\nceptual coherence its architecture will exhibit, and the\nless useful it will be as a scientific instrument. If Kent\nwas right, we need not data science but data history.\nA historical approach would treat tweets and posts\nand comments and links and all the rest not as scientific\nobservations but as\nexploits, human doings which have been performed,\nutterances which have been pronounced, artefacts\nwhich have been made, fragments of the bygone pur-\nposive engagements of their perhaps unknown authors\nwhich have survived (although sometimes recognizably\ndamaged) and are themselves now present. (Oakeshott,\nThis shift, from viewing Big Data as scientific measure-\nments toward viewing them as traces left by past\nengagements, changes the character of Big Data-\ndriven inquiry. Treated as the subject of a scientific\ninquiry, 100 million tweets are a series of observations\ngenerated by the same implicit and unchanging mech-\nanism, the nature of which is to be discerned via stat-\nistical generalization from that series. Treated as the\nsubject of a historical inquiry, 100 million tweets are\nan assembly of individual utterances, the circumstantial\nrelations among which must be discerned through a\nprocess of mutual criticism and interpretation. From\nthese circumstantial relations one may be able to infer\nsomething about the practices and conventions of\nTwitter users and designers. Twitter users participate\nin various complexes of purposive activity--fandom,\nrecruitment, hashtag activism, bots, spam, ``weird\nTwitter,'' and so on. These practices leave traces that\nare interpreted by Twitter designers--not only the\ndesigners employed by Twitter, but anyone who designs\nsoftware that interoperates with Twitter.1 The designers\nin turn discourage certain practices and encourage\nothers via architectural decisions, decisions that are\ninfluenced not only by their interpretations of user\npractices but also by available technologies, competing\nproducts, and prevailing fashions of software develop-\nment. This process does not work to stabilize a set of\nassumptions about social reality; but perhaps careful\ninterpretation of the big and small data this process\nleaves behind can tell us something about the social\nreality of which it is a part.\nLouis Mink, arguing for the independence of histor-\nical understanding from scientific explanation, wrote\nthat:\n. . . the more I know about the facts of the case, the\nmore necessary it becomes to use something like empa-\nthy in order to convert an indigestible heap of data into\na synoptic judgment by which I can ``see together'' all\nthese facts in a single act of understanding. Otherwise,\nif I am asked what I have learned, I can only point\nThe study of Big Data could lead to a more compre-\nhensive understanding of social reality. But achieving\nthat understanding will require developing a sense of\nthe complex materiality of our Big Data-producing\ninformation systems, and empathy for the people who\nfund, design, build, use, and exploit them. Without that\nsense and empathy, when we are asked what we have\nlearned from Big Data, we may be left pointing mutely\nat our data centers.\nDeclaration of conflicting interests\nThe author(s) declared no potential conflicts of interest with\nrespect to the research, authorship, and/or publication of this\narticle.\nFunding\nThe author(s) received no financial support for the research,\nauthorship, and/or publication of this article.\nNote\nopers had created applications interoperating with their\n``ecosystem.''\nReferences\nAnderson C (2008) The end of theory: The data deluge makes\nthe scientific method obsolete. Wired, 23 June. Available\narchive.wired.com/science/discoveries/magazine/16-07/\nKilgour Lecture, Chapel Hill, NC, 16 March. Available at:\nHoopes H (2004) Reply to discussion thread ``Unfound?''.\nFlickrCentral. Available at: https://web.archive.org/web/\nKent W (1978) Data and Reality: Basic Assumptions in Data\nProcessing Reconsidered. Amsterdam: North-Holland.\nKirschenbaum M (2008) Mechanisms: New Media and the\nForensic Imagination. Cambridge, MA: MIT Press.\nLoukides M (2010) What is data science? O'Reilly Radar,\n2 June. Available at: https://web.archive.org/web/\nMink L (1966) The autonomy of historical understanding.\nOakeshott M (2002) Experience and its Modes. Cambridge:\nCambridge University Press.\nQi W (2005) Yahoo, UC Berkeley join to create research lab.\nThe Daily Californian, 18 July. Available at: https://web.\nQue\n/https://plus.google.com/\u00feJeanBaptisteQueru/posts/dfyd\nShamma DA, Shaw R, Shafton PL, et al. (2007) Watch what\nI watch: using community activity to understand content.\nIn: Proceedings of the international workshop on multimedia\nTaylor RN, Medvidovic\nArchitecture: Foundations, Theory, and Practice. Hoboken:\nWiley.\nTwitter (2011) One million registered Twitter apps. Available\nblog.twitter.com/2011/one-million-registered-twitter-apps\nWikipedia (2014) Frictionless sharing. Available at: https://\nen.wikipedia.org/w/index.php?title\u00bcFrictionless_sharing\nWilliamson B (2014) The death of the theorist and the emergence\nof data and algorithms in digital social research. Impact\nof Social Sciences blog, The London School of Economics\nand Political Science, 10 February. Available at: https://\nThis article is part of a special theme on Colloquium: Assumptions of Sociality. To see a full list\nof all articles in this special theme, please click here: http://bds.sagepub.com/content/colloquium-\nassumptions-sociality.\n4 Big Data & Society"
}