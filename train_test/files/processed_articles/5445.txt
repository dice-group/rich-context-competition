{
    "abstract": "Abstract\nManual coding of political events from news reports is extremely expensive and time-consuming, whereas completely\nautomatic coding has limitations when it comes to the precision and granularity of the data collected. In this paper, we\nintroduce an alternative strategy by establishing a semi-automatic pipeline, where an automatic classification system\neliminates irrelevant source material before further coding is done by humans. Our pipeline relies on a high-performance\nsupervised heterogeneous ensemble classifier working on extremely unbalanced training classes. Deployed to the Mass\nMobilization on Autocracies database on protest, the system is able to reduce the number of source articles to be human-\ncoded by more than half, while keeping over 90% of the relevant material.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons\nAttribution-NonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use,\nreproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open\nAccess pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nIntroduction\nLarge-scale extraction of event data from unstructured\nnews reports produced by global news agencies has been a\ntopic in political science for almost three decades. Indeed,\nmany leading data collections in political science follow\nthis strategy. We can distinguish two different approaches:\none involving manual reading and coding of events by\ntrained human coders, used for example by the Uppsala\nConflict Data Program's Geo-referenced Event Dataset\n(Sundberg and Melander, 2013) or the Armed Conflict\nLocation and Events Dataset (Raleigh et al., 2010). The\nother approach, automated coding, uses various algorithms\nto extract events computationally from the source text, as\nfor the Kansas Event Data System (Schrodt and Gerner,\n1994) or the recent ICEWS event dataset (Boschee et al.,\nHuman coding, while producing high-quality content\nand being extremely flexible in terms of the type of infor-\nmation that can be extracted, is very costly and time con-\nsuming (Schrodt and Van~Brackle, 2013). Automated\ncoding, while quite successful in categorizing news articles\ninto various topics of interest or extracting actors from\nknown actor lists (dictionaries), has been demonstrated to\nhave shortcomings when it comes to extracting actual con-\ntent from the text (e.g. the number of fatalities, the date of\nincident rather than the date of reporting, or the location),\nthe actors involved, or the relations between them (Boschee\nis relative agreement that the signal-to-noise ratio of auto-\nmatically coded datasets make their usage at the most gran-\nular level (the individual incident) difficult, and that various\naggregation techniques are required to eliminate unwanted\nIn this paper, we present a coding pipeline that uses the\nbest of both worlds by combining the two approaches\ndescribed above--in other words, a semi-automatic proce-\ndure. In this pipeline, the source material is first screened for\nrelevant news articles through machine learning techniques.\nImproving the selection of news\nreports for event coding using\nensemble classification\nMihai Croicu1 and Nils B Weidmann2\n Keywords\nProtest data, text classification, machine learning\nDepartment of Peace and Conflict Research, Uppsala University,\nSweden\nDepartment of Politics and Public Administration, University of\nKonstanz, Germany\nCorresponding author:\nNils B Weidmann, Department of Politics and Public Administration,\nEmail: nils.weidmann@uni-konstanz.de\nResearch Article\n2 Research and Politics \nThe remaining, much smaller set of potentially relevant arti-\ncles are assigned to human coders for further processing.\nThis paper introduces the first part of this pipeline, the auto-\nmatic pre-selection of source material. In a typical event\ncoding project, the large amount of articles irrelevant for\ncoding is a huge issue, and the prime reason why human\ncoders are so slow in comparison to computers. Therefore,\nimproving the pre-selection of relevant articles is a key issue\nwe need to resolve. We proceed by describing our use\ncase--a protest event data project focusing on autocracies.\nWe then implement a pipeline for the pre-selection of arti-\ncles using a versatile bagging ensemble classifier. In an out-\nof-sample validation, we show that this pipeline is able to\nachieve high predictive performance when applied to a\nnumber of different countries.\nUse case and technical considerations\nWhen coding political events from news reports, coders\ntypically encounter large amounts of articles unrelated to\nthe dataset under construction. The main objective of the\nfirst part of our semi-automatic coding process is therefore\nto eliminate as many of these \"irrelevant\" news articles,\nwhile retaining as many as possible of all \"relevant\" arti-\ncles such that the latter can be given to a trained human for\nfurther coding. We demonstrate this approach in the context\nof an ongoing coding project, the Mass Mobilization on\nAutocracies database, a new event dataset on mass mobili-\nzation under autocratic regimes (R\u00f8d and Weidmann,\n2013). However, the design is project-agnostic, such that it\ncan be used in other data projects.\nSources. The source material for the coding are news arti-\ncles furnished by global news bureaus (e.g. Reuters,Agence\nFrance Presse, Associated Press, or BBC Monitoring)\nthrough aggregators such as LexisNexis or Factiva. This\ntype of source material is probably the most common basis\nfor large-scale event datasets used in political science and\ninternational relations (Brandt et al., 2011).\nWe obtain the source material through a simple keyword\nsearch with different synonyms for political protest. No\npre-filtering of articles is done at the time of retrieval.\nGiven the fact that the keywords used are extremely com-\nmon, a vast majority of the retrieved articles do not contain\nrelevant information (i.e., they cover topics such as sports,\nfinance, education, etc.). Thus, the number of irrelevant\narticles vastly exceeds those covering events of interest to\nthe project. Alternative pre-filtering through proprietary\ntools such as the categories provided by LexisNexis was\nruled out for lack of transparency and replicability. The\nextracted articles consist of a headline, a dateline, a body,\nand a unique ID.\nTraining set. During the first one year phase of the project,\naccording to the coding procedure described in R\u00f8d and\nWeidmann (2013).1 This set of articles constitutes the\ntraining set of our machine learning-based procedure.\nWhile the coders extracted all the information relevant to\nthe project such as the number of protesters, the issue, and\nthe actors involved, we only use the information whether\nan article was actually considered relevant--in other\nwords, whether at least one protest event was coded from\nit by the coder. This way, a large training set with human-\nannotated binary class labels (relevant/irrelevant) was\ngenerated. Descriptives for the training set are presented\ngraphically in Figure 1.\nA machine learning task. Our goal is to create a pre-filtering\npipeline keeping as many of the relevant articles as pos-\nsible, while discarding most of the irrelevant ones. There-\nfore, the overall requirements for this pipeline are\nsomewhat different than those for most machine learning\nprocedures. Traditionally, machine learning algorithms\nattempt to achieve the best trade-off between precision\nand recall. For our application, as the machine filtering is\njust one stage of the complete coding pipeline, recall\nclearly has a higher priority than precision. Ideally, we\nwant all relevant articles to be labeled as true (recall as\nclose to 100% as possible), whereas the number of cor-\nrectly labeled irrelevant articles is of lesser importance (if\nsome irrelevant articles are wrongly labeled as relevant,\nthey can still be eliminated by coders). Therefore, the\nmain performance indicator we aim to optimize is the\nrecall of truly relevant articles, and we consider values of\nImplementation\nWe implement the solution as a pipeline, with the starting\npoint being raw, unprocessed news articles extracted\ndirectly from LexisNexis. Following standard procedures,\nfrom each article we extract a set of features that is later\nused for classification.\nText processing and feature extraction\nWe apply standard natural language pre-processing such\nas sentence and word tokenization, lemmatization and\nstop-word removal, as well as removal of punctuation.\nWe also experimented with part-of-speech tagging meth-\nods with the goal of eliminating low-information words\nsuch as adjectives and numerals. Most of these methods\nperformed extremely well at their task, but were not\nincluded in the final pipeline due to slow performance\nand extremely modest improvements to the classifica-\ntion results. Similarly, named entity recognition was\nattempted in order to make the classifying pipeline as\ngeography- and name-agnostic as possible; however,\nCroicu and Weidmann 3\nagain, the computational performance costs were mas-\nsive and produced only small improvements in predic-\ntive performance.\nTherefore, training and classification is done at the\nword-level. For each article in each iteration, a vector of\nclassification features consisting of individual words in that\narticle is extracted (\"bag of words\"). We extract the most\nused individual word stems (unigrams) as well as the most\nused two-word groupings/expressions (bigrams) from the\ntraining set used for each classifier, and test for their exist-\nence in each article. Some classifiers (such as Naive Bayes\n(NB) can only deal with a limited number of features. For\nthat reason, we selected the most frequent 750 unigrams\nand 250 bigrams for each corpus of text used by each indi-\nvidual classifier.\nThis number was reached through multiple small-scale\ntests on blocks of 5000 articles, and offers an excellent\nspeed (computer performance)\u00adaccuracy (model perfor-\nmance) compromise for classification. In small scale tests,\nincreasing the number of unigrams in the features vector\nfrom 50 to 250 increased the recognition of relevant arti-\nincrease tapered off at approximately 500 unigrams.\nSimilarly, increasing the number of bigrams, as well as\nincreasing the proportion of bigrams to unigrams provided\nincreases in performance, but at the cost of increasing the\nnumber of false positives substantially (at 500 unigrams\nand 165 bigrams, almost 40% of all irrelevant articles were\ncorrectly labeled and thus eliminated; while at 500 uni-\ngrams and 500 bigrams, this dropped to under 20%; recog-\nnition of relevant articles was one percentage point better\nin the latter case). We considered this trade-off (20% more\nirrelevant articles kept for manual observation in exchange\nof 1% more relevant articles saved) unacceptable, and thus\nset a low number of bigram features.\nThe ensemble classifier\nThe next step in the pipeline is the classification stage. As\ndescribed above, the key challenge we face is the extreme\nimbalance of the class distribution; only about 2% of all\narticles are truly relevant. Most classifiers tend to perform\nextremely poorly in such an environment, sometimes sim-\nply ignoring the smaller class by labeling all instances as\nbelonging to the majority class (Tang et al., 2009; Chawla\net al., 2004). Multiple solutions have been proposed, such\nas using weighting, various random sampling techniques\n(Chawla et al., 2004), and even multi-step classification\nwith a large-class structured sampling methodology as a\npre-selection phase (Tang et al., 2009).\nOur approach to solving the problem of class imbal-\nance is inspired by the random sampling methodology:\nemploying an ensemble classifier consisting of multiple\nbase classifiers, each of which is trained on a balanced\nFigure 1. The composition and distribution of the dataset employed for training and testing.\n4 Research and Politics \nsubset of the training set. Ensemble classification is an\napproach where, rather than a single classifier, a set of so-\ncalled base classifiers is created during the training phase.\nWhen classifying a new instance, each of these classifiers\nthen votes on the new instance, and the aggregated vote\n(for example, the majority) is then used as the ensemble's\nprediction. Ensemble prediction has long been used in\nmachine learning, and is known to have a number of\nconstruction of ensemble classifiers can be done in a num-\nber of ways. Bagging (short for \"bootstrap aggregation\")\nis one of the simplest procedures (Breiman, 1996).\nBagging involves the random creation of multiple training\nsamples (\"bags\"), each of which is then used for the crea-\ntion of one base classifier. These samples are created by\nrandomly drawing (with replacement) from the set of all\ntraining instances. This, however, means that the bags\nexhibit the same class distribution as the entire training\ndataset. In our case, this would inevitably lead to the prob-\nlem described above, i.e. \"degenerated\" base classifiers\npredicting the majority class only.\nFor that reason, we modify the standard bagging proce-\ndure to address the problem of class imbalance. As pro-\nposed by Weidmann (2008), we draw balanced random\nsamples from the training instances that have an equal pro-\nportion of relevant and irrelevant articles. Since having\npartially overlapping data is generally considered as\nadvantageous for classification (Dietterich, 2000), we\ndeviate from Weidmann's approach of using the entire set\nof relevant articles along with randomly selected irrelevant\nones. Instead, we split the relevant training articles into a\nnumber of \"shards\" (five shards, each 1200 articles long),\nand include a single shard with an equal-sized random\nsample from the irrelevant articles category in each bag.\nTherefore, each bag has a total of 2400 training articles\neach base classifier can be trained on a balanced sample,\nwhich avoids the problem discussed above (see Figure 2,\nleft). When given a new article for classification as rele-\nvant/irrelevant, each base classifier first generates an\nindividual prediction. Then, the ensemble's aggregate pre-\ndiction is generated through a voting procedure where an\narticle is classified as relevant if a certain percentage of the\nbase classifiers have classified the article as relevant\n(Figure 2, right).\nThree questions remain before we can launch the ensem-\nble classifier. First, we need to select the base classifiers to\nbe used in the ensemble. We chose algorithms that are well\nsuited for text classification purposes: support vector\nmachines (SVM) and NB (Joachims, 1998; Manning et al.,\n2008). Due to their high computational costs, other com-\nmonly used algorithms used in text classification such as\nnot considered. The individual classifiers themselves are\nsimple SVMs (with a radial basis function) provided by the\nPython scikit-learn module, and the NB classifier with a\nfrom Python's Natural Language Toolkit. For the SVM,\nlinear kernels were also considered, but prior experimental\ntesting indicated worse results (a decrease of 1.5\u00ad5%) on\nthe same dataset compared to RBFs. The combination of\nboth classifiers was optimized such that every pair of SVM\nand NB classifiers share single a bag of data, which speeds\nup the process.\nThe second question we need to resolve is the number of\nbags. Theoretically, we expect that the number of bags\nrequired to maximize the quality of prediction is fairly lim-\nited. Both SVM and NB classifiers tend to be relatively\nFigure 2. General architecture of the classifier ensemble. Dark squares represent the relevant articles in the training set.\nCroicu and Weidmann 5\nstable (Bousquet and Elisseeff, 2002; Ting and Zheng,\n2003), which means that changes in the training data have a\ncomparatively small impact on the structure of the classi-\nfier. This means that the number of required bags need not\nbe larger than the amount of new information brought to the\nmodel. Given the relative homogeneity of the \"relevant\"\ncategory, we presume that the value of additional bags will\nbe limited after a certain threshold has been reached. In\nlimited experiments, this stability point was identified at\napproximately 10\u00ad15 bags. However, to be on the safe side,\ngiven some concerns with regards to data heterogeneity,\nand since performance did not alter beyond reason, all real-\nusage training was conducted with 50 bags. In total, 100\nbase classifiers are trained, i.e. 50 pairs of one NB and one\nSVM classifier. Each such pair is trained on a bag of nega-\ntive data and one of the five shards (discussed above) of\npositive data.\nThe third question that remains is the voting threshold.\nFor example, a threshold of 0.01 means that at least 1% of\nall classifiers must predict \"relevant\" for an article to be\nclassified as \"relevant.\" We implement and test different\nvalues of this parameter between 0.01 and 1. The evalua-\ntion below describes the performance of the ensemble at\ndifferent threshold values.\nEvaluation\nIn order to assess the applicability of our classification\napproach to the problem of pre-selecting news reports\naccording to their relevance, we need to determine its pre-\ndictive accuracy out-of-sample, i.e. on unseen news arti-\ncles. The standard way to do this would have been a\nstandard N -fold cross validation, where articles are split\nrandomly into N folds, N -1 to use for teaching and the\nremaining one for evaluation (the whole procedure repeated\nN times). However, we expect strong regional or country\npatterns in our data, where relevant reports about protest\nmay be characterized by certain region- or country-specific\nwords or combinations. In essence, this would mean that a\nsubstantial number of the features determining the result of\nclassification of any given article article are dependent on\nthe country the article referred to. In a real application,\nhowever, users may want to filter articles from previously\nunseen countries.\nTherefore, we perform a variation of the standard cross-\nvalidation approach by binning the articles by country\n( N =18 ) to which they belong. In each loop, the classifier\nis trained on data from every country except one, and the\nexcluded country constitutes the test set. Such a strategy\nreflects the actual usage of the classifier in practice and\nexposes the potential country bias discussed above. In par-\nticular, this evaluation methodology eliminates any over-\nestimation of performance due to country-level information\npresent in both the training and testing sets: a normal k\n-fold validation strategy would contain country data from\nany given country in both training and testing, allowing\ncountry-specific terms, names or locations to affect the\nlabeling of articles in the test set. This is something we need\nto avoid.\nCross validation results are presented graphically in\nFigure 3. Overall, the ensemble exhibits good perfor-\nmance, identifying an unweighted average of 93% of\nrecall across the cross-validation test sets at a 0.05 cutoff,\nwhile eliminating 56% of all irrelevant articles across the\nset.2 In effect, the classifier is exceeding the goals set out\nat the beginning of the project, while conducting an evalu-\nation similar to actual use in practice. This is equivalent to\n18-country set similar to the sample used in this example,\nsaving up to 200 work-days by trained humans. Results\nhold across sample sizes as well as various levels of bal-\nance (proportion of relevant articles) in the test sets. In\neffect, there is no observable difference between the clas-\nsification of Belarus and that of Kyrgyzstan, even though\nin Belarus this proportion is more than three times higher\nChoosing a more demanding voting cutoff substantially\nincreases the performance of the ensemble in eliminating\nirrelevant articles, with 80% being eliminated at a 0.75 cut-\noff. The cost is a converse loss of recall, dropping to only\n80% of all positives. However, as recall-false increases\nmuch faster than recall-true decreases (i.e. the number of\nfalse articles eliminated grows much faster than the number\nof lost true articles), users should choose the cutoff level\ndepending on the ambitions and human resources available\nto the project.\nFurther, the classifier performs very well in areas with\nvery low numbers of positives, such as Turkmenistan or\nBelarus, where it identifies 100% of all positives in their\nrespective test samples. This is essential in practice. In\nregions with low reporting rates we do not expect that other\n(potentially duplicated) articles describing the same inci-\ndent would make up for a wrongly eliminated article;\ninstead, losing a single article can mean omitting the entire\nprotest event.\nMoreover, predictive performance is more dependent on\nthe choice of cutoff for geographic regions with less data\n(such as Latin America). For countries in the former Soviet\nUnion, performance is nearly unaffected by an increase in\ncutoff (indicating that most classifiers actually vote the\nsame). In Kyrgyzstan, 90% of relevant articles are identi-\nfied at a cutoff of 0.75, whereas performance drops sharply\nas the cutoff is increased for countries such as Venezuela;\nhere, only a little over 52% of all relevant articles are cor-\nrectly identified. However, we consider such behavior\nacceptable from a practical point of view, as a vast majority\nof recall values are in the 0.9\u00ad1.0 range. Further, as no\ncountry remains to be coded in areas with the lowest\n6 Research and Politics \nFigure 3. Cross-validation results for the proposed ensemble.\nCroicu and Weidmann 7\ntraining data density (such as Latin America), the resulting\nbias will be further attenuated in practice. When deploying\nthe coding pipeline in practice, we chose a low cutoff (0.05)\nthat provides more homogeneous results.\nFurther, we assess the suitability of the classifier for\nother potential tasks it may be applied to. First, we analyze\nthe classifier's performance when trained on an existing\ndata set, in order to predict the relevance of new, incoming\narticles. We train the classifier on the first 80% of the arti-\nout-of-sample on the latest 20% of the data (combing all\ncountries). The results are excellent. The classifier identi-\nfies 94% of all relevant articles while discarding 58% of all\nirrelevant articles at the 0.05 cutoff.3\nSecond, we test the classifier's performance when\ntrained on a much smaller sample, since not all projects\nmay have a training set as big as ours. We randomly select\nanother random sample of 15,000 articles. The performance\nis again perfectly within an acceptable range, with the clas-\nsifier identifying 97% of all positives in the sample at the\nstandard 0.05 cutoff. However, trained with less data, the\nfiltering performance suffers, and the classifier eliminates\nabout one third of all irrelevant articles as opposed to over\n50% when trained on the full dataset. One solution for bet-\nter performance is to alter the value of the cutoff parameter,\nwith values such as 0.25 or 0.5 being probably more appro-\npriate for many users.4\nConclusion\nThe screening of large amounts of texts can quickly and\nefficiently be done by computers, whereas humans are bet-\nter at extracting individual pieces of information from\nthese texts. In order to combine the strengths of both auto-\nmatic and human coding into a feasible coding pipeline,\nwe devised a hybrid, semi-automatic approach for coding\nprotest events from news reports. This paper describes the\nfirst stage of our pipeline. We presented a machine learning\nensemble classifier for the pre-selection of news reports\nfor event coding. In order to overcome the problem of a\nhugely imbalanced training set, this classifier relies on a\nlarge number of base classifiers, each built on a balanced\nrandom sample of the training data. We have shown that\nthis approach is able to achieve good results in an out-\nof-sample validation, and can therefore be of great use also\nin other coding projects. While we have not explored the\npossible parameters and settings of our classifier exhaus-\ntively, we believe that our results provide sufficient reason\nto pursue this line of development in future research.\n"
}