{
    "abstract": "Abstract\nIn this study, we examined whether phonotactic constraints of the first language affect speech\nprocessing by Japanese learners of English and whether their proficiency of the second language\ninfluences it. Native English speakers and second language speakers with a high level of language\nproficiency and those with a low level took part in a monitoring task. They were given two kinds of\nsound stimuli as target syllables (i.e., consonant\u00advowel and consonant\u00advowel\u00adconsonant) and\nwere asked to detect them in lists of words that have stress on the first or second syllable\n(e.g., biscuit and beside). The results showed that both stress and phonotactics facilitated\nsegmentation strategies by the three groups. The Japanese groups did not rely on either\nphonotactics or morae to segment the target syllables. They rather used stress to detect the\ntarget syllables in the English words, which is a different segmentation strategy from their first\nlanguage. This study showed that phonotactic constraints did not interfere with second language\nprocessing by native Japanese speakers and provided evidence that second language speakers use\nthe segmentation strategy that is used by native speakers of the target language.\n",
    "reduced_content": "Article\nEffect of Phonotactic\nConstraints on Second\nLanguage Speech Processing\nTamami Katayama\nDepartment of Life Science, Prefectural University of Hiroshima, Japan\n Keywords\nSecond-language speech perception, phonotactics, stress\nIntroduction\nBackground\nCommunicating with others in a second language (L2) is a necessary skill in the globalized\nsociety today. To process speech efficiently, segmenting words in the stream of speech is a\nvital skill as well. One of the issues in the study of speech processing is how words are\nsegmented and how the mental dictionary, the lexicon, is accessed. Segmentation strategies\nfor words depend on listeners' exposure to a particular language (Cooper, Cutler, & Wales,\n2002). Language-specific strategies include use of suprasegmental information, such as stress\nand rhythm, and a segmental inventory that consists of consonants and vowels (Dupoux,\nCorresponding author:\nTamami Katayama, Department of Life Science, Prefectural University of Hiroshima, 562 Nanatsuka Shoabara, Hiroshima,\nJapan.\nEmail: katayamat@pu-hiroshima.ac.jp\ni-Perception\nipe.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (http://www.\nus.sagepub.com/aboutus/openaccess.htm).\nPallier, Sebastian, & Mehler, 1997). Listeners segment words on the basis of the minimal unit\nof their language, but it is not still clear how L2 learners segment the target language and\nwhether they develop their own segmentation strategies. Investigation of how native and\nnonnative speakers auditorily detect words will contribute to the elucidation of how\nspeech in L1 and that in L2 are processed.\nPrevious studies have shown that the segmentation strategies differ depending on the\nlistener's first language. Cutler, Mehler, Norris, and Segui (1986) used a monitoring task\nto determine whether native English speakers (ES) perceive a phoneme or syllable itself\ndirectly. The study used seven pairs of unambiguous English content words (nouns\nand verbs) of similar frequencies sharing the same initial three phonemes (a consonant\u00ad\nvowel\u00adconsonant [CVC] sequence; e.g., bal), and in another context, each pair has a\nsyllable boundary after the initial CVC (e.g., balcony). In the other context, the third\nphoneme was ambisyllabic, that is, the second consonant of the initial CVC could be\nconsidered to belong to both syllables (e.g., balance). Native ES were asked to give\nresponses as quickly as possible to the target syllable when presented alongside a list of\nwords. By measuring response time (RT), the authors examined perceptual word\nsegmentation by syllables. The results for English-speaking subjects showed no sign of a\nsyllabification effect, which was found in another study with French subjects (Mehler,\nDommergues, & Frauenfelder, 1981). Cutler et al. concluded that segmentation strategies\nin continuous speech perception might well be language-specific and thus the syllable's\nfunction in speech segmentation differs depending on the speaker's languages. Mehler\net al. suggested that native French speakers syllabified regardless of whether they were\nlistening to familiar French words that were easy to syllabify or unfamiliar English words\nthat were hard to syllabify. On the other hand, native ES did not syllabify in the same way.\nIf listeners segment words based on the unit of syllables, phonotactics is likely to affect\nspeech processing. Phonotactics of every language has restrictions ``on which phonemes and\nsequences of phones are permissible in different syllabic position'' (McQueen, 1988, p.24).\nAccording to McQueen, the restriction of a phonotactic sequence differs depending on the\nlanguage and it marks the starting and ending points of syllables, which is highly correlated\nwith word boundaries. McQueen (1988) examined the effects of phonotactics and stress on\nspeech segmentation by Dutch listeners. Forty monosyllabic Dutch words were embedded in\nnonsense words in the initial positions (e.g., pil in [pil.vrem]), which had four different\nconditions. One context had a syllable boundary that was aligned with the offset of the\ntarget word followed by strong stress (StrongStrong, Aligned, such as in [pIl.vrem]), and\nanother context was the same but with phonotactic constraints of the consonant sequence\nwith a syllable boundary being misaligned with the offset of the target word (StrongStrong,\nMisaligned, such as in [pIlm.rem]. In the third and fourth contexts, the second strong syllables\nwere replaced with the weak vowel schwa, respectively, (StrongWeak, Aligned, as in\n[pIl.vr@m], and Strong Weak, Misaligned, as in [pIlm, r@m]). Phonotactic constraints\ndiffered under the Aligned and Misaligned contexts depending on the final phoneme of the\ntarget. In addition, another 40 words were used as targets in the final positions of nonsense\ntarget words. As well as the first targets, the second ones had four conditions depending on\nwhether the preceding vowels are stressed or not and whether the pnototactics of the target\nonset is legal or not between the two vowels: StrongStrong, Aligned, as in [fIm.r.uk];\n[fIm.ruk]; StrongStrong, Misaligned, as in [fI.druk]; Weak Strong, Aligned, as in [f@m.ruk];\nand WeakStrong, Misaligned, as in [f@.druk]. The Dutch participants were instructed to\nspot the target words embedded in a list of nonsense bisyllables, press a button as fast as\npossible when they found them, and say aloud what words they heard. McQueen analyzed\nthe RTs and the number of errors (ERR) the participants made. The Dutch speakers in his\nexperiments failed more frequently to detect words that were misaligned with syllable\nboundaries cued by phonotactic constraints than to detect words that were aligned with\nsuch boundaries. Furthermore, the participants were affected by the targets in the final\nposition, where phonotactic cues preceded the targets' onsets, more than the targets in the\ninitial position, where the cues followed the targets' offset. McQueen suggested that\nphonotactic legality is taken into consideration and helps listeners to segment words. He\nalso stated that phonotactic constraints are likely to be one of the sources of information,\nsuch as silence and metrical cues, when listeners segment words.\nIn addition to the effects of syllables, Cutler and Norris (1988) examined how stress affects\nnative ES since English is a stress-based language. They tested their model of segmentation\nbased on strong syllables. In the model of speech segmentation in a stress language, the\noccurrence of a strong syllable triggers segmentation of the speech signal, while that of a\nweak syllable does not. In their study, the participants were instructed to detect real words in\nnonsense strings. For example, mint embedded in mintayve and mint embedded in mintesh\nwere presented to the listeners. Because the second syllable (tayve) in mintayve is strong,\nlexical access starts at tayve. When mint belongs partly to both accompanying syllables,\nthis inappropriate intersyllabic segmentation interferes with the detection of the word mint.\nSince the weak second syllable in mintesh does not hinder segmentation, Cutler and Norris\nspeculated that listeners would be able to detect mint faster in mintesh than in mintayve. The\nresults supported their hypothesis that the detection of a word is delayed when the word\nconsists of two strong syllables but not when it consists of a strong syllable followed by a\nweak syllable. Cutler and Norris explained that the strong syllable triggered a segmentation\neffect. Thus, the detection of the word was delayed in intersyllabic segmentation because the\nlistener needed to collect speech information across a segmentation point. The results of that\nstudy supported the prediction by Mehler et al. (1981) of a stress-based segmentation model,\nwhich argues that a syllable forms a unit of speech processing.\nTo test the rhythmic segmentation hypothesis (Cutler & Carter, 1987; Cutler & Norris,\n1988), Cutler and Butterfield (1992) used errors of spontaneous misperception. They found\nthat strong syllables tend to be the initial syllables of lexical words, while weak syllables are\nmore likely not to be word-initial or, if so, are more probably grammatical words. They\nconcluded that listeners use rhythmic segmentation when perception is difficult. Filippi,\nGingras, and Fitch (2014) examined prosodic effects on word learning using an Extraction\nor Inference or Mapping. They provided Austrian students at the University of Vienna with\nfour conditions: consistent co-occurrence of the target label with the corresponding image\ncategory (Co-occurrence), Co-occurrence with constantly exaggerated pitch, that with\nrandomly highlighted pitch, and that with increased duration of the target label. The\nparticipants showed a significantly higher level of performance when pitch was constantly\nprominent on the target. Filippi et al. suggested that pitch is more influential than duration\nbut that attentiveness of pitch enhancement of the target label is not sufficient in learning new\nwords. According to Filippi et al., in addition to statistical learning of the regular input,\nexposure to prosodically highlighted stimuli-accelerated language learning.\nIf listeners segment words on the basis of the rhythmic pattern in their first language,\nnative Japanese speakers are assumed to employ the unit of mora for their segmentation\nstrategy. According to Ladefoged (2001), ``Japanese may be analyzed in terms of the classical\nGreek and Latin unit called mora. A mora is a unit of timing. Each mora takes about the\nsame length of time to say'' (Ladefoged, 2001, p.233). Japanese is called ``mora-timed\nlanguage'' because its basic unit is the mora (Kubozono & Ota, 2008). Although\nresearchers in psycholinguistic studies of speech perception have agreed on the importance\nof the mora as a unit of Japanese language, its role in timing remains a matter of dispute\nKatayama 3\n(Warner & Arai, 2001). Warner and Arai (2001) offer several hypotheses to account for\ntiming by morae (rather than by stress); one of them is that morae are utilized to\nnormalize word duration. Another feature of mora in Japanese is that a nasal consonant\n(N) is considered as one mora as well as the combination of a consonant and a vowel (CV).\nOtake, Hatano, Cutler, and Mehler (1993) examined the perception of segmentation of\nspoken Japanese words by native and nonnative listeners using a nasal in the coda\nposition. They found, as expected, that the patterns of performance by native and\nnonnative listeners differed, and they confirmed the theory that speech segmentation is\nlanguage-specific. Their results showed that the response patterns to CV targets were\nidentical in CVCVCV and CVNCV (a consonant\u00advowel\u00adnasal\u00adconsonant\u00advowel sequence)\nwords, being inconsistent with the prediction of syllable-based segmentation. They argued\nthat the pattern of the response by Japanese listeners can best be described in terms of mora-\nbased segmentation because the initial mora was CV in both CVCVCV and CVNCV words.\nOtake et al. also claim that the mora hypothesis explains why the response pattern to CVN\ntargets differed across CVNCV words and CVCVCV words. The Japanese listeners\nresponded to CVN targets in CVNCV, but the RT was comparatively long. According to\nOtake et al., this is due to a complex target, two-mora words, while CVN targets in\nCVCVCV words received a mixed response. Otake et al. reported that this was predicted\nby the mora hypothesis because the target and stimulus do not match at the mora level. They\nconcluded that Japanese listeners did not decompose the spoken words into syllables but into\nmorae and that Japanese listeners thus use morae to segment speech. These results were\nsupported by the results of Cutler and Otake's study (1994), which confirmed the theory\nthat Japanese listeners segment speech sound targets on the basis of morae and that mora-\nbased segmentation is language-specific, as are French listeners' syllabic segmentation and\nEnglish listeners' stress-based segmentation. Cutler and Otake argue that Japanese listeners\nmap their moraic pattern of speech processing onto whatever foreign languages they are\nlearning. They therefore believe that their findings about language-specific processing have\ncritical implications for clarifying the processes of L2 acquisition.\nA bilingual study by Cutler, Mehler, Norris, and Segui (1992) provided further evidence\nthat word segmentation strategies are language-specific. They examined how French\u00adEnglish\nbilinguals segment word strings in both English and French. Once the participants had been\njudged as having native-speaker competence in both languages by native speakers of French\nand by native speakers of English, they were asked about their language preference in order\nto divide them into English-dominant bilinguals and French-dominant bilinguals. Cutler\net al. used the stimuli created by Mehler et al. (1981) for the French experiment and the\nstimuli created by Cutler et al. (1986) for the English experiment. As used in the study by\nCutler et al. (1986), the bilingual participants were asked to spot words as quickly as possible\nwhen they heard the target syllables when presented alongside a list of words. As in the study\nby Cutler et al., the study used seven pairs of unambiguous English content words (nouns\nand verbs) of similar frequencies sharing the same initial two or three phonemes (CV or CVC;\ne.g., ba and bal). In one context, a syllable boundary was placed after the initial CVC (e.g.,\nbalcony). In the other context, the boundary was ambiguous because the coda (the second\nconsonant) of the initial CVC could be considered to belong to both syllables (e.g., balance).\nThe results showed that English-dominant bilinguals produced exactly the same pattern as\nthat produced by monolingual ES not only when listening to English but also when listening\nto French, while the results for French-dominant participants were exactly the same as those\nfor French speakers when the French-dominant participants listened to French materials and\nwere exactly as those for English monolingual speakers when they listened to English. Cutler\net al. argue that syllabic segmentation is a restricted procedure and can be ``switched off''\nwhen it is inefficient. On the other hand, they claim that stress-based segmentation\n(i.e., English in this case) is an unrestricted (a general) procedure and is generally available\nto all speakers. Thus, while French-dominant bilinguals employed stress-based segmentation\nwhen they were presented with input in English, the reverse process is not possible, since\nEnglish does not encourage the development of syllabic segmentation. Cutler et al. explain\nthat English-dominant bilinguals use only generally available processing procedures for\nsegmentation. They argue that the right input at the right time is required to develop a\nrestricted processing procedure. Consequently, according to this view, even the most\nskilled bilingual speakers will be restricted one segmentation procedure. They therefore\nconcluded that rhythmically based segmentation procedures are mutually exclusive as well\nas language-specific and that they are restricted in their availability. In this one particular\naspect of language processing, even bilinguals may be functionally monolingual.\nIn summary, French speakers segment speech on the basis of the syllable, while Japanese\nspeakers use the unit of mora. Although English and Dutch languages are stress-controlled\nlanguages, different segmentation strategies are adopted (Cooper et al., 2002). Rhythmic\nsegmentation using a metrical prosody plays an important role in segmenting English\nspeech, and lexical stress does not facilitate speech segmentation. Furthermore, the study\non bilinguals shed light on how human speech processing is restricted to the dominant\nlanguage. In particular, a rhythmically based procedure is incompatible with procedures\nfor other languages, especially procedures that are syllabic in structure. Thus, speech\nsegmentation can be considered as language-specific. Although Cutler and Otake (1994)\nclaim that a listener's segmentation strategy in L1 is transferred into that in L2, there has\nnot been much research on transfer of word segmentation strategies to L2. Cutler and Otake\nreported that native Japanese speakers segmented speech based on mora, but their\nmethodology employed Japanese words as material. We have evidence that L2 learners\ndevelop their perception at the phoneme level. Studies on perception of L2 phonemes have\nrevealed that the perception develops as the L2 proficiency improves (Rochet, 1995). Flege,\nBohn, and Jang (1997) provided further evidence of a link between an L2 learner's perception\nof English vowels and the learner's L2 experience in general. They reported that adult L2\nlearners were able to perceive certain L2 vowels more accurately as they gained experience of\ntheir L2 language but that the L1 background did play a role in the accurate perception of L2\nvowels. According to Leather and James (1991), learners adjust their perception of the target\nlanguage by subsequently creating their own prototypes. However, previous studies have\nmainly focused on segmental level, although a unit of speech is the syllable level. Thus, it\nis not still clear whether perception of L2 syllable structure develops and how it affects oral\ncommunication in L2. Investigation of how words or syllables are segmented using materials\nin L2 is needed to unveil the development of L2 speech processing.\nHypotheses and prediction\nThe aim of this study was to determine the effects of phonotactic constraints of the first\nlanguage on L2 acquisition. The following research question was raised: How do native\nEnglish speakers and L2 speakers of different levels in English proficiency identify target\nsyllable units (i.e., CV and CVC) in bisyllabic or trisyllabic English words? Otake et al.\n(1993) reported that native Japanese speakers used the unit of mora to segment words in\nspeech, and CVC is an illegal segmental sequence in Japanese except for the case in which the\ncoda is nasal. Therefore, I predicted that the Japanese groups would segment CV syllables\nfaster than CVC syllables. In addition, since Cutler and Norris (1988) reported that native ES\nused stressed syllables to segement words, the factor of stress was also included in this study\nKatayama 5\nand the effects of phonotactics were compared. Japanese accents are realized by pitch rather\nthan stress. Native Japanese speakers recognize a word accent on the syllable on which pitch\nfalls (Kubozono, 1998). Although stress facilitates segmentation of continuous speech by\nnative English speakers, this is not the case with native Japanese speakers. Thus, the\nfollowing hypotheses were raised:\n(1) Since Japanese is not a stress-timed language, placement of stress makes no difference to\nJapanese learners of English (and less so for Japanese speakers with low proficiency of\nEnglish than for those with high proficiency) for syllable recognition in English.\n(2) Since most of Japanese moras consist of CV units, Japanese learners of English will\nrecognize CV units faster than CVC units.\nBased on these hypotheses, native English speakers and L2 speakers with a high level of\nlanguage proficiency and those with a low level took part in a monitoring task.\nExperiment: Monitoring Task\nMaterials\nTwenty bisyllabic or trisyllabic words with target syllables (CVC or CV) were selected\nas target words (see Table 1). Half of them have stress on the first syllables, and the\nothers have stress on the second syllables. Thus, stimuli under the following four\nconditions were created.\n(1) Target words have stress on the first syllables whose structure is the same as the target\nsyllable structure: a target word is biscuit and its target syllable is bis (CVC).\n(2) Target words have stress on the first syllables whose structure is partly the same as the\ntarget syllable structure; a target word is biscuit and its target syllable is bi (CV).\n(3) Target words have stress on the second syllables and the target syllables are placed over\nthe first syllables and the second syllables in a word; a target word is beside and its target\nsyllable is bis (CVC).\n(4) Target words have stress on the second syllables whose structure is the same as the target\nsyllable structure; a target word is biscuit and its target syllable is bi (CV).\nTable 1. Target Words and Syllables.\nNo.\nStressed\nsyllable Target word Version I Version II\nStressed\nsyllable Target word Version I Version II\n1 first biscuit bis bi second beside bi bis\n2 bister bi bis besiege bis bi\n3 Bigfoot big bi begin bi big\n4 bicker bi bik became bik bi\n5 piddle pid pi pedometer pi pid\n6 picnic pi pik pecan pik pi\n7 mote mote mou motel mou mote\n8 niggle ni nig neglect nig ni\n9 picture pik pi peculiar pi pik\n10 mosaic mouz mou mosey mou mouz\nIn addition to the target words, 20 distractors (the first consonants being the same as the\ntarget syllables but the following vowels being different) and 400 fillers were selected, and the\nratios of words with stress on the first syllables and words with stress on the second syllables\nwere 50%. Two versions of a monitoring task were created with the aid of E-prime 2.0 software\n(2012) for ES and Japanese speakers. The instructions were in English for the ES and in\nJapanese for the Japanese speakers. Each version of instructions had 40 lists including 20\npositive lists with the target words and 20 negative lists without them. Each list had eight to\ntwelve words. Each positive list had one target word and the rest of the spaces were occupied by\nfillers, and each negative list consisted of a foil and fillers. The target words had stress either on\nthe first or second syllables, and they were each presented ten times. All of the conditions for\nthe English and Japanese versions were the same except for the target syllables. The different\ntypes of segmental sequences (i.e., CV and CVC) were presented as target syllables in the two\nversions. For example, when a target word was biscuit, bis was presented in version I and bi was\npresented in version II. The proportions of different types of target words (i.e., different\nplacement of stress) and target syllables (i.e., CV and CVC) were counterbalanced in each list.\nA target syllable was set up to be presented auditorily followed by a blank screen for\n750 ms. Then the words in the list were programmed to run while the mark ``\u00fe'' appeared in\nthe center of the screen. If a response was made while the words in a particular list were being\npresented, this list was programmed to jump to the next list. If there was no response by the\nend of the list, the screen informed the participant of the end of list and instructed the\nparticipant to press the button to continue.\nParticipants\nTwenty native English speakers, 19 Japanese speakers with high proficiency of English (JH),\nand 20 Japanese speakers with relatively low English proficiency (JL) took part in the\nexperiment. ES were native English speakers in the Boston area with a mean age of 48.5\nyears. Most of the JH were English or Japanese instructors at a college or at a private English\nconversation school, and their mean scores for TOEIC and TOEFL (PBT) were 862 and 600,\nrespectively. Their mean age was 33.8 years and the mean period of living in English-speaking\ncountries was 2 years and 7 months. JL were English learners at the university level in Japan,\nand their ages ranged from 19 to 22 years. None of them had lived overseas. All of the\nparticipants reported that they had no hearing impairment.\nProcedure\nThe participants were instructed to sit in front of the computer in a quiet room and put on the\nheadphones. Instructions were presented on the screen, and they were asked to respond as\nquickly as possible by pressing button ``1'' on the response box when they heard the target\nsyllable in a word. The RT from the onset of each target word was recorded. When they did\nnot hear the target syllable, they were instructed to press button ``5'' to continue. The target\nsyllable and lists of words were presented after the auditory instruction ``Please listen for.''\nEach word in the list was followed by a 1,000-ms interval, and the next words were presented\nautomatically.\nResults\nEach participant listened to the target words under four conditions. The duration of\ntarget syllables was subtracted from the response time (RT) for analysis. Figure 1\nKatayama 7\nshows RT for the four conditions in the three groups. The mean numbers of errors\n(ERR) was also counted. Figure 2 shows ERR when the participants were given four\ntarget words. Two-way repeated measures ANOVA was performed for RT and ERR in\nthe three groups. The esults for ES indicated significant effects of stress and phonotactics\nindicated significant effects of stress and phonotactics in RT, stress: F(1, 19) \u00bc 13.58,\nThe results revealed that the participants detected the target words significantly faster and\nmore accurately when they were given the target syllable CVC (mean response time\n[M RT] \u00bc 555.4 ms, mean ERR [M errors] \u00bc 1.0) than when they were given the target\nsyllable CV (M RT \u00bc 715.2 ms, M errors \u00bc 1.4). In addition, RT was significantly faster\nand ERR was smaller when stress was on the first syllable (M RT \u00bc 561.2 ms,\nM errors \u00bc 1.0) than when it was on the second syllable (M RT \u00bc 709.5 ms,\nM errors \u00bc 1.4). JL responded to the words significantly faster (M \u00bc 578.4 ms) than did ES\nFigure 1. Mean responses times in the three groups are shown. F indicates target words with stress on\nthe first syllables and S indicates target words with stress on the second syllables. ES: English speakers,\nJH: Japanese speakers with high proficiency of English, JL: Japanese speakers with low proficiency of English;\nCVC: consonant\u00advowel\u00adconsonant; CV: consonant\u00advowel.\n(M \u00bc 727.6 ms), but the difference between RT of JL and JH (M \u00bc 600.1 ms) was not\nsignificant. Target words were missed most frequently by JH (M \u00bc 1.3), second most\nfrequently by JL (M \u00bc 1.2), and least by ES (M \u00bc 1.1), though the differences were not\nsignificant.\nIn short, all of the groups tended to find the target words faster and more accurately\nwhen the given target syllable structure was CVC and the target words to be identified\nhad stress on the first syllables. Moreover, ES responded to the target words slowest but\nmost accurately among the three groups. Compared with the results for JH, JL tended to\nidentify the targets faster and more accurately, though the differences were not statistically\nsignificant.\nDiscussion\nThis study was carried out to determine the effects of phonotactic constraints of the first\nlanguage on acquisition of a L2 by examining how native English speakers and L2 speakers\nof different levels in proficiency segment target syllable units (i.e., CV and CVC) in bisyllabic\nor trisyllabic English words with stress either on the first or second syllables. The following\ntwo hypotheses were raised:\n(1) Since Japanese is not a stress-timed language, placement of stress makes no difference to\nJapanese learners of English (and less so for Japanese speakers with low proficiency of\nEnglish than for those with high proficiency) for syllable recognition in English.\n(2) Since most of the Japanese morae consist of CV units, Japanese learners of English will\nrecognize CV units faster than CVC units.\nFigure 2. Mean numbers of errors in the three groups are shown. F indicates target words with stress on\nthe first syllables and S indicates target words with stress on the second syllables. ES: English speakers,\nJH: Japanese speakers with high proficiency of English, JL: Japanese speakers with low proficiency of English;\nCVC: consonant\u00advowel\u00adconsonant; CV: consonant\u00advowel.\nKatayama 9\nIt was found that participants in all of the groups responded to the target words faster\nwhen stress was on the first syllables and the target syllable structure was CVC. ES identified\nthe target syllables the most accurately but the slowest. JL showed better performance both in\naccuracy and speed than JH. The Japanese groups did not rely on either phonotactics or\nmora to segment the target syllables. They rather used stress to detect the target syllables in\nthe English words, which is a different segmentation strategy from that of their L1. Therefore,\nboth hypotheses were rejected in this study. However, there is a limitation in the methodology\nof this study. Some given target syllables were ambiguous as to whether they were [big] or\n[bik] because the stop consonants were not pronounced clearly in the coda position.\nIn addition, the difference in sound quality between stressed syllables and unstressed\nsyllables was likely to have affected the results. Since the target syllables were recorded\nseparately, they were pronounced with stress. The target syllables were embedded in either\nstressed syllables or the unstressed syllables in the target words, and thus the sound quality\nwas not exactly the same. Further study is needed to control these variables.\nThe results for ES were consistent with those in a study by Mehler et al. (1981) with\nrespect to the fact that stress and phonotactics facilitate ES' segmentation strategies. ES\nfound it difficult to detect the target syllables in the words when they were unstressed\ndespite the matched target syllables (e.g., [bi] in besiege). These results provide further\nevidence that stress facilitates native ES' segmentation. In addition, ES found it easier to\nidentify the target syllables (e.g., [bis]) when the coda of the target syllable (e.g., [s] of [bis])\ncrossed the second syllable with stress (e.g., besiege) than when the target syllable (e.g., [bi])\nwas a part of the stressed first syllable (e.g., bister). A syllable is the minimal unit of speech\nprocessing and English does not have a monosyllabic word pronounced [bi]. That is, CV units\n(e.g., [bi]) are ill-formed syllables unlike Japanese, while CVC units are well formed in\nEnglish. Thus, it was likely that ES did not match the target syllables (CV) in words.\nAlthough their accuracy was the best among the three groups, their RT was the longest\namong the groups. Since English words were used in this study, ES might have tried to\nmatch the target syllable and the corresponding words while considering possible acoustic\nvariations. According to Jusczyk (1997), innately guided learning enables infants to pay\nattention to particular acoustic information more than others in order to process their L1\nspeech effectively. Although they can perceive various types of acoustic information that\nsurrounds them, they choose only necessary information to create the categories of their\nL1 phonemes. Thus, ES were likely to have put the allophones into the same category\nwhile selecting the critical information. In this sense, word familiarity was likely to have\naffected the results since some words (e.g., picture or became) are common, while others\n(e.g., bister or piddle) are not common. A possible reason why ES responded the most\nslowly is aging effect. The Japanese groups, especially JH, were younger than ES.\nAlthough the aging effect on RT has not been proved in previous studies, further study\nwith control of these variables is needed.\nOn the other hand, JL responded to the words fastest, but they missed the most among the\nthree groups. Since they do not have sufficient linguistic knowledge to access a mental lexicon\nin L2, it is assumed that they focused on only acoustic information to agree with that of the\ntarget syllables in the words and thus reacted quickly. At the same time, however, it is\nassumed that their lack of sufficient L2 phonological categories to cover acoustic\nvariations was responsible for their failure to find target syllables that have only slight\nacoustic differences. Compared with JL, JH have advanced L2 knowledge and have\ndeveloped phonological categories to deal with English sound variations. Thus, more time\nis required to connect the acoustic input and their linguistic knowledge, which might have\ncaused JH to respond to the targets more slowly than JL.\nJL and JH showed the same tendency as that of ES with respect to RT and ERR. The\nresults showed that JL and JH responded faster and more correctly when they segmented\nCVC syllables than when they segmented CV syllables. This indicates that they neither used a\nsegmentation strategy based on mora-timing or the Japanese syllable structure. Since Otake\net al. (1993) reported that native Japanese speakers used morae to segment words in speech,\nI predicted that the Japanese groups would segment CV syllables faster than CVC syllables.\nHowever, they responded to CVC syllables faster regardless of the placement of stress (even\nwhen the coda of the target syllable crossed the second syllable (e.g., [bis] in besiege)). The\nreason why JH and JL did not rapidly respond to CV syllables might be that acoustic\ninformation was not sufficient to detect the target words. The stimuli [bi] and [pi] could be\nacoustically ambiguous without context, but [bis] has additional information of the coda [s].\nThis allows the participants to specify the target syllables and to find the target words easily.\nWhen English words were used as stimuli in this study to investigate L2 speech processing,\nthe results were not consistent with those in a study by Otake et al. (1993). In addition, both\nJL and JH responded more rapidly and more accurately to words with stress on the first\nsyllables than to words with stress on the second syllables, indicating that stress facilitated\ndetection of target syllables. They used stress to segment the target syllables as the native ES\ndid. It is assumed that stress makes the syllables acoustically prominent, helping the Japanese\nspeakers to find the target easily. Again, target syllables were pronounced separately for the\nrecording and thus they were produced as stressed syllables. This is likely to have allowed the\nJapanese groups to match the words with stress on the initial syllables better than words with\nstress on the second syllables. Another possible reason is that stress on the first syllable is\nmore common in English, so that the Japanese groups found it easy to segment syllables in\nthe words. Cutler et al. (1986) claim that rhythmically based segmentation procedures are\nmutually exclusive as well as language-specific and that they are restricted in their\navailability. The results of this study, however, showed that the Japanese speakers did not\nuse their L1 segmentation strategy to segment target syllables in English words regardless of\ntheir L2 proficiency. Although they use morae for speech processing in L1, this strategy can\nbe adjusted when they process L2 speech, and thus phonotactic constraints did not hamper\nthe Japanese groups' segmentation strategies in this study.\nPerception of L2 phonemes develops as learners gain more experience, but this was not the\ncase with L2 phonotactics. There was not a significant difference depending on L2 proficiency\nwhen they segmented the syllables of the target language. The results suggest that the native\nJapanese speakers employed not their moraic pattern but English stress pattern for L2 speech\nprocessing, which was different from the prediction. In short, phonotactics of the L2 was\nperceived by the learners regardless of their L2 experience unlike phonemes, and phonotactic\nconstraints of L1 do not prevent them from segmenting L2 speech. These results provided\nadditional evidence to reveal how learners process a second language and what aspect they\ndevelop for speech processing. However, the findings of this study raised some fundamental\nquestions as to whether speech processing strategies are language-specific or not. If they are,\nfurther study is required to examine how native Japanese speakers segment words both in L1\nConclusion\nIn conclusion, this study showed that phonotactic constraints did not interfere with L2\nprocessing by native Japanese speakers and provided evidence that L2 speakers use the\nsegmentation strategy that is used by native speakers of the target language. Further\ninvestigation is needed to determine how native Japanese speakers segment a syllable in a\nnonword and to determine whether there is a difference between groups with different levels\nof English proficiency. Results of such studies may help to reveal the mechanism of L2 speech\nprocessing.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or\npublication of this article.\nFunding\nThe author(s) disclosed receipt of the following financial support for the research, authorship, and/or\npublication of this article: This study was supported by Grant-in-Aid for Scientific Research, Japan\nReferences\nCooper, N., Culter, A., & Wales, R. (2002). Constraints of lexical stress on lexical access in English:\nCutler, A., & Butterfield, S. (1992). Rhythmic cues to speech segmentation: Evidence from juncture\nCutler, A., & Carter, D. M. (1987). The predominance of strong initial syllables in the English\nCutler, A., Mehler, J., Norris, D., & Segui, J. (1986). The syllable's differing role in the segmentation of\nCutler, A., Mehler, J., Norris, D., & Segui, J. (1992). The monolingual nature of speech segmentation\nCutler, A., & Norris, D. (1988). The role of strong syllables in segmentation for lexical access. Journal of\nCutler, A., & Otake, T. (1994). Mora or phoneme? Further evidence for language-specific listening.\nDupoux, E., Pallier, C., Sebastian, N., & Mehler, J. (1997). A distressing ``deafness'' in French? Journal\nE-prime 2.0 [Psychology software tools]. Tokyo, Japan: IBS Japan.\nFilippi, P., Gigras, B., & Fitch, W. T. (2014). Pitch enhancement facilitates word learning across visual\ncontexts. Frontiers in Psychology, 5, 1\u00ad7.\nFlege, J. E., Bohn, O., & Jang, S. (1997). Effects of experience on non-native speakres' production and\nJusczyk, P. (1997). The discovery of spoken language. Cambridge, MA: MIT Press.\nKubozono, H. (1998). Onseigaku to oninron. Tokyo: Kuroshio Shuppan [Phonetics and Phonology].\nKubozono, H. & Ota, S. (2008). Oninkouzou to akusento [A phonological structure and an accent].\nTokyo: Kenkyusha Shuppan.\nLadefoged, P. (2001). A course in phonetics: 4th edition. Boston: Thomas Learning.\nLeather, J., & James, A. (1991). The acquisition of second language speech. Studies in Second Language\nMcQueen, J. M. (1988). Segmentation of continuous speech using phonotactics. Journal of Memory and\nMehler, J., Dommergues, J. Y., & Frauenfelder, U. (1981). The syllable's role in speech segmentation.\nOtake, T., Hatano, G., Cutler, A., & Mehler, J. (1993). Mora or syllable? Speech segmentation in\nRochet, B. (1995). Perception and production of second-language speech sounds by adults.\nIn W. Strange (Ed.), Speech perception and linguistic experience (pp. 379\u00ad432). Baltimore, MD:\nYork Press.\nAuthor Biography\nTamami Katayama received an MA and PhD at Hokkaido University\n(International Media and Communication) and MSc in Developmental\nLinguistics at University of Edinburgh. She is now working as an\nassociate professor at Prefectural University of Hiroshima. Her research\nfields are second language acquisition, phonetics, and psycholinguistics.\nHer main research interests are L2 speech processing and auditory\nspeech perception."
}