{
    "abstract": "Abstract\nThis paper discusses how an interactive artwork, the Crowd-Sourced Intelligence Agency (CSIA), can contribute to\ndiscussions of Big Data intelligence analytics. The CSIA is a publicly accessible Open Source Intelligence (OSINT) system\nthat was constructed using information gathered from technical manuals, research reports, academic papers, leaked\ndocuments, and Freedom of Information Act files. Using a visceral heuristic, the CSIA demonstrates how the statistical\ncorrelations made by automated classification systems are different from human judgment and can produce false-\npositives, as well as how the display of information through an interface can affect the judgment of an intelligence\nagent. The public has the right to ask questions about how a computer program determines if they are a threat to\nnational security and to question the practicality of using statistical pattern recognition algorithms in place of human\njudgment. Currently, the public's lack of access to both Big Data and the actual datasets intelligence agencies use to train\ntheir classification algorithms keeps the possibility of performing effective sous-dataveillance out of reach. Without this\ndata, the results returned by the CSIA will not be identical to those of intelligence agencies. Because we have replicated\nhow OSINT is processed, however, our results will resemble the type of results and mistakes made by OSINT systems.\nThe CSIA takes some initial steps toward contributing to an informed public debate about large-scale monitoring of open\nsource, social media data and provides a prototype for counterveillance and sousveillance tools for citizens.\n",
    "reduced_content": "Commentary\nCrowd-Sourced Intelligence Agency:\nPrototyping counterveillance\nJennifer Gradecki1 and Derek Curry2\n Keywords\nDataveillance, information access, transparency, social media, counterveillance, sousveillance\nIntroduction\nWith the release of the Snowden leaks, debates about\ndataveillance practices used by intelligence agencies\nhave finally entered public discourse. Unfortunately,\npeople without familiarity with techniques for data col-\nlection or analysis often do not understand how large\ntroves of unstructured data (and metadata) become\nintelligence and often assume that if they have nothing\nto hide, these systems should not concern them.\nConsequently, dataveillance of social media or other\npublicly available information has not faced the same\npublic scrutiny over privacy as bulk collection of emails\nor cell phone data. To address this deficit, we have\ncreated an interactive artwork, the Crowd-Sourced\nIntelligence Agency (CSIA), that replicates the data\nprocessing of an open source intelligence (OSINT) sur-\nveillance system monitoring the popular microblogging\nplatform, Twitter. By allowing users to experience how\nthese systems frame social media posts and (mis)inter-\npret natural language, especially slang, jokes, and sar-\ncasm, we hope to provide a visceral heuristic of the\nprocess to help participants of our app to ask questions\nand make informed decisions about the large-scale\nmonitoring of open source, social media data. This\ntype of awareness can facilitate new tactics for sousveil-\nlance and counterveillance.\n1Department of Art, Art History and Design, Michigan State University,\nUSA\n2Department of Media Study, State University of New York at Buffalo,\nUSA\nCorresponding author:\nJennifer Gradecki, Department of Art, Art History and Design, Michigan\nState University, 600 Auditorium Road, 113 Kresge Art Center,\nEmail: jgradeck@buffalo.edu\nBig Data & Society\njournals.sagepub.com/home/bds\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://\nwww.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nThe state of the public debate\nPart of what impedes a public understanding of large-\nscale OSINT surveillance is made possible by the rise of\nBig Data and analytic tools to process it. Big Data is\n``fundamentally networked'' (boyd and Crawford,\n2011) and has been facilitated by the ``widespread avail-\nability of electronic storage media, specifically main-\nframe computers, servers and server farms, and\nstorage area networks'' (Gitelman and Jackson, 2013:\n6\u00ad7). However, Big Data is inherently inaccessible to\nthe public, both in terms of access to the database and\nthe ability to process it. The public does not have access\nto the same amount of data that intelligence agencies\ndo, but even when the public does gain access to\nmassive troves of data, they generally do not have the\ncomputational capacity to quickly process and analyze\nall of the data or the time to develop the technical\ncompetencies needed to understand the intentionally\ncoded, specialized documents. Twitter, for example,\nonly offers a limited amount of its datastream to the\npublic and academic researchers through its\nApplication Program Interface (API). However, the\nfull ``firehose'' is available to companies (including gov-\nernment contractors) who have the ability to pay for\nand process it. Lev Manovich created a hierarchy of\n``data-classes'' for a ``Big Data society'' that places\nthose with the expertise to analyze it at the top. The\nmiddle class is comprised of people and organizations\nwho have the ability to collect Big Data, while the\nbottom contains those who only make data, con-\nThe danger of using Big Data to identify threats to\nnational security is that it tends to provoke apophenia,\nor the perception of meaningful patterns in random\ndata (boyd and Crawford, 2011: 2). The tools for hand-\nling Big Data, such as machine-learning classification\nand techniques for making different data types compat-\nible with one another, have restrictions and difficulties\nthat data scientists are often in disagreement over how\nto address. Statistician and computer scientist Jesper\nAndersen points out that simply the process of cleaning\nthe data (determining which characteristics of the data\nare important) ``removes the objectivity from the data\npresented to an agent and the context in which it is\n(re)framed can also influence how the data is perceived,\nthus affecting the agent's judgment. In the CSIA, we are\ninterested in reproducing problems faced when process-\ning and displaying data for intelligence analysis.\nIntelligence agencies and Big Data\nThe dataveillance practices currently employed by intel-\nligence agencies are spawned from a `collect-it-all'\nmentality that assumes that if enough data can be col-\nlected, future actions can be predicted with a high level\nof accuracy. Consequently, the amount of data now\nbeing collected and processed necessitates the use of\ntools developed for handling Big Data. This toolkit\nnot only includes instruments for data capture, but\nalso software that scans massive troves of unstructured\ndata, returning elements determined to be suspicious\nthrough an algorithmic process. In the CSIA, we are\nusing some of the same classification techniques used to\nparse and analyze Big Data for predictive policing pur-\nposes. We focus on open-source intelligence (OSINT)\ndata because it is the easiest to obtain, and perhaps the\nleast controversial because it is already publicly\navailable.\nFor the last 20 years, intelligence agencies have been\ndeveloping and refining large-scale, automated data\ngathering and processing software (Arnold, 2015:\n36),1 in order to address the growing problem of\n``data deluge'' or ``ever-growing data sets [sic]'' (IBM\nagencies routinely process massive amounts of struc-\ntured and unstructured data, derived from both private\nand public sources, including: financial, medical, pro-\nfessional and academic records, transactional data,\nsearch queries, emails, texts, telephony metadata, geo-\ngraphic information system (GIS) data, public records,\nsocial media posts (Facebook, Instagram, Twitter),\nwebsites and blogs, news articles, video, audio,\nimages, and the list goes on. The Big Data that intelli-\ngence analytics systems have been developed to deal\nwith consists mainly of public data: in 2004, it was\nestimated that over 80% of the intelligence database\nthe number of people using social media has grown\nsubstantially since 2004, it is likely that this percentage\nis even higher today. Agencies feel the need to automate\nthe processing of this disparate data in order to gain\nsituational awareness and predict outcomes. We are\ninterested in how this process of automation impacts\nthe conclusions that intelligence agents come to.\nThe Crowd-Sourced Intelligence\nAgency (CSIA)\nCSIA is an online application and interactive artwork\nthat replicates and displays some of the known tech-\nniques used by intelligence agencies to collect and pro-\ncess open source information.3 The app uses technical\nmanuals, research reports, academic papers, leaked\ndocuments, and Freedom of Information Act files to\nconstruct an OSINT system that is accessible to the\npublic. OSINT is intelligence collected from publicly\navailable sources, such as the media (including social\nmedia), academic records, and public data, and has\n2 Big Data & Society\nbeen described as ``the basic building block for secret\nThe purpose of the CSIA is to openly show how\npublicly available information is processed and ana-\nlyzed, with a focus on social media posts. We pieced\ntogether an incomplete mosaic of information that\nbecame the basis for constructing a technological arti-\nfact that replicates many of the features commonly used\nto process publicly available data, including: nai\u00a8ve\nBayes supervised machine-learning classification for\npredictive analytics, keyword search results for words\nknown to be used by intelligence agencies,4 and an\ninterface that allows users to evaluate social media\nposts based on their threat to national security. Once\nwe were able to build and interact with this surveillance\nsystem, assumptions and problems inherent in the\nsystem started to become visible. For example, we rea-\nlized that if someone has a similar speech pattern or\nTwitter user description as a known target, they could\npotentially end up on a watch list.\nThe CSIA app consists of several components: (1)\nThe Social Media Monitor, a surveillance interface\nwhere users evaluate Tweets based on their threat to\nnational security. (2) Two nai\u00a8ve Bayes supervised\nmachine-learning classifiers that automatically label\ntweets as suspicious or not suspicious. The Agent\nBayes classifier is trained on a corpus of manually\nlabeled tweets created by researching and simulating\nthe process and judgments of intelligence agents. The\nCrowd-Sourced Classifier is trained on a corpus labeled\nby visitors to Science Gallery Dublin's SECRET exhib-\nition.5 Users can review the algorithms' suggestions for\naccuracy and idiosyncrasies. (3) The Social Media Post\nInspector, where users can submit text to see if a post is\nlikely to be considered threatening by intelligence agen-\ncies and choose whether or not to share it on social\nmedia. (4) The Watchlist, where users can target them-\nselves and others as subjects of social media monitor-\ning, and which provides automated evaluations\nfrom our machine-learning classifiers to show how\nsocial media posts may be treated by OSINT surveil-\nlance systems (Figure 1). (5) A Resource Library\nthat links to documents that informed the creation of\nthe app.\nThe goal of the CSIA is to expose potential prob-\nlems, assumptions, or oversights inherent in current\ndataveillance processes in order to help people\nunderstand the effectiveness of OSINT processing\nand its impact on our privacy. We aim to facilitate a\ncritical and practice-based understanding of a socio-\ntechnical system that typically evades public scrutiny.\nUltimately, the CSIA provides firsthand experience\nwith social media monitoring, allowing users to\nchoose how they want to navigate social media\nsurveillance.6\nCreating an informed public debate and\nmodel for resistance\nThe release of the Snowden documents revealed the\nextent to which governments and private contractors\nare monitoring the communications of their citizens,\nincluding social media posts and exchanges. This type\nof dataveillance would fall under what Bakir (2015) has\ntermed the ``veillant panoptic assemblage'', which\nincludes, among other things, governmental re-appro-\npriation of citizen's social media communications for\ndisciplinary purposes. Technology and tactics for coun-\nterbalancing the power differential amplified by older\nforms of optical surveillance have already been devel-\noped and are currently in use by the public. Among\nthese is counterbalancing surveillance by the state\n(oversight) with citizen-based sousveillance (under-\nsight) to achieve a `democratic homeostasis', or equi-\nveillance, where the veillant forces of the state and\ncitizens are balanced. Since sousveillance is at a\npower disadvantage, a socio-technical assemblage of\nnew media and social networks may need to be lever-\naged to compensate for the power differential. A\ncommon example of equiveillance is when citizens use\ncell phone cameras to film abuses of power by police or\nthe power elite and post the videos online.\nto achieve an ``equiveillant panoptic assemblage'' where\nthe intelligence-power elite could face public scrutiny\nfor their dataveillance practices in a similar way that\ncitizen-produced videos can hold police accountable for\nthat the current civic infrastructure for ``genuine public\ndebate'' over mass surveillance is currently too weak to\nfacilitate ``change from below'', and her assessment that\nwhen it comes to surveillance, counterveillance and uni-\nveillance ``making people understand and care about\nsuch issues is challenging given their abstract, complex\nnature'' (18).7 We would add to this list of difficulties\nthe inability of citizens to either collect or process Big\nData. Despite these obstacles, we intend the CSIA to be\na step toward facilitating a genuine public debate about\nthe dataveillance of social media and a prototype for\ncounterveillance and sousveillance tools for citizens. By\ndemonstrating that what a dataveillance program `sees'\nwhen it `reads' social media posts is nothing like what a\nhuman being sees, we hope to create a debate over cur-\nrent dataveillance technologies as well as the efficacy\nand ethics of mass automated dataveillance more\nbroadly.\nThe CSIA highlights the importance of the training\ncorpus in machine-learning by allowing participants to\ncreate a corpus used to train the Crowd-Sourced\nClassifier and by providing another classifier, Agent\nBayes, for comparison. The algorithm in both classifiers\nGradecki and Curry 3\nis identical--the only difference between the two classi-\nfiers is the data, which was selected and labeled by users\nof the CSIA application. The ratio of tweets found to\nbe suspicious versus not suspicious is surprisingly simi-\nlar between the two corpuses. In the Crowd-Sourced\nClassifier, museum visitors in Dublin labeled 22.11%\nof the tweets they reviewed as suspicious (Figure 2).\nIn the Agent Bayes corpus, 21.00% of the tweets were\nidentified as threatening by an individual who simu-\nlated the judgment criteria used by intelligence agents\nbased on leaked documents and ethnographic\naccounts.8 However, the predictions made by the two\nclassifiers varied greatly. When Agent Bayes and the\nCrowd-Sourced Classifier were tested against each\nother using a dataset containing 9,430 Twitter posts,\nthey disagreed 35% of the time.\nAutomated classification does not make erroneous\ndata more accurate, it only automates the same errors\nacross a larger dataset. This raises questions about the\naccuracy of the data intelligence agencies use to train\ntheir predictive policing systems, and whether the\npublic should have access to that data for transparency\nand oversight purposes. There are already documented\ninstances of intelligence agencies misinterpreting social\nmedia data as threatening. In 2012, two British students\nwere detained by the US Department of Homeland\nSecurity and denied entrance to the US for posts they\nmade on Twitter. In one post identified as a threat,\nLeigh Van Bryan tweeted a joke from the cartoon\nFamily Guy about ``diggin' Marilyn Monroe up'',\nwhich prompted authorities to search the couple's lug-\ngage for shovels (Compton, 2012). Less humorously, in\nthe trial of Dzhokhar Tsarnaev, the man convicted of\nplanting a bomb made from a pressure cooker at the\nBoston Marathon, the evidence initially presented from\nhis Twitter account was exceptionally flawed. Song\nlyrics and jokes from the television show Key and\nPeel were presented as evidence of wrongdoing and\nthe background image of Tsarnaev's home mosque in\nGrozny had been labeled as ``Mecca'' by the FBI. Upon\nFigure 1. Crowd-Sourced Intelligence Agency watchlist interface.\n4 Big Data & Society\ncross-examination, the agent admitted that they did not\nbother to look at a picture of Mecca for a comparison\n(Woolf, 2015). Because there was ample physical evi-\ndence linking Tsarnaev to the bombing, the FBI may\nhave simply assumed that his Twitter posts were incri-\nminating. If all of the social media posts made by\nknown terrorists are labeled as threatening and used\nin a training corpus for a machine-learning classifier,\nwe can expect to find Twitter users who have similar\ntaste in television and music being algorithmically iden-\ntified as threats to national security. People who believe\nthey will not be targeted by these systems because they\nare not doing anything wrong need to understand that\nautomated classification systems only find statistical\ncorrelations between data: if you happen to make\nposts using language similar to a known target, you\nmay be flagged as a potential threat by the system.\nThe CSIA also provides a model for possible coun-\nterveillance and sousveillance tools. The Social Media\nPost Inspector feature, which allows users to type a\ntweet and process the text with both keyword and algo-\nrithmic analysis to see if it might be flagged as suspi-\ncious by an OSINT dataveillance system, enables\ncounterveillance by showing social media users how\ntheir posts might be interpreted. The user then has\nthe option to tweet directly from the Post Inspector's\ninterface, giving them the option to rephrase the post to\navoid algorithmic scrutiny or even overload a post with\nlanguage that creates false positives. An informed user\nmay even decide to refrain from tweeting altogether.\nThe CSIA Watchlist can be used for sousveillance:\nusers may choose to include law enforcement, intelli-\ngence agencies, government contractors or other mem-\nbers of the intelligence\u00adpower elite, to keep track of\ntheir social media posts using dataveillance techniques\nand participate in a crowd-sourced and distributed\nwatching of the watchers.\nConclusion\nThe CSIA fosters an informed public debate by making\nabstract ideas about surveillance into concrete, inter-\nactive replications of intelligence techniques and tech-\nnologies to allow participants to see some aspects of\nhow dataveillance works in practice. The CSIA pro-\nvides a visceral heuristic: as CSIA agents (users of the\napp) monitor their own posts and the posts of their\nfriends, they can see how the automated processing\nchanges, reinterprets, reframes, and recontextualizes\ntheir posts without needing a background in data\nFigure 2. Visitors to Science Gallery Dublin reviewing Twitter posts.\nGradecki and Curry 5\nscience. The inaccessibility of Big Data keeps the pos-\nsibility of performing effective sousveillance on OSINT\ntechnologies out of reach, prohibiting the prospect of\nachieving equiveillance under the current situation.\nHowever, technologies in this area are developing rap-\nidly enough that it is conceivable that consumer grade\nequipment will be able to perform these types of ana-\nlytics in the near future. The CSIA is taking some of the\nfirst steps towards creating tools for sous-dataveillance\nand counter-dataveillance.\nIdeally, the effectiveness of specific algorithms for\nlanguage processing, translation, and classification\ncould become topics of public debate and scrutiny.\nThe public has the right to ask questions about how a\ncomputer program determines if they are a threat to\nnational security and to question the practicality of\nusing statistical pattern recognition algorithms in\nplace of human judgment. Ethical and legal questions\nwill also need to be addressed, such as who is held\naccountable when someone is wrongfully detained or\narrested due to a statistical similarity to a known\nthreat? What is badly needed for both the public\ndebate and to create effective counterveillance and\nsousveillance tools is the actual data intelligence agen-\ncies use to train their dataveillance algorithms. Without\nthis data, the results returned by the CSIA will only\nresemble the results and mistakes made by OSINT sys-\ntems currently in use. These limitations may be over-\ncome in the near future through leaked information,\nFOIA requests, or public pressure. Despite these limi-\ntations, by reproducing the type of problems inherent\nin the processing and displaying of Big Data for intel-\nligence analysis, the CSIA fosters a critical awareness of\nthe assumptions in dataveillance technology and begins\nto enable the development of counterveillance tactics.\n"
}