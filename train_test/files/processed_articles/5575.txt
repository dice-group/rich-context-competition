{
    "abstract": "Abstract\nIt has been claimed that working memory training programs produce diverse beneficial effects. This article presents\na meta-analysis of working memory training studies (with a pretest-posttest design and a control group) that have\nexamined transfer to other measures (nonverbal ability, verbal ability, word decoding, reading comprehension, or\narithmetic; 87 publications with 145 experimental comparisons). Immediately following training there were reliable\nimprovements on measures of intermediate transfer (verbal and visuospatial working memory). For measures of far\ntransfer (nonverbal ability, verbal ability, word decoding, reading comprehension, arithmetic) there was no convincing\nevidence of any reliable improvements when working memory training was compared with a treated control condition.\nFurthermore, mediation analyses indicated that across studies, the degree of improvement on working memory\nmeasures was not related to the magnitude of far-transfer effects found. Finally, analysis of publication bias shows that\nthere is no evidential value from the studies of working memory training using treated controls. The authors conclude\nthat working memory training programs appear to produce short-term, specific training effects that do not generalize\nto measures of \"real-world\" cognitive skills. These results seriously question the practical and theoretical importance\nof current computerized working memory programs as methods of training working memory skills.\n",
    "reduced_content": "Perspectives on Psychological Science\nReprints and permissions:\nsagepub.com/journalsPermissions.nav\npps.sagepub.com\nWorking memory refers to the idea that there is a general\ncapacity-limited system (or set of systems) responsible for\nthe storage and manipulation of information in the human\nmind. In the last few years, studies of working memory\ntraining have bourgeoned. In this article, we review the\nbackground on working memory training and provide a\nmeta-analytic review of the many studies in this field. Our\nconclusion is that there is no good evidence that working\nmemory training improves intelligence test scores or other\nmeasures of \"real-world\" cognitive skills. We finish with\nsome recommendations for future research.\nCorresponding Authors:\nThomas S. Redick, Department of Psychological Sciences, Purdue\nE-mail: tredick@purdue.edu\nMonica Melby-Lerv\u00e5g, Department of Special Needs Education,\nE-mail: monica.melby-lervag@isp.uio.no\nCharles Hulme, Division of Psychology and Language Sciences,\nUniversity College London, Chandler House, 2 Wakefield Street,\nLondon, WC1N 2PF, United Kingdom\nE-mail: c.hulme@ucl.ac.uk\nWorking Memory Training Does Not\nImprove Performance on Measures\nof Intelligence or Other Measures of\n\"Far Transfer\": Evidence From a\nMeta-Analytic Review\nMonica Melby-Lerv\u00e5g1, Thomas S. Redick2,\nand Charles Hulme3\n1Department of Special Needs Education, University of Oslo; 2Department of Psychological Sciences,\nPurdue University; and 3Division of Psychology and Language Sciences, University College London,\nand Department of Special Needs Education, University of Oslo\n Keywords\nworking memory, training, meta-analysis, transfer\nWorking Memory Training on Intelligence Measures 513\nThe Foundation of Working\nMemory Training\nThe rationale for working memory training developed\nfrom suggestions that limitations in working memory\ncapacity may have wide-reaching effects on other aspects\nof cognition. For example, according to one theory varia-\ntions in fluid intelligence reflect to a large degree varia-\ntions in working memory capacity (Engle, 2002; but see\nHeitz et al., 2006). Others have argued that limitations in\nworking memory capacity may be responsible for impair-\nments in the development of reading (Swanson, 2006),\nlanguage (Archibald & Gathercole, 2006), and mathemat-\nical skills (Passolunghi, 2006). The idea that working\nmemory limitations may place constraints on diverse higher\ncognitive functions leads directly to the suggestion that if\nworking memory capacity can be increased by training, this\nshould produce transfer effects to diverse untrained tasks\nthat depend on such a capacity (Shipstead, Redick, &\nA specific example makes the approach clearer. In a\npioneering study, Klingberg, Forssberg, and Westerberg\n(2002) assessed whether they could improve working\nmemory capacity by training and whether this, in turn,\nwould produce improvements on other cognitive tasks.\nThe participants were children diagnosed with attention-\ndeficit/hyperactivity disorder (ADHD). Children in the\nworking memory training group were given computer-\nized training on three working memory tasks (visuospa-\ntial span, backward digit span, letter-span) and a choice\nreaction time task. For each of the working memory\ntraining tasks, the difficulty level for the trained group\nwas adjusted adaptively across trials by changing the\nnumber of stimuli to be remembered in response to the\nparticipant's performance. The logic of such an adaptive\ntraining regime is that it makes the task challenging and\nensures that participants are performing at the limits of\ntheir working memory capacity. Participants in the trained\ngroup were required to complete 30 trials on each task\neach day (roughly 25 min training per day) for a total of\n24 days of training, giving a total training dose of roughly\n10 hr spread over 5 to 6 weeks. Participants in the control\ngroup performed easy versions of the same tasks where\nthe difficulty level was fixed to a low level in each task.\nHowever, children in the control group only did 10 trials\nper session on each task and had less contact with the\nexperimenter than the children in the treatment group.\nKlingberg et al. (2002) reported that working memory\ntraining improved performance in comparison to the con-\ntrol group on a variety of tasks, including measures\ndirectly trained (visuospatial working memory) and tasks\nsimilar to those trained (span board, another visual memory\ntask). More surprisingly, they also reported significant\nincreases in scores on a standardized measure of intelli-\ngence (Raven's progressive matrices) after working memory\ntraining. This is an example of far transfer--improvement\non a task that is seemingly remote from the tasks that\nhave been trained. The claim that playing a set of com-\nputer games for roughly 10 hr can improve a child's intel-\nligence test scores is provocative and has led many others\nto try to confirm such an effect.\nThis study proved highly influential (and formed the\nbasis of the popular commercial working memory train-\ning program CogMed; www.cogmed.com). However,\nthere are some obvious weaknesses in this study: The\ngroup sizes (7 children per group) are tiny--this study is,\nlike many other studies in this area, severely underpow-\nered. Another problem with the design of this study is\nthat any differences found as a result of training may\nreflect differences in the duration of the computerized\ntasks performed by the two groups, rather than being\neffects of adaptive working memory training, per se. Nev-\nertheless, if the claim that working memory training\nimproves intelligence test scores could be confirmed and\nsubstantiated it would have potentially important impli-\ncations for education and economic productivity, as well\nas for theories about the nature and limitations of human\ncognitive abilities. Because studies in this area are gener-\nally underpowered, summarizing them in a meta-analysis\nmay be valuable as a way of clarifying the conclusions\nthat can be drawn from the many inconsistent findings.\nPrior Reviews of Working Memory\nTraining\nIn this article, we report an updated meta-analysis of cur-\nrent evidence for the effectiveness of working memory\ntraining. We focus on two key issues: (a) Does working\nmemory training improve the skills directly trained? (b) If\nso, does such training transfer to improve other skills\nsuch as intelligence test performance? Recently, there\nhave been many working memory training studies that\nhave been summarized in several qualitative and quanti-\ntative reviews, with seemingly conflicting conclusions\n(see Appendix in the Supplemental Materials available\nonline for references to other reviews and meta-analyses\nof interest).\nMelby-Lerv\u00e5g and Hulme (2013) reported a meta-anal-\nysis of all 23 published studies (at the date of their review)\nof working memory training in both adults and children.\nThey found evidence that the training programs produced\nreliable short-term improvements in working memory\nskills. However, these improvements in working memory\nskills did not appear to be maintained a few weeks after\ntraining had ended. Furthermore, there was no convinc-\ning evidence of generalization from working memory\ntraining to other skills (nonverbal and verbal ability, inhib-\nitory processes in attention, word reading, and arithme-\ntic). They concluded that working memory training\nprograms appear to produce short-term effects that do\nnot generalize to tasks that have not been directly trained.\nClearly, the most critical issue, theoretically and practi-\ncally, is whether or not working memory training pro-\nduces far-transfer effects (Barnett & Ceci, 2002; \nTaatgen,\n2013), for which our previous review (Melby-Lerv\u00e5g &\nHulme, 2013) failed to find any convincing evidence. These\nfindings of small immediate effects with no reliable effects\nat follow-up have also been replicated by Schwaighofer,\nRecently, two meta-analyses have claimed that working\nmemory training can be effective in enhancing cognitive\nskills in adulthood (Au et al., 2014) and stemming cogni-\ntive decline in old age (Karbach & Verhaeghen, 2014).\nHowever, the conclusions from these articles can be ques-\ntioned because of the following: (a) The failure to take\naccount of baseline differences when calculating effect\nsizes is of great importance, because relying only on post-\ntest differences can cause biased effect-size estimates,\nespecially in a field with small sample sizes and studies\nthat have imbalance at baseline; (b) the failure to base\nconclusions on studies that include treated control groups,\nbecause only these studies provide adequate control for\nnonspecific effects (such as familiarity with being assessed\non a computer) that may arise in computerized working\nmemory training studies; and (c) the merging of measures\nsuch as reading comprehension and executive measures\ninto one far-transfer construct. Melby-Lerv\u00e5g and Hulme\n(2016) reanalyzed the studies from these two meta-analy-\nses and concluded that there was no convincing evidence\nthat working memory training produces general cognitive\nbenefits (see also Dougherty, Hamovitz, & Tidwell, 2016).\nHowever, recently there have been other meta-analyses of\nworking memory and cognitive training concluding that it\nis effective for specific age/patient groups (Karr,\n\nAreshenkoff, Rast, & \nPrevious reviews seem to show conflicting findings for\ntwo main reasons: First, some methodological decisions\nmay have led to biased conclusions (see Melby-Lerv\u00e5g &\nHulme, 2016). Second, previous reviews typically have\nexamined subsets of studies restricted to specific types of\nworking memory training, age groups, or participant sta-\ntus. An alternative approach is to include as broad a\nrange of studies as possible and then examine whether\ncertain study characteristics can explain variations\nbetween them. This is the approach adopted here. We\nhave identified a total of 87 publications with 145 differ-\nent experimental comparisons (up from 23 publications\nwith 30 experimental comparisons identified by Melby-\nexperimental comparisons identified by Schwaighofer\net al. (2015), which is a considerably larger number than\nall other reviews in this field. We have designed our\nreview to be as inclusive as possible to reach a clear\nstatement as to what conclusions are justified by current\nevidence in this contentious field.\nAlso, in this review we will pay particular attention to\npossible effects of publication bias using a novel method,\np-curve analysis (Simonsohn, Nelson, & Simmons, 2014).\nIn the past few years, there has been much attention paid\nto issues of replicability in psychology (e.g., Ioannidis,\n2012). By analyzing publication bias in more detail, we\ncan get a better understanding of how it can affect a field\nwhen one study shows a newsworthy finding with an\nextremely large effect size (as in the study by Klingberg\net al., 2002). Our analysis using p-curves provides evi-\ndence of publication bias in studies of working memory\ntraining.\nThe Current Review\nThe current meta-analysis focuses on four interrelated\nquestions:\n1. Does working memory training improve perfor-\nmance on working memory tasks (i.e., produce\nimprovements on the tasks trained and on visuo-\nspatial and verbal working memory tasks)?\n2. Does working memory training improve perfor-\nmance on tests of nonverbal skills (particularly\nmeasures of nonverbal reasoning, such as Raven's\nmatrices)?\n3. Does working memory training improve perfor-\nmance on tests of verbal skills (verbal ability, word\ndecoding, reading comprehension, and arithmetic)?\n4. Is there a relationship between intermediate-trans-\nfer and far-transfer effects (i.e., if training produces\nimprovements on measures of working memory,\nare improvements on more distant measures of\ntransfer such as nonverbal reasoning proportional\nto the improvements found in working memory)?\nQuestion 1 addresses whether training is effective in\nimproving performance on measures that are identical or\nclosely related to tasks that have been trained (near-\ntransfer measures). Questions 2 and 3 address aspects of\nfar transfer; more specifically, they are relevant to per-\nhaps the most provocative claim for working memory\ntraining--that it can increase scores on measures of intel-\nligence and attainment. Question 4 addresses the mecha-\nnisms responsible for possible far-transfer effects.\nTheoretically, such far-transfer effects are only expected\nin the presence of improvements in working memory\ncapacity, because effects of working memory training on\nfluid intelligence or academic attainment are typically\nseen as being mediated by (i.e., dependent on) increases\nin working memory capacity (Harrison et al., 2013; Lange\n& S\u00fc\u00df, 2015; Tidwell, Dougherty, Chrabaszcz, Thomas, &\nWorking Memory Training on Intelligence Measures 515\ning memory training studies find increases in fluid intel-\nligence or attainment in the absence of increases in\nworking memory capacity, this pattern would be difficult\nto explain theoretically. It would have to be argued, for\nexample, that the component of intelligence that is\nimproved by working memory training is different from\nthe components of intelligence that are dependent on\nworking memory capacity.\nThe four questions outlined above are critical for any\nattempt to assess evidence for the effectiveness of work-\ning memory training. However, in reaching a satisfactory\nanswer to these broad questions, a number of method-\nological issues need to be taken into account. One poten-\ntially critical factor is the type of activity to which the\ncontrol group is exposed (Boot, Simons, Stothart, &\ncategorized studies into those with untreated controls\n(i.e., where the control group receives no intervention)\nor treated controls (i.e., where the control group receives\na non-working memory training intervention of a similar\ntype and of equivalent intensity and duration). Arguably,\nonly studies with treated control groups can provide con-\nvincing evidence of specific benefits from working mem-\nory training. If a study only has an untreated control\ngroup, any effects of training could arise from quite gen-\neral effects--for example, familiarity with computerized\ntasks, additional contact with the experimenter (demand\ncharacteristics), and motivational differences related to\nbelief about the study purpose (expectancy effects)\nrather than effects on working memory processes, per se.\nBecause untreated control conditions are likely to overes-\ntimate the true size of any training effect, they are poten-\ntially most useful for preliminary evaluations of novel\neffects, but not for understanding causal mechanisms or\nevaluating whether a treatment offers an improvement\nover current practice (see Mohr et al., 2009, for a review).\nThus, if there are true effects of training, this should be\napparent both in studies using treated controls and in\nstudies using untreated controls. For these reasons, we\nwill analyze studies with untreated controls separately\nfrom those with only treated controls, and our discussion\nand interpretation of the meta-analytic results will largely\nfocus on analyses of studies with treated controls.\nStudies of working memory training also differ on\nnumerous other dimensions that potentially may affect the\nresults of training. Therefore, we will conduct analyses to\nexamine whether training effects differ according to:\n1. The age of participants. On the basis of ideas about\nbrain plasticity (Buschkuehl, Jaeggi, & Jonides,\n2012), some researchers have suggested that indi-\nviduals at different ages may be more receptive to\nbenefits from working memory training (L\u00f6vd\u00e9n,\nB\u00e4ckman, Lindenberger, Schaefer, & Schmiedek,\n2010). This finding could take the form of working\nmemory training being more effective for children or\nyoung adults relative to older adults, although other\nresearch (Zinke et al., 2014) indicates that age may\nmoderate working memory training gains even\nwithin the older population.\n2. The duration or dose of training received. One of\nthe first working memory training studies (Jaeggi,\nBuschkuehl, Jonides, & Perrig, 2008) reported that\nthe amount of transfer to fluid intelligence was\ndependent on the number of training sessions\ncompleted. Working memory training studies have\nvaried widely in terms of training duration, which\nmay account for some of the variation in transfer\nobserved between studies.\n3. Learner status. Under the assumption that individ-\nuals who have cognitive impairments (e.g., ADHD,\ndyslexia, low working memory, or other neuropsy-\nchological disorders) may be more receptive to\nbenefits from working memory training, we sepa-\nrated studies according to whether participants\nwere selected for having a learning difficulty. Sta-\ntistically, individuals with lower cognitive abilities\nbefore training may have more room for improve-\nment, and thus training and transfer might be more\neffective for these individuals relative to healthy,\ntypically developing participants.\n4. Study design. We examined whether studies used\nrandom allocation of participants to conditions. Tra-\nditionally, it has been argued that random allocation\nof participants to conditions is the surest way of\nensuring that the results of an intervention reflect\na causal effect (Shadish, Cook, & Campbell, 2002).\nHowever, meta-analyses indicate that under some\ncircumstances, data from nonrandomized experi-\nments may yield similar estimates of effect size\nto those from randomized experiments (e.g.,\nHeinsman & Shadish, 1996; Shadish & Ragsdale,\n1996). The analyses presented later will examine this\nissue further.\n5. Publication bias. Several studies have found that\nintervention studies are particularly vulnerable to\npublication bias and for lacking replicability\n(\nCuijpers, Smit, Hollon, & Andersson, 2010;\nScherer, Langenberg, & von Elm, 2007). Here, we\nwill pay particular attention to this, both by retriev-\ning as much grey literature as possible (see\n\nRothstein, Sutton, & Borenstein, 2005) and by esti-\nmating the impact of publication bias using statis-\ntical procedures. Studies from the grey literature\nwere retrieved through electronic database\nsearches, by e-mailing researchers in the field, and\nby attending conferences and asking for posters/\nunpublished material (see Fig. 1).\nAdapted from The PRISMA Statement. www.prisma-statement.org. (Mohr et al. 2009)\nScreening\nIncluded Eligibility\nRecords after duplicates removed:\nAbstracts screened\nFull-text articles assessed\nfor eligibility\nStudies included in meta-\nanalysis\ncomparisons)\nInclusion criteria\nSearch features:\n\u00b7 Electronic databases (ERIC, Medline, PsychAPA, ProQuest dissertations,\nPsychInfo, and all Citation Databases included in ISI web of knowledge from 1980\nto August 10, 2015, with keywords \"working memory training\").\n\u00b7 Citation search on author names\n\u00b7 Scanning reference lists\n\u00b7 Hand search of journals that specialize in publishing research on learning\ndisabilities\n\u00b7 Search in prior reviews (see Appendix in the Supplemental Material available\nonline)\n\u00b7 Google scholar\n\u00b7 E-mail request to researchers in the field\nSearch\nIncluded studies must:\n\u00b7 Be a randomized controlled trial or quasiexperiment with a treatment and either\na treated or untreated control group tested pre- and posttest.\n\u00b7 The treatment group had to receive an intervention based on an computerized\nprogram that aimed to train working memory skills (verbal, visuospatial, or both)\nacross more than 1 session/day.\n\u00b7 The studies must provide data so that an effect size can be computed for the\ntransfer measures.\nFull-text articles excluded (n = 50)\nReasons:\n1. Included none of the far transfer tests in our inclusion criteria\n2. Did not have a control group or compared two types of working\nmemory training\n3. Sample overlaps with included study\n4. Did not involve computerized working memory training\n5. Did not involve working memory training\n6. Outcomes on categorical scales\n7. Retracted by authors\nCarretti, Borella, Fostinelli, et al. (2013)4; Carretti, Borella, Zavagnin,\nFig. 1. Flow diagram for the search and inclusion criteria for studies in this review.\nWorking Memory Training on Intelligence Measures 517\n6. Type of working memory training program.\nNumerous training programs have been used in\nworking memory training studies. For example,\nthe commercial CogMed program includes vari-\nous versions of verbal and visuospatial memory\nspan tasks. Other commonly used working mem-\nory training programs include: (a) running mem-\nory span tasks, where participants must recall in\norder only a specified number of items at the end\nof a long list of stimuli; and (b) complex memory\nspan tasks, where the participant completes a dis-\ntractor processing task interleaved with to-be-\nremembered stimuli within the span task. Finally,\nvariations of the N-back task have been used fre-\nquently; in these tasks participants indicate\nwhether or not the currently presented stimulus\nmatches one that was presented n stimuli back in\na list (e.g., Jaeggi et al., 2008). The different types\nof working memory training tasks used could\naccount for the variation in results across studies\nand is examined here as a potential moderator. In\naddition, the stimulus content (verbal, visuospa-\ntial, or both) of the working memory training pro-\ngrams was coded.\nIn summary, we present a meta-analysis to synthesize evi-\ndence from all studies of working memory training (both\npublished and unpublished) that we have been able to\nidentify. We will focus particularly on evidence for the\nmost provocative claim of working memory training (that\nit improves intelligence), though we will also consider\nother claims, particularly those concerning generalized\nimprovements on untrained measures of both verbal and\nvisuospatial working memory. This study will clarify previ-\nous working memory training meta-analyses. We will pro-\nvide more fine-grained analyses of several different\noutcomes, analyzing follow-up effects and controlling for\nbaseline differences, while examining multiple working\nmemory training programs (in contrast to Au et al., 2014)\nand including participants across the entire life span (in\ncontrast to Au et al., 2014, and Karbach & Verhaeghen,\nSchwaighofer et al. (2015), we have a considerably larger\nsample of studies, providing us much more robust esti-\nmates of effect size and allowing us to perform moderator\nanalyses that have more power (Hedges & Pigott, 2004).\nFurther, in comparison to Melby-Lerv\u00e5g and Hulme (2013)\nand Schwaighofer et al. (2015), we will also provide a\nmore fine-grained analysis of working memory measures\nand publication bias. Critically, we will use mediation\nmodels to examine whether differences among the studies\nin terms of gains in working memory are related to differ-\nences in gains on the transfer measures.\nMethod\nThis meta-analysis was designed in line with the state-\nment for systematic reviews developed by PRISMA (Pre-\nferred Reporting Items for Systematic Reviews and\nMeta-Analyses, www.prisma-statement.org).\nSearch, inclusion criteria and coding\nThe literature search, criteria for inclusion and exclusion\nand flow of studies are shown in Figure 1. Only studies\nthat had a control group and a training group with pretest\nand posttest measures before and after working memory\ntraining were included in the review. Our decision as to\nwhether a study trained working memory was guided by\nprevious research examining the factor structure of work-\ning memory measures (see Ackerman, Beier, & Boyle,\n\nGathercole, Pickering, Ambridge, & Wearing, 2004; Kane\nincluded, the training had to involve tasks that typically\nhave been found to load on working memory in latent-\nvariable studies. Examples include visuospatial and verbal\nversions of simple span (forward and backward), complex\nspan, running span, updating (e.g., keep track), and N-back\n(single and dual). In cases where an intervention had mul-\ntiple components, the working memory tasks had to consti-\ntute at least 50% of the intervention. Studies based purely on\ntraining task-switching, inhibition, or reasoning were\nexcluded in an attempt to isolate the construct being trained\nas working memory and not other potentially related con-\nstructs (e.g., executive functioning). We included only stud-\nies of computerized working memory training.\nThe studies examined could be randomized or quasi-\nexperiments, but they had to include tests of either non-\nverbal ability, verbal ability, reading comprehension,\nword decoding, or arithmetic as outcome measures. We\nalso included verbal and visuospatial working memory\noutcomes, but only if the study included one of the far-\ntransfer constructs. Measures of problem solving without\na clear reliance on language were coded as nonverbal\nability tests (common examples include Raven's progres-\nsive matrices and the Cattell Culture Fair test). Measures\nof verbal ability were largely tests of receptive or expres-\nsive vocabulary knowledge, along with reasoning based\non alphanumeric content (e.g., Letter Sets, Number\nSeries). Measures that involved text reading with subse-\nquent questions about the meaning of a passage were\nclassified as reading comprehension tests. Measures of\nword decoding included tests of the accuracy or fluency\nof word or nonword reading. Arithmetic measures\nincluded tasks involving addition, subtraction, multiplica-\ntion, or division. Near-transfer measures were tests that\nwere similar or identical to the tasks trained. For working\nmemory measures, we distinguished between measures\nof verbal and visuospatial working memory (where the\nparticipant had to remember verbal or visuospatial mate-\nrial, respectively). Simple memory span tests such as digit\nspan were excluded from the analyses if only the forward\nversion was administered, unless the forward-only ver-\nsion of the task was considered a criterion measure that\nwas identical to a training task. If the backward version\nof a simple span task was administered separately, or a\ncombined score for the forward and backward versions\nwas reported, the outcome was included as well.\nWe separated the measures into three different catego-\nries: (a) near-transfer measures (tests that were similar or\nidentical to the tasks trained), (b) intermediate-transfer\nmeasures (verbal and visuospatial working memory mea-\nsures), and (c) far-transfer measures (measures that dif-\nfered substantially from those trained, i.e. nonverbal\nability, verbal ability, reading comprehension, word\ndecoding, or arithmetic).\nWe also took steps to account for the nonindependence\nof effect sizes from the same or different studies. Violating\nthe assumption of independence by computing an overall\neffect size based on information from the same sample\nmore than once can lead to biased estimates (Borenstein\net al., 2009). For this reason studies from the same author\nwere examined to detect duplicate samples, and studies\nbased on the same participants were only coded once (see\nFig. 1 for detailed information). When a study had multiple\nindicators for the same construct (for instance more than\none measure of verbal working memory) the mean of the\nindicators was used to yield a single effect size for that\nstudy. Finally, some studies compare the same control\ngroup to different experimental groups and are included\nin the same analysis of a mean effect size for treated and\nuntreated controls. Because the correlations between the\nmultiple comparisons and their outcomes are not reported\nin the original studies reviewed, we included these studies\nin the analyses, assuming zero correlation between the\noutcomes. Note, however, that we also did analyses where\nwe combined the different treatment groups into one in\nthese studies, and this produced essentially identical\nresults to those reported here.\nAll the studies were coded by two independent raters.\nThe interrater correlation (Pearson's) for outcomes was\nAny disagreements between raters were resolved by con-\nsulting the original article or by discussion.\nMeta-analytic procedure and analysis\nThe analyses were conducted using the \"Comprehensive\nmeta-analysis\" program (Borenstein, Hedges, Higgins, &\nRothstein, 2005). We calculated effect sizes by dividing\nthe differences in gain between pre- and post-test in the\ntreatment and the control group by the pooled standard\ndeviation for each group at pretest; this method of effect-\nsize calculation for pretest-posttest designs is recom-\nmended (Morris, 2008). Thus, when the effect size is\npositive, the group receiving working memory training\nmade greater pretest-posttest gains than the control\ngroup. We adjusted the effect size for small samples using\nHedges's g (Hedges & Olkin, 1985). Effect sizes for fol-\nlow-up tests were calculated in an analogous way (pre-\ntest to follow-up).\nThe mean effect sizes were calculated by a weighted\naverage of individual effect sizes using a random-effects\nmodel. Because previous meta-analyses have found a\nlarge difference between studies using treated and\nuntreated controls (Dougherty et al., 2016; Lilienfeld,\nRitschel, Lynn, Cautin, & Latzman, 2014; Melby-Lerv\u00e5g &\nHulme, 2013), these two designs were analyzed sepa-\nrately. Treated controls received computerized training\neither based on nonadaptive memory tasks or other tasks\nthat did not involve memory training but which were of\nsimilar duration to the training received by the interven-\ntion group. Untreated controls had no contact other than\ncompleting the pretest, posttest, and follow-up transfer\nsessions.\nFor moderator analyses, studies were separated into\nsubsets based on the categories in the categorical mod-\nerator variable (e.g., children vs. young adults vs. older\nadults). A Q test was used to examine whether the effect\nsizes differed between subsets. When there were fewer\nthan four studies in a subset (k < 4), this analysis was not\nconducted. The overlap between confidence intervals\nwas used to examine the size of the difference between\nsubsets of studies. Because of the limited number of\ntraining studies examining follow-up effects, moderator\nanalyses were not conducted. In addition, moderator\nanalyses were not calculated for decoding, reading com-\nprehension, and arithmetic, given the limited number of\ncomparisons for treated and untreated controls.\nPublication bias refers to the notion that a mean effect\nsize can be upwardly biased because only studies with\nlarge or significant effects get published (i.e., file-drawer\nproblem with entire studies) or that authors only report\ndata on variables that show effects (often referred to as\np hacking, or the file-drawer problem for parts of studies;\nsee Simmons, Nelson, & Simonsohn, 2011; Simonsohn\net al., 2014). In line with recommendations for meta-anal-\nyses, we made special efforts to retrieve studies from the\ngrey literature and used this as a moderator when possi-\nble (Higgins & Green, 2011). To estimate the impact from\npublication bias statistically, commonly funnel plots have\nbeen used in combination with a trim-and-fill analysis.\nHowever, there are several problems with the funnel\nWorking Memory Training on Intelligence Measures 519\nplot/trim-and-fill method (Lau, Ioannidis, \nTerring, Schmid,\n& Olkin, 2006). P-curve is a recently developed method\nthat deals with the weaknesses in the funnel plot/trim-\nand-fill analysis (Simonsohn et al., 2014). A p-curve plots\nthe distribution of statistically significant p values (p <\n.05) in published studies, and the shape of the p-curve is\na function only of the effect size and sample size, when\nthe power level is taken into account. If there are true\neffects, one expects the distribution of published p values\nto be right-skewed with more low (.01) than high (.04) p\nvalues. However, if a set of studies is affected by publica-\ntion bias (because researchers discard entire studies or\ndiscard analyses or parts of studies), the p-curve becomes\nleft-skewed or flat. Such a form of p-curve is said to pro-\nvide no evidential value (i.e., no support for an appre-\nciable effect size).\nWhen coding articles, it became clear that there were\nnumerous instances of missing data. If data were critical\nto calculate an effect size, articles with missing data were\nexcluded if authors did not respond to an e-mail request\nto provide the data (see inclusion criteria in flow chart).\nIn cases where an effect size could be computed on one\noutcome but data were missing on other outcomes or\nmoderator variables, the study was included in all the\nanalyses for which sufficient data were provided.\nModerator variables\nModerators are variables that may explain why different\nstudies show different results (Pigott, 2012). The follow-\ning moderator variables were used:\nAge.The average age of participants in each study was\ncoded. Because of a nonnormal distribution, it was not pos-\nsible to analyze age as a continuous variable. Studies were\ntherefore separated into three groups: studies of children\nTraining dose.The duration of training (total number\nof hours in training) was coded. Again, because of a non-\nnormal distribution, training duration was divided into\ndiscrete bands (studies with a total training duration of\nup to 10 hr vs. those with more than 10 hr).\nDesign type.The procedure for separating participants\ninto training and control groups was coded (randomized\nor nonrandomized).\nLearner status.The sampling of participants in the\nstudy was coded: learning disorder (e.g., ADHD, reading,\nmath, or other learning disorders) or unselected.\nTraining type.The training programs were split into\nfour categories: N-back, CogMed, complex span, and\nother tasks (which could include combinations of the\nfour specific types of training listed). There were too few\nrunning span studies to consider them as a separate\ncategory.\nPublication type. Each experiment was coded as grey\nliterature (theses, dissertations, conference posters) or\npublished studies (journal articles, chapters, peer-\nreviewed conference proceeding papers).\nResults\nCharacteristics concerning the studies included in the\nreview are shown in Table S1 in the Supplemental Mate-\nrial available online. This table shows sample age, sam-\nple size, outcome measures, and effect sizes for each\ntime point (posttest or follow-up) for each comparison\nwithin each study. Table S2 in the Supplemental Material\navailable online shows how each study was coded on the\nmoderator variables. Sample size and mean sample size\nfor all studies included in each analysis are shown in\nTable S3 in the Supplemental Material available online.\nFor forest plots for each outcome, see Figures S1 to S8 in\nthe Supplemental Material available online. The full data-\nset is provided in an Excel sheet in the Supplemental\nMaterial available online.\nImmediate effects of working memory\ntraining on far-transfer measures\nFigure 2 shows a summary of the effects of working\nmemory training on far-transfer measures. In studies with\ntreated controls, the effects of training on nonverbal abil-\nity, verbal ability, word decoding, and arithmetic are close\nto zero and not significant (see Table 1). For reading\ncomprehension, the effect size for studies with treated\ncontrols is small though significant (g = 0.15). Closer\ninspection shows that 10 comparisons with treated con-\ntrols give small to large positive effects (g = 0.10 to 0.87).\nOf these, six comparisons across four studies (Jaeggi,\npattern with the control group showing decreases in\nreading comprehension from pretest-posttest (see Redick,\n2015, for examples of this pattern). This pattern of results\nmakes these studies very difficult to interpret. Theoreti-\ncally, there is no reason to expect decreases in a rela-\ntively stable construct such as reading comprehension\nability between pretest and posttest in a control group;\nsuch decreases presumably can only reflect error of mea-\nsurement. Also, one of these studies (Shiran & Breznitz,\n2011) showed large effects compared with the others\n(above 3 SDs from the mean effect size). When the com-\nparisons with the problematic pretest-to-posttest decline\nin the control group are excluded, the effects of working\nmemory training compared with treated controls for\nreading comprehension becomes trivial (g = 0.08) and\nsimilar in magnitude to the other far-transfer outcomes.\nAs seen in Table 1, even studies using passive controls\ndid not produce significant effects on verbal abilities,\ndecoding, and reading comprehension. Thus, when\ndirectly comparing treated and untreated controls, the dif-\nference in effect size was only statistically significant in the\nmeasures the pattern of findings across studies were con-\nsistent (the true variation between studies was zero or not\nsignificant, see Table 1). Note that N-back training shows a\nsignificant effect on nonverbal ability (g = 0.15, p = .02) in\nstudies with treated controls. Further examination of the\nstudies with the largest effect sizes for N-back training\ntransferring to nonverbal ability revealed several short-\ncomings, which unfortunately are common in the working\nmemory training literature (see Redick, 2015). For the five\nlargest effect sizes, all comparisons had (a) small sample\nsizes (less than the minimum 20 observations per group as\nrecommended by Simmons et al., 2011) and (b) used only\none test to measure nonverbal ability (in contrast to mul-\ntiple indicators of the intended nonverbal ability construct;\nShipstead, Redick, & Engle, 2012; von Bastian & Oberauer,\n2014). In addition, four of the five comparisons had large,\nunexplained pretest-to-posttest decreases for the control\ngroup, which were larger than the training group's pretest-\nto-posttest increases. These crossover interactions thus\nartificially produced large effect sizes (Boot, Blakely, &\n\nWebster, 2014), and are responsible for the significant\neffect of N-back training and transfer to nonverbal ability.\nAfter only the most problematic study has been removed\nfrom the analysis (Schweizer, Hampshire, & Dalgleish,\n2011), the effects of N-back training on nonverbal ability\nwith treated controls is negligible (g = 0.10). In the mod-\nerator analyses, studies with adults showed a small though\nsignificant effect (g = 0.10) as did studies with a small\ntraining dose (g = 0.13); however, both of these significant\neffects within the moderator analyses include the prob-\nlematic comparisons described above, and when these\nstudies are excluded the effect sizes reduce to trivial levels.\nIn any case, these effect sizes are arguably too small to be\nrelevant to educational practice (Cooper, 2008; also see\nPromising Practices network at www.promisingpractices\n.net and the What Works Clearinghouse at www.w-w-c\n.org). For further details of the moderator analyses of far\ntransfer, see Table S4\u00adS5 in the Supplemental Material\navailable online.\nTreated controls (k = 10)\nUntreated controls (k = 7)\nTreated controls (k = 22)\nUntreated controls (k = 16)\nVerbal working\nmemory\nVisuospatial\nworking memory\nCriterion\nmeasures\nFar-transfer\neffects\nNear-transfer\neffects\nTreated controls (k = 67)\nUntreated controls (k = 53)\nNonverbal\nabilities\nVerbal abilities\nDecoding\nReading\ncomprehension\nTreated controls (k = 19)\nUntreated controls (k = 7)\nArithmetic Treated controls (k = 15)\nUntreated controls (k = 14)\nTreated controls (k = 60)\nUntreated controls (k = 38)\nTreated controls (k = 40)\nUntreated controls (k = 25)\nTreated controls (k = 22)\nUntreated controls (k = 16)\nConstruct\nCondition\n(number of studies)\nMean effect size\nIntermediate-\ntransfer effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 2. Mean effects (g) on the transfer measures for studies with treated and untreated controls (k = number of studies)\nWorking Memory Training on Intelligence Measures 521\nFollow-up effects from working\nmemory training on far-transfer\nmeasures\nTable 2 shows the effect sizes for each of the far-transfer\nmeasures at follow-up (on average, 5 months after train-\ning). For treated controls there are no statistically signifi-\ncant effects on nonverbal ability, verbal ability, word\ndecoding, or reading comprehension. There is, however, a\nsignificant effect at follow-up for treated controls on arith-\nmetic. Unfortunately, once again this effect seems to be\ndriven by three comparisons across two studies (\nAlloway,\nBibile, & Lau, 2013; Nussbaumer, Grabner, Schneider, &\nStern, 2013), where the control group shows decreases in\nperformance between pretest and posttest. When these\nstudies are excluded the effect size is negligible (g = 0.14).\nThe findings are consistent between studies; there is no\nsignificant variation between studies on any measure.\nImmediate effects of working memory\ntraining on intermediate-transfer\nmeasures\nFigure 2 shows a summary of the effects from working\nmemory training on verbal and visuospatial working\nmemory measures. These effects are significant and mod-\nerate in size, and there is evidence of true heterogeneity\nbetween studies (see Table 1). On verbal working memory\ntests (see Table S6 in the Supplemental Material available\nonline), children and older adults show significantly larger\neffects of training than adults, and samples with learning\ndisorders show larger effects than samples without\nlearning difficulties. Also, for verbal working memory,\nCogMed shows a significantly larger effect than the other\ntraining programs (see Table S6 in the Supplemental Mate-\nrial available online). For visuospatial working memory,\nnone of the moderators for treated controls were reliably\nrelated to variation between studies (see Table S7 in the\nSupplemental Material available online), although studies\nwith treated controls produced smaller effects compared\nFollow-up effects of working memory\ntraining on intermediate-transfer\nmeasures\nTable 2 shows the effect sizes for each of the intermedi-\nate-transfer outcomes at follow-up (on average, 5 months\nafter training). At follow-up, the effects for verbal working\nmemory were no longer significant in studies with treated\ncontrols but still significant for studies with untreated con-\ntrols. For visuospatial working memory, there were still\nsignificant effects at follow-up (see Table 2). It is impor-\ntant to note, however, that for many studies, the visuospa-\ntial working memory tests consisted of tasks that were\nsimilar in stimuli or method to those that were trained\n(e.g., the CogMed program and the span tests used in\nmany of the CogMed studies).\nEffects of working memory training\non near-transfer measures\nThere were large effects on tasks that are similar or iden-\ntical to those that are trained (see Fig. 2). The findings,\nTable 1. Effects of Working Memory Training Compared to Treated and Untreated Control Groups at Immediate Posttest\nConstruct\nComparison\ntype\nMean effect\nsize (g)\nNo. of\nthough statistically significant, vary across studies (true\nheterogeneity is present, see Table 1) and are maintained\nat follow-up (5 months after training; see Table 2). At\nposttest, studies with treated controls show significantly\nsmaller effects than those with untreated controls, Q(1) =\nsignificant moderator, with smaller effects among chil-\ndren than younger and older adults. In addition, near-\ntransfer effects were larger in subjects that were healthy\nand did not have a condition associated with impaired\nworking memory. There were no other significant mod-\nerators (for details, see Table S8 in the Supplemental\nMaterial available online).\nThe relationship between measures\nof near and far transfer\nA series of mediation models were used to assess the\nextent to which improvements in working memory per-\nformance following training accounted for improvements\non far-transfer measures (nonverbal ability and verbal\nability). The first model contains the subset of studies that\nreported measures of both nonverbal ability and working\nmemory measures similar to the tasks that were trained.\nBecause treated controls showed significantly smaller\ntraining effects on nonverbal ability than untreated con-\ntrols, we ran a meta-regression model (random effects\nmodel, method of moments) where we controlled for the\ntype of control group with a dummy variable (coded as\n1 for treated and 0 for untreated). The results showed that\ntype of control was a predictor of gains on tests of non-\nverbal ability ( = .27, p = .04). However, improvements\nin nonverbal ability were not significantly related to\nimprovements on the near-transfer working memory\nwhether improvements on visuospatial working memory\nmediated the effects from working memory training on\nnonverbal ability. The meta-regression model showed\nthat type of control (untreated vs. treated controls) was a\nsignificant predictor of gains in nonverbal ability ( = .20,\np < .01) but improvements in the visuospatial working\nmemory measures did not explain any further variance\nthe degree of verbal working memory improvement\nmediates the degree of improvement on measures of ver-\nbal ability. We ran a meta-regression model where we\nfirst controlled for type of control group in the subset of\nstudies that reported data on both verbal working mem-\nory and verbal ability. The results showed that type of\ncontrol group did not account for any statistically signifi-\ncant differences in gains in verbal abilities ( = -.03,\np = .75). Second, we examined whether improvements\non verbal working memory mediated the effects from\nworking memory training on verbal abilities, and there\nOverall, these analyses fail to provide support for the\nidea that improvements on measures of far transfer are\nmediated by improvements in working memory capacity.\nThe absence of such effects calls into question the theo-\nretical rationale for the training studies reviewed here,\nbecause such studies are predicated on the notion of a\nmediated relationship (that improvements on far-transfer\nmeasures are causally dependent on the degree of\nimprovement in working memory capacity, which is seen\nTable 2. Effects of Working Memory Training Compared With Treated and Untreated Control Groups at Delayed Posttest\nConstruct\nComparison\ntype\nMean effect\nsize (g)\nNo. of\nWorking Memory Training on Intelligence Measures 523\nas a limiting factor for performance on measures of far\ntransfer). In contrast, the meta-regression results pro-\nvided additional evidence that the type of control group\nused (treated vs. untreated) accounts for significant vari-\nance in the nonverbal ability outcomes.\nAnalyses of publication bias\nTo address publication bias, we analyzed whether there\nwere differences between published studies and the grey\nliterature (see Table S9 in the Supplemental Material\navailable online). Overall, there was a tendency for stud-\nies in the grey literature to have smaller effect sizes, but\nthis was only significant for verbal working memory and\nvisuospatial working memory (note that we did not con-\nduct tests for outcomes where there were four or fewer\ngrey studies).\nWe did a p-curve analysis of published studies that\nhave reported significant (p < .05, two-tailed) far-transfer\neffects from working memory training. In a p-curve anal-\nysis, if the studies have evidential value, we expect to\nfind a right-skewed curve with more low p values than\nhigher p values. If the opposite is the case, and the stud-\nies have no evidential value (likely due to publication\nbias or p-hacking), we expect to find a left-skewed\np-curve with more higher than lower p values. Because\nof the differences between treated and untreated controls\nfound in the previous analyses, we did one p-curve for\nstudies with untreated controls and one for treated con-\ntrols. For studies with untreated controls we expected to\nfind evidential value (a right skewed p-curve with more\nlower p values), because these studies are likely to show\nevidential value because the untreated control compari-\nson overestimates the \"true\" effects of intervention. For\nthe studies with treated controls, we expected to find no\nevidential value in the p-curve because the overall mean\neffect size concerning far transfer from studies that use\nactive controls is close to zero. In spite of this, numerous\npublished studies have reported significant far-transfer\neffects. We therefore expect to find evidence of publica-\ntion bias here (exclusion of studies or analyses).\nDetailed rules for including studies in the p-curve\nanalysis are shown in Table S10, in the Supplemental\nMaterial available online. For details of the studies that\nthe p-curves are based on, as well as excluded studies,\nsee Table S11 in the Supplemental Material available\nonline. Unfortunately, there has been an extensive use of\none-tailed significance tests in published studies in this\nfield (multiple studies with analysis of variance and anal-\nysis of covariance models, which is not possible given\nthat the F distribution is asymmetric). In the p-curve anal-\nysis these p values were transformed into two-tailed val-\nues. Six of the studies that claimed significant far transfer\nwhen using a one-tailed test exceeded p = .05 when we\nused this more conventional two-tailed approach (see\nTable S11 in the Supplemental Material available online).\nBecause a p-curve analysis only includes studies that\nreport significant findings, these six studies were excluded\nfrom the p-curve analysis.\nFigure 3 (panel A) shows the p-curve for published\nstudies with untreated controls. In line with our hypoth-\nesis, the p-curve shows that these studies have evidential\nvalue with a right-skewed p-curve (i.e., significantly more\np values of .01 or below than higher p values, z(16) =\nstudies of working memory training using treated con-\ntrols only; this confirms our hypothesis, as there is no\nevidential value from the studies of working memory\np-curve is flat or slightly left-skewed, providing evidence\nof publication bias (exclusion of whole studies or parts of\nstudies). If a real effect existed, we would expect the\np-curve to be right-skewed.\nDiscussion\nOur meta-analysis of working memory training reveals a\nclear pattern. Current working memory training programs\nyield short-term improvements on both verbal and visuo-\nspatial working memory tasks. For verbal working mem-\nory, these short-term near-transfer effects are not\nsustained when they are reassessed after a delay of a few\nmonths. For measures of visuospatial working memory,\nmodest training effects appear to be maintained at fol-\nlow-up, but these outcome tasks often share features\n(memoranda, method) with the tasks trained. Most seri-\nously, however, there is no evidence that working mem-\nory training convincingly produces effects that generalize\nto important real-world cognitive skills (nonverbal ability,\nverbal ability, word decoding, reading comprehension,\narithmetic) even when assessments take place immedi-\nately after training, especially when compared against a\ntreated control group.\nThere were two cases where there appears to be weak\nevidence for transfer from working memory training to\nmeasures of real-world cognitive skills: improvements in\nreading comprehension immediately after training (that\nwere not sustained at follow-up) and improvements in\narithmetic at follow-up (in the absence of effects at imme-\ndiate posttest). However, we believe that there are strong\nreasons to doubt that these effects are genuine. Both\neffects appear to be driven by studies in which the control\ngroup show a decrease between pretest to posttest, and\nsuch a pattern inflates estimates of the effect size obtained\n(in extreme cases a large decrease in scores in the control\ngroup, coupled with no significant change in scores for\nthe trained group, would lead to a significant but arguably\nartefactual effect of training). This pattern of declines in\nthe controls on stable constructs such as reading compre-\nhension and arithmetic is completely unexpected and\npresumably reflects measurement error. Contrary to the\nclaims made in the original articles, we believe that such\na pattern provides no evidence for a training effect\n(Redick, 2015). In addition, neither of these effects was\nsignificant in the untreated control comparisons.\nGiven our broad inclusion criteria, we were able to\nexamine a number of possible moderators of working\nmemory training effects on transfer. In contrast to specu-\nlation in the literature that the characteristics of the par-\nticipants (age, learning difficulties) and training procedure\n(dose, training type) are critical for producing far transfer\nin certain studies, we found virtually no evidence of sig-\nnificant moderator effects, especially for nonverbal abil-\nity. We did find significant moderator effects on\nintermediate and near transfer (verbal and visuospatial\nworking memory), suggesting that we had sufficient\npower to detect such effects if they were present. We\nbelieve that the lack of significant moderator effects on\nnonverbal ability is important since it contradicts many\nsuggestions in the literature.\nMethodological issues in the studies of\nworking memory training\nOne major methodological issue comes out strongly from\nour review: the problem of using untreated control\ngroups. In our analyses, we separated studies with treated\ncontrols from those with untreated controls. It is clear\nfrom our analyses that the effects of working memory\ntraining on measures of far transfer are absent (nonverbal\nability, verbal ability, word decoding, arithmetic) in stud-\nies using treated control groups. As noted in the Intro-\nduction, only studies using treated control groups provide\na sound basis for claiming support for specific causal\neffects of working memory training. Just as new medica-\ntions are compared against inert pills in clinical trials to\ncontrol for placebo effects, working memory training\ninterventions should be compared against treated control\ngroups to provide evidence for specific effects of work-\ning memory training in causing gains in unpracticed abil-\nities. We recommend that investigators stop conducting\nworking memory training studies with untreated control\ngroups and that journals stop publishing them. It should\nalso be noted that the type of active control group used\nis also potentially important (see Mohr et al., 2009). For\nexample, whether an active control group uses a non-\nadaptive training regime or a different, adaptive activity\nmay have effects (Weicker et al., 2016). Because many of\nthe studies reviewed did not describe the active control\ngroup scheme in much detail, there were too few studies\nto do a more fine-grained analysis of this. However, this\nis potentially important to consider in future studies.\nAnother important methodological issue is the use of\nmediation analyses to relate changes on far-transfer mea-\nsures to changes in working memory capacity. Such anal-\nyses allow us to investigate the extent to which changes\non a far-transfer measure can be accounted for by\nchanges in the theoretically critical mediating variable--\nworking memory capacity (see Hulme et al., 2012, for an\napplication of such analyses to explaining effects of inter-\nvention procedures in studies of reading development).\nIn this article, we reported meta-regression models to\nassess the extent to which, across studies, improvements\nin outcome measures (e.g., verbal ability) were related to\nimprovements in theoretically relevant mediators (e.g.,\nverbal working memory capacity). Those analyses\nrevealed no evidence for any mediated relationships.\nThese findings undermine the rationale for working\nmemory training studies. If working memory training\nA\nB\nObserved p-curve\nNull of zero effect\nll\nll\nll\nll\nFig. 3. p-curve analysis of studies of working memory training for\narticles that have tested a hypothesis of transfer effects from training to\nother cognitive measures. A: p-curve analysis for studies with untreated\ncontrols. B: p-curve analysis for studies with treated controls.\nWorking Memory Training on Intelligence Measures 525\nproduces far transfer because of increased working\nmemory capacity, there should be a direct relationship\nbetween the degree to which working memory skills\nincrease and the extent of increases in measures of far\ntransfer, such as fluid intelligence. We believe it is impor-\ntant for future studies of working memory training to be\nadequately powered to allow for convincing tests of\nmediated relationships.\nOn a related point, many of the studies included in the\nmeta-analyses we have reported here contain small sam-\nple sizes, which result in very low power. For instance in\nthe 120 studies that reported transfer effects to nonverbal\nIQ, the mean sample size was 22.4 participants in the\ntraining group and 22.1 in the control group (see Table S3\nin the Supplemental Material available online). The larg-\nest effect size for working memory training on nonverbal\nability came from the earliest study (d = 2.18; Klingberg\net al., 2002), which included only 7 subjects in each of\nthe training and control groups. Given the tendency for\nsmall sample sizes to produce inflated effect-size esti-\nmates (Button et al., 2013), future studies should include\nlarger samples to produce more precise effect-size\nestimates.\nHere is one demonstration of how small sample sizes\nproduce inflated effect sizes in the current dataset.\n\nSimmons et al. (2011) recommended that at a minimum,\n20 subjects/observations need to be present in each cell\n(in this case, group). Therefore, we analyzed posttest\nnonverbal ability effect sizes for studies that had at least\n20 subjects in each of the training and the control group\nversus studies that had less than 20 subjects in each of\nthe training and the control group. For treated controls,\nthe k = 34 studies meeting the minimum recommended\nsample sizes produced no effect, g = 0.01, whereas the\nk = 25 comparisons with fewer subjects produced a sig-\nnificant effect, g = 0.26. For untreated controls, the k = 31\nstudies meeting the minimum recommended sample\nsizes produced a significant effect, g = 0.16, as did the\nk = 18 comparisons with fewer subjects, g = 0.33. These\nresults provide clear evidence that nonverbal ability\ntransfer is largest in studies with untreated controls and\nsmall sample sizes, and no effects are observed in studies\nwith treated controls and at least the minimum recom-\nmended number of subjects in the training and control\ngroups.\nThe problem with studies of low power is that pub-\nlished studies are likely to be biased because only those\nwith large or very large effect sizes will generate statisti-\ncally significant results and therefore get published (the\nso-called \"file-drawer problem\"). This is termed by Bogg\nand Lasecki (2015) a \"winner's curse\" because such very\nlarge effect sizes are unlikely to be true. Kraemer,\nGardner, Brooks, and Yesavage (1998) have argued force-\nfully that meta-analyses should exclude studies that are\nunderpowered, as this will go a long way to removing\nthe problem of misleading conclusions arising from the\nfile drawer problem. The p-curve analyses presented ear-\nlier provide evidence of publication bias in studies with\ntreated control groups in this field.\nPractical implications\nWorking memory training has frequently been claimed to\nincrease intelligence and other important real-world\nskills. However, based on an analysis of the 87 studies\ncontaining 145 independent experiments reviewed here,\nwe observed no evidence of such effects. The general\npattern of a lack of transfer to real-world constructs fits\nwith other recent meta-analyses that assess the potential\ntherapeutic benefit of working memory training. For\nexample, there is no evidence that working memory\ntraining reduces symptoms in individuals with ADHD\n(Cortese et al., 2015; Rapport, Orban, Kofler, & Friedman,\nTheoretical implications\nGiven the strong relationship between working memory\ncapacity and fluid intelligence (for a review, see Unsworth,\n2015), the lack of transfer effects from working memory\ntraining to nonverbal and verbal abilities may appear sur-\nprising. The absence of such effects may simply reflect the\nfact that there is no causal relationship between working\nmemory capacity and fluid intelligence (Harrison et al.,\n2013). However, working memory capacity and intelli-\ngence share approximately 50% common variance (mea-\nsured with latent variables; Kane, \nHambrick, & Conway,\n2005). If working memory training did work to produce\nincreases in intelligence, increases in working memory\nafter training must be responsible for increasing the pro-\ncesses that are shared with intelligence. As we have\nshown with our mediation analyses, the available evi-\ndence suggests that is not the case--gains on measures\nof working memory were not related to the size of gains\non measures of intelligence (nonverbal and verbal abil-\nity). This result may reflect the fact that individual differ-\nences in working memory capacity are multifaceted\n(Gibson & Gondoli, 2013). Having participants repeat-\nedly practice a working memory task may not necessarily\nengage those aspects of working memory that reflect\ncommon processes shared with measures of intelligence.\nFor example, according to one prominent view of\nindividual differences in working memory capacity, indi-\nviduals vary in (a) the number of items that can be held\nin primary memory; (b) the ability to search strategically\namong items in secondary memory; and (c) the ability\nto control attention according to goals (Unsworth,\ndemonstrated that all three sources of variance (primary\nmemory, secondary memory, attention control) were\nnecessary to fully mediate the relationship between\nworking memory capacity and fluid intelligence. An\nimportant question, then, is how likely is it that repeat-\nedly practicing a task such as N-back leads to changes in\nany or all of these three sources of working memory\ncapacity? Hypothetically, even if repeatedly practicing an\nN-back task were to improve an individual's attention\ncontrol, it may be that this would not be sufficient to\nincrease the individual's intelligence score from pretest to\nposttest. In addition, it is important to remember that \"the\nvariance of the score gains can have a radically different\ncomposition than the variance of the scores themselves\"\n(Hayes, Petrov, & Sederberg, 2015, p. 9). Therefore, even\nthough variation in working memory capacity may\naccount for up to 50% of the variance in fluid intelligence\nat pretest, pretest-to-posttest increases in scores on an\nintelligence test do not necessarily reflect working mem-\nory increases, even after a working memory training\nintervention (see also Estrada, Ferrer, Abad, Rom\u00e1n, &\ngence gain scores in the context of training).\nAs is evident in our figures, even though the meta-\nanalytic effect sizes tended to be nonsignificant, there\nare certainly individual studies that demonstrate trans-\nfer effects. If working memory training is not respon-\nsible for these changes, then what is? For studies in\nwhich untreated controls are used (and even in studies\nwith treated controls where the subjects have different\nexpectancies than the training group about the inter-\nvention) motivation could partially explain differences,\ngiven previous research showing effects of motivation\non intelligence tests (Duckworth, Quinn, Lynam,\nLoeber, & Stouthmaer-Loeber, 2011). For intermediate-\nand near-transfer effects to verbal and visuospatial\nworking memory tasks, the development of task- or stim-\nulus-specific strategies is likely to explain a large amount\nof the pretest-to-posttest improvement (Dunning &\nHolmes, 2014; Gibson, Gondoli, Johnson, & Robison,\n2012), high- and low-ability individuals performing the\nsame complex task may strategically use their cognitive\nresources differently so that repeatedly practicing the\nsame task over sessions may not address the same cog-\nnitive processes to the same degree across all people.\nWhether the correct explanation is in terms of changes\nin motivation or strategy, it seems quite possible that\nimprovements on working memory tasks following\ntraining do not reflect genuine increases in working\nmemory capacity. It is also possible that some positive\neffects in this literature reflect the effects of publication\nbias (because positive effects are more likely to be\npublished than null results).\nConclusions\nOur meta-analysis included an impressive number of\nstudies from the burgeoning working memory training\nliterature, but the pattern of results is consistent with an\nearlier review (Melby-Lerv\u00e5g & Hulme, 2013). Although\nthere is evidence for near transfer to similar verbal and\nvisuospatial working memory tasks, all immediate and\ndelayed comparisons of nonverbal ability, verbal ability,\nreading comprehension, word decoding, and arithmetic\nwere not significantly different from zero when com-\npared against treated controls and eliminating outlier\nstudies. Using mediation analyses, we showed that the\nsize of working memory gains was not related to the size\nof gains on measures of \"far transfer.\" We conclude that\nthere is no evidence that working memory training yields\nimprovements in so-called far-transfer abilities.\nIt cannot be concluded from the current review that\nworking memory training could never produce improve-\nments on measures of intelligence or other real-world\ncognitive skills. However, the extensive efforts in this\nfield to date are discouraging. We believe that current\nevidence suggests that further attempts to increase work-\ning memory capacity by repetitively practicing simple\nmemory tasks on a computer are unlikely to lead to gener-\nalized cognitive benefits. We believe new training\napproaches, likely based on deeper theoretical analyses,\nwill need to be developed and tested if the field of working\nmemory training is to move forward. As we have discussed\nelsewhere (Redick, Shipstead, Wiemers, Melby-Lerv\u00e5g, &\nHulme, 2015), given the repeated finding that training\nproduces near transfer, training specific skills with inter-\nventions that are similar to the targeted outcome will\nlikely be a more fruitful approach than current working\nmemory training programs. In this vein, there is good\nevidence that difficulties with word reading and prob-\nlems with reading and language comprehension can be\nimproved by intensive, targeted educational interventions\n(see Hulme & Melby-Lerv\u00e5g, 2015). We believe that\nattempts to produce lasting improvements in attainment\nand intelligence may be better pursuing these more \"con-\nventional\" approaches (particularly approaches that\ninvolve more varied and stimulating educational inter-\nventions) than using repetitive computerized memory\ngames. Finally, we have highlighted a number of critical\nmethodological weaknesses in many studies in this area\nand have made some recommendations that we believe\nare important for guiding future studies in the field of\ncognitive training more generally.\n"
}