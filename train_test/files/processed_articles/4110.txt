{
    "abstract": "Abstract\nInformation from faces and voices combines to provide multimodal signals about a person. Faces and voices may offer redundant,\noverlapping (backup signals), or complementary information (multiple messages). This article reports two experiments which\ninvestigated the extent to which faces and voices deliver concordant information about dimensions of fitness and quality. In\nExperiment 1, participants rated faces and voices on scales for masculinity/femininity, age, health, height, and weight. The results\nshowed that people make similar judgments from faces and voices, with particularly strong correlations for masculinity/femininity,\nhealth, and height. If, as these results suggest, faces and voices constitute backup signals for various dimensions, it is hypothetically\npossible that people would be able to accurately match novel faces and voices for identity. However, previous investigations into\nnovel face\u00advoice matching offer contradictory results. In Experiment 2, participants saw a face and heard a voice and were required\nto decide whether the face and voice belonged to the same person. Matching accuracy was significantly above chance level, suggesting\nthat judgments made independently from faces and voices are sufficiently similar that people can match the two. Both sets of results\nwere analyzed using multilevel modeling and are interpreted as being consistent with the backup signal hypothesis.\n",
    "reduced_content": "Article\nConcordant Cues in Faces and Voices:\nTesting the Backup Signal Hypothesis\nHarriet M. J. Smith1, Andrew K. Dunn1, Thom Baguley1, and Paula C. Stacey1\n Keywords\nface, voice, static, dynamic, backup signal\nTogether, faces and voices convey multimodal signals. Such\nsignals are common in animals and occur when information\nabout an underlying trait is communicated by more than one\nmodality. As most research has focused on face and voice\nratings independently of each other (Wells, Baguley, Sergeant,\ntively little is known about multimodal signals in humans.\nMultimodal signals are either backup signals (Johnstone,\n1997), or multiple messages (M\u00f8ller & Pomiankowski,\n1993), and are likely to have adaptive value in terms of mate\nchoice. Backup signals are redundant in meaning: they offer\nsimilar information and elicit the same response, thereby help-\ning to reduce inaccurate trait assessments (M\u00f8ller & Pomian-\nkowski, 1993). It is therefore possible to distinguish between\nmultiple messages and backup signals by empirically testing\nthe effect of multimodal signals on a recipient (Partan & Mar-\nler, 1999). If a multimodal signal present in human faces and\nvoices is a backup signal for a certain dimension, ratings on this\ndimension should correlate, whereas uncorrelated ratings\nwould reflect the presence of multiple messages (Wells et al.,\nMultimodal Signals in Faces and Voices\nFaces and voices are salient social stimuli, offering a multitude\nof identity and affective information (Belin, Fecteau, &\nBedard, 2004). From an evolutionary perspective, faces and\nvoices provide valuable clues about fitness. For example, in\nterms of attractiveness they appear to constitute reliable and\nconcordant signals of genetic quality (e.g., Collins & Missing,\nZahavi & Zahavi, 1997; see also Puts, Jones, & DeBruine,\n2012 for a review), and a number of studies have found that\npeople who have faces that rate highly for attractiveness also\n1 Psychology Division, Nottingham Trent University, Nottingham, UK\nCorresponding Author:\nHarriet M. J. Smith, Psychology Division, Nottingham Trent University, Burton\nStreet, Nottingham, NG1 4BU, UK.\nEvolutionary Psychology\nReprints and permissions:\nsagepub.com/journalsPermissions.nav\nevp.sagepub.com\nCreative Commons CC-BY-NC: This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 3.0 License\n(http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (https://us.sagepub.com/en-us/nam/open-access-at-sage).\ntend to have voices that rate highly for attractiveness (e.g.,\nWith the exception of the attractiveness literature, previous\nresearch has rarely compared judgments made from faces and\nvoices, focusing instead on judgments informed by a single\nmodality (e.g., Neiman & Applegate, 1990; Penton-Voak &\n2012). However, there are a number of reasons as to why we\nmay expect concordance between face and voice ratings in\nterms of masculinity and femininity, health, age, height, and\nweight. Some of these reasons are detailed below.\nMasculinity/femininity. Levels of reproductive hormone levels are\nlikely to influence perceptions of both facial and vocal femi-\nninity and masculinity. For example, testosterone increases the\nsize and thickness of vocal folds (Beckford, Rood, & Schaid,\nwhich influences perceptions of masculinity (Pisanski et al.,\n2012). In addition, high levels of testosterone are associated\nwith characteristics of facial masculinity (Penton-Voak &\nand noses (Miller & Todd, 1998). In women, estrogen slows\ndown vocal fold development and is associated with higher\nvocal pitch (Abitbol, Abitbol, & Abitbol, 1999; O'Connor,\nRe, & Feinberg, 2011). Estrogen levels are also related to mar-\nkers of facial femininity (Thornhill & Grammer, 1999) such as\nlarger lips, smaller lower faces, and fat deposits on the upper\nHealth. We might also expect ratings of health made from faces\nand voices to be similar. Previous research suggests that cues\nrelating to higher levels of reproductive hormones are reliable\nindicators of fitness and quality (Folstad & Karter, 1992;\nindeed, some studies suggest that measures of sexual dimorph-\nism are linked to health ratings and actual health in both men\n(Gray, Berlin, McKinlay, & Longcope, 1991; Rhodes, Chan,\nAge. Faces and voices index information about biological age, a\ncue which is relevant to reproductive fitness in both males and\nfemales (Thornhill & Gangestad, 1999). Numerous visual mar-\nkers act as indicators of older age, such as decreased elasticity\nin the skin, wrinkles, discoloration, and reduced clarity in skin\ntone (Burt & Perrett, 1995). In terms of voices, older people\nspeak with a slower speech rate (Linville, 1996), and age-\nrelated hormonal changes affect pitch. For example, female\nvoice pitch lowers after the menopause, whereas older male\nvoices become higher pitched (Linville, 1996). People can esti-\nmate a speaker's age from their voice relatively accurately (to\nHeight and weight. Body size is a further indicator of quality\nHowever, although people tend to agree about height and\nweight judgments made from a voice (Collins, 2000), this does\nnot indicate that they are necessarily accurate (Bruckert,\nLi\u00b4\nenard, Lacroix, Kreutzer, & Leboucher, 2006; Collins,\ninaccuracy of height judgments made from voices, people\njudge height from faces with relative accuracy (Schneider,\nHecht, Stevanov, & Carbon, 2013), using cues such as facial\nelongation. People with longer faces are judged as being taller\n(Re et al., 2013). Judgments from faces are also accurate for\nweight estimates (Coetzee, Chen, Perrett, & Stephen, 2010).\nLass and Colt (1980) compared visual and auditory height and\nweight ratings. Results showed significant differences between\nweight ratings from female faces and voices, suggesting that\nfor some characteristics, faces and voices may not offer con-\ncordant information. Recent research has not addressed the\nextent of concordance between body size information offered\nby faces and voices. Although Krauss, Freyberg, and Morsella\n(2002) asked participants to rate the age, height, and weight of\nspeakers from faces and voices, they only tested whether the\nratings were accurate, rather than whether there was a relation-\nship between face and voice ratings.\nStatic and Dynamic Faces\nThe extent to which faces and voices offer concordant infor-\nmation might be affected by whether the face is static or\ndynamic. For example, Lander (2008) found that male face and\nvoice attractiveness was only related when faces were\ndynamic. Studies investigating facial attractiveness and human\nmate preferences most frequently use static facial stimuli\n(photos). However, there has been a recent move to use\ndynamic facial stimuli (videos) in order to improve ecological\nvalidity (Gangestad & Scheyd, 2005; Penton-Voak & Chang,\nthat facial stimulus type (static or dynamic) influences attrac-\ntiveness judgments, although the overall results are somewhat\nprevious studies and investigating methodological differences\nbetween them, Roberts, Saxton et al. (2009b) reported that\ncorrelations between ratings from static and dynamic facial\nstimuli were stronger when rated by the same participants,\nlikely because of carryover effects. As patterns of facial move-\nment vary according to sex (Morrison, Gralewski, Campbell, &\nPenton-Voak, 2007), it is conceivable that masculinity/femi-\nninity ratings will be more extreme when viewing dynamic\nfaces. In light of these findings, it is necessary to consider the\ninfluence of facial stimulus type when testing the concordance\nof face\u00advoice judgments.\nFace\u00advoice matching provides a further test of the extent to\nwhich faces and voices offer redundant information. However,\nit is not clear from the literature whether accurate face\u00advoice\nmatching using static facial stimuli is possible. While Kamachi,\nHill, Lander, and Vatikiotis-Bateson (2003) showed that parti-\ncipants could match dynamic muted faces saying different\n2 Evolutionary Psychology\nsentences to voices of the same identity, participants performed\nat chance level when the facial stimuli were static. Similar\nresults were reported by Lachs and Pisoni (2004). However,\nMavica and Barenholtz (2013) observed above chance level\naccuracy on trials featuring static faces, suggesting that above\nchance matching ability is not dependent on being able to\nencode visual articulatory patterns but rather on concordant\ninformation offered by faces and voices.\nAims\nThis article investigates the extent to which faces and voices\noffer concordant information, thereby providing a test of the\nbackup signal hypothesis (Johnstone, 1997). Using both static\nand dynamic facial stimuli, we tested cross-modal concordance\nby asking participants to make judgments from faces and\nvoices about perceived femininity/masculinity, health, age,\nheight, and weight. In a further test of face\u00advoice concordance,\nwe investigated whether it is possible to accurately match novel\nstatic or dynamic faces and voices of the same identity. If faces\nand voices offer similar information, and it is possible to match\nthe two, this would offer support for the backup signal\nhypothesis.\nExperiment 1\nExperiment 1 tested whether faces and voices offer concordant\ninformation about dimensions of fitness and quality, aiming to\nestablish whether people make similar judgments about a novel\nperson, regardless of whether they see their face or hear their\nvoice. We expect that as the previous literature suggests that\nboth faces and voices honestly signal quality, judgments made\nindependently from faces and voices should be similar. In light\nof the contradictory findings regarding judgments made from\nstatic and dynamic facial stimuli, the study also tested whether\nthe relationship between face and voice ratings differs accord-\ning to facial stimulus type (static vs. dynamic).\nMethod\nDesign\nThis experiment employed a mixed design. The between-\nsubject factor was facial stimulus type (static or dynamic), and\nthe within-subject factor was modality (face or voice)\nParticipants\nThe participants (n \u00bc 48) were recruited from the Nottingham\nTrent University Psychology Division's Research Participation\nScheme. There were 12 male and 36 female participants (age\ninformed consent and received a research credit in line with\ncourse requirements. The College Research Ethics Committee\nfor Business, Law and Social Sciences granted ethical approval\nfor the study (ref: 2013/37). All participants reported having\nnormal to corrected hearing and vision.\nApparatus and Materials\nStimulus faces and voices were taken from the Grid audiovisual\nsentence corpus (Cooke, Barker, Cunningham, & Shao, 2006),\na multi-talker corpus featuring head and shoulder videos of\nBritish adult speakers saying 1,000, six-word sentences each\nin an emotionally neutral manner recorded against a plain blue\nbackground. Each sentence follows the same six-word struc-\nture: (1) command, (2) color, (3) preposition, (4) letter, (5)\ndigit, and (6) adverb, for example, ``Place blue at J 9 now.''\nNone of the speakers in the corpus say the same sentence. A\ntotal of 18 speakers were selected from the corpus: 9 males and\n9 females. Speakers were matched for ethnicity (White Brit-\nish), accent (English), and age (18\u00ad30).\nThe stimuli were presented on an Acer Aspire laptop (screen\nAdvanced Audio) placed approximately 8.5 cm away from the\nedge of the desk at which participants sat. The experiment was\nsoftware package designed for running experiments in Python.\nThree videos (.mpegs) were selected at random from the GRID\ncorpus for each speaker, using an online research randomizer\n(Urbaniak & Plous, 2013). The study used static faces, dynamic\nfaces, and voices. One of the three videos was used to create\nstatic pictures of faces. Pictures were extracted using the snap-\nshot function on Windows Movie Maker (2012) and presented\nin .png format. The static picture for each talker was the first\nframe of the video. Another of the three video files was used to\nconstruct the dynamic stimuli. The file was muted using Win-\ndows Movie Maker and converted back into .mpeg format. All\nfacial stimuli measured 384 \u00c2 288 pixels and were presented in\ncolor for 2 s, with brightness settings at the maximum level.\nVoice recordings were also played for 2 s, from the third .mpeg\nfile, but the face was not visible at presentation. To reduce the\nbackground noise, participants listened to the recordings binau-\nrally through Apple earphones with a frequency range of 5\u00ad\n21,000 Hz. This exceeds the range of human hearing (Feinberg\net al., 2005). Voices were played at a comfortable listening\nvolume (30% of the maximum volume). Two versions of the\nexperiment were constructed: one using static faces and voices\nand the other using dynamic faces and voices. In both versions,\nall 18 faces and voices appeared.\nProcedure. Participants were randomly allocated to either the\nstatic face or the dynamic face version of the experiment. They\nread the information sheet, completed the consent form, and\nprovided demographic information. Testing took place in a\nquiet cubicle. Participants completed two counterbalanced\nblocks of testing. In one block participants viewed faces, in the\nother they heard voices. Participants were not told that the\nvoices and faces featured in the experiment belonged to the\nsame people. Each block consisted of a practice trial followed\nby 18 randomly ordered experimental trials. After each face or\nvoice, participants estimated the age of the stimulus person in\nyears and completed the 7-point Likert-style rating scales in the\nfollowing order: femininity/masculinity (1 \u00bc very feminine,\nSmith et al. 3\n7 \u00bc very masculine), health (1 \u00bc very unhealthy, 7 \u00bc very\nhealthy), height (1 \u00bc very short, 7 \u00bc very tall), and weight\n(1 \u00bc very underweight, 7 \u00bc very overweight).\nData Analysis and Multilevel Modeling\nData were analyzed using multilevel models, rather than per-\nforming conventional analyses on data averaged over either par-\nticipants or stimuli (see Wells et al., 2013). This avoids the\necological fallacy which arises when it is falsely assumed that\npatterns observed for participant means also hold for data at a\nlower level of analysis such as individual trials repeated within\nlevel modeling allows both participants and stimuli to be simul-\ntaneously treated as random effects, thereby maximizing\nWhen the random effects are fully crossed (i.e., when all parti-\ncipants experience all stimuli), conventional analyses (including\nseparate by-items or by-subjects analyses) can lead to massive\n2012). The most appropriate analysis therefore takes into\naccount both sources of variability. Unless the ignored source\nof variability is negligible, this is always more conservative than\nseparate by-stimuli or by-participants analyses.\nResults\nWe calculated the absolute difference between face and voice\nratings by comparing each rating participants had given to a face\nand voice belonging to the same person. Then we calculated the\nmean absolute difference (MAD) for each stimuli person on each\nrating scale (age, masculinity/femininity, health, height, and\nweight). Descriptive statistics (Table 1) indicate that typical rat-\nings for faces and voices fall within a similar range.\nOn all scales apart from age, face and voice ratings only\ndiffer on average by about 1 point (14%) on a 7-point rating\nscale, and MADs were similar across static and dynamic facial\nstimuli. The difference between face and voice ratings in terms\nof age appears larger than that of the other rating scales. How-\never, rather than being rated on a 7-point scale, age estimates\nwere given in years. This prevents a neat comparison between\nthe rating scales.\nThe results in Table 1 show that face and voice ratings tend to\nbe close together in terms of the range they fall into. A logical\nnext step is to quantify the extent to which voice and face ratings\ncovary in the same individual. For this purpose, a simple corre-\nlation coefficient between voice and face ratings would either\nignore the dependency within participants or rely only on aggre-\ngate data (mean ratings for each participant). We therefore used\nmultilevel models to account for both participant and stimuli\nvariation when correlating voice ratings with face ratings for\nestimated age and ratings for femininity/masculinity, health,\nheight, and weight. For each variable, we fitted an intercept-\nonly model with the rating as an outcome, using the lme4 pack-\nage in R (Bates, Maechler, Bolker, & Walker, 2014). A crucial\npart of each model was to estimate separate variance for face and\nvoice ratings as well as the correlation between face and voice\nratings across both stimuli and participants. The correlation\nbetween face and voice ratings within participants is, for present\npurposes, a nuisance term (merely indicating that participants\nwho give high ratings to voices also tend to give high ratings to\nfaces) and is not reported here. The correlations reported in\nTable 2 are those within stimuli and demonstrate that, for a given\nitem, voice and face ratings are positively correlated.\nTable 2 provides evidence that mean face and voice ratings\nfor the same target appear to be positively related for all rating\ntypes. Correlations between face and voice ratings on scales for\nmasculinity/femininity, health, and height were particularly\nhigh, regardless of whether the facial stimuli were static or\ndynamic. Correlations between mean face and voice ratings for\nage and weight were moderate when facial stimuli were sta-\ntic--with some suggestion that the correlations were dimin-\nished for dynamic stimuli. However, correlations did not vary\naccording to facial stimulus type in direction or by more than .3\non any scale. The difference between the static and dynamic\ncorrelations was tested by fitting models with separate variance\nterms for each stimulus type. Comparing a model which\nincludes separate variance and covariance terms for static and\ndynamic stimuli with one that does not did not improve the\nmodel fit for any of the ratings (p > .14). This complements the\nresults shown in Table 1, suggesting that the extent to which\nfaces and voices offer similar information is not greatly influ-\nenced by whether the facial stimuli is static or dynamic.\nDiscussion\nExperiment 1 showed that observers glean concordant infor-\nmation about different dimensions of quality from faces and\nTable 1. MAD and 95% Confidence Intervals for the MAD Between\nFace and Voice Ratings by Stimulus-Type Condition.\nRating scale\nStatic Facial Stimuli Dynamic Facial Stimuli\nNote. MAD \u00bc mean absolute difference.\nTable 2. Within-Stimulus Correlations Between Face and Voice\nRatings.\nCondition\nCorrelation coefficient\nAge Masc/fem Health Height Weight\n4 Evolutionary Psychology\nvoices, particularly in terms of masculinity and femininity,\nhealth, and height. On each dimension, the relatedness of face\nand voice ratings is not affected by facial stimulus type, show-\ning that the signals tested here are stable across static and\ndynamic faces. These results support the hypothesis that on\nvarious dimensions of quality, faces and voices constitute\nbackup signals.\nExperiment 2\nExperiment 2 tested whether faces and voices offer sufficiently\nconcordant information that people can match novel faces to\nvoices. Previous studies have addressed this question, with\nconflicting results. Krauss et al. (2002) showed that people are\nrelatively accurate at inferring physical information from a\nvoice. After only hearing a voice excerpt, participants selected\nthe speaker's full-length photograph from one of two possible\noptions with above chance accuracy. Mavica and Barenholtz\n(2013) tested whether people could use information from a\nvoice to distinguish between two static images of different\nfaces. Accuracy was significantly above chance level, despite\ncontradictory results presented in previous studies (Kamachi et\nmatching of faces and voices depends on the ability to encode\ndynamic properties of speaking (muted) faces (Mavica & Bare-\nPrevious face\u00advoice matching studies (Kamachi et al., 2003;\ntwo-alternative forced choice paradigm (2AFC), which unlike a\nsame\u00addifferent paradigm does not model whether people are\nalso able to correctly reject a match when a face and voice are\nfrom different people. The 2AFC tasks therefore give no infor-\nmation about possible response biases. Experiment 2 uses a\nsame\u00addifferent paradigm to give a clearer picture of face\u00advoice\nmatching ability.\nExperiment 2 addresses three main questions. First, whether it\nis possible to accurately match novel faces and voices of the same\nage (20\u00ad30), sex, and ethnicity (White British). Second, whether\nmatching accuracy is affected by facial stimulus type (static or\ndynamic). Third, in line with cross-modal matching procedures\nwhether people are more accurate at face\u00advoice matching when\nvisual information (a face) is presented first, compared to when\nauditory information (a voice) is presented first. If faces and\nvoices primarily constitute backup signals, people should be able\nto match novel faces and voices above chance level.\nMethod\nThe methods for Experiment 2 were the same as for Experi-\nment 1, with exceptions explained in the following subsections.\nDesign\nThis experiment employed a 2 \u00c2 2 \u00c2 2 mixed factorial design.\nThe between-subject factor was facial stimulus type (static or\ndynamic). The within-subject factors were identity (same or\ndifferent) and order (face first or voice first). The dependent\nvariable was accuracy.\nParticipants\nThere were 40 male and 40 female adult participants (n \u00bc 80)\nMaterials\nFour different versions of the experiment were created so that\nmatching and not-matching pairs of faces and voices could be\nconstructed using different stimulus people. Stimuli were ran-\ndomly selected to be used for either one of the eight same\nidentity or eight different identity trials. None of the faces or\nvoices appeared more than once in each version. On different\nidentity trials, the face and voice were matched for age, gender,\nand ethnicity. The stimuli that remained were used for the\npractice trials. Each version was repeated for static and\ndynamic conditions. In total, there were eight versions.\nProcedure\nParticipants were randomly allocated to one of the eight ver-\nsions of the experiment. In the dynamic facial stimulus condi-\ntion, participants were also correctly informed that the face in\nthe muted video and the voice in the recording were not saying\nthe same thing. This was to prevent them using speech reading\nto match the face and voice (Kamachi et al., 2003).\nParticipants completed two counterbalanced experimental\nblocks, each consisting of a practice trial followed by eight\nrandomly ordered experimental trials. In one block, partici-\npants saw the face first, and in the other they heard the voice\nfirst. None of the stimuli appeared more than once in each\nversion of the experiment. In each trial, there was a 1-s gap\nbetween presentation of the face and voice stimuli. At test,\nparticipants pressed ``1'' if they thought the face and voice\nwere ``matching'' (same identity), and ``0'' if they thought it\nwas ``not matching'' (different identity).\nResults\nPerformance accuracy was analyzed using multilevel logistic\nregression with the lme4 version 1.06 package in R (Bates\net al., 2014). Four nested models with accuracy (0 or 1) as the\ndependent variable were compared (and all models were fitted\nusing restricted maximum likelihood). The first model included\na single intercept (and was later used to obtain confidence\nintervals for the overall accuracy). The second model also\nincluded the main effects of each factor (identity, order, and\nstimulus type). The third model added all two-way interactions\nand the final model added the three-way interaction. Setting up\nthe model in this way allows us to test for individual effects in a\nmanner similar to that of a traditional analysis of variance.\nHowever, as F-tests-derived multilevel models are not, in gen-\neral, accurate, we report the more robust profile likelihood ratio\nSmith et al. 5\ntests provided by lme4. These were obtained by dropping each\neffect in turn from the appropriate model (e.g., testing the\nthree-way interaction by dropping it from the model including\nall effects, and testing the two-way interactions by dropping\neach effect in turn from the two-way model).\nTable 3 shows the profile likelihood chi-square statistic (G2)\nand p-value associated with dropping each effect. Table 3 also\nreports the coefficients and standard errors (on a log odds scale)\nfor each effect in the full three-way interaction model. In the\nthree-way model, the estimate of SD of the face random effect\nthe participant effect was less than 0.0001. A similar pattern\nheld for the null model. Thus, although individual differences\nwere negligible in this instance, a conventional by-participants\nanalysis that did not incorporate both voice and face variation\ncould be extremely misleading.\nOnly the main effect of identity and the two-way interaction\nof identity and order were statistically significant. To aid inter-\npretation of these effects, we obtained means and confidence\nintervals for the percentage accuracy of the eight conditions in\nthe factorial design. These confidence intervals were obtained\nthrough simulations of the posterior distributions of the cell\nmeans using arm package version 1.6 in R (Gelman & Su,\nvals are shown in Figure 1.\nFrom Figure 1 it is clear that overall matching performance\nCI [51.9, 66.9]. Static face\u00advoice matching was above chance,\nreveals the main effect of identity, with performance for same\ntrials consistently higher than for different trials (and the for-\nmer but not the latter consistently above chance). It also reveals\nthe basis of the identity by order interaction. The results from\nthe face first trials are shown in Panel A. The results from the\nvoice first trials are shown in Panel B. Although same identity\ntrials showed better performance than different trials for both\nface first and voice first trials, this advantage is greater in the\nface first conditions. Given that performance on the face first\ndifferent trials is on average worse than chance (and signifi-\ncantly so for the static stimuli), this pattern suggests the oper-\nation of a response bias, such that participants exhibited a bias\nto accept faces and voices as belonging to the same identity\nwhen they saw the face before hearing the voice.\nDiscussion\nIn Experiment 2, we observed that both dynamic faces and\nvoices, and static faces and voices, can be matched for identity\nabove chance level. These results are consistent with the\nhypotheses informed by the results of Experiment 1, which\nshow that faces and voices offer a high level of concordant\ninformation on various dimensions. Face\u00advoice matching per-\nformance does not differ according to facial stimulus type.\nTherefore, accuracy does not appear to depend on encoding\nvisual information about speaking style but rather on redundant\nsignals available in voices and static faces.\nGeneral Discussion\nThe results of Experiment 1 are consistent with the hypothesis\nthat faces and voices offer redundant signals for various dimen-\nsions of quality. Mean face and voice ratings for the same target\nwere positively related for all rating types. Correlations\nbetween face and voice ratings on scales for masculinity/fem-\nininity, health, and height were particularly strong, regardless\nof whether the facial stimuli were static or dynamic. The results\nof Experiment 2 show that the information signaled by faces\nand voices is so similar that people can match novel faces and\nvoices of the same sex, ethnicity, and age-group at a level\nsignificantly above chance. Taken together, results suggest that\nfaces and voices constitute backup signals, reinforcing the\nsame information about quality (Johnstone, 1997) rather than\nTable 3. Parameter Estimates (b) and Profile Likelihood Tests for the\n2 \u00c2 2 \u00c2 2 Factorial Analysis of Accuracy in Experiment 2.\nSource df b SE G2 p\nIdentity \u00c2 Order \u00c2 Facial\nStimulus Type\nFigure 1. Face\u00advoice matching accuracy on face first (Panel A) and\nvoice first (Panel B) trials. Error bars show 95% CI for the condition\nmeans. CI \u00bc confidence interval.\n6 Evolutionary Psychology\ncomplementary but different information (M\u00f8ller & Pomian-\nFace and Voice Ratings\nWith the exception of the attractiveness literature, previous\nresearch has rarely compared judgments made from faces and\nvoices, focusing instead on judgments informed by a single\nmodality (e.g., Penton-Voak & Chen 2004; Perrett et al.,\nso on) or comparing face and voice ratings to actual measure-\nments of physical characteristics (e.g., Krauss et al., 2002)\nrather than to each other. The results of Experiment 1 show\nthat not only do face and voice ratings fall within a small range\nbut independent ratings of an individual's face and voice are\npositively correlated. These results complement other studies,\nshowing that faces and voices offer related information about\nfitness and mate value (Collins & Missing, 2003; Feinberg,\nThe strongest correlations between face and voice ratings\noccurred on scales for masculinity/femininity, health, and\nheight. Despite the previous literature suggesting that unimodal\nvoice ratings of body size are less accurate than unimodal face\nExperiment 1 showed that regardless of accuracy, the MAD\nbetween body size judgments made from faces and voices was\nsmall. However, correlations were strong for height but only\nweak-moderate for weight. This corresponds with Lass and\nColt (1980) who found significant differences between weight\nratings for female faces and voices.\nFace and Voice Matching\nOverall, face\u00advoice matching accuracy in Experiment 2 was\nsignificantly above chance. This result is consistent with pre-\nvious findings (Krauss et al., 2002; Mavica & Barenholtz,\n2013) and shows that people can use redundant information\nto match faces and voices of the same identity. Furthermore,\nthe use of multilevel modeling allows us to generalize these\nfindings beyond the sample of faces and voices used, thereby\novercoming a common limitation of previous studies.\nAlthough overall matching accuracy is at 59.7%, there is\nstill a substantial proportion of unexplained variance which\ncould be due to the existence of discordant rather than concor-\ndant face\u00advoice information. Beyond the characteristics tested\nin Experiment 1, faces and voices also convey a multitude of\nother information, including personality characteristics and\nof which might be complementary. Nevertheless, the results\nfrom Experiment 2 suggest that on balance, faces and voices\nprovide concordant information because overall performance is\nsignificantly above chance level. These results are consistent\nwith the results presented in Experiment 1.\nOn different identity trials, participants performed at chance\nlevel (voice first trials), or below chance level (face first trials),\nand were significantly less accurate than on same identity trials.\nThis indicates that participants were better at detecting a correct\nmatch than rejecting an incorrect one. In line with the argument\npresented above, based purely on the findings from Experiment\n1, we might have expected that accurately rejecting mismatches\nwould be possible because the ratings were so closely related. It\nseems that participants are using other information to inform\ntheir matching decisions on different identity trials. On the other\nhand, the pattern of results across same\u00addifferent trials might be\npartially explained by the existence of a response bias.\nWhile previous face\u00advoice matching studies using 2AFC pro-\ncedures have found no difference between face first and voice\nour results using a same\u00addifferent task suggest people exhibit a\nbias to respond that a face and voice belong to the same identity,\nparticularly when the face is presented before the voice. A per-\nformance asymmetry, according to stimuli order, is consistent\nwith the previous literature. For instance, studies have consis-\ntently found asymmetries between faces and voices in terms of\nrates of recognition accuracy, which have been attributed to\ndifferential link strength in the two perception pathways (e.g.,\nnage, Hugill, & Lewis, 2012). Therefore, there is no reason to\nassume that face first and voice first matching performance\nshould be identical. However, based on the finding that familiar\nfaces prime familiar voices better than familiar voices prime\nfamiliar faces (Stevenage et al., 2012), we might have expected\nthe asymmetry to operate the other way around. Nevertheless, it\nis feasible that voices give more information about faces than\nfaces do about voices, and aside from conveying semantic infor-\nmation about the spoken message, the other important role of\nvoices is to allow people to infer socially relevant visual infor-\nmation about the speaker, such as information about masculi-\nnity/femininity, body size, health, and age. This idea is in\nkeeping with the finding that showing participants mismatched\ncelebrity face\u00advoice pairs disrupts voice recognition to a greater\nextent than it disrupts face recognition (Stevenage, Neil, & Ham-\nlin, 2014). During social interactions, it is common to hear a\nvoice while not looking in the direction of the speaker. Being\nable to accept or reject a face match quickly may aid social\ncommunication by facilitating attention shifts.\nStatic and Dynamic Faces\nInformed by contradictory findings relating to the effect of static\nand dynamic facial stimuli on ratings of attractiveness (e.g.,\nand face\u00advoice matching ability (Kamachi et al., 2003; Lachs &\nfacial stimulus type affected the extent of face\u00advoice concor-\ndance. In both experiments, performance was unaffected by\nwhether the facial stimuli were dynamic or static. This suggests\nthat information on these dimensions is stable across dynamic\nand static faces. Novel face\u00advoice matching ability is not due to\nencoding visual articulatory patterns (Mavica & Barenholtz,\n2013) but to the availability of redundant information.\nSmith et al. 7\nStimulus Sample Size\nThe findings of the multilevel models we report emphasize the\nimportance of stimulus sample size in estimating effects. These\nmodels provide the tools to generalize over both participants\nand stimuli, but obtaining large samples of stimuli is challen-\nging. The corpus (Cooke et al., 2006) we used only contained 18\nstimulus individuals matched for age, gender, and ethnicity.\nThis reduced the set of stimuli available for study but also\nreduced extraneous variability. In addition, all of the people\nin this stimulus set were from similar educational backgrounds\n(Cooke et al., 2006), and none of them exhibited strong regional\naccents. As there is a high level of interstimulus variability in\nboth faces (Valentine, Lewis, & Hills, 2015) and voices (Ste-\nvenage & Neil, 2014), we would encourage future face\u00advoice\nmatching studies to aim for larger samples of stimuli, having\ndemonstrated that it is variation in faces and voices that is the\nlimiting factor on statistical power in experiments such as these\n(as face and voice variation is consistently higher than partici-\npant variation). However, many published studies have used\nsamples of stimuli far smaller than 18 when investigating per-\nson perception (see G. L. Wells & Windshitl, 1999), as have\nother face\u00advoice matching studies (e.g., Lachs & Pisoni, 2004).\nCrucially, only by accounting for variability in stimuli is it\nreasonable to generalize from stimuli as well as participants.\nEven in studies using large sample of stimuli, generalizability is\nlimited by the common practice of aggregating over stimuli\nthe adequate sample size of stimuli or participants in experi-\nmental designs such as those reported here is a question of\nstatistical power (e.g., see Westfall, Kenny, & Judd, 2014).\nConclusion\nFaces and voices of the same identity offer redundant signals\nabout a number of dimensions associated with quality and\nfitness. Information about masculinity/femininity, height, and\nhealth is particularly similar across faces and voices. We have\nshown that the level of redundancy between faces and voices is\nsufficient that it is possible to accurately match them for iden-\ntity. In summary, the results of Experiments 1 and 2 are more\nconsistent with the backup signal hypothesis (Johnstone, 1997)\nthan the multiple messages hypothesis (M\u00f8ller & Pomian-\nkowski, 1993). As multimodal signals for various indicators\nof quality, faces, and voices offer concordant rather than com-\nplementary information.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect to\nthe research, authorship, and/or publication of this article.\nFunding\nThe author(s) disclosed receipt of the following financial support for\nthe research, authorship, and/or publication of this article: The first\nauthor was supported by a Ph.D. studentship from the Division of\nPsychology, Nottingham Trent University.\nReferences\nAbitbol, J., Abitbol, P., & Abitbol, B. (1999). Sex hormones and the\nBaguley, T. (2012). Calculating and graphing within-subject confi-\ndence intervals for ANOVA. Behavior Research Methods, 44,\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2014). lme4:\nLinear mixed-effects models using Eigen and S4. R package\nR-project.org/package\u00bclme4\nBeckford, N. S., Rood, S. R., & Schaid, D. (1985). Androgen stimula-\ntion and laryngeal development. Annals of Otology, Rhinology,\nBelin, P., Fecteau, S., & Bedard, C. (2004). Thinking the voice: Neural\ncorrelates of voice perception. Trends in Cognitive Sciences, 8,\nBraun, A. (1996). Age estimation by different listener groups. Inter-\nnational Journal of Speech Language and the Law, 3, 65\u00ad73. doi:\nBruckert, L., Li\u00b4\nenard, J. S., Lacroix, A., Kreutzer, M., & Leboucher,\nG. (2006). Women use voice parameters to assess men's charac-\nteristics. Proceedings of the Royal Society, B: Biological Sciences,\nBurt, D. M., & Perrett, D. I. (1995). Perception of age in adult Cau-\ncasian male faces: Computer graphic manipulation of shape and\ncolour information. Proceedings of the Royal Society, B: Biologi-\nClark, H. H. (1973). The language-as-fixed-effect fallacy: A critique\nof language statistics in psychological research. Journal of Verbal\nCoetzee, V., Chen, J., Perrett, D. I., & Stephen, I. D. (2010). Decipher-\ning faces: Quantifiable visual cues to weight. Perception, 39,\nCollins, S. A. (2000). Men's voices and women's choices. Animal\nCollins, S. A., & Missing, C. (2003). Vocal and visual attractiveness\nCooke, M., Barker, J., Cunningham, S., & Shao, X. (2006). An audio-\nvisual corpus for speech perception and automatic speech recog-\nnition. The Journal of the Acoustical Society of America, 120,\nDamjanovic, L., & Hanley, J. R. (2007). Recalling episodic and\nsemantic information about famous faces and voices. Memory &\nEllison, P. T. (1999). Reproductive ecology and reproductive cancers.\nIn C. Pater-Brick & C. Worthman (Eds.), Hormones, health, and\nbehavior: A socio-ecological and lifespan perspective (pp.\nFant, G. (1960). The acoustic theory of speech production. The Hague,\nthe Netherlands: Mouton.\nFeinberg, D. R. (2008). Are human faces and voices ornaments sig-\nnaling common underlying cues to mate value? Evolutionary\n8 Evolutionary Psychology\nFeinberg, D. R., Jones, B. C., DeBruine, L. M., Moore, F. R., Law\nSmith, M. J., Cornwell, R. E., . . . Perrett, D. I. (2005). The voice\nand face of woman: One ornament that signals quality? Evolution\nFolstad, I., & Karter, A. J. (1992). Parasites, bright males, and the\nimmunocompetence handicap. American Naturalist, 139,\nFraccaro, P. J., Feinberg, D. R., DeBruine, L. M., Little, A. C., Wat-\nkins, C. D., & Jones, B. C. (2010). Correlated male preferences for\nfemininity in female faces and voices. Evolutionary Psychology, 8,\nGangestad, S. W., & Scheyd, G. J. (2005). The evolution of human\nphysical attractiveness. Annual Review of Anthropology, 34,\nGelman, A. E., & Su, Y. S. (2013). arm: Data analysis using regression\nand multilevel/hierarchical models. R package version 1.6-05.\nRetrieved September 12, 2014, from http://CRAN.R-project.org/\npackage\u00bcarm\nGray, A., Berlin, J. A., McKinlay, J. B., & Longcope, C. (1991). An\nexamination of research design effects on the association of\ntestosterone and male aging: Results of a meta-analysis. Journal\nHanley, J. R., & Turner, J. M. (2000). Why are familiar-only\nexperiences more frequent for voices than for faces? The Quarterly\nJohnstone, R. A. (1997). The evolution of animal signals. In J. R.\nKrebs & N. B. Davies (Eds.), Behavioural ecology: An evolution-\nJudd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stimuli as a\nrandom factor in social psychology: A new and comprehensive\nsolution to a pervasive but largely ignored problem. Journal of\nKamachi, M., Hill, H., Lander, K., & Vatikiotis-Bateson, E. (2003).\nPutting the face to the voice: Matching identity across modality.\nKrauss, R. M., Freyberg, R., & Morsella, E. (2002). Inferring speak-\ners' physical attributes from their voices. Journal of Experimental\nLachs, L., & Pisoni, D. B. (2004). Crossmodal source identification in\nLander, K. (2008). Relating visual and vocal attractiveness for moving\nLass, N. J., & Colt, E. G. (1980). A comparative study of the effect of\nvisual and auditory cues on speaker height and weight identifica-\nLaw Smith, M. J., Perrett, D. I., Jones, B. C., Cornwell, R. E., Moore,\nF. R., Feinberg, D. R., . . . Hillier, S. G. (2006). Facial appearance\nis a cue to oestrogen levels in women. Proceedings of the Royal\nLinville, S. E. (1996). The sound of senescence. Journal of Voice, 10,\nMavica, L. W., & Barenholtz, E. (2013). Matching voice and face\nidentity from static images. Journal of Experimental Psychology:\nMiller, G. F., & Todd, P. M. (1998). Mate choice turns cognitive.\nM\u00f8ller, A. P., & Pomiankowski, A. (1993). Why have birds got mul-\ntiple sexual ornaments? Behavioral Ecology and Sociobiology, 32,\nMorrison, E. R., Gralewski, L., Campbell, N., & Penton-Voak, I. S.\n(2007). Facial movement varies by sex and is related to attractive-\nNeiman, G. S., & Applegate, J. A. (1990). Accuracy of listener judg-\nments of perceived age relative to chronological age in adults.\nO'Connor, J. J., Re, D. E., & Feinberg, D. R. (2011). Voice pitch\ninfluences perceptions of sexual infidelity. Evolutionary Psychol-\nOguchi, T., & Kikuchi, H. (1997). Voice and interpersonal attraction.\nPartan, S., & Marler, P. (1999). Communication goes multimodal.\nPeirce, J. W. (2009). Generating stimuli for neuroscience using Psy-\nPenton-Voak, I., & Chang, H. (2008). Attractiveness judgements of\nindividuals vary across emotional expression and movement con-\nPenton-Voak, I. S., & Chen, J. Y. (2004). High salivary testosterone is\nlinked to masculine male facial appearance in humans. Evolution\nPerrett, D. I., Lee, K. J., Penton-Voak, I., Rowland, D., Yoshikawa, S.,\nBurt, D. M., . . . Akamatsu, S. (1998). Effects of sexual dimorphism\nPisanski, K., Mishra, S., & Rendall, D. (2012). The evolved psychol-\nogy of voice: Evaluating interrelationships in listeners' assess-\nments of the size, masculinity, and attractiveness of unseen\nPtacek, P. H., & Sander, E. K. (1966). Age recognition from voice.\nPuts, D. A., Jones, B. C., & DeBruine, L. M. (2012). Sexual selection\nSmith et al. 9\nRe, D. E., Hunter, D. W., Coetzee, V., Tiddeman, B. P., Xiao, D.,\nDeBruine, L. M., . . . Perrett, D. I. (2013). Looking like a leader\u00ad\nfacial shape predicts perceived height and leadership ability. PloS\nRhodes, G., Chan, J., Zebrowitz, L. A., & Simmons, L. W. (2003).\nDoes sexual dimorphism in human faces signal health? Proceed-\nings of the Royal Society of London B: Biological Sciences, 270,\nRoberts, S. C., Little, A. C., Lyndon, A., Roberts, J., Havlicek, J., &\nWright, R. L. (2009a). Manipulation of body odour alters men's\nself-confidence and judgements of their visual attractiveness by\nwomen. International Journal of Cosmetic Science, 31, 47\u00ad54.\nRoberts, S. C., Saxton, T. K., Murray, A. K., Burriss, R. P., Rowland,\nH. M., & Little, A. C. (2009b). Static and dynamic facial images\nRobinson, W. S. (1950). Ecological correlations and the behavior of\nRubenstein, A. J. (2005). Variation in perceived attractiveness: Dif-\nferences between dynamic and static faces. Psychological Science,\nSaxton, T. K., Caryl, P. G., & Roberts, C. S. (2006). Vocal and facial\nattractiveness judgments of children, adolescents and adults: The\nSchneider, T. M., Hecht, H., Stevanov, J., & Carbon, C. C. (2013).\nCross-ethnic assessment of body weight and height on the basis of\nSmith, H. M. J., & Baguley, T. (2014). Unfamiliar voice identifica-\ntion: Effect of post-event information on accuracy and voice rat-\nStevenage, S. V., Hugill, A. R., & Lewis, H. G. (2012). Integrating\nvoice recognition into models of person perception. Journal of\nStevenage, S. V., & Neil, G. J. (2014). Hearing faces and seeing\nvoices: The integration and interaction of face and voice process-\nStevenage, S. V., Neil, G. J., & Hamlin, I. (2014). When the face fits:\nRecognition of celebrities from matching and mismatching faces\nThornhill, R., & Gangestad, S. W. (1999). Facial attractiveness.\nThornhill, R., & Gangestad, S. W. (2006). Facial sexual dimorphism,\ndevelopmental stability, and susceptibility to disease in men and\nThornhill, R., & Grammer, K. (1999). The body and face of woman:\nOne ornament that signals quality? Evolution and Human Beha-\nUrbaniak, G. C., & Plous, S. (2013). Research randomizer (Version 4.0)\n[Computer software]. Retrieved from http://www.randomizer.org/\nValentine, T., Lewis, M. B., & Hills, P. J. (2015). Face-space: A\nunifying concept in face recognition research. The Quarterly Jour-\nvan Dommelen, W. A., & Moxness, B. H. (1995). Acoustic parameters\nin speaker height and weight identification: Sex-specific beha-\nWells, G. L., & Windschitl, P. D. (1999). Stimulus sampling and social\npsychological experimentation. Personality and Social Psychology\nWells, T., Baguley, T. S., Sergeant, M. J. T., & Dunn, A. K. (2013).\nPerceptions of human attractiveness comprising face and voice\nWells, T., Dunn, A. K., Sergeant, M. J. T., & Davies, M. N. O. (2009).\nMultiple signals in human mate selection: A review and framework\nfor integrating facial and vocal signals. Journal of Evolutionary\nWestfall, J., Kenny, D. A., & Judd, C. M. (2014). Statistical power and\noptimal design in experiments in which samples of participants\nrespond to samples of stimuli. Journal of Experimental Psychol-\nWheatley, J. R., Apicella, C. A., Burriss, R. P., Ca\n\u00b4rdenas, R. A.,\nBailey, D. H., Welling, L. L., & Puts, D. A. (2014). Women's faces\nand voices are cues to reproductive potential in industrial and\nZahavi, A., & Zahavi, A. (1997). The handicap principle. New York,\nNY: Oxford University Press.\n10 Evolutionary Psychology"
}