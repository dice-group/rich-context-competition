{
    "abstract": "Abstract\nMassive Open Online Courses appear to have high attrition rates, involve students in peer-\nassessment with patriotic bias and promote education for already educated people. This paper\nsuggests a formative assessment model which takes into consideration these issues. Specifically,\nthis paper focuses on the assessment of open-format questions in Massive Open Online Courses.\nIt describes the current assessment methods in Massive Open Online Courses and it argues that\nself-assessment should be the only way of formative assessment for the essays of xMOOCs and\nreplace the peer-assessment.\n",
    "reduced_content": "Article\nSelf-assessment in Massive\nOpen Online Courses\nOurania Maria Ventista\nDurham University, UK\n Keywords\nMassive Open Online Courses, self-assessment, formative assessment, attrition, fairness, motivation\nIntroduction: Assessment in Massive Open Online Courses\nMassive Open Online Courses (MOOCs) have become increasingly popular over the last\ndecade (Jordan, 2015). They are called `massive', in relation to the number of registered\nstudents, and `open', because the course content is free of charge. They can be accessed\nonline and they are courses since they have a specific structure with a definite material to be\nThis paper focuses on the assessment in MOOCs. Assessment is an important topic to be\nexamined because it can be a powerful learning tool even in the case of MOOCs. Boud and\nFalchikov (2007: 3) argued that `assessment, rather than teaching, has a major influence on\nstudents' learning'. They supported that assessment has an impact on what learners do and\nhow they do it, whilst it can help them understand what they can or cannot do. Additionally,\nassessments can have positive or negative `washback', which means that they can have\nimpact on students' learning and motivation. Therefore, the relationship between assess-\nment and learning should be examined (Baird et al., 2017), since assessments can both\nmeasure and support learning. Furthermore, the assessment can be considered a learning\nCorresponding author:\nOurania Maria Ventista, School of Education, Durham University, UK.\nEmail: o.m.ventista@durham.ac.uk\nE-Learning and Digital Media\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\njournals.sagepub.com/home/ldm\nevent. Testing what is learnt is more likely to lead to the retention of knowledge in memory\ncompared to restudying the same material (Halamish and Bjork, 2011). Therefore, this\npaper focuses on the assessment as an important part of the learning process in MOOCs.\nSince there are thousands of MOOCs, it is not possible to examine each course individ-\nually. This paper scrutinises the assessment for xMOOCs, which are MOOCs `offered in\na traditional university model' (Siemens, 2013: 7). Two popular examples of platforms\noffering xMOOCs are Coursera and EdX. There are xMOOCs which can be evaluated\nwith multiple-choice items or computational activities. These activities can be automatically\ngraded by a computer in a reliable way. However, for some MOOCs open-ended questions\nor essays are used as a preferable assessment method since it is debatable whether multiple-\nchoice items evaluate particular high-order skills as effectively as open-response questions\nUndoubtedly, it is infeasible to involve tutors in assessing thousands of open-response\nassignments (Koller, 2012), since MOOCs are massive and hundreds to thousands of stu-\ndents can take each of these courses (Siemens, 2013). For this reason, EdX uses a system\ncalled Automated Essay Scoring (Balfour, 2013). This system has been criticised as being\nrestricted to grading based on superficial factors, lacking the ability to point creativity,\nhumour or sarcasm (Zhang, 2013) and not being able to assess the texts in the same way\nas human assessors (Balfour, 2013). Therefore, it is questionable to what extent computer-\nbased assessment could evaluate the assignments of the students. Hence, it has been found\nthat there is a need for human assessors.\nSince it is infeasible to have the assignments of thousand students corrected by the uni-\nversity tutors, both Coursera (2016) and EdX (2016) use peer-assessment and self-\nassessment when a course demands a written assignment to evaluate the large number of\nassignments. Specifically, Coursera uses a calibrated peer review assessment system. In the\ncalibrated peer review, the students initially mark some assignments which have been pre-\nviously marked by the instructor. The system checks the agreement between the instructor\nand the student grade and it calibrates the grades given by the students. According to the\nextent of the agreement, the students are attributed a Reviewer Competency Index. Then,\nwhen the students grade the assignments of their peers, this grade can be considered more or\nless crucial for the final grade of their peer according to the Reviewer Competency Index\nDuring peer-assessment, students grade the assignments of four or five of their peers\nusing a rubric and they also have their assignment graded by the same number of peers.\nSome courses offer the option of self-assessment. If students self-assess their work from the\ncourse, they do it by using the same rubric after having completed the peer-assessment.\nNevertheless, in EdX for the open-response assignments when students are both peer- and\nself-evaluated, they are awarded only the peer grade and the self-assessment is not taken into\naccount (EdX, 2016). This implies that self-assessment is underestimated to some extent.\nEven though the feedback remains formative, self-assessment could contribute equally.\nAssessments in Coursera and EdX can be either summative or formative. Assessment for\nLearning or formative assessment intends to support learning, while summative assessment\nis associated with grading, certifications and accountability (Gardner et al., 2010). When\nusers are registered in a course in Coursera or EdX, they can choose between two routes.\nThey can either choose the direction which will lead them to a certification or to simply audit\nthe course. Thus, the summative assessment could be defined as the type of assessment\nwhich is focused on certification. In this case, learners are required to pay a fee to gain a\ncertificate on both platforms. The option of auditing the course for free is also provided to\nthe students. In this case, students can still complete the assessments to examine their level of\nknowledge, understand what they have to improve and promote their learning. This latter\ncase is considered as formative assessment and it is the type of assessment on which this\npaper focuses.\nSpecifically, this paper focuses on formative assessment, because the second route corre-\nsponds to the needs of the majority of learners registered in MOOCs. Shrader et al. 2016\nasked participants, who were registered in different courses, the reasons why they were\ntaking these MOOCs. The study found that only a small percentage of participants\n(3.3%) were registered in MOOCs in order to gain a course certificate. The majority of\nthe participants reported the broadening of knowledge (65.6%), and the curiosity and gen-\neral interest for the topic (35.6%) as prime motivation to take a MOOC. Similarly, Salmon\net al. (2017) found that the motivation of MOOC students is mostly intrinsic. Furthermore,\nBarak, Watted and Haick (2016) reported high ratings in intrinsic motivation of the MOOC\nlearners in their study. Therefore, the majority of the students who participate in a MOOC\nappear not to pursue a certification, but they are merely interested in the learning of the\nMOOC content. Hence, it becomes apparent that formative assessment, which supports the\nstudents to evaluate their own learning, could be considered more meaningful for the major-\nity of MOOC learners compared to the summative assessment, which is focused on the\naccreditation.\nFinally, despite the high number of registered students, MOOCs appear to have high\nattrition rates and low engagement. Even though the success of the courses should not\nnecessarily entail completion (Pursel et al., 2016), these are indicators that enhancement\nshould take place concerning several aspects of MOOCs. This paper tries to address these\nissues by examining the best possible way of assessment for these courses. In the following\nsections, the reasons why peer-assessment in MOOCs should be abandoned and how self-\nassessment could possibly provide a solution for the improvement of the MOOCs experience\nis discussed.\nPeer-assessment and interaction\nPeer-assessment does not appear to be implemented in the most ideal conditions in the case\nof MOOCs. A meta-analysis with peer-assessment articles published in the last 15 years\n(Li et al., 2016) found that peer-assessment appears to be more correlated to the teacher\nassessment when it is paper- instead of computer-based. Moreover, the same meta-analysis\nshowed that this correlation is higher when peer-assessment is voluntary. In the case of\nCoursera and EdX, the peer-assessment is computer-based and compulsory and its comple-\ntion is pre-requisite for the students to have their own marks returned. Last but not least,\npeer-assessment is more accurate when the students participate in the creation of the rating\ncriteria (Li et al., 2016). This is definitely not the case in the MOOCs when the scoring rubric\nis given to the students.\nPeer-assessment in MOOCs can be argued to be beneficial for the students when they\nreflect on and evaluate the work of their peers (Comer and White, 2016). However, peer\nfeedback is heavily criticised in MOOCs. Particularly, peer feedback in Coursera has been\nheavily criticised due to the fact that it is anonymous and it does not incorporate a check for\nplagiarism (McEwen, 2013). Further, peer feedback in Coursera has been accused of being\ninconsistent with a lack of feedback on the peer-assessment itself (Watters, 2012). Thus, the\nlearners do not always appear satisfied by the peer feedback received. Concurrently, almost\nhalf of the participants in a large-scale survey supported that they put a lot of effort to\nevaluate their peers, but they feel that the peers do not comprehend their work (Kulkarni\net al., 2013). Meanwhile, the peer-assessment is questioned concerning its trustworthiness by\nAs a result, the evidence for the use of peer-feedback in MOOC is not encouraging for the\ncontinuation of implementation of this assessment. Lee and Rofe (2016) found that the peer\ninteraction in MOOCs does not play a role in the final grade of the students. Additionally,\nresearch with MOOC participants revealed negative correlation between the students' perfor-\nmance and the extent to which they prioritise interaction with their peers (Phan et al., 2016).\nHowever, if peer-assessment is abandoned, it is likely that social interaction in MOOCs would\nbe reduced. This might be problematic since the social interaction as a crucial factor for the\nlearning process has been emphasised by educators, such as in the zone of proximal develop-\nment of Vygotsky (1978). Nevertheless, the students who participate in MOOCs can still be\noffered the choice of peer interaction. This does not have to occur during the process of peer-\nassessment. The students could still interact through peer forums.\nAfter demonstrating that the research findings do not fully support the implementation of\npeer-assessment and the benefits of peer interaction, the following section will demonstrate\nhow self-assessment could potentially address crucial issues in MOOCs which peer-\nassessment fails to solve and address.\nThe current issues in MOOCs\nAs it has been discussed, the learners sometimes appear unsatisfied with the implementation\nof peer-assessment in MOOCs. There is a need to correspond better to the needs of the\nlearners, and platforms providing MOOCs have still crucial matters to consider. Specifically,\nthe issues that MOOCs face are high attrition rates, patriotic bias emerging in peer-\nassessment and the lack of providing accessibility to all the learners. This paper argues\nthat self-assessment as an only assessment method which could correspond more effectively\nto each of the issues appearing in MOOCs and to the learning needs of the MOOC students\ncompared to the current assessment method.\nHigh attrition rate\nOne of the main problems in MOOCs is the high dropout rate. Jordan (2015) examined 221\nMOOCs and she found that the completion rate varies from 0.7 to 52.1%. Specifically, the\ncourses using peer grading or a combination of peer grading and autograding were com-\npleted by less than 10% of the students who were enrolled. On the other hand, courses with\nautograding were usually completed by more than 20% of students. Therefore, she con-\ncluded that courses with peer grading have high attrition rates. The use of autograding could\nbe a solution, but not all the courses can be evaluated with autograding and multiple-choice\nitems. Thus, the investigation of an assessment method to reduce the attrition rates for\nopen-ended questions is crucial.\nWhat is more, a recent survey identified (Nawrot and Doucet, 2014) that among\n12 sample reasons more than 50% of the participants chose time as a reason for withdrawal.\nSpecifically, by `time' the authors meant `time organisation, real life responsibilities and too\nmuch time consuming course' (Nawrot and Doucet, 2014). The participants argued that\nthey lack time and hence they decided to quit. Subsequently, in order to reduce the attrition\nrates, MOOCs should implement less time-consuming assessment methods.\nTime as a factor could also explain why Jordan (2015) found out that courses implement-\ning peer-assessment are having the highest attrition rates. Peer-assessment can be extremely\ntime-consuming as it demands from the students to grade at least four assignments of other\nstudents. Meek et al. (2016) used a survey in a MOOC to investigate the appropriateness of\npeer review. Their findings are in line with the argument that peer review is a time-\nconsuming method. They found that women with full-time jobs were less likely to be\ninvolved in peer review process, while students retired by their jobs tend to complete\nthe peer review the most. As a result, the authors also concluded that the time constraint\ncan be a dispiriting factor for students to be involved in peer review process. Likewise, the\ncombined peer- and self-assessment can be time-consuming. On the other hand, it takes less\ntime for the students to grade only their own assignments. Consequently, self-assessment\ncan correspond better to the needs of the MOOC learners as they appear to regard time as\nan important factor for their participation in the course.\nRating bias in peer-assessment\nBias is an important factor to be considered when the quality of an assessment framework is\njudged. A test is regarded as fair and unbiased when it does not attribute differences in\ndifferent groups, when these differences do not exist, or do not attribute larger or smaller\ndifferences in the test than the differences which exist in reality (Hunter and Schmidt, 1976).\nKoretz (2008) argued that it is not the test itself which can be (un)biased, but a test inference.\nAssessment in MOOCs should not promote biased inferences. MOOCs are open to people\nfrom different social, economic and cultural backgrounds. Hence, MOOCs have immense\ndiversity concerning their students. However, Kulkarni et al. (2013: 15) identified `patriotic\ngrading' which means that students tend to grade higher the students who come from the\nsame country. Thus, peer-assessment in MOOCs appears to be biased. On the other hand,\nself-assessment can provide a solution to the rating bias found by MOOC research since it\nwill leave no space for the patriotic bias. In research for self-assessment, students were found\nto agree that self-assessment is `fairer' because it enables them to include complementary\nperformance dimensions, such as effort (Ross et al., 1998). It is likely that self-assessment\ncan reduce bias deriving from the diverse background of the participants in MOOCs.\nDuring self-assessment the students do not have the opportunity to promote bias regarding\ntheir peers, since they are only evaluating their work and they are aware of their own\nbackground, effort and learning goals.\nEducation not accessible to all\nThe most important element to be considered is probably who can access MOOCs. Even\nthough MOOCs are promoted as a way of promoting `education for all' and making the\neducation accessible to everyone, research evidence has shown that the courses are usually\ntaken by educated and employed people coming from developed countries (Christensen\nparticipants who hold a master or a PhD degree are twice more likely to complete the course\nThis means that research, which examines the demographic characteristics of the MOOC\nparticipants, clearly demonstrates that the students who take MOOCs come from a partic-\nular educational and economic background. This might be due to the fact that some\nMOOCs are designed only for educated participants. There are examples of research\nwhich examined MOOCs specifically designed to be taken by students who have already\nEven though this is a discouraging finding for the accessibility of MOOCs to all learners,\nit reveals that there are particular students attending these courses. As a consequence,\nspecific adjustments in the assessment model could correspond better to the needs of\nthese students. The type of students who are mostly involved in MOOCs have high levels\nof self-regulation and the assessment model should acknowledge this element. Particularly,\nresearch disclosed that learners working as professionals in a field relevant with the MOOC\ncontent and students working towards a higher education degree have higher self-regulation\nlevels (Hood et al., 2015; Kizilcec et al., 2017). Hence, formal education and prior knowl-\nedge are associated with higher self-regulation and performance in the course.\nIn addition, about 10% of participants in a recent survey (Shrader et al., 2016) reported\nrefreshing and reviewing existing knowledge as the reason they registered in a MOOC. Thus,\nthere is a group of MOOC students who take a course to expand upon existing knowledge.\nIf the students have a basic previous knowledge related to the content of the course or they\nare familiar to the task, their self-assessment is more accurate (Boud and Falchikov, 1989;\nFitzgerald et al., 2003). A recent study about online assessment verified that students who\nperform poorly overestimated their abilities, whilst the accuracy in self-assessment improved\nwhen the students increased their skills (Dom\nConcerning the participants of MOOCs, self-assessment can fit the model of their self-\nregulatory learning (Zimmerman et al., 1996) and therefore it can be the most appropriate\nassessment method for these self-regulated students, particularly when they are interested in\ncourse content and knowledge instead of obtaining a qualification. Nevertheless, the fact\nthat students do not always perceive their self and their performance in an objective way\ncannot be disregarded. Bias may be introduced even in self-assessment based on the way the\nstudents perceive themselves. It is usually questionable to what extent students tend to be\nlenient and overrate themselves. Students sometimes regard themselves and their perfor-\nmance above average, underestimate the time needed to complete a task and have little\ninsight of errors of omission (Dunning et al., 2004). Thus, self-image bias is likely to be\npresent in the self-assessment procedure.\nSelf-image bias should not be deemed more problematic in a formative self-assessment\nthan in peer-assessment context. It has been found that a large proportion of the students\nundertake an online course to satisfy their curiosity (Hew and Cheung, 2014).These students\nwant to learn about the course topic, have an intrinsic motivation for the course, and as long\nas the assessment remains formative, they do not have a reason to cheat. Furthermore, it has\nbeen found that there are several `auditing' students who watch the lectures that they do not\ncomplete the assessments (Kizilcec et al., 2013). This entails that these students are interested\nin the content of the course, but not interested in gaining a certificate or the assessment of a\nMOOC. Self-assessment will benefit their own learning and as they have intrinsic motivation\nto complete the assessment and evaluate their learning. They are also expected to be con-\ncerned about the real outcomes of their learning and try to assess accurately.\nThe fact that the students are intrinsically motivated does not exclude the possibility of\nthem being incapable of recognising their own omissions. Nevertheless, peers should not be\njudged more capable assessors, because peer assessors can also fail to identify the mistakes\nin the assignments. For instance, Suen (2014) highlighted the likelihood of having peers\ngrading in favour of some assignments based on their own misconceptions.\nIt is also possible that those students who are willing to keep a false self-image about their\nperformance can still disregard the feedback of their peers. Several students who attend\nMOOCs support that their peers did not understand their work (Kulkarni et al., 2013),\nchallenge the ability and the accuracy of their peers to assess their work (Floratos et al.,\n2015) and express the opinion that their peers are not qualified to provide feedback (Meek\net al., 2016). Consequently, students can merely retain their self-image and be biased against\nthe peer assessors. As an answer to this issue, it can be argued that it is more plausible for the\ncourse instructors to develop the self-regulation of students and try to limit the students' self-\nimage bias rather than persuading them that their peers are competent evaluators. Students\nwith weaker metacognitive skills and self-regulation can be supported with scaffolding\nLimitations\nThe arguments of this conceptual paper are based on available empirical research evidence.\nTherefore, in this section the limitations of the available published research evidence, on\nwhich the arguments of this paper are based, will be reported. The two main limitations of\nthe research on MOOCs are the small response rate and the lack of representative sample for\nall the types of MOOC learners. This means that the research of MOOCs usually has small\nresponse rate compared to the overall number of students registered in the MOOCs (Lee and\nRofe, 2016) and the research sample includes mainly educated participants (Loizzo and\nPursel et al. (2016) recognised that their survey sample significantly differs from the\ngeneral population of students registered to MOOCs and could be involved in the study.\nMOOC research projects, which are mostly surveys, manage to recruit as research partic-\nipants these students who are already more engaged and involved in the course. For\ninstance, Barba et al. (2016) examined the students' motivation in participating in an\neight-week MOOC. Their sample consisted of students who remained engaged in the\ncourse for the last three weeks. Considering the high attrition rates in the MOOCs, it is\napparent that the majority of the students had already dropped out by that point. It is\napparent that the motivation of the students who were still engaged during the last three\nweeks of a course cannot represent or explain the motivation of all the students initially\nregistered in the course.\nTo summarise, since the response rate is low and the cases participating in MOOC survey\ncannot be representative for the whole population of the MOOC learners, the research\nfindings cannot be generalised. Thus, the external validity of the currently available research\nstudies is not well established. As a result, the argumentation of this conceptual paper which\nis based on the current research on MOOCs might have a partial insight into the topic.\nFuture research\nThis paper argued in favour of the self-assessment as the most effective and appropriate\nmethod of formative assessment when open-ended questions and essays in MOOCs are\nconcerned. This conceptual paper can be followed up by the collection and analysis of\nempirical evidence. As it has been discussed, research has already been conducted to exam-\nine the perceptions of the learners about peer-assessment. Similarly, in future research,\ninterviews can be conducted to investigate whether the learners in MOOCs perceive that\nself-assessment support their learning more than peer-assessment and whether they feel they\ncan track their learning via their participation in self-assessment. Moreover, an evaluation\nof MOOCs which have adopted different approaches to open-ended assessments could\nprovide with empirical evidence about potential causal relationships between the assessment\nmethods used and learning occurred. Finally, since this paper argued that self-assessment\ncan reduce the attrition in MOOCs, a study similar to the research conducted by Jordan\n(2015) could take place. Specifically, a study can be conducted in order to identify whether\ncourses which use only self-assessment have higher completion rates compared to MOOCs\nwhich use auto-grading, peer-assessment or a combination of peer- and self-assessment.\nConclusions\nThis paper discussed the existing assessment system implemented for open-ended questions\nin MOOCs. The emphasis was put on formative assessment, because the majority of the\nMOOC students report to be more focused on learning and the course content instead of\ngetting an accreditation when they are registered in a MOOC. Moreover, MOOC learners\nare usually educated, employed people from developed countries taking the courses with\nintrinsic motivation and tending to drop out when they lack the time to complete them. The\nreasons why peer-assessment is an inappropriate assessment method for these learners were\nexplained. Instead, self-assessment was argued to be the most suitable assessment method to\ncorrespond to the needs of these self-regulated learners and a potential solution to the high\nattrition rates and the patriotic grading bias during peer-assessment.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or\npublication of this article.\nFunding\nThe author(s) received no financial support for the research, authorship, and/or publication of\nthis article.\nReferences\nBaird J, Andrich D, Hopfenbeck TN, et al. (2017) Assessment and learning: Fields apart?. Assessment\nBalfour SP (2013) Assessing writing in MOOCs: Automated essay scoring and calibrated peer review.\nBarak M, Watted A and Haick H (2016) Motivation to learn in massive open online courses:\nExamining aspects of language and social engagement. Computers & Education 94: 49\u00ad60.\nBarba PD, Kennedy GE and Ainley MD (2016) The role of students' motivation and participation in\npredicting performance in a MOOC Motivation and participation in MOOCs. Journal of Computer\nBennett RE, Rock DA and Wang M (1991) Equivalence of free-response and multiple-choice items.\nBoud D and Falchikov N (1989) Quantitative studies of student self-assessment in higher education:\nBoud D and Falchikov N (2007) Introduction: Assessment for the longer term. In: Boud D and\nFalchikov N (eds) Rethinking Assessment for Higher Education: Learning for the Longer Term.\nChristensen G, Steinmetz A, Alcorn B, et al. (2013) The MOOC phenomenon: Who takes massive\nopen online courses and why? Available at: https://papers.ssrn.com/sol3/papers.cfm?abstract_\nComer DK and White EM (2016) Adventuring into MOOC writing assessment: Challenges, results,\nDom\ninguez C, Jaime A, Sa\n\u00b4 nchez A, et al. (2016) A comparative analysis of the consistency and\ndifference among online self-, peer, external-and instructor-assessments: The competitive effect.\nDunning D, Health C and Suls JM (2004) Flawed self-assessment: Implications for health, education,\nand the workplace. Psychological Science in the Public Interest 5(3): 69\u00ad106.\nEdX (2016) Building and running an EdX course. Available at: http://edx.readthedocs.io/projects/edx-\npartner-course-staff/en/latest/exercises_tools/open_response_assessments/\nOpenResponseAssessments.html#pa-scoring (accessed 29 April 2016).\nEichhorn S and Matkin GW (2016) Massive open online courses, big data, and education research.\nFitzgerald JT, White CB and Gruppen LD (2003) A longitudinal study of self-assessment accuracy.\nFloratos N, Guasch T and Espasa A (2015) Recommendation on formative assessment and feedback\nGardner J, Harlen W, Hayward L, et al. (2010) Developing Teacher Assessment. Maidenhead:\nMcGraw Hill.\nHalamish V and Bjork RA (2011) When does testing enhance retention? A distribution-based inter-\npretation of retrieval as a memory modifier. Journal of Experimental Psychology: Learning,\nHancock GR (1994) Cognitive complexity and the comparability of multiple-choice and constructed-\nHew KF and Cheung WS (2014). Students' and instructors' use of massive open online courses\n(MOOCs): Motivations and challenges. Educational Research Review 12: 45\u00ad58.\nHood N, Littlejohn A and Milligan C (2015) Context counts: How learners' contexts influence learning\nHunter JE and Schmidt FL (1976) Critical analysis of the statistical and ethical implications of various\nJordan K (2015) Massive open online course completion rates revisited: Assessment, length and attri-\nKizilcec RF, Pe\n\u00b4 rez-Sanagust\nin M and Maldonado JJ (2017) Self-regulated learning strategies predict\nlearner behavior and goal attainment in Massive Open Online Courses. Computers & Education\nKizilcec RF, Piech C and Schneider E (2013) Deconstructing disengagement: Analyzing learner sub-\npopulations in massive open online courses. In: Third international conference on learning analytics\nKoller D (2012) What we `re learning from online education. TEDGlobal. Available at: www.ted.com/\ntalks/daphne_koller_what_we_re_learning_from_online_education (accessed 1 June 2015).\nKoretz D (2008) Measuring up: What Educational Testing Really Tell Us. Cambridge: Harvard\nUniversity Press.\nKulkarni C, Wei KP, Le H, et al. (2013) Peer and self assessment in massive online classes. ACM\nLee Y and Rofe JS (2016) Paragogy and flipped assessment: Experience of designing and running a\nMOOC on research methods. Open Learning: The Journal of Open, Distance and e-Learning\nLi H, Xiong Y, Zang X, et al. (2016) Peer assessment in the digital age: A meta-analysis comparing\nLoizzo J and Ertmer PA (2016) MOOCocracy: The learning culture of massive open online courses.\nMcEwen K (2013) Getting to know Coursera: Peer assessments. Vanderbilt University. Available at:\nMeek SE, Blakemore L and Marks L (2016) Is peer review an appropriate form of assessment in a\nMOOC? Student participation and performance in formative peer review. Assessment & Evaluation\nNawrot I and Doucet A (2014) Building engagement for MOOC students \u00ad Introducing support for\ntime management on online learning platforms. In: 23rd International World Wide Web conference,\nPhan T, McNeil SG and Robin BR (2016) Students' patterns of engagement and course performance\nin a Massive Open Online Course. Computers & Education 95: 36\u00ad44.\nPursel BK, Zhang L, Jablokow KW, et al. (2016) Understanding MOOC students: Motivations and\nbehaviours indicative of MOOC completion. Journal of Computer Assisted Learning 32: 202\u00ad217.\nRieber LP (2017) Participation patterns in a massive open online course (MOOC) about statistics.\nRoss JA, Rolheiser C and Hogaboam-Gray A (1998) Skills training versus action research in-service:\nImpact on student attitudes to self-evaluation. Teaching and Teacher Education 14(5): 463\u00ad477.\nSalmon G, Pechenkina E, Chase AM, et al. (2017) Designing massive open online courses to take\naccount of participant motivations and expectations. British Journal of Educational Technology\nShrader S, Wu M, Owens D, et al. (2016) Massive open online courses (MOOCs): Participant activity,\nSiemens G (2013) Massive open online courses: Innovation in education? In: McGreal R, Kinuthia W\nand Marshall S (eds) Open Educational Resources: Innovation, Research and Practice. Vancouver:\nCommonwealth of Learning and Athabasca University, pp. 5\u00ad16.\nSuen HK (2014) Peer assessment for massive open online courses (MOOCs). The International Review\nof Research in Open and Distributed Learning 15(3). Available at: http://www.irrodl.org/index.php/\nVygotsky L (1978) Mind in Society: The Development of Higher Psychological Processes. Cambridge:\nHarvard University Press.\nWatters A (2012) The problems with Coursera's peer assessments. Available at: http://hackeducation.\nZhang B (2013) Contrasting Automated and Human Scoring of Essays. R & D Connections, 21.\nPrinceton, NJ: Educational Testing and Services.\nZimmerman BJ, Bonner S and Kovach R (1996) Developing Self-Regulated Learners: Beyond\nAchievement to Self-Efficacy. Washington: American Psychological Association.\nAuthor Biography\nOurania Maria Ventista is a doctoral researcher and a teaching assistant in the School of\nEducation at Durham University. She has published articles on educational assessment,\ncontrolled trials in education, school effectiveness and Philosophy for Children."
}