{
    "abstract": "Abstract\nMalaysia is fast becoming a major attraction for candidates from all over the world to pursue their higher education. Currently\nstudents (local and international) who pursue postgraduate (hereafter, PG) education in Malaysia use the Test of English as\na Foreign Language (TOEFL) or International English Language Testing System (IELTS) scores as indicators of their English\nability. These are tests from the United States and the United Kingdom, respectively, tailor-made for university education\nin those countries. Recent literature in testing and evaluation describes the need for more localized tests, developed for\nthe \"local\" context of a particular country. Thus, the need for a test that could be utilized and customized to the needs\nof the students studying in Malaysia is foreseeable. This is in line with the concept of test localization. It stipulates that for a\ntest to be valid, its design and development must take into consideration the population, context, and the domain in which\nthe test is used. A project was undertaken where a new English test named Graduate Admission Test of English (GATE) was\ndeveloped for PG admission into universities in Malaysia. This article describes the process of developing a new test that\nmeasures English language competency of PG students who intend to pursue their studies in Malaysia. It includes the use of\na test specification/blueprint that contains validity elements adopted from a test validation framework developed by Weir.\nThe article emphasizes the rigor of developing such a test, which includes aspects of test development, operation, analysis,\nand validation.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access page (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nArticle\nIntroduction\nThis research was undertaken with the hope of improving the\npostgraduate (PG) intakes at local universities in Malaysia in\nterms of candidates' language ability, which would in turn\nenhance skills and competencies required to succeed in the\nrespective programs. At present, candidates who intend to\ntake up a PG course in the country are required to obtain a\nscore from tests such as the Test of English as a Foreign\nLanguage (TOEFL) and International English Language\nTesting System (IELTS), which shows their English lan-\nguage ability at entry point into a university. In fact, since\n2010, even local candidates were subject to the same English\nlanguage requirement. However, there is evidence to show a\nmismatch between a test score and a candidate's actual writ-\ning and speaking performance in the PG classrooms, albeit\nreports from foundation courses offered at various universi-\nties for foreign candidates who do not make the mark, and\nothers who are exempted from such courses.\nIn their article on processing strategies in reading, Ponniah\nand Tay (1992) stated that in Malaysian tertiary institutions, it is\nessential for students to have a near-native English competency\nin reading to be able to comprehend/read academic texts in vari-\nous disciplines. This will subsequently produce graduates who\nare competent and able to demonstrate the required skills at the\nworkplace as well as for other endeavors.\nOne of the main reasons for the development of a new test\nfor PGs is the influx of foreign nationals into Malaysia, mostly\nfor higher education. Their lack of English language ability to\npursue PG education and the requirement to secure an IELTS or\nTOEFL score to enter universities in Malaysia are contributing\nfactors. In 2008, the Council of Deans of Post Graduate Studies,\nScience University of Malaysia (USM; minutes of meeting\nintroduce an English language test developed in Malaysia for\nthese international candidates, just as the Malaysian University\nEnglish Test (MUET) is a measure of Malaysian students' pro-\nficiency at the undergraduate level. As reported in a research\narticle by Juliana Othman and Nordin (2013), it has been argued\nthat to cope with the linguistic demands of the courses, a certain\nlevel of proficiency in the English language is required. In most\n1MARA University of Technology, Shah Alam, Selangor, Malaysia\nCorresponding Author:\nSaidatul Akmar Zainal Abidin, MARA University of Technology, 40450\nEmail: akmarza@msn.com\nToward an English Proficiency Test for\nPostgraduates in Malaysia\nSaidatul Akmar Zainal Abidin1 and Asiah Jamil1\n Keywords\npostgraduate education, test localization, high-stakes tests, GATE, CEFR, English language competence.\n2 SAGE Open\nif not all applications for universities, jobs, and even govern-\nment agencies, the English language is obligatory; indeed, even\nMalaysian graduates have at times demonstrated below-average\nlevels of English in university tasks and on tests such as the\nAlong these lines, if candidates need to take IELTS or\nTOEFL to study in the United Kingdom and the United\nStates respectively, it is only appropriate that they take\nGraduate Admission Test of English (GATE) to study in\nMalaysia. The requirement of taking a foreign test as opposed\nto a local test to study locally in itself warrants justification.\nThis is so because IELTS and TOEFL were designed for\nthose who want to pursue their studies in the English-\nspeaking countries. Based on the justification provided ear-\nlier on the need for a localized test, no one test or two tests in\nthis context fit all. If all elements and principles of testing\ncan be incorporated in the development of a localized test,\nthat should provide a sound basis for its use locally.\nThe proposal for developing a localized test is inevitable\nas it can only be beneficial for those who intend to pursue\nhigher education in Malaysia, and where their English lan-\nguage abilities require filtering. This is to prevent the mis-\nmatch between test scores and true performance of candidates\nin the classroom, and cases where even Malaysians take a\nforeign test to study in Malaysia.\nThus, the significance of this project will be evident in\nterms of saving cost for many candidates and the universi-\nties, encouraging candidates to enhance their English lan-\nguage ability, the potential for universities to venture into the\ntesting field, and more importantly, achieving standardiza-\ntion of PG intake and quality among Malaysian universities.\nTo develop a test that would be acceptable, it has to have\nthe qualities of validity, reliability, test usefulness, and test\nfairness. To produce a test that has value, the test develop-\nment process needs to be multifaceted and rigorous, and\nincorporate many parties. Indeed, experience in the current\nproject has shown that these test qualities are not achievable\nwhen considerable time, resources, administrative support,\nand budget are not made available.\nObjective\nThe main objective of the new English test for postgraduates\nis to ascertain their ability in the English language to be able\nto cope academically in universities in Malaysia. Because\nthe literature shows that Malaysia is currently a favorite des-\ntination for higher education, and the number of students\nentering the country for this purpose has increased drasti-\ncally, it is essential that the language level be ensured before\na candidate can continue to study at PG level. In addition,\nthis is more than ever vital seeing that the mode of instruc-\ntion in most universities (public and private) in Malaysia is\nEnglish. With this objective in mind, the thrust of this article\nis a description of the process involved in developing the\nproposed test of English for PG purpose.\nLiterature Review\nThis section provides a brief overview of test processes and\nthe comparison of some major tests in the literature.\nTest development begins with the concern that a test can\nbe shown to produce scores that are an accurate reflection of\na candidate's ability in a particular area, such as reading for\nspecific ideas, writing a research proposal, breadth of vocab-\nulary knowledge, or speaking in a class presentation. We\nneed to understand the trait (underlying construct) we wish\nto measure and the method (instruments we need to develop)\nwe would use to provide us with the information about these\nconstructs (Weir, 2005). In addition, a key part of this pro-\ncess is test validation in which evidence is gathered to cor-\nroborate the inferences we make regarding these traits from\ntest scores obtained. Finally, testing also has an ethical\ndimension, which many in the testing field have referred to\nas consequential validity. This is the impact that the test has\non individuals, institutions, and society as a whole, that is,\nthe stakeholders (IELTS Handbook, 2007).\nIn the case of the proposed new English test for PG stu-\ndents, an early analysis of the literature found similarities\nbetween the MUET and IELTS. These similarities are pre-\nsented in the comparison table (refer to Table 1).\nBecause there are close similarities between the MUET and\nthe IELTS in terms of exam type, language components, and\nmarking on a band scale, a new test should be fashioned after\nthese tests, targeted at PG level. However, major changes will\ntake place in terms of topics, levels of difficulty of tasks, cover-\nage, and more importantly, taking into consideration the local\ncontext and test domain. Like the IELTS and TOEFL, which are\ndeveloped by major exam boards, the MUET by the Malaysian\nExamination Council (MPM), the proposed new test would be\ndeveloped by a group of academic professionals, and supported\nby the university for its operations. The introduction of such a\ntest would eventually involve more parties (such as the\nMalaysian Examination Council and the Ministry of Higher\nEducation) and could be implemented on a wider scale through-\nout the country and beyond.\nThe models below outline the test development process\nby some major exam boards (Cambridge English for\nSpeakers of Other Languages [ESOL] Test Development\nThese models are used as references for test development in\nthe present study (Figures 1 and 2).\nIt is evident that in both models, the processes include test\ndesign and development, test operations, data analysis, and\nan evaluation or validation, although these are labeled differ-\nently in each model.\nIn the context of the present study, the development process\nfollows a similar outline as presented in Figure 3. In addition,\nthere is an attempt at placing candidates according to levels in\nthe Common European Framework of Reference (CEFR; refer\nto the appendix). The CEFR is a framework used widely in lan-\nguage programs, curriculum, schools, and other language\nZainal Abidin and Jamil 3\npractitioners in Europe (see Cambridge ESOL Main Suite\nvarious uses of the CEFR). It describes in a comprehensive way\nwhat language learners have to learn to use a language for com-\nmunication, and what knowledge and skills they have to develop\nto be able to perform effectively. Little (2006) discussed the\nimpact that the CEFR has had in European schools and higher\ninstitutions where it was used as a major reference for develop-\ning courses and tests in their respective programs.\nThus, the proposed new test in the study follows a model\nthat has two major considerations:\n i. the test development process and\nii. the test levels, based on existing models of language\nlearning and testing.\nThis model ensures test validity in terms of its develop-\nment and reliability in terms of its scoring outcome.\nThe GATE (Graduate Admission Test\nof English)\nBefore a test can be developed, pertinent questions need to\nbe put forward such as the following:\n\u00b7\n\u00b7 Why must we develop a new test when there are such\nEnglish tests in the market?\n\u00b7\n\u00b7 What would the test contain, and what would it look\nlike?\n\u00b7\n\u00b7 Who would be developing the test?\n\u00b7\n\u00b7 How will the test be graded?\n\u00b7\n\u00b7 What about issues of cost, operations and administra-\ntion, and security of the test?\nA possible solution to address these questions is the use of a\nframework for test development, which can ensure a systematic\nand effective process, and essentially a valid test. The GATE\nwill use a framework proposed by Weir (2005) labeled Socio-\nCognitive Framework for Developing and Validating tests of\nreading, writing, listening, and speaking. This framework will\nbe the backbone for the test from the initial development pro-\ncess, scoring, and to its final evaluation process. We need clear\nTable 1. Comparison Between IELTS and MUET.\nObjective Assess a candidate's ability to study\nat pre-degree or postgraduate levels\nin an English-speaking country\nAssess a candidate's English language ability\nfor admission into university at pre-degree\nType of exam Criterion-referenced Criterion-referenced\nComponents Reading, writing, listening, speaking Reading, writing, listening, speaking\nPlace offered and recognized International Malaysia and two universities in Singapore\nAdministration British Council and IDP Australia Malaysian Examination Council\nMode Paper and pencil Paper and pencil\n Online \nNote. IELTS = International English Language Testing System; MUET = Malaysian University English Test; IDP = Internaltional Development Programme.\nNeeds analysis of test context\nPlanning (relang needs to test usefulness, i.e. test\nvalidity)\nMonitoring\nTest Design\nTest item construcon\nOperaons\nEvaluaon and revision\nFigure 1. Test development cycle.\nSource. Adapted from Cambridge ESOL test development cycle 2002.\nNote. ESOL = English for Speakers of Other Languages.\nQuestion Development Selection for pre-test 2\nReview groups Review groups\n1ST Pre-test 2nd Pre-test\nMarking Marking VALIDATION\nData capture Data capture\nAnalysis of item functioning Analysis of item functioning\nCOMPLETED\nFigure 2. Test development process.\nSource. Adapted from Pearson test of English document, 2008.\n4 SAGE Open\nobjectives, expressed in a set of terms of reference; what is the\ntest used for, for which target group of test takers, and what\nstructure will the test take in terms of its format, content, and\nlevels. In addition, the new test would need to have qualities of\nsustainability if it is to be useful not just for its immediate pur-\npose, but also for a longer shelf life when it can be extended to\na more global level and application. For this purpose, it is pro-\nposed that the test would eventually be administered via com-\nputers, or a computer-based test (CBT).\nTo generate a test of this significance and scale, it has to be\ngiven a name, or branding, that reflects its purpose and impact.\nThe proposed test, GATE, is a test that measures a candidate's\nlanguage ability and competency in that it measures beyond\nEnglish language proficiency, it ascertains the candidate's abil-\nity to demonstrate competence required to partake and succeed\nin PG education. As indicated by Moon and Siew (2004), the\nlevel of proficiency in the English language is a contributing\nfactor for a better academic performance. This includes the abil-\nity \" . . . to participate in scholarly discussions, to be able to\ndefend their arguments, explain their opinions, develop hypoth-\neses, all of the things they need to write and defend their dis-\nSimilarly, in the context of the present study, the skills and\nthe level of proficiency, which are required at PG level are\nmeasured to ascertain if graduates have achived the profi-\nciency needed in postgraduate education. Thus, it is labeled\na test of language competence administered at the point of a\ncandidate's entry into a PG university program. The GATE\nconsists of three sections (refer to Table 2).\nThe test consists of three sections. They are\ni. Reading\nii. Grammar, and\niii.Writing.\nIn each of the sections, different skills are tested. As indi-\ncated in the table, the skills tested in the Reading section are\nmaking inferences, determining author's purpose/point of\nview, determining author's argument, and evaluating the\nauthor's argument. In this section, students are required to\nrespond to 18 items. The items are tested in the multiple-\nchoice question format.\nThe second section, which is Grammar, is divided into\ntwo types: error correction and transformation items. The\nskills tested are identifying errors, making corrections, and\nparaphrasing sentences. Students are required to respond to\n23 items in this section.\nThe last section is Writing, and the skills tested are sum-\nmarizing a text and writing an argumentative essay. Students\nare required to respond to one item for each of these skills.\nDevelopment of GATE\nThis section discusses the stages of developing the GATE.\nThe study utilized the test process (refer to Figure 3) as a\nbasis of the development of the GATE. The three phases in\nthe process are further elaborated and explained.\nPhase 1--Test Development\nDeveloping the GATE. The GATE was developed based on the\nfollowing principles:\n1. Literature on high-stakes standardized tests such as\nTOEFL and IELTS, the framework for validating lan-\n2. The test specifications focus on the elements of \"con-\ntext validity\" or the test tasks or questions, such as\npurpose, text length, lexical and structural range,\ntopic familiarity, and so on. The objectives of the test\nwere spelled out according to the major sections,\nReading, Writing, and Grammar, and the subsections\nfor each component. The Listening and Speaking\nsections were left out as they required a lot more time\nFigure 3. Test process.\nSource. Developed by researcher.\nZainal Abidin and Jamil 5\nand resources to develop, and at PG level, candidates\nare expected to write extensively to produce a thesis.\nThey need to be able to read extensively and inten-\nsively, be critical on their readings and discussions,\nand be able to summarize and paraphrase ideas that\nsubsequently need to be organized in the thesis. In\naddition to the conventional methods of testing\nReading and Writing, the test was creative in that a\ndifferent technique for testing Grammar called\n\"transformation items\" was introduced; it tests a can-\ndidates' ability to paraphrase sentences and very\nshort paragraphs using appropriate ad accurate lan-\nguage. Thus, not only did the test contain the neces-\nsary skills that PG candidates are expected to\ndemonstrate at entry level, it also had innovative\nideas for a test of this nature. Subsequently, the num-\nber of items per section, weightage, and duration for\neach section were determined.\n3. \"Scoring validity\" concerns how the test is marked\nincluding elements of rating criteria, raters, rater\ntraining, and so on, and the marking scheme for\nReading, Grammar, and rating scale for Writing were\nconsequently developed. The rating schemes were\nbased on the sections as follows:\nReading--four reading tasks: graded in terms of diffi-\nculty, range from 2 to 3 marks per item\nWriting--two tasks: summary and argumentative essay,\nGrammar--two tasks: error correction and transformation\nitems, carry 30 and 20 marks, respectively\n4. \"Theory-based validity\" concerns the test takers; ele-\nments include language and content knowledge that\ntest takers possess and the processes or strategies that\nthey utilize when performing the test task. These are\nimportant considerations regarding the test takers,\nwhich test developers need to be mindful of when\ncreating the test. In fact, the test takers should be the\nstarting point of a test development process; we need\nto ask the following questions:\n Who are the candidates?\n What is required of them in the test task?\n Do the tasks match the candidates'levels of ability,\ncontent knowledge, and language knowledge?\n Are they well prepared for the test tasks?\n What processes would the candidates need to use\nto fulfill the test tasks?\nIn the case of the GATE test, the factors were spelled out\nin the test specifications, for example, the topics selected for\nthe Reading passages were fairly academic yet general\nenough and were within candidates' scope of knowledge and\nability. The topics ranged from education, international trade\nto technology and social ills. The skills tested include apply-\ning inductive and deductive reasoning to determine author's\nargument, making inferences and drawing conclusions and\nevaluating the argument in the text. The Writing tasks require\nthat the candidates demonstrate their ability to discuss a topic\nin an argumentative manner using appropriate and accurate\nlanguage. The Grammar tasks require error recognition, cor-\nrection, and paraphrasing skills.\nThese are skills that are expected of PG students in their\nprograms; they need to be able to read critically, write in a\ncohesive and coherent manner, and display language knowl-\nedge and proficiency that is up to mark.\nSampling techniques and analytical procedures.In the begin-\nning stages of the test development process, the participants\nwere randomly sampled from PG candidates (Malaysian and\ninternational) currently enrolled in Universiti Teknologi\nMara (UiTM) and some selected from other universities.\nWhen a candidate has completed the test and obtains a\nscore, she was placed on a band, in line with the levels of the\nCEFR (although it was anticipated that candidates may reach\nTable 2. GATE: Components Tested.\nSection Item type/format Skill tested Number of items\nReading MCQ Making inferences and drawing conclusion based on\nevidence or information found in text\n Determining author's purpose and point of view 4\n Using deductive or inductive reason to determine author's\nargument/ interpreting graphic aids, which authors use to\npresent information\n Evaluating author's argument to determine objectivity,\ncompleteness, validity, and credibility\nGrammar Error correction Identifying errors and making corrections 15\nTransformation items Paraphrasing sentences given first words 8\nWriting Summary Summarizing a text 1\nEssay Writing an argumentative essay 1\nNote. GATE = Graduate Admission Test of English; MCQ = multiple-choice question.\n6 SAGE Open\na maximum of B2; the highest level in the CEFR is C2 =\nproficient user).\nPhase II--Test Operations (Implementing the\nAfter many months of developing, vetting, and refining the\nGATE, it was administered to PG candidates who were already\nenrolled in various university programs and faculties.\nThe test was administered in two stages as it was depen-\ndent on the availability of the test takers.\nTest examiners were assigned, and the scores were\nreported (refer to the test analysis).\nPhase III--Test Analysis\nBefore test scores can be analyzed for further validation and\nrevision of test items and the test as a whole,\n i.\nitems were calibrated accordingly so they match the\nobjectives and\nii.\nbias was detected to find items where one group per-\nforms much better than the other group: Such items\nfunction differentially for the two groups, and this is\nknown as Differential Item Functioning (DIF). Bias\nis \"systematic error that disadvantages the test per-\nformance of one group\" (Shepard, 1981) or \"system-\natic under- or overestimation of a population\nparameter by a statistic\" (Jensen, 1980).\nData analyzed in the project were based on several\nsources:\n i.\nTest trials/pilot studies: Test scores from these trials\nwere analyzed using the SPSS and results placed\naccording to the CEFR band.\nii.\nThe new English test for PG students: Test scores\nwere rendered in SPSS and results placed according\nto the CEFR band.\nResults\nThis section provides the findings of the GATE taken by stu-\ndents in the two stages above.\nA. The distribution of scores is reported as follows:\n\u00b7\n\u00b7 According to each section--Reading, Writing,\nGrammar (refer to Table 3)\n\u00b7\n\u00b7 The total score for each candidate (refer to Table 3)\n\u00b7\n\u00b7 Data on descriptive statistics--mean, median,\nmode, range (refer to Table 3)\n\u00b7\n\u00b7 Distribution of scores (refer to Figure 4)\n\u00b7\n\u00b7 Distribution of final scores (refer to Figure 5)\n\u00b7\n\u00b7 Placement of student scores against the CEFR\n(refer to Table 4)\nB. Summary of distribution of scores:\n\u00b7\n\u00b7 The scores on the GATE showed a distribution of\nscores that are erratic and fluctuating\n\u00b7\n\u00b7 The mean scores for individual sections are just\naverage or below the average expected\n\u00b7\n\u00b7 The candidates performed best in the Reading sec-\ntion and scored lowest in the Grammar section\n\u00b7\n\u00b7 The range between the highest and the lowest\n\u00b7\n\u00b7 The distribution of final scores showed several peaks\n(5 scores in the 70s) and several scores in the low 20s\nC. Placement of students against the CEFR\n1. Before students are placed along the CEFR table, cut-\noff scores needed to be determined for the individual\nsections and the final scores of the test. In doing so,\nthe following factors were taken into consideration:\n\u00b7\n\u00b7 The GATE was developed solely on research, the\nteam members' expertise and experience, and its\nown specifications\nTable 3. Mean Distribution of Scores for Each Section, the Final Scores, and Descriptive Scores.\nReading Grammar Writing Final score\nZainal Abidin and Jamil 7\nFigure 4. Distribution of scores.\nFigure 5. Distribution of final scores.\nTable 4. Placement of Students Against the CEFR.\nUiTM scoring MUET CEFR\nScore Grade Band Levels GATE\nNote. CEFR = Common European Framework of Reference for Languages; MUET = Malaysian University English Test; GATE = Graduate Admission Test\nof English.\n8 SAGE Open\n\u00b7\n\u00b7 The GATE did not have an equivalent; however, it\nbenchmarked items against the MUET, IELTS, and\nother internal tests\n\u00b7\n\u00b7 Results from the test indicated that the test per-\nformance for the sample of candidates ranged\nfrom good to below average and weak; in fact,\n50% of the candidates scored below the proposed\ncutoff scores, and 50% scored above the pro-\nposed cutoff scores.\n2. Thus, it was decided that we could take a central\npoint to determine the cutoff scores as follows:\n3. Thus, referring to the GATE results, we derive the\nfollowing details:\nThese results were transferred on to the following table to\nhighlight the levels attained by the candidates in the GATE.\nIt appears that almost 10% of the candidates managed\nto A2 range, and the others scored below the range. Although\nthe candidates demonstrated performance that were below\nthe expectations of PG candidacy, especially because many\nwere already in the respective courses, some inferences\ncould be made of the situation.\nAs mentioned earlier, there seems to be a mismatch in the\nperformance of the candidates in the PG classes although\nthey were able to produce an IELTS or TOEFL score when\napplying to the university. One possible explanation is in the\nnature of these tests; many have questioned whether they are\ntests for academic purpose or tests of proficiency. Test tak-\ners may have been able to succeed in the tests, but the\ndemands of PG tasks, especially in critical reading and\nwriting, are far different from the tasks in these standard-\nized tests. Aspects of bias have often been one of the criti-\nLast, the GATE may have been a rather overwhelming\nexperience for the candidates; they may not have taken such\ntests for some time, and results of the GATE trial showed\ntheir weakest performance in the grammar followed by the\nWriting section. Overall, despite the test trials and results,\nthe likelihood of developing such a test within the local con-\ntext in Malaysia is now evident. Given time, resources, and\nthe wealth of test development \"experts\" in the country, the\nGATE can be realized as a promising option to the foreign\nand costly tests.\nDiscussion\nThe main reasons for developing a \"localized,\" \"home-\ngrown\" test for entry into PG education in the country is\nclear: Recent literature suggests it, and a clear mismatch is\nfound in language ability at PG intake into universities and\ncandidates' performance in the classroom.\nThus, it is proposed that the GATE is used to measure a\ncandidate's ability to use the English language at entry level\ninto a PG program in Malaysia. This in itself encourages\npotential candidates to develop and improve their English\nlanguage competency so that they are able to communicate\nand function effectively and successfully in PG education\nand in a global world where English is the lingua franca of\ntrade, international business, political governance, mass\ncommunication, and so on. More importantly, we have reit-\nerated the significance of standardization in the PG enroll-\nment across faculties in Mara University of Technology and\nacross universities in Malaysia.\nThe GATE was developed systematically according to\nknowledge and literature in test development, and details in\nits Test Specification document are based on elements\ndescribed according to components in the framework for test\nvalidation (Weir, 2005). Accordingly, several raters carried\nout the marking while the Writing section was graded based\non the Graduate Record Examination (GRE) scale for\nwriting.\nResults of the test (detailed in the section \"Results\") indi-\ncate that candidates performed fairly well in spite of con-\nstraints such as time, exposure to test format and level, and\nlittle or no preparation leading to the test. More importantly,\nthe results draw attention to the mismatch between student\nability and actual performance within a course, and the con-\ncerns of teaching, learning, and testing at PG level in\nMalaysia.\nRecommendations\nThe following recommendations are made based on factors\nthat need to be considered to enhance and facilitate the test\ndevelopment process, and the research project as a whole,\nleading to the production of a more reliable and valid GATE\ntest.\nThe test development process requires many hours of\nresearch into best practices of test development from organi-\nzations such as Cambridge ESOL, English Testing Services\n(ETS), the Malaysian Examination Board, and other more\nlocalized exam boards. The process and procedures need to\nSections Final score\naThe cutoff for the final scores is the same as the median of the\ndistribution.\nZainal Abidin and Jamil 9\nbe examined and discussed, and test specifications need to be\ndrawn up before the test development process begins.\nIf the project is to be a success, time, resources in terms of\npersonnel, and money are essential. Teams of human resource\nare needed for the following:\na. test development: writing, vetting, refining, and\ncollating\nb. administering the test, analyzing the outcome, and\nreviewing the test\nc. test operations: administering the test, monitoring,\nand collecting\nd. grading: marking/rating the test/task using grading/\nrating/marking schemes, and moderating\ne. analyzing test scores and reporting test results, and\nf. test validation, which incorporates all of the above to\nensure construct validity and reliability.\nFurthermore, infrastructure needs to be made available\nfor the project to run smoothly, and this includes facilities\nsuch as office space, basic office equipment such as comput-\ners, printers, scanners, photocopying facility, statistical\nsoftware, and other applications to assist in item analysis,\nscore analysis, organization, and reporting of test results.\nConclusion\nThe topic of localization in testing is new in the field\n(O'Sullivan, 2011a), yet one of the major considerations of\nthe test development process is the purpose or needs of the\ntest we are developing. Before we can consider the type of\ntask, test content, scoring, and test outcomes, we have to\nfirst determine its purpose for the target test takers. Given\nthis important aspect in testing, it can be concluded that\ndeveloping the GATE is a viable project as it bears signifi-\ncance and could have a big impact on the quality of PG\ncandidates as well as the PG system in universities in\nMalaysia. Given much more consideration in terms of time,\nbudget, human resource assistance, and proper manage-\nment, a test such as the GATE could help raise the status of\nlocal academic institutions by having quality PG education,\nstudents, and system, which will ultimately produce com-\npetent graduates equipped to face a challenging and global-\nized world.\nAppendix\nCommon Reference Levels: Global Scale.\nProficient user C2 Can understand with ease virtually everything heard or read. Can summarize information from\ndifferent spoken and written sources, reconstructing arguments, and accounts in a coherent\npresentation. Can express himself or herself spontaneously, very fluently, and precisely,\ndifferentiating finer shades of meaning even in more complex situations.\nC1 Can understand a wide range of demanding, longer texts, and recognize implicit meaning. Can\nexpress himself or herself fluently and spontaneously without much obvious searching for\nexpressions. Can use language flexibly and effectively for social, academic and professional\npurposes. Can produce clear, well-structured, detailed text on complex subjects, showing\ncontrolled use of organizational patterns, connectors, and cohesive devices.\nIndependent user B1 Can understand the main ideas of complex text on both concrete and abstract topics, including\ntechnical discussions in his or her field of specialization. Can interact with a degree of fluency and\nspontaneity that makes regular interaction with native speakers quite possible without strain for\neither party. Can produce clear, detailed text on a wide range of subjects and explain a viewpoint\non a topical issue giving the advantages and disadvantages of various options.\nB2 Can understand the main points of clear standard input on familiar matters regularly encountered in\nwork, school, leisure, etc. Can deal with most situations likely to arise whilst travelling in an area\nwhere the language is spoken. Can produce simple connected text on topics that are familiar or of\npersonal interest. Can describe experiences and events, dreams, hopes, and ambitions, and briefly\ngive reasons and explanations for opinions and plans.\nBasic user A2 Can understand sentences and frequently used expressions related to areas of most immediate\nrelevance (e.g., very basic personal and family information, shopping, local geography,\nemployment). Can communicate in simple and routine tasks requiring a simple and direct\nexchange of information on familiar and routine matters. Can describe in simple terms aspects of\nhis or her background, immediate environment, and matters in areas of immediate need.\nA1 Can understand and use familiar everyday expressions and very basic phrases aimed at the\nsatisfaction of needs of a concrete type. Can introduce himself or herself and others, and can\nask and answer questions about personal details such as where he or she lives, people he or she\nknows, and things he or she has. Can interact in a simple way provided the other person talks\nslowly and clearly and is prepared to help.\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with\nrespect to the research, authorship, and/or publication of this\narticle.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nIELTS, especially on candidates and teachers. Paper presented\nat Going Global. The UK International Education Conference,\nEdinburgh, Scotland.\nIELTS Handbook. (2007). University of Cambridge web publica-\ntion. Available from www.ielts.org\nJaschik, S. (2010, August 2). New questions on test bias. Inside\nHigher Ed. Available from http://www.insidehighered.com/\nJensen, A. R. (1980). Methods for identifying biased test items. In\nG. Camilli & L. A. Shepard (Vol. 4, Eds.). Thousand Oaks,\nCA: Sage.\nLittle, D. (2006). The Common European Framework of Reference\nfor Languages: Content, purpose, origin, reception and impact.\nMoon, T. S., & Siew, H. O. (2004). A study on the factors that\nimpact on academic performance of the computer science and\nthe information technology students in University of Malaya.\nO'Sullivan, B. (2011a). Language testing: Theories and practices\n(Advances in linguistics). London, UK: Palgrave Macmillan.\nOthman, A. B., & Nordin, J. (2013). MUET as a predictor of aca-\ndemic achievement in ESL teacher education. GEMA Online\nPonniah, K. S., & Tay, B. (1992). Processing strategies in reading.\nRedden, E. (2008). English for graduate students. Inside Higher Ed.\nSaville, N. (2006). The CEFR and common standards for interna-\ntional language examinations a QMS approach. Paper pre-\nsented at ALTE conference, Copenhagen, Denmark.\nShepard, L. A. (1981). Identifying bias in test items. In B. F. Green\n(Ed.), Issues in testing: Coaching, disclosure and ethnic bias\n(pp. 79-104). San Francisco, CA: Jossey-Bass. Retrieved from\nhttp://www.eric.ed.gov\nTannenbaum, R. J., Wylie, E. C. (2005) Mapping English language\nproficiency test scores onto the Common European Framework\n(ETS Research Rep. No. RR-05-18; TOEFL Research Rep.\nNo. RR\u00ad80). Princeton, NJ: ETS.\nWeir, C. J. (2005). Language testing and validation: An Evidence-\nbased approach. New York, UK: Palgrave Macmillan.\nhttp://www.pearsonvue.co.uk/\nhttp://www.cambridgeenglish.org/Images/23122-research-\nhttp://www.ets.org/toefl/\nAuthor Biographies\nSaidatul Akmar Zainal Abidin is associate professor and senior lec-\nturer in the field of English language and Linguistics. She has more\nthan 28 years experience in teaching English to second language speak-\ners (TESL), and the programme `English for Professional\nCommunication'. Dr Saidatul's research interest is in the fields of lan-\nguage testing & evaluation, professional communication and curricu-\nlum development.\nAsiah Jamil is associate professor and senior lecturer in the field of\nEnglish language and Linguistics. She has more than 25 years expe-\nrience in teaching English to second language speakers (TESL), and\nthe programme `English for Professional Communication'.\nDr Asiah's research interest is in the fields of language testing &\nevaluation, test taking strategies and qualitative research."
}