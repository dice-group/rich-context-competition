{
    "abstract": "Abstract\nMovement detection for a virtual sound source was measured during the listener's horizontal head\nrotation. Listeners were instructed to do head rotation at a given speed. A trial consisted of two\nintervals. During an interval, a virtual sound source was presented 60 to the right or left of the\nlistener, who was instructed to rotate the head to face the sound image position. Then in one of a\npair of intervals, the sound position was moved slightly in the middle of the rotation. Listeners\nwere asked to judge the interval in a trial during which the sound stimuli moved. Results suggest\nthat detection thresholds are higher when listeners do head rotation. Moreover, this effect was\nfound to be independent of the rotation velocity.\n",
    "reduced_content": "Short Report\nDetection of Sound Image\nMovement During Horizontal\nHead Rotation\nAkio Honda\nYamanashi Eiwa College, Yamanashi, Japan\nKagesho Ohba\nTohoku University, Sendai, Japan\nYukio Iwaya\nTohoku Gakuin University, Miyagi, Japan\nYo\n^iti Suzuki\nTohoku University, Sendai, Japan\n Keywords\nsound localization, virtual sound source, active listening, rotation velocity\nIntroduction\nFor azimuthal sound localization, we use the interaural time difference and the interaural\nlevel difference (Blauert, 1997). This information changes according to the listener's head\nmovement, thereby influencing sound localization accuracy. Wallach (1939) demonstrated\nthat this information provides clues for the elevation angle judgment at sound image\nlocalization and for front or back judgment. Similar findings were obtained using a virtual\nCorresponding author:\nAkio Honda, Faculty of Human Sciences and Cultural Studies, Yamanashi Eiwa College, 888 Yokone-machi, Kofu, Yamanashi\nEmail: akio.honda6@gmail.com\ni-Perception\nipe.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without\nfurther permission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sage-\npub.com/en-us/nam/open-access-at-sage).\nreality system. For instance, Kawaura, Suzuki, Asano, and Sone (1989) investigated effects of\nhead movement on sound image localization using a 3D virtual auditory display (VAD).\nThey reported that front or back judgment of the sound localization task is improved by\nreflecting a listener's head movement. As described herein, many reports have described that\nsound localization is facilitated by head movement, which creates dynamic changes to the\ninformation input to both ears.\nIt is particularly interesting that recent reports have described that these effects differ\ndepending on the sound features (Iwaya, Suzuki, & Kimura, 2003; Perrett & Noble, 1997),\nand that the sound image localization accuracy is reduced by head movement depending on\nthe timing of the sound presentation (Cooper, Carlile, & Alais, 2008; Leung, Alais, & Carlile,\n2008). For instance, Iwaya et al. (2003) revealed that the listener's head rotation reduces the\nfront\u00adback error effectively when the sound duration is long. Moreover, Cooper et al. (2008)\nperformed a task by which a test sound was presented while the listener was engaged in the\nhead rotation movement, and the listener was asked to report the sound image position when\nthe rotation movement was finished. They reported the result that presentation of sound\nstimulation during head rotation movement caused reduction in the sound image localization\naccuracy compared with a case in which the head is not moved. Leung et al. (2008) examined\nauditory spatial perception during rapid head motion. They demonstrated that the\nperception of auditory space was compressed during rapid head motion for stimuli\noccurring in the perisaccadic interval. The authors' group assessed whether listener's\nhorizontal head rotation affects sound localization accuracy at the subjective front\n(Masumi, Honda, Suzuki, & Sakamoto, 2014; Masumi, Suzuki, Honda, & Sakamoto,\n2014). Results showed that reduction in sound localization accuracy during head rotation\nmovement occurred with passive rotation movement (Masumi, Honda, et al., 2014) and\nactive rotation movement (Masumi, Suzuki, et al., 2014). Recently, Brimijoin and Akeroyd\n(2014) reported that the moving minimum audible angle for virtual sound sources during self-\nmotion is larger than in the still condition but is smaller than that during sound source\nmotion. Nevertheless, few researchers have examined sound source localization when\nlisteners and sounds rotate (Yost et al., 2015). Therefore, this study examined sound image\nmovement detection during horizontal head rotation using the VAD.\nExperiment 1\nObservers (listeners) were eight male students with normal hearing ability (average age:\n22.9 years old). To present a sound source with arbitrary movement with no mechanical\nnoise, it was presented virtually via a VAD, which outputs binaural output signals via the\nheadphones after convolving head-related impulse responses (HRIRs) of a specified sound\nsource position. Actually, HRIRs are the time domain representation of head-related transfer\nThis VAD was developed by the authors' group (Yairi, Iwaya, & Suzuki, 2007, 2008). The\nsystem consisted of a pair of headphones, a magnetic position sensor, and a personal\ncomputer (3.06 GHz Pentium 4 CPU, 2 Gbyte memory). The operating system of the\npersonal computer was Linux (kernel 2.6). The sound driver was Open Sound System with\na sound card (Audiophile 2496; M-Audio). Actually, HRIRs (head-related transfer\nfunctions) are determined according to anthropometrical parameters such as the sizes and\nshapes of the listener's ears, head, and body. They are, therefore, strongly idiosyncratic,\ndiffering among individuals (Morimoto & Ando, 1980; Watanabe, Iwaya, Suzuki, Takane,\n& Sato, 2014; Watanabe, Ozawa, Iwaya, Suzuki, & Aso, 2007). Therefore, the use of\nindividual HRIRs (individualization) is extremely important to achieve good sound\nlocalization with VADs (Morimoto & Ando, 1980). To do this, HRIRs were measured for all\nlisteners with a spherical speaker array installed in an anechoic room at the authors' institute.\nHRIRs for sound sources located 1.5 m from the center of the spherical array, where the\ncenter of listener's head is aligned during HRIR measurements, were measured using an\nelevation angle, as well as the two poles. The sampling frequency was 48 kHz. To achieve\nsmooth rendering, measured HRIRs were spatially interpolated for any direction with\nresolution of 0.1 based on an average with weights depending on the angle separation\nbetween the direction and the four directions around it where the measured HRIRs are\nTakeda, & Itakura (1999) evaluated the performance of the linear interpolation of HRIR\nusing an objective measure based on spectral distortion and by a subjective listening test to\nshow that sufficient precision in terms of sound localization is obtained if the spatial\nresolution of the measured HRIR is as small as 10. For the four HRIR used for the\nlinear interpolation, temporal alignment was introduced so that the interpolation works\neffectively (Watanabe et al., 2005). The peak of the main response of each HRIR was\nfinely adjusted using eight times oversampling, that is, with eight times higher temporal\nresolution. Because of this time alignment as well as the horizontal HRIR resolution of\nfive degrees, we expected that our VAD had sufficiently good precision. The HRIRs were\nalways updated by reflecting the listeners' head rotations (3DOF of yaw, pitch, and roll)\nobserved using a motion sensor at the rate of 120 Hz (Fastrak, Polhemus) so that the virtual\nsound sources can be presented stably at specified positions in terms of the virtual world\ncoordinates. The output signals were presented via headphones (SR202, STAX Ltd.). The\nthe sum of the latencies of head-tracking, position data transmission, interpolation of head-\nposition, and delay by sound buffer (128 samples). This total system latency value is much\nless than the detection threshold (DT) of the delay of head-tracking in VADs, which is 50 to\nincongruity with their own motion.\nIn this experiment, the stationary condition and head movement conditions were used. In\nthe stationary condition, listeners listened to the stimulus without moving the head. In a head\nmovement condition, listeners listened to the stimulus while horizontally rotating the head.\nThe sound stimulus in both conditions was pink noise (frequency range, 40 Hz\u00ad21.8 kHz;\nsampling frequency, 48 kHz) convolved with the observer's HRIR of specified direction. The\nsound was presented using the VAD. The sound pressure levels of the stimuli were measured\nusing an artificial ear (4153; Bruel and Kjaer) to set it to 65 dB. Both conditions comprised\nexperimental stimulus of two types. These were a still virtual sound source and a moving\nvirtual sound source. The sound source was virtually placed on a circle of 150 cm radius on\nthe horizontal plane centered at the listener's head. When a still virtual sound source was\npresented, it was presented at 60 to the right or left of the listener and did not move. In\ncontrast, when a moving virtual sound source was presented, a virtual sound source was first\npresented at 60 right or left of the listener's orientation and moved, later in the middle of the\npresentation, only once on the circle with constant angular velocity from 60 to a more lateral\nangle with a specified deviation. It was subsequently returned to the original position. These\nmoving processes took 500 ms. In the experiment, a trial consisted of two intervals. In an\ninterval, the still virtual sound source was presented. In the other interval, the moving virtual\nsound source was presented. The order of these two intervals was randomized in each trial.\nListeners were asked to judge in which interval in a trial the sound stimuli moved\n(two-interval two-alternative forced-choice task, chance rate \u00bc 50%). The duration of each\nHonda et al. 3\nof the virtual sound sources was 6 s. After the presentation of the first stimulus, the listener\npushed a key to start the second stimulus in 1 s.\nUnder stationary head conditions, a listener faced forward with the head front 0 and then\nthe two stimuli were presented as described earlier. When a moving sound source was\npresented, the sound source started moving 2 to 3 s after the start of the presentation; the\nelapsed time length was selected randomly for each stimulus. The deviation of the moving\nsound source, which was incremental in terms of absolute values, was chosen randomly from\ntrials in total (7 sound image movement angles \u00c2 2 directions \u00c2 10 repetitions). The stimuli\nwere presented with fully randomized order.\nUnder the head movement condition, a listener faced forward with the head front 0;\nthen the two stimuli were presented. As soon as each virtual sound source was presented\nfrom 60 right or left, listeners were asked to orient toward the sound image in front in 1 s.\nIn other words, the head rotation velocity was 60/s. A 440 Hz pure tone (duration, 100 ms)\nwas presented three times at 1 s intervals 1 s after sound stimulation presentation to\nassist observers because it is difficult to control head rotation velocity by instruction\nalone. Listeners were asked to start head rotation upon completion of the third pure\ntone presentation. We expected that users would use this step to learn this target rotation\nvelocity. After each stimulus presentation, the average head speed during one head motion\nwas fed back to the listener. In this condition, the virtual sound source started moving\nwhen the listener turned his head 30. Although the manner of the virtual sound source\nmovement was identical to that of the stationary condition, the angular increment was one\nsound condition. Listeners performed the head movement condition after the stationary\ncondition.\nResults and Discussion\nResults are presented in Figure 1. The average head rotation velocity was 63.5/s (SD \u00bc 3.52/s).\nWe calculated and analyzed the DTs of sound source movement (correct rate of 75%) for\neach listener based on maximum likelihood fitting to the psychometric functions\n(0.5 \u00c2 cumulative normal distribution function \u00fe 0.5). Statistical analyses demonstrated\nthat the DT in the head movement condition (17.7) is significantly greater than that in\nthe stationary condition (3.6), t(7) \u00bc 9.93, p < .01. Therefore, our results showed that\nduring head rotation, listeners were less sensitive to sound image movement than in the\nhead stationary state. Previous reports have described that if a sound is presented during\nhead movement, the sound localization accuracy is reduced (Brimijoin and Akeroyd, 2014;\nindicate that the DT of a moving sound image increases during head rotation, as found in the\nprevious studies. However, this experiment involves the following issues: First, the angular\namount of virtual sound source movement differed between two conditions. Second, the\norder effect of head movement was not controlled. Third, only one head rotation velocity\nwas examined. To resolve these issues, we conducted Experiment 2.\nExperiment 2\nListeners were nine male students with normal hearing ability (average age: 22.8 years old).\nSix of them also participated in Experiment 1. The apparatus and stimuli were identical to\nthose used in Experiment 1.\nExperiment 2 consisted of the stationary condition and two head movement conditions\n(30/s and 60/s). When the head rotation velocity was 30/s, the listener was asked to orient\nin the direction of the sound image in front in 2 s and to assist observers in maintaining the\nhead rotation velocity. An auxiliary tone similar to that used in Experiment 1 was presented\nas follows: A 440 Hz pure tone (100 ms) was presented three times at 2 s intervals. Then the\nlistener was asked to start rotating the head after the third presentation of a pure tone. The\nsound stimulus duration was 10 s. When the velocity was 60/s, similarly to Experiment 1,\na pure tone was presented three times at 1 s intervals. The duration was 6 s. After each\nstimulus presentation, the average head speed during one head motion was fed back to the\nlistener. Each condition was constructed using three levels of the sound image movement\nangular amount (5, 10, 20) and two levels of sound image position (60 in right and left).\nThe head movement condition (head stationary, head rotation of 30/s, and 60/s) was\ncounterbalanced across listeners. The experiment was performed in three sessions\n(stationary, 30/s and 60/s sessions). The number of trials in each session was 60 in all\n(3 sound image movement angles \u00c2 2 directions \u00c2 10 repetitions). The sound image\nmovement angular amount and sound image position were fully randomized.\nResults and Discussion\nResults are presented in Figure 2. Average head rotation velocities for 30/s and 60/s\nway within-subject analysis of variance (ANOVA) of the correct detection rate (DR) was\nperformed, considering the sound image movement angular amount (5, 10, 20), the sound\nimage position (60 in right and left), and the head movement condition (head stationary,\nhead rotation of 30/s, 60/s) as factors. Results revealed that main effects of the angular\nwere significant. The other main effect and the interaction were not significant. We conducted\npost hoc analyses using Ryan's method to elucidate the main effects. Regarding the angular\n(DR \u00bc 0.64), it was detectable but less easily in this order (ps < .05). Regarding the head\nmovement, the DR of sound source movement under the head stationary condition\nFigure 1. Average rate of correct detection in Experiment 1. Error bars represent the standard deviations.\nHonda et al. 5\n(DR \u00bc 0.91) was significantly higher than under conditions of head rotation of 30/s\nobserved between head rotation speeds of 30/s and 60/s (p \u00bc .28). Therefore, results\ndemonstrated that similar to Experiment 1 and in agreement with previous studies, the\ndetection of sound source movement is more difficult under head rotation than without\nhead movement. Results demonstrated that this is true not only at the rotation velocity of\nGeneral Discussion\nSound image movement detection during horizontal head rotation was investigated using the\nVAD. Previous reports of some studies have described that when a sound is presented during\nhead movement, the sound localization accuracy is reduced (Brimijoin and Akeroyd, 2014;\n2014). Results of Experiment 1 suggest that the DT of a moving sound source during head\nrotation is greater than that under the stationary condition. Experiment 2 revealed that, in\naddition to the replication of the results of Experiment 1, the same phenomena occur even\nwith head rotation movement of 30/s. Our results indicate that an increase in the DRs of\nsound source movement during head movement is observed in both head velocity conditions.\nPrevious reports of studies showing that sound image localization accuracy for that sound is\nreduced fall into two categories. Some studies (Cooper et al., 2008; Leung et al., 2008) have\nrevealed that the phenomenon occurs when the head rotation movement is made rapidly. For\ninstance, Leung et al. (2008) reported that observers' average head turn speed had mean\nvelocity of 256/s. Moreover, Cooper et al. (2008), from findings based on data of\nparticipants' head turn velocity, found a mean of 124/s. Results of other studies suggest\nlittle velocity dependence (Masumi, Honda, et al., 2014; Masumi, Suzuki, et al., 2014).\nFor instance, Masumi, Honda, et al. (2014) reported that sound localization accuracy\nis independent of listeners' passive head turn speeds of 5 to 20/s. In this study, the\nobservers' average head rotation velocities were 63.5/s for Experiment 1. The average head\nrespectively. Therefore, results of this study correspond to those of studies that suggest little\nvelocity dependence (Masumi, Honda, et al., 2014; Masumi, Suzuki, et al., 2014).\nFigure 2. Average rate of correct detection in Experiment 2. Error bars represent the standard deviations.\nMore specifically, our results indicate that deterioration of sound localization accuracy is\ncommonly observed not only from rapid head movement but also from moderate or slow\nhead movement.\nMoreover, our results can be regarded as consistent with results reported by Brimijoin and\nAkeroyd (2014), which were also obtained using VAD. Among the four conditions examined,\ntwo were comparable to ours. They are Condition 1, in which both the observer's head and\nsound sources were stationary, and Condition 4, in which observers rotated the head several\ntimes and the virtual sound sources were moved properly in the egocentric coordinate so that\nthe virtual sound source positions are presented as stationary in the virtual world coordinate.\nThat is, their minimum audible angle (MAA) obtained in their Condition 1 might be\ncompared with our DT obtained in the stationary condition if effects of the unconscious\nmicro head movements could be disregarded. Moreover, their MAA obtained in their\nCondition 4 might be compared with our DT obtained in the head movement conditions,\nalthough the experimental conditions differ somewhat. They presented male and female\nvoices at fixed but different positions in the virtual world coordinate, whereas we\npresented, in our head movement condition, a pink noise burst initially at 60 in the\nvirtual world coordinate. It was then moved away to a more lateral angle from 60 and\nback to 60 with a specified directional deviation of 1 to 12. They reported that the MAA,\nwhich they call moving MAA (MMAA), is larger when the head is moving under the proper\ncontrol of the virtual sound source being fixed in virtual world coordinate (ca. 6, Condition\n4) than when the head and the virtual sound source positions are stationary (ca. 3, Condition\n1). In summary, their results and ours both indicate that the sound localization resolution\nbecomes larger (worse) when observer (listener) is rotating the head. Brimijoin and Akeroyd\n(2014) also report that MMAA, which shows auditory resolution for static relative positions\nbetween sound sources, is larger when the head is moving than when it is stationary,\nirrespective of whether the virtual sound sources are stationary or moving in the virtual\nworld coordinate. In addition, our results newly indicate that the detection of sound\nsource motion in virtual world coordinate worsens when the head is rotating.\nHow can localization accuracy deterioration during a listener's head movement be\nexplained? We attempt to explain this issue based on two models. The first applies a basic\ncognitive information processing model. In general, in information processing, a tradeoff\nrelation prevails between speed and accuracy (Wickelgren, 1977). On the basis of this\nmodel, we have anticipated that the DR of sound source movement under the head\nrotation of 30/s was significantly higher than those with head rotation of 60/s. However,\nno significant difference was observed between head rotation of 30/s and 60/s. Then, the\nvelocity dependence of the phenomena observed in this study is weak. Therefore, an\nexplanation based solely on this model seems inappropriate. The second is a perceptual\ninformation processing model. For instance, Viemeister and Wakefield (1991) proposed an\nauditory information processing model known as the multiple-look model. According to this\nmodel, one can expect that sound localization performance worsens with head movements\nbecause extra sources of error arise due to the dynamic change of auditory input, rather than\nsimply more information to be integrated. Results of this study showed the expected tendency\nin that regard. In the multiple-look model, when the sound space perception was examined\nwith this model, auditory input information was first divided into time windows of several\nmillisecond units. Then localization was performed for each divided window. For input\nsound information, the sound image position was estimated in several-millisecond units.\nTheir outputs were finally integrated to conclude sound image localization. For the head\nmovement conditions examined in this study, in addition to dynamic changes of the\ninformation input to the ears because of sound source movement, the information\nHonda et al. 7\nobtained using the listener's head movement should be integrated with that information.\nTherefore, under a head movement condition, the information amount to be integrated is\nsubstantially greater, in terms of both amount and complexity, than that under a head-\nstationary condition. Accordingly, the multiple-look model can qualitatively explain the\nexperimentally obtained results that sound localization resolution is worsened by head\nmovements.\nAs future research directions intended to enrich our understanding, it is important to\nconduct experiments under actual environments. Furthermore, future studies must\ninvestigate sound image movement DTs during faster and slower horizontal head rotation.\n"
}