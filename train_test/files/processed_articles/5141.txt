{
    "abstract": "Abstract\nThe increasing availability of digital data reflecting economic and human development, and in particular the availability of\ndata emitted as a by-product of people's use of technological devices and services, has both political and practical\nimplications for the way people are seen and treated by the state and by the private sector. Yet the data revolution\nis so far primarily a technical one: the power of data to sort, categorise and intervene has not yet been explicitly\nconnected to a social justice agenda by the agencies and authorities involved. Meanwhile, although data-driven discrim-\nination is advancing at a similar pace to data processing technologies, awareness and mechanisms for combating it are not.\nThis paper posits that just as an idea of justice is needed in order to establish the rule of law, an idea of data justice \u00ad\nfairness in the way people are made visible, represented and treated as a result of their production of digital data \u00ad is\nnecessary to determine ethical paths through a datafying world. Bringing together the emerging scholarly perspectives on\nthis topic, I propose three pillars as the basis of a notion of international data justice: (in)visibility, (dis)engagement with\ntechnology and antidiscrimination. These pillars integrate positive with negative rights and freedoms, and by doing so\nchallenge both the basis of current data protection regulations and the growing assumption that being visible through the\ndata we emit is part of the contemporary social contract.\n",
    "reduced_content": "Original Research Article\nWhat is data justice? The case\nfor connecting digital rights\nand freedoms globally\nLinnet Taylor\n Keywords\nPrivacy, ethics, development, discrimination, representation, surveillance\nIntroduction: The case for data justice\nAs digital data become available on populations that\nwere previously digitally invisible, policymakers and\nresearchers worldwide are taking advantage of what\nthe UN has termed the `data revolution' (United\nNations, 2014). The increasing availability of digital\ndata reflecting economic and human development,\nand in particular of `data fumes' (Thatcher, 2014) \u00ad\ndata produced as a by-product of people's use of\ntechnological devices and services \u00ad is driving a shift\nin policymaking worldwide from being data informed\nto being data driven (Kitchin, 2016). These granular\ndata sources which allow researchers to infer people's\nmovements, activities and behaviour have ethical, pol-\nitical and practical implications for the way people are\nseen and treated by the state and by the private sector\n(and, importantly, by both acting in combination).\nThis distributed visibility has even clearer social and\npolitical implications in the case of low-income envir-\nonments, where authorities' ability to gather accurate\nstatistical data has previously been limited. Yet the data\nrevolution is so far primarily a technical one: the power\nof data to sort, categorise and intervene has not yet\nbeen explicitly connected to a social justice agenda by\nthose agencies and authorities who collect, manage and\nuse data. Nor is there a high level of awareness amongst\nimplementers of how new data technologies may not be\nneutral in terms of access, use or impacts, something\nthat the available research into this phenomenon shows\nTilburg Institute for Law, Technology and Society (TILT), Netherlands\nCorresponding author:\nLinnet Taylor, Tilburg Institute for Law, Technology and Society (TILT),\nEmail: l.e.m.taylor@uvt.nl\nCreative Commons NonCommercial-NoDerivs CC BY-NC-ND: This article is distributed under the terms of the Creative Com-\nmons Attribution-NonCommercial-NoDerivs 4.0 License (http://www.creativecommons.org/licenses/by-nc-nd/4.0/) which permits\nnon-commercial use, reproduction and distribution of the work as published without adaptation or alteration, without further permission provided the\noriginal work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nBig Data & Society\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\njournals.sagepub.com/home/bds\nto be the case (Dalton et al., 2016). In fact, while data-\ndriven discrimination is advancing at a similar pace to\ndata processing technologies, awareness and mechan-\nisms for combating it are not.\nTwo trends make developing a global perspective on\nthe just use of digital data urgently necessary: one is the\nexponential rise in technology adoption worldwide, and\nthe other the corresponding globalisation of data\nanalytics. Of the world's seven billion mobile phones,\n5.5 billion are in low- and middle-income countries\n(LMICs), where 2.1 billion people are also online\n(ITU, 2015). India and China have commissioned the\ncreation of hundreds of smart cities that will provide\nthe ability to track and monitor citizens in every aspect\nof their lives (Greenfield, 2013), digital and biometric\nregistration are becoming the new norm in even the\npoorest countries, and practices in international aid,\ndevelopment and humanitarian response increasingly\nuse vast amounts of digital data to map, sort and inter-\nvene on the mass scale in lower income regions (Taylor,\n2015). The reach of the global data market has also\nchanged to take account of these new sources of data,\nwith multinational corporations scrambling to profile\nbillions of potential new consumers (Taylor, 2016a).\nMeanwhile, research and praxis on the ways in\nwhich datafication can serve citizenship, freedom and\nsocial justice are minimal in comparison to corpor-\nations and states' ability to use data to intervene and\ninfluence.\nThis paper posits that just as an idea of justice is\nneeded in order to establish the rule of law, an idea\nof data justice is necessary to determine ethical paths\nthrough a datafying world. Several framings of data\njustice are emerging within different fields, which have\nthe potential to build on each other. I will therefore\nanalyse the existing work on data justice and place\nthe different viewpoints in dialogue with each other,\nthen argue that by finding common principles we can\nbring them together into a single framing for further\nresearch and debate. The paper is structured as follows:\nI will first outline the reasons for concern relating to the\nnew public\u00adprivate interfaces of big data, namely the\ndisciplinary and frequently discriminatory nature of\nlarge-scale databases used on the population level,\nNext, I will use empirical examples to demonstrate\nthat these concerns are not only amplified but funda-\nmentally different in the context of big data. I will then\nexplore current framings of data justice and identify\nwhich aspects of the problems arising from big data\nthey propose to address. Next, I will propose an over-\narching conceptualisation of data justice that can\nbridge existing approaches and form a basis for dia-\nlogue between them. Finally, I will argue for Sen and\nNussbaum's Capabilities Approach as a framing for\nthis conceptualisation, with the aim of providing an\necosystemic approach that can address institutions,\nmarkets, legal systems and public debates.\nA note on methodology: the theoretical and empir-\nical starting points for the framework proposed here\nare based on a research project comprising fieldwork,\nobservation and interviews conducted over the period\nmal interviews and periods of observation conducted\nwith academic researchers, development aid and\nhumanitarian organisations, independent technology\ndevelopers, activist organisations in the field of data\nand rights, large technology firms and policymakers\nfrom the US, EU and several African and Asian coun-\ntries. The observation portion of the research was con-\nducted at international events relevant to the\n`Responsible Data' movement, through participation\nin advisory groups and in public discussions on data\nethics. The interviews were conducted at these events,\nat multinational mobile network operators in France\nand Norway, and on a public-sector datafication pro-\nject in Bangalore, India.\nThe problem: Data at the public\u00adprivate\ninterface\nWhy look for ways to relate social justice concerns to\ndatafication, and vice versa? Why not, for example, pri-\noritise making sure that commercial digital innovation\ncan proceed unfettered, since this has been argued to\nbenefit everyone in society, or that data fully supports\nefficiency in the public sector, thus serving the interests\nof taxpayers and public security? Both of these latter\narguments have been made by both high-level private\nsector (World Economic Forum, 2011) and public-\nsector actors (European Commission, 2016). What is\nit about the social impacts of digital data that suggests\na social justice agenda is important? For one thing, the\nimpacts of big data are very different depending on\none's socio-economic position. The work of Gilliom\n(2001) and more recently of scholars such as Eubanks\nburden of dataveillance (surveillance using digital\nmethods) has always been borne by the poor.\nBureaucratic systems designed to assure that people\nare not misusing state welfare funds and other publicly\nfunded support are part of the apparatus of govern-\nmentality (Lemke, 2001); data-driven law enforcement\nfocuses unequally on poor neighbourhoods which\nexperience certain types of criminality (O'Neil, 2016);\nand undocumented migrants are tracked and acted\nupon by digital systems in more invasive ways than\nhigher income travellers (Taylor, 2015).\nBeyond socio-economic status, gender, ethnicity and\nplace of origin also help to determine which databases\n2 Big Data & Society\nwe are part of, how those systems use our data and the\nkinds of influence they can have over us. Kang's work\non trafficking (2015), for example, shows how the sur-\nveillance of women's behaviour and movements by\ninternational anti-trafficking and anti-sex work autho-\nrities has historically been shaped by very different\nmethodologies and types of expertise depending on\nsubjects' national origin and ethnic group, so that dif-\nferent types of data were fed into the international\nsystem from different regions, with corresponding vari-\nance in the conceptualisation of who should be the sub-\nject of anti-trafficking provisions and of anti-sex-work\nprogrammes for control and discipline. Similarly,\nMoore and Currah's (2015) research on how trans-\ngender citizens have been dealt with by population\ndatabases in the US shows that one's ability to legally\nidentify as a different gender depends to a great extent\non one's income. Jiwani's (2015) work on citizenship\nand conformity also demonstrates the ways in which\nsurveillance as an `active social process' reinforces\nstructural and social boundaries.\nMoreover, these problems intersect and multiply at\nthe boundaries created by the linking and merging of\ndatasets. This intersectionality (Cho et al., 2013) in the\neffects of datafication is an important component of the\nargument for a social justice perspective. A range of\ninteracting characteristics \u00ad race, ethnicity, religion,\ngender, location, nationality, socio-economic status \u00ad\ndetermine how individuals become administrative and\nlegal subjects through their data and, consequently,\nhow those data can be used to act upon them by pol-\nicymakers, commercial firms and both in combination.\nIn turn, the possibility of being identified as a target of\nsurveillance multiplies depending on the number of\ncategories of interest one belongs to.\nFor example, a teenager from an immigrant family,\nliving in a low-income area, whose parents are poor and\nwho belongs to a minority ethnic group and religion is\nexponentially more likely to be targeted for surveillance\nby both protective (social services) and preventive at\n(law enforcement) authorities, and is also likely to\nhave less opportunity to resist that surveillance or inter-\nvention than her friend who lives in a high-income area\nand belongs to the majority ethnic group.\nThat data systems discriminate is not news. Nor is it\nnews that they tend to further disadvantage those who\nare already marginalised or socially excluded, or that\nthose people experience the greatest obstacles in seeking\nredress. The evidence for this is well documented and\ndoes not per se argue for a new conceptualisation of\ndata justice \u00ad everyone has the right to be treated fairly\nby public (and private) authorities of all kinds. What\ndoes argue for paying special attention to the current\nimplications of datafication for social justice, however,\nis the particular dynamic of contemporary datafication\nwhere methods of data collection and analysis are no\nlonger easily divisible into `volunteered' (direct surveys\nor other collection of administrative data, where the\ncitizen is aware her data is being gathered) versus\n`other' (digital surveillance via devices and sensors).\nFor the surveilled teenager in the above example, the\nproblem multiplies when the functions of data collec-\ntion and analysis are shared between public authorities\nand the commercial firms that provided her phone, her\ninternet access or the apps she uses. The economics of\nsurveillance also have implications for fair representa-\ntion and access to services, since access to technology\nincreasingly determines who can be seen: Shearmur\n(2015) has warned that those who use big data to\nstudy behaviour or shape policy are seeing not society\nbut `users and markets'.\nThe public\u00adprivate interface is important because\nmany of what we perceive as public-sector functions\n(counting, categorising and serving our needs as citi-\nzens) are in fact performed by the private sector, with\ncorresponding implications for transparency and\naccountability. The number of public-sector data scien-\ntists equipped to analyse big data is tiny in comparison\nto the number of bureaucrats interested in what big\ndata can tell them, with the consequence that the data-\nfication of government has been, and will always be,\nexecuted primarily by the private sector. For example,\nthe whistle-blower Edward Snowden was employed by\nthe consulting firm Booz Allen Hamilton when he per-\nformed surveillance for the US intelligence agencies.\nThis suggests that markets are a central factor in estab-\nlishing and amplifying power asymmetries to do with\ndigital data, and that new strategies and framings may\nbe needed that can address the public\u00adprivate interface\nas an important site for determining whether data tech-\nnologies serve us or control us.\nIn response to this problem, arguments are emerging\nwithin different domains for a broader, social-justice-\noriented conceptualisation of our rights with regard to\ndata. As population data become by-products of infor-\nmational capitalism, this has consequences both for the\nway we can be monitored and the avenues we have to\nseek redress if we are subjected to unfair treatment.\nThis is because the tools of law and democratic repre-\nsentation that provide the possibility of redress where\npersonal data is misused become more difficult to use as\ndata starts to flow more freely between commercial and\npublic sector actors. Responsibility and accountability\ngrow fuzzy, partly because each actor can shift the\nresponsibility onto the other, and partly because moni-\ntoring is indirect and invisible, making people less likely\nto identify harms as data related.\nThe public\u00adprivate interface involved in large-scale\ndata collection, and its inevitable engagement with the\nglobal data market, raise fundamental questions about\nhow rights can be secured across borders and legal sys-\ntems, and even about whether individual rights should\nbe the only instrument used to combat data harms\n(Taylor et al., 2017). One important shift is that the\nsurveillance (or monitoring) relationship, which under-\npins many other, often positive, functions of data, is no\nlonger one to one with a fixed aim and geography, but\nrather many to many, virtual and has aims that may\nshift from governmental to commercial and back again.\nA panopticon (Foucault, 1977) where continuous sur-\nveillance drives people to modulate their behaviour is\nno longer the most useful metaphor for a contemporary\ndatafied surveillance that is invisible and plural, oper-\nating through a myriad different platforms. Instead of\ncensoring our behaviour to please our watchers, we\nmake ourselves accidentally visible through our every-\nday behaviour to a huge range of actors, from the cor-\nporations that make the devices and systems we use,\nand the service providers who facilitate their content,\nto the data brokers who track our use of them and the\nmyriad consumers of their products, which include gov-\nernments, marketing firms, intelligence agencies and\npolitical parties. Even where self-censorship is the aim\nof a technological system (as with the Chinese Social\nCredit scheme, which is designed to create citizen\nbehaviour that aligns with governmental priorities\n(Creemers, 2016)), it can be argued that it is not realistic\nfor users to remain constantly in a state of struggle\nagainst an all-encompassing system of surveillance.\nInstead, evidence shows that the increasing necessity\nof data technologies in everyday life causes people to\nresign themselves to this distributed visibility rather\nthan engaging with it politically (Turow et al., 2015).\nUntil now, within the global North freedoms and\nneeds with regard to data technologies have been\napproached through a fundamental rights framework\nthat includes data protection, framings of informa-\ntional privacy and the right to free speech and commu-\nnication. However, this framing presents two problems\nwhen applied in relation to the global data market.\nFirst, the liberal individual framing of Human Rights\nrequires that abuses are clear and visible so that those\ninjured can respond, and second, it assumes that redress\nwill be sought on the individual level. This is rendered\nproblematic by the invisible and many-to-many nature\nof `seeing' through data technologies, but also by the\nfact that many of the negative impacts of data occur on\nthe group as much as the individual level (Taylor et al.,\nInstead of applying a fundamental rights framework\nwhose application demands identifiable violations, this\nnew situation requires a more multifaceted approach\nthat can address the breadth of actors and possibilities\ninherent in contemporary data collection and use. By\nidentifying the new ways in which power is inscribed in\nlarge-scale digital data, we can better debate what we\nwant and do not want from the information we emit\nabout ourselves. The next section will explore two\nexamples from the public\u00adprivate interface of datafica-\ntion that illustrate the ways in which that interface may\nbe a locus of structural discrimination (embedded in\ninstitutions, rules and practices) that is also intersec-\ntional (multiplying disadvantage to people due to inter-\nsecting aspects of their identity).\nIdentifying data injustices\nI will explore the problem of data-driven discrimination\nusing two illustrative cases, both of which demonstrate\nthat a specific articulation of social justice is now\nrequired with regard to contemporary data technolo-\ngies. The first case is that of India's biometric popula-\ntion database, known as Aadhaar. The database is the\nworld's largest with over a billion records and was\nlaunched in 2009 with the stated aim of combating wel-\nfare fraud by allowing those below the poverty line to\nprove their identity with a fingerprint or iris scan when\ncollecting entitlements. However, the design of the tech-\nnologies that enable inclusion in the system \u00ad iris and\nfingerprint scanners and the networks, wired and\nhuman, that translate inputs of data into outputs of\nconfirmed identities \u00ad in fact ensures that the poorest\nare the worst served by Aadhaar.\nThe system's design does not acknowledge the\nmateriality of poverty, being unable to `authenticate\nthose who work with stone, cement, limestone and\noften have no fingerprints due to hard labour, or\nusable iris scans due to malnutrition. It also misses\nthe day-to-day precarity of poor people's existence by\nonly allowing each family's single registered claimant to\ndraw rations, so that if that claimant is sick, working or\notherwise unable to come to the ration provider, the\nfamily cannot access its allocation (Priya and Priya,\n2016). Moreover, the backup authentication system\noperates by sending a password to the registrant's\nmobile phone, thus excluding the poor who cannot\nafford a phone, or anyone who has not written down\nthe number they had when they enrolled (Yadav, 2016).\nIt also increases the bureaucratic burden of poverty,\nsince despite compulsory participation there is no way\nfor people to correct entries in the database on the local\nlevel. There is no independent oversight in terms of\naddressing technological faults: the redress system\nrefers people back to the Unique Identification\nAuthority of India, the agency that runs Aadhaar,\nbut there is no legal obligation for the authority to\nprovide a solution to authentication problems, leaving\nthem instead to individual citizens and local ration\nshops to resolve (Thikkavarapu, 2016). Despite its\n4 Big Data & Society\nunresponsiveness to registrants, the database does,\nhowever, make it possible for the ultra-poor to be\ntransformed into consumers: its chairman has said\nthat he envisages it as having strong potential for\ndirect marketing to registrants (Nilekani, 2013) and\nplans are underway to partner with Google so the\nfirm can reach and profile its `next billion users'\nThe problem of Aadhaar's uneven burden on the\npoor was illustrated by the government's 2016 demon-\netisation, which threw the cash-based elements of\nIndia's economy into chaos, and thus also the lives of\nthe poor and marginalised. The demonetisation put\ndemands on automated payment systems in ways that\nwere discriminatory against the poor, since they had the\nleast access to mobile phones, formal saving and bank-\ning systems, and applications that could help tide them\nover the cash crisis that ensued \u00ad and suffered the high-\nest cost if Aadhaar-related technologies failed to iden-\nAadhaar is what Johnson (2014) in his work on\ninformation justice terms a `disciplinary system'. It\nraises several issues to do with justice that are specific\nto its use of data technologies, specifically the way it\nrecords, stores and processes registrants' data. First, at\nthe point of collection and processing claims the system\nforces registrants to confirm to a `standard of nor-\nmalcy' (Johnson, 2014) by having legible fingerprints\nand irises, by possessing mobile phones, by having a\nstable family life where the same registrant can collect\nrations from week to week, among other standards.\nThese standards point to a middle-class standard for\nnormality rather than the precarity and unpredictabil-\nity of the lives of the poor. Second, it raises problems of\ndistributive fairness. Although the claim is made that\nAadhaar furthers distributive justice by reducing cor-\nruption in welfare transactions and giving the poor\naccess to previously inaccessible services and represen-\ntation, in fact it offers radically different possibilities\ndepending on one's resources and socio-economic\nstatus. Third, the system amplifies inequality: for\nricher citizens, it is a way to ease one's passage through\nthe world. One can acquire a phone or a utility account,\nprove one's identity in everyday transactions and sim-\nplify dealings with the bureaucracy. For poorer citizens,\noften lower caste and/or female, it is a way of formalis-\ning precarity. For those whose bodies the system\ncannot process, or for those whose identity is misread,\nthere is no apparent path back to administrative legi-\nbility. Finally, it does not allow for fair redress of abuse\nor grievances. The complaints procedure is not\ndesigned for emergencies: instead of access to a local\nofficial, problems must be directed by phone or email to\na processing centre with `no timelines, no designated\ngrievance redress officers, no written dated\n"
}