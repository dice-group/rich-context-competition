{
    "abstract": "Abstract\nIn this results-oriented era of accountability, educator preparation programs are called upon to provide comprehensive\ndata related to student and program outcomes while also providing evidence of continuous improvement. Collaborative\nAnalysis of Student Learning (CASL) is one approach for fostering critical inquiry about student learning. Graduate educator\npreparation programs in our university used collaborative analysis as the basis for continuous improvement during an\naccreditation cycle. As authors of this study, we sought to better understand how graduate program directors and faculty\nused collaborative analysis to inform practice and improve programs. Our findings suggested that CASL has the potential\nto foster collective responsibility for student learning, but only with a strong commitment from administrators and faculty,\npurposefully designed protocols and processes, fidelity to the CASL method, and a focus on professional development.\nThrough CASL, programs have the ability to produce meaningful data related to student and program outcomes and meet\nthe requirements for accreditation.\n",
    "reduced_content": "sgo.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License\n(http://www.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of\nthe work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nSpecial Issue - Accreditation\nIntroduction\nIn this results-oriented era of accountability, educators at all\nlevels are increasingly evaluated by their impact on student\nachievement. Reeves (2007) noted that educators exist in a\nworld where too often assessment is equated with high-\nstakes testing. This narrow view of assessment has reached\nits way into the arena of educator preparation (Norman &\nSherwood, 2015). Outside agencies evaluate quality of data\nwith, at times, little correlation to student and program effec-\ntiveness (Fuller, 2014). Value-added measures that assess the\nperformance of educator preparation programs using K-12\nstudent data are increasingly popular (Darling-Hammond,\npotential \"for good and for ill\" (Harris & Herrington, 2015,\np. 71). The Council for the Accreditation of Educator\nPreparation (CAEP) discussed these practices in their\nrecently drafted CAEP Accreditation Handbook:\nMany states are moving toward linking P-12 student achievement\nback to a teacher-of-record--and to the provider that prepared\nthat teacher. They also are initiating data systems that collect\ninformation on other dimensions of educator preparation provider\nperformance, such as those demonstrated by metrics associated\nwith completers' performance, employer and completer\nsatisfaction, and teacher evaluations that can be linked to\ncompletion, licensure, and employment rates. (CAEP, 2016, p. 6)\nCAEP (2016) had, however, attempted to \"increase the value\nof accreditation\" (p. 5) by expanding accreditation evidence\nto include data related to continuous program improvement.\nEducator preparation providers now submit evidence of\ncompleter proficiency and evidence that a \"culture of evi-\ndence\" has been created that promotes \"the practice of using\nevidence to increase the effectiveness of preparation pro-\ngrams\" (CAEP, 2016, p. 6). Given this new reality, it is\nincreasingly important for educator preparation providers to\ndevelop methods, tools, and processes that accomplish mul-\ntiple purposes.\nThe journey to create a meaningful and coherent assess-\nment system for our educator preparation programs began 3\nyears ago prior to our college's most recent National Council\nfor Accreditation of Teacher Education (NCATE) review.\nCollege administrators and program directors were chal-\nlenged to develop an assessment system that would meet\nunit-level accreditation requirements while meeting the\nneeds of numerous programs with discipline-specific\nstandards and practices. Fullan and Quinn (2016) offered\n1Appalachian State University, Boone, NC, USA\nCorresponding Author:\nSusan Colby, Reich College of Education, Appalachian State University,\nEmail: colbysa@appstate.edu\nUtilizing Collaborative Analysis of\nStudent Learning in Educator Preparation\nPrograms for Continuous Improvement\nSusan Colby1, Monica Lambert1, and Jennifer McGee1\n Keywords\neducator accreditation, teacher education, program evaluation\n2 SAGE Open\nguidance for attending to these multiple demands: \"Internal\naccountability must precede external accountability if lasting\nimprovement in student achievement is the goal\" (p. 111).\nFullan and Quinn further explained this shift from compli-\nance to collective responsibility:\nConstantly improving and refining instructional practice so that\nstudents can engage in deep learning tasks is perhaps the single\nmost important responsibility of the teaching profession and of\neducational systems as a whole. In this sense, accountability as\ndefined here is not limited to mere gains in test scores but on\ndeeper and more meaningful learning for all students.\" (p. 110)\nAs administrators and program directors worked to create a\nsound assessment system, Hattie's (2012) Visible Learning\n(VL) research provided a starting point.\nSearching for an Approach: The VL Framework\nHattie's VL research was based on a synthesis of 1,200 meta-\nanalyses from over 50,000 individual studies. Each research\nstudy examined the influence of a program, policy, or inno-\nHattie's VL research was the largest meta-analysis ever con-\nducted in the field of education and provided insight into\nwhat educators should, and should not, focus on in their\nefforts to improve practice. The key finding from this\nresearch was learning should be the explicit and primary goal\nand teachers need to \"know thy impact\" (Hattie, 2012).\nEducators should see their fundamental role as an evaluator\nof their own effect on students (Hattie, 2015). Student learn-\ning is increased (a) when teachers believe their major role is\nto evaluate their impact and (b) when teachers work together\nHattie's VL research provides guidance for educator prep-\naration programs that not only wish to study their impact, but\nare also required to do so for accreditation. Hattie translated\nhis findings for higher education: \"Faculty [in higher educa-\ntion] need to go beyond merely collecting data, creating\nreports, and asking students to fill in surveys, but to become\nexcellent interpreters of evidence about their impact\" (Hattie,\n2015, p. 89). Hattie's findings had implications for our educa-\ntor preparation programs. To create a culture of evidence, our\ncollege needed to build capacity in our programs so faculty\nwould have the skills, data, and structures for studying impact\nmethod of building capacity for professional learning is\nthrough the use of professional learning communities (PLCs).\nEstablishing PLCs.PLCs build the collective capacity of\nstakeholders to improve practice and have become the norm\nin schools since the early work of DuFour and Eaker (1998).\nDuFour and Eaker believed that PLCs are \"the most promis-\ning strategy for sustained, substantive school improvement\"\n(p. xi). Researchers and scholars found PLCs a promising\ntechnique to improve practice and increase student learning\n(Mitchell, 2014; Ronfeldt, Farmer, McQueen, & Grissom,\non PLCs, stated,\nIf there is anything that the research community agrees on, it is\nthis: The right kind of continuous, structured teacher\ncollaboration improves the quality of teaching and pays big,\noften immediate, dividends in student learning and professional\nmoral in virtually any setting. (p. xii)\nUtilizing PLCs as a strategy to foster reflection on prac-\ntice in a continuous improvement cycle seemed evident\ngiven the core principles of PLCs: (a) ensuring that students\nlearn, (b) creating a culture of collaboration for improve-\nment, and (c) focusing on results (DuFour, 2015; DuFour &\nEaker, 1998). These core principles aligned with CAEP stan-\ndards and the PLC literature provided direction for how edu-\ncator preparation programs in our college could engage in\ncollective inquiry. Ronfeldt et al. (2015) examined the impact\nof collaboration on student learning in a large-scale study\nand found while naturally occurring collaboration across a\nwide range of contexts is beneficial, the strongest student\nachievement gains resulted from collaboration focused on\nassessment. Van Lare and Brazer's (2013) sociocultural\nmodel conceptualized how teachers learn in a PLC and pro-\nposed that learning must be the focus of a PLC in order to\nmove beyond the routine outcomes of consistency and pre-\ndictability. Gallimore, Ermeling, Saunders, and Goldenberg\n(2009) found a positive impact on student learning when set-\ntings were stable, teachers were focused on concrete learning\ngoals, and progress was tracked. Most importantly, they\nfound momentum increased after teachers saw the positive\nresults of their collaboration on student learning. In the\nhigher education arena, Mitchell (2014) found that PLCs\nafforded academics the ability to engage in meta-cognitive\nprocesses that helped faculty better understand the impact of\ntheir teaching on their students. These studies confirmed the\nimportance of creating inquiry-focused protocols and struc-\ntured organizational routines to guide efforts for critical,\nrecursive reflection (Gallimore et al., 2009; Thompson,\nThe PLC model provided our college with a sustainable\nstructure for collaborative action; however, impact on stu-\ndent learning cannot be determined without sufficient skill in\nanalyzing multiple sources of data and drawing accurate con-\nclusions. One challenge that our college encountered was\ndeveloping capacity for data-based decisions.\nDeveloping capacity for effective use of data. Educational organi-\nzations struggle with similar problems when using data to\ninform decision making: a lack of access or ability to select\nappropriate data, a lack of skill in the use of data, and a lack of\ncollaboration around the use of data (Schildkamp, Karbautzki,\nColby et al. 3\nrized this issue: \"Most educators lack efficient, flexible access\nto these mountains of new data and have been afforded little\npreparation for productive organization and analysis of these\ndata\" (p. 464). Regardless of these limitations, public school\nsystems have long been expected to use data for decision mak-\ning and this practice is likely to persist (Jimerson, 2014; Marsh\n& Farrell, 2015). CAEP's focus on evidence-based measures\nfor accreditation echoes this growing trend (CAEP, 2016).\nThe K-12 literature was helpful in providing direction\nabout how to build capacity for data-based decision making.\nThe literature outlined the skills educators need for data-based\ndecisions (Staman, Visscher, & Luyten, 2014; Wayman &\nJimerson, 2014) and described effective practices for building\ncapacity (Marsh & Farrell, 2015). While effective professional\ndevelopment has the potential to yield positive results (Staman\net al., 2014), it is difficult to make decisions about what it\nshould look like. Data-driven decision making is complex and\nrarely linear (Wayman & Jimerson, 2014) and educators must\nemploy a set of data literacy skills in order to make sound\njudgments. Utilizing these skills in a culture of collaborative\ninquiry is essential for success (Marsh & Farrell, 2015). Much\nof the utility in using data for improvement depends on the\nability to support collaborative critical inquiry for improving\net al., 2014). Professional learning strategies such as modeling\ndata experts (Schildkamp et al., 2014) have proven to be suc-\ncessful. Norms should be established that foster shared expec-\ntations and coordinated conversations around data use (Coburn\nUsing Hattie's (2015) principle \"know thy impact\" as a\nfocus, our college began searching for a method that utilized\nPLCs to make effective data-based decisions about our edu-\ncator preparation programs. One method that translated these\nprinciples into practice was the Collaborative Analysis of\nStudent Learning (CASL; Colton, Langer, & Goff, 2015).\nSelecting a Method: The CASL\nThe CASL\"is a professional learning design in which teacher\ngroups analyze student work samples and assessments to\nlearn how to effectively support students' learning of com-\nframework provides guidance for instructor teams that exam-\nine the strengths and weaknesses of student responses, deter-\nmine whether students meet standards and objectives, and\nidentify implications for improvement (Langer, Colton, &\nGoff, 2003). Instructors look for \"clues in the work\" (Langer\net al., 2003, p. 34) that can provide helpful information for\ndata-informed decisions about student learning and instruc-\ntion. Student work is defined as \"any data or evidence teach-\ners' collect that reveals information about student learning\n(e.g., standardized test data, classroom assessments, writing\nsamples, projects, oral reports, videotapes, pictures, or stu-\ndent observation data)\" (Langer et al., 2003, p. 4). Critical to\nthe reflective inquiry process is the need for a \"trusting, col-\nlaborative environment\" (Langer et al., 2003, p. 27) \"without\nfear of being judged or criticized\" (Colton et al., 2015, p. 40).\nIn this supportive environment, teachers are asked to con-\nsider other viewpoints as they construct meaning that goes\nbeyond more immediate assumptions and generalizations.\nCASL is grounded in decades of work fostering reflective\nand collaborative inquiry in teachers as a means to improve\nstudent learning. CASL research began in 1986 with gradu-\nate students (Langer et al., 2003; Sparks-Langer, Simmons,\nPasch, Colton, & Starko, 1990) and progressed to work in\npublic schools. CASL had strong connections with peer\ncoaching models (Costa & Garmston, 2002; Stallings,\nNeedels, & Sparks, 1987). The insights learned from CASL\nwere used to inform the National Board for Professional\nTeaching Standards (NBPTS) and for professional develop-\nment for public school teachers. At the classroom level,\nCASL can assist teachers in interpreting student learning evi-\ndenced in assessments, identifying areas for improvement,\nand reflecting on teaching practice. At the program/school\nlevel, CASL can be used to determine collective student\nlearning; examine alignment among curriculum, instruction,\nand assessment; and identify areas for program improvement\n(Langer et al., 2003). This reflective thinking results in a\nclearer vision of needed improvements (Sparks-Langer et al.,\nPurpose of the Study\nThe purpose of this study was to examine how collaborative\nanalysis was used in our college's graduate accredited pro-\ngrams for continuous program improvement. The research-\ners sought to understand how graduate programs used results\nfrom collaborative analysis to inform practice and what facil-\nitators of collaborative analysis sessions learned from the\nprocess. The research questions for this study were the\nfollowing:\nResearch Question 1: How do program directors and\nprogram faculty use results from CASL to inform practice\nin a continuous improvement cycle?\nResearch Question 2: What can we learn from the expe-\nriences of program directors who have facilitated collab-\norative analysis sessions?\nThis research study and the corresponding protocols\nreceived approval from our Office of Research Protections\nInternal Review Board.\nMethod\nThis qualitative research study was conducted in a college of\neducation at a midsize comprehensive university with a large\neducator preparation program that graduates approximately\n4 SAGE Open\nboth the undergraduate and graduate level are NCATE\napproved. Our college is currently transitioning to CAEP\nstandards and will undergo CAEP accreditation in our next\ncycle. One CAEP principle in particular provided an impetus\nfor our work, \"There must be solid evidence that the provid-\ner's educator staff have the capacity to create a culture of evi-\ndence and use it to maintain and enhance the quality of the\nprofessionalprogramstheyoffer\"(CAEP,2015,Introduction).\nTo more closely align CASL with the multiple measures\nrequirement of CAEP, the original student work definition\nwas broadened to include collaborative analysis of any appro-\npriate data source (e.g. survey data, enrollment and gradua-\ntion numbers, graduate student evaluations).\nResearch Design\nQualitative researchers typically gather data from multiple\nsources to help ensure accurate results. Researchers \"review\nall of the data, make sense of it, and organize it into categories\nor themes that cut across all of the data sources\" (Creswell,\nauthors of this article, consisted of two college administrators\nand one former director of teacher education assessment. The\nteam analyzed documents and survey responses from graduate\nprogram directors to establish emerging themes and draw con-\nclusions related to the research questions. For the document\nreview, the team analyzed assessment reflections from gradu-\nate program directors about collaborative analysis sessions.\nAssessment reflections provided documentation of continuous\nprogram improvement with program area faculty for accredi-\ntation purposes. Assessment reflections served as the primary\ndata source for this study. Creswell (2014) noted that there are\nadvantages and limitations to using each type of data in quali-\ntative research. An advantage of document review is that the\noriginal language becomes the source for analysis. Documents\nhave limitations, however, and participants may not be equally\narticulate or perceptive. Further, a document may not compre-\nhensively capture an event (Creswell, 2014). Survey responses\nto open-ended questions from program directors about their\nexperiences facilitating collaborative analysis sessions served\nas a secondary data source for this study. The research team\nfound the advantages of survey research to be particularly\nhelpful in our study due to the economy of the design and the\nrapid turnaround in data collection (Creswell, 2014). For this\nstudy, researchers were able to sample program directors effi-\nciently about their experiences at the close of the assessment\nreflection cycles.\nParticipants\nThe participants in this study (n = 13) were graduate program\ndirectors from NCATE accredited educator preparation pro-\ngrams. A single-stage sampling procedure was utilized as the\nresearchers had access to all of the names in the population\nand were able to access the documents and participant per-\nceptions directly (Creswell, 2014). These graduate programs\nincluded Curriculum Specialist, Elementary Education,\nHigher Education, Educational Media and Technology,\nLibrary Science, Mathematics Education, Middle Grades\nEducation, Music Education, Professional School\nCounseling, Reading Education, Romance Languages,\nSchool Administration, and Special Education. Program\ndirectors were tenure-track faculty selected by departmental\nchairs and affiliated program faculty due to their program\nexpertise and administrative skills. Program directors\nattended college level meetings and worked closely with\ndepartmental chairs and deans to manage programs. Program\narea faculty, while not direct participants in this study, did\nparticipate in collaborative analysis sessions which served as\nthe basis of the assessment reflections. Program area faculty\nwere primarily tenure-track faculty that taught courses in the\nprogram and participated in program area meetings. Program\ndirectors and program faculty met regularly to discuss advis-\ning, curriculum, program assessments, and other program-\nrelated topics. Participants had no prior knowledge of CASL\ntechniques and most programs did not routinely review stu-\ndent work for program improvement purposes.\nInstruments\nThe data collected and analyzed for this study included\nassessment reflections based on collaborative analysis ses-\nsions and survey responses from program directors describ-\ning their experiences as facilitators of collaborative sessions.\nThe assessment reflection was developed by the director of\nteacher education assessment. The assessment reflection\ntemplate included a series of prompts to assist programs in\nattending to the various stages in the collaborative analysis\nprocess. The survey for program directors was developed by\nthe authors of this study. The survey was cross-sectional and\ndata were collected electronically using university approved\nsurvey software for research. Both the assessment reflection\ntemplate and the survey for program directors were approved\nby Institutional Review Board (IRB).\nAssessment reflections.Each program director was asked to\ncomplete two assessment reflections based on collaborative\nyear. The assessment reflection template was designed to\nstimulate collaborative inquiry using multiple sources of data\nand to provide a framework for the written reflection. Guid-\ning questions assisted the program director in leading a dis-\ncussion around five key areas. For the first section, program\ndirectors were asked to list the action item(s) for the session.\nProgram directors identified topics for discussion that were\nbased on program priorities. In the second section, program\ndirectors reported on the status of past improvements. Pro-\ngram directors were asked to describe the status of changes\nthat were made to the program as a result of recent reviews. In\nColby et al. 5\nthe third section, program directors cited the data sources that\nwould be reviewed in the current session. These data sources\nwere to align with the action items. For the fourth section,\nprogram directors described how data were interpreted.\nDirectors were to report on what was learned about student\nperformance and how the curriculum impacts student perfor-\nmance based on a review of data. In addition, program direc-\ntors were to identify which program goals and objectives\nfindings applied. In the final section of the written reflection,\nprogram directors responded to a series of questions about\nprogram improvement. Directors were asked to describe the\nchanges that the program would make based on the analysis\nof data. Directors were prompted to specifically describe\nchanges to the curriculum and the assessment system. Direc-\ntors were also asked to provide detail on the next steps.\nThe assessment reflections were discussed in program\ndirector meetings in the fall of 2014. The dean of the college\nexpressed the need for programs to use a common data source\nfor providing evidence of continuous improvement and intro-\nduced the assessment reflection template. Program directors\nwere informed that data would be aggregated across pro-\ngrams. Program faculty were to engage in multiple cycles of\ncollaborative analysis for continuous improvement purposes.\nSurvey responses. Survey responses from program directors\nabout their experiences as facilitators of collaborative analy-\nsis sessions served as the secondary data source for this\nstudy. Survey responses were anonymous. There were five\nopen-ended survey items related to facilitating collaborative\nanalysis. The first question asked program directors if they\nfound collaborative analysis a useful process. They were\nasked to provide a rationale for their response. The second\nquestion asked program directors if the prompts provided in\nthe assessment reflection proved useful in guiding the col-\nlaborative discussions. Again, directors were asked to pro-\nvide a rationale for their answer. The third question prompted\ndirectors to describe the level of difficulty with conducting\ncollaborative analysis sessions. Program directors, for the\nfourth question, were to describe the challenges they faced in\nleading a group of faculty through this process. Last, direc-\ntors were asked for their suggestions on what would help\nthem facilitate the process better in the future.\nSurveys were sent electronically to program directors in\nearly spring 2016 after two cycles of collaborative analysis\nsessions. The introduction to the survey included a descrip-\ntion of the research study, the purpose for the study, the\nresearchers involved, and an overview of the survey. The\nsurvey overview and questions were IRB approved. Program\ndirectors were given a choice to opt out of the survey. The\nsurvey remained open for 2 weeks.\nData Analysis\nCreswell's (2014) six-step process for analyzing qualitative\ndata was used as a guiding framework: (a) organizing and\npreparing the data for analysis, (b) reading for a general\nsense of the information, (c) coding of all data, (d) identify-\ning themes from the coding and searching for theme connec-\ntions, (e) representing the data, and (f) interpreting the larger\nmeaning of the data. The researcher's purpose was to deter-\nmine how faculty were using collaborative analysis to reflect\non student learning and determine areas for improvement. In\naddition, the researchers hoped to better understand the\nexperiences of program directors as facilitators of collabora-\ntive analysis sessions.\nDuring Phase 1 of data analysis, the researchers analyzed\nthe assessment reflections. First, each researcher indepen-\ndently open-coded the assessment reflections from the fall of\n2014. The research team met on several occasions during the\ncoding process to discuss findings and to cross-check devel-\noping codes. Once individual coding was completed, the\nresearch team met to discuss findings and create a list of\ncodes that appeared across the fall assessment reflection nar-\nratives. Assessment reflections from the spring of 2015 were\nanalyzed using the same process. The research team met to\nrefine the codes and check for themes emerging from both\nsets of data.\nIn Phase 2, the same process was followed for analyzing\nthe survey data. The researchers each independently coded\nthe survey data collected from anonymous program direc-\ntors. The research team then met to determine the emerging\npatterns evident in the set of responses.\nAfter all three data sources were coded, the researchers\nmet to discuss emergent themes across the data collection\npoints. The researchers finalized the primary themes related\nto the research questions and developed conclusions and\nimplications for the study.\nIt is well known that unconscious bias can be a limitation\nor source of error when analyzing and interpreting data. Few\nresearchers achieve complete objectivity (Best & Kahn,\n1998). Because this study was primarily qualitative, it was\nimportant for the researchers to examine their own bias and\npotential influence on the study. The research team incorpo-\nrated multiple strategies to enhance the accuracy of the find-\nings. These strategies included triangulation, consideration\nof bias, reporting of negative and discrepant information,\nand cross-checking of codes during analysis (Creswell,\n2014). The research team collected data from two data\nsources to develop emerging themes and build a cohesive\ninterpretation of the data. Because the assessment reflection\ntemplate was created by the current assessment director with\nno input from the research team, bias was not a factor in the\ndesign of the template or in the responses recorded in the\nassessment reflections. In addition, program directors were\nrequired to complete assessment reflections, and participa-\ntion rate was not influenced by the researchers. To mitigate\nresearcher bias during the analysis, assessment reflections\nwere not reviewed until both cycles were completed.\nFurthermore, the research team met on several occasions\nduring the coding process to ensure interrater reliability by\n6 SAGE Open\ncross-checking codes and reconciling discrepancies. The\nresearch team did, however, develop the survey and there-\nfore greater potential existed for influencing the data and the\ninterpretation of the data. The research team disclosed the\npurpose of the survey in the survey introduction and high-\nlighted that there were no consequences for choosing not to\nparticipate.\nResults\nIn total, 21 collaborative analysis sessions in 13 graduate\nprograms were held over the course of the academic year.\nProgram directors submitted assessment reflections for fall\nfor the assessment reflections was higher in the fall as a\nresult of a stronger effort for compliance. The dean sent out\nmultiple reminders in the fall to program directors to com-\nplete the assessment reflections. The number of program fac-\nulty participating in the collaborative analysis sessions\nranged from two to 18 depending on the size of the program.\nAfter coding the assessment reflections and identifying\nemerging themes, the research team found it helpful to note\nfrequencies for specific topics discussed, types of data used\nin analysis, and action steps identified by program faculty.\nSurveys were sent electronically to the 13 graduate pro-\ngram directors after all collaborative analysis sessions were\nconducted. Two reminders were sent by the associate dean,\none of the researchers for this study. Six program directors\ncompleted the anonymous survey. Seven program directors\nopted out of the survey. Surveys were coded after a 2-week\nopen window. The researchers were aware of the potential\nfor nonresponders to influence the overall survey data\nFindings were categorized into three primary themes:\nOutcomes of Collaborative Analysis, Use of Data for\nCollaborative Analysis, and Facilitating Collaborative\nAnalysis. These themes aligned with the research questions.\nFindings from the analysis of the assessment reflections pri-\nmarily informed Research Question 1: How do program\ndirectors and program faculty use results from CASL to\ninform practice in a continuous improvement cycle? Findings\nfrom the analysis of the survey responses primarily informed\nResearch Question 2: What can we learn from the experi-\nences of program directors who have facilitated collabora-\ntive analysis sessions?\nOutcomes of Collaborative Analysis\nEach assessment reflection opened with the action item(s)\nfor the collaborative analysis session. Action items were spe-\ncific and served as the focus for the session. A survey of the\naction item topics revealed that programs had conversations\naround course development, curricular changes, and student\nproducts. Action items separated into two categories: items\nrelated to program goals and items related to student learning\noutcomes (SLOs). Program-related action items included\naligning documents and protocols to standards, revising\nassignment guidelines and rubrics, and examining and\napproving unit-level standardized rubrics. Student learning\naction items included implementing strategies for promoting\ndeeper critical thinking, integrating skills in student prod-\nucts, and addressing perceived student deficiencies.\nPrograms conducted collaborative analysis sessions using\na variety of data sources aligned with the action item. Use of\ndata is explained in the second theme below. Program direc-\ntors and faculty reached consensus at the conclusion of each\nsession about recommendations for improvement. These rec-\nommendations were recorded in the reflections. A review of\nthe continuous improvement section of the assessment\nreflections revealed that programs identified a wide range of\naction steps. A majority of the programs intended to review\nand revise current rubrics or create new rubrics. These pro-\ngrams cited a need for continuous monitoring of goals. In\naddition, many programs realized consensus was needed by\nprogram faculty on course expectations, assignments, and/or\nevaluations. Specific actions included revisions to assign-\nments, rubrics, and courses. A few programs realized they\nlacked important program documents such as curriculum\nmaps or assessment plans and action steps were included for\nthe development of these program documents.\nUse of Data for Collaborative Analysis\nThe prompts on the assessment reflection template were\nintended to elicit group discussions about student learning\nand program quality. Program directors were asked to cite\nthe data sources used for the review and describe what was\nlearned through collaborative analysis. Programs varied\ngreatly in the type of data used for collaborative analysis and\nhow data were interpreted.\nData sources cited. A review of the 21 assessment reflections\nrevealed that a total of 37 data sources were identified\nthroughout the reports. These data represented five types:\nstudent assessments (62%), graduate student exit surveys\nand interviews (19%), evaluations of faculty and supervisors\n(8%), documents (5%), and other (5%). Data from student\nassessments were predominantly used to make decisions\nabout program improvement.\nFrom the student assessment category, the assessment\nmost often cited for analysis was the Product of Learning\n(POL). The POL was the capstone assessment for all NCATE\naccredited graduate programs and consisted of a collection of\nartifacts and accompanying synthesis reflections providing\nevidence that five state standards were met. The POL rubric\nwas common across all programs, and results from each cri-\nterion on the POL rubric were aggregated for unit-level\naccreditation. Although the rubric for all programs was the\nsame, the contents of the portfolio and the synthesis reflec-\ntions varied depending on program. Each program had the\nColby et al. 7\nlatitude to create course and program assignments that met\nspecific objectives. A second source of assessment data often\nused in the collaborative analysis sessions was the depth of\ncontent evidence. This evidence was also required at the state\nlevel and each graduate program required students to demon-\nstrate discipline-specific depth of content. A third student\nassessment data source cited was capstone course assess-\nments. Multiple program directors used data collected from\nrubrics to determine next steps for improvement. Other\nassessments, such as comprehensive exams, portfolios, and\nterm papers, were cited less frequently.\nIt was evident that programs relied heavily on aggregated\nquantitative data when discussing student learning. These\ndata were gathered from ratings on proficiency levels (does\nnot meet, meets, exceeds expectations) for specific criteria\nfrom the assessment rubrics. Nearly all programs analyzed\ndata from multiple criteria on the rubric. Most programs used\nthis quantitative data without examining individual student\nwork to better understand the results. For example, one grad-\nuate program (GP 4) stated, \"Our data from the previous\nsemester demonstrates that most graduate students are per-\nforming at `proficient' or better on all assessment points of\nprogram (GP 2) stated, \"We achieved the first of our two\ncriteria for graduate student demonstration of depth of con-\ntent knowledge with 100 percent of our students (n = 19)\nachieving a score of 8 or higher, indicating proficient perfor-\nmance.\" This type of reporting shows a cursory glance at\naggregated data devoid of an analysis of student work. This\nwas true for nearly all programs that used assessment data.\nOnly one graduate program (GP 11) indicated that they had\nreviewed individual student products:\nIn previous Product of Learning [final project] presentations we\nhave noticed that non-native speakers of the target language do\nnot always have a strategy in place to make ongoing, independent\nprogress in their target language skills outside of our graduate\nclasses, thus they struggle somewhat in the completion of target\nlanguage papers, oral presentations, or both.\nThis graduate program reviewed student work from multiple\nassignments to determine what students were learning.\nProgram faculty found a difference in the use of target lan-\nguage between graduate students with and without interna-\ntional experiences. The program now strongly recommends a\nvariety of international experiences for second-language\nlearners to enhance their target language competence.\nA second category of data sources was graduate student\nperceptions collected from surveys and interviews. These\ndata provided programs with rich descriptions about pro-\ngram strengths, weaknesses, and areas for improvement.\nPrograms relied on survey and interview data to provide con-\nfirmation and triangulation. For example, one graduate pro-\ngram (GP 2) used survey responses to assist in developing a\nprogram of study for an accelerated admissions program.\nData provided the program with feedback about which\ncourses were not appropriate for the condensed summer\nschedule and which courses were not appropriate for a dou-\nble-count (undergraduate and graduate). Student perceptions\nalso revealed that more technology integration was needed\nthroughout the course work. The exit survey data from a sec-\nond graduate program (GP 9) revealed that graduate students\nneeded earlier exposure to specific skills in order for mastery\nby graduation.\nA third category of data related to faculty. These data\nincluded quantitative and qualitative data from course evalu-\nations of teaching, field supervisor evaluations, and faculty\nperceptions related to courses and/or program. One graduate\nprogram (GP 6) used end of course evaluations from a new\ncourse to offer student perceptions about course effective-\nness. The qualitative comments were helpful in determining\nwhat students were gaining from the course and how the\ncourse could be refined to better meet student needs. A sec-\nond graduate program (GP 9) used graduate student evalua-\ntions of university and school-based field supervisors to\nevaluate the program. In this program, high-quality field\nexperiences are crucial to gaining knowledge and skills\nrequired in the field. Data were used to determine whether\nsupervisors were effective in their positions and if changes\nneeded to be made in the selection and mentoring of supervi-\nsors. In addition, one graduate program (GP 7) used faculty\n(instructor) perceptions about the quality of writing from\nspecific course assignments to make recommendations about\nhow to improve the quality of student writing throughout the\nprogram.\nThe fourth and fifth data categories, much less frequently\ncited, referred to a review of documents and grades. A total\nof four programs used these two data sources to determine\nareas for improvement. Interestingly, neither of the graduate\nprograms that reviewed grades (GP 1 and GP 12) found stu-\ndent grades useful in their deliberations about student learn-\ning. Faculty in two graduate programs (GP 3 and GP 5)\nanalyzed the alignment of program documents to state and/or\nnational standards. Both of these programs did report areas\nfor improvement based on their review of documents.\nInterpretation of data.All program directors were able to\ndescribe how they interpreted meaning from the data sources\ntheir programs reviewed. There was strong alignment\nbetween the sources cited and the interpretation of these\ndata. A review of the 21 assessment reflections revealed a\ntotal of 51 specific findings about how data were interpreted\nand would be used for program improvement. These findings\nseparated into four categories and were related to the follow-\ning: assessments (62%), program improvement (21%), sup-\nporting students (12%), and faculty (5%).\nNearly all comments in the interpretation section\ndescribed findings related to assessments. This was not sur-\nprising given that a majority of data reviewed were from pro-\ngram assessments. Half of the interpretive comments\n8 SAGE Open\nreported proficiency levels of students on course assessments\nand satisfaction with the percentage of graduate students\nrated as meeting and exceeding specific criteria. The second\nhalf of assessment related comments focused on rubrics.\nProgram directors explained how data provided evidence\nthat rubrics needed refining. Typically, rubric revisions were\nneeded to yield more variance or more specificity in the\ndescriptors. A few directors noted that there was duplication\nin rubrics, and one director reported that a large amount of\nmissing data made it difficult to determine next steps.\nA second set of findings were related to the overall pro-\ngram. Nine program directors recommended changes in the\nprogram of study or in a specific course. A few directors\nnoticed there was duplication in courses while others noted\ncourses that were no longer useful based on changing stan-\ndards and practices in the field. A couple of directors stated\nthat new data sources were needed to better make decisions\nabout program improvement. One director noted that there\nwas a large discrepancy between graduate student self-report\nand faculty ratings on dispositions. This program intended to\nlearn more about this disparity.\nA third set of findings focused on students. Programs\nunderstood the need to develop and implement formative\nchecks for capstone portfolios and products. A couple of pro-\ngram directors identified a need for more scaffolding, sup-\nport, and remediation of students enrolled in the program.\nOne program found a need to better meet the needs of differ-\nent student groups.\nThe last set of findings were related to faculty. After\nreviewing various data sources, programs learned that fac-\nulty needed to be more consistent in their teaching and when\ngrading program assessments.\nConclusions about use of data.After reviewing all of the\nassessment reflections for findings related to the use of data,\nit was evident that programs consulted a wide variety of data\nto make decisions about program improvement. As evi-\ndenced in the first assessment reflections, the focus of many\ncollaborative analysis sessions shifted from a discussion\nabout student learning to a discussion about the program.\nImplications from a review of these quantitative rubric rat-\nings generally resulted in recommendations about revising\nthe rubric with little confirmation from the examination of\nstudent work. Only one program analyzed actual student\nwork. In general, programs selected important program\nassessments and appropriate data sources, but nearly always\nmade their decisions about how to improve their program\nbased on quantitative scores rather than rich, descriptive\nqualitative findings that emerged from examining student\nwork.\nPrograms that participated in two collaborative analysis\nsessions appeared to have stronger results in the second ses-\nsion. They posed fewer questions and used less data sources\nbut produced deeper reflections. One program director\nreflected on this progression in the survey: \"It [the\ncollaborative analysis session] was difficult both times, but\neasier the second time, mainly because I had a better idea of\nhow to use it more to my and the program's advantage.\"\nFacilitating Collaborative Analysis\nProgram directors were asked to complete a survey about\ntheir experiences as a facilitator. The first question asked\ndirectors whether collaborative analysis was a useful pro-\ncess. Overwhelmingly, directors indicated that collaborative\nanalysis sessions were useful or somewhat useful. Program\ndirector (PD 4) discussed the benefits:\nThe process forced us to take a positivist look at the progress/\nsuccess of our students according to our own metrics and analyze\njust what those metrics were telling us. We had spirited debate\nabout (1) the content of our courses and entire curriculum, (2)\nthe sequencing of our courses, (3) our teaching styles and\nmethods, (4) the quality of our students, (5) the support\nmechanisms in place--and whether they were utilized or even\nappropriate--to help develop the areas that we believed were\nnecessary for success, and (6) whether we need to make\nadjustments to any of these aspects, including the curriculum.\nWe even took the time to do a bit of an environmental scan to\nassess whether there were changes in the profession requiring an\nadjustment in our curriculum, including needing new courses.\nOnly one program director (PD 3) reported \"not particularly\"\nuseful when responding to this question. This director stated,\n\"Because the faculty meet to discuss program goals and cur-\nriculum twice a semester already, this really did not add to\nwhat we already do--so I guess you could say it was useful\n(but not a new thing).\"\nThe second survey question asked directors about the\nhelpfulness of the assessment reflection prompts. A majority\nof program directors found the prompts helpful. One pro-\ngram director (PD 1) stated, \"I felt that I was starting from\nscratch with a new program and the prompts were very help-\nful guidance.\" A second program director (PD 5) stated,\n\"They got a bit tedious and detailed, but ultimately I have to\nadmit they were valuable as they drilled down.\" It does\nappear that the prompts assisted in providing a focus for the\nconversations, especially when directors followed the\nsequence outlined in the template.\nThe third question in the survey asked program directors\nabout the level of difficulty in conducting collaborative analy-\nsis with program faculty. The responses were mixed with half\nof the program directors stating the process was easy.\nAlternatively, two program directors (PD 5 and 6) found the\nprocess difficult because they had to select and organize the\ndata for examination. Faculty played no role in preparing for\nthe session. These mixed responses were not surprising given\nthe various comfort levels in leading data-driven discussions.\nThe fourth survey question asked program directors to\ndescribe the challenges they faced while leading faculty\nthrough the collaborative analysis process. Program\nColby et al. 9\ndirectors provided a variety of responses to this question.\nTwo program directors (PD 5 and 6) found challenges in\ncoming to agreement on ratings, metrics, and principles. A\nsecond program director (PD 2) stated, \"We struggle with\ndetermining how to be sure we are uniformly applying the\nratings in the POL rubric so that the variability in the results\nis really related to the program outcomes and not the raters.\"\nTwo program directors (PD 4 and 6) reported a lack of inter-\nest from faculty and PD 4 reported faculty hostility toward\nthe process.\nThe final question asked program directors what would\nhelp facilitate the collaborative analysis process. Five direc-\ntors responded to this question with over half discussing the\nneed for more faculty engagement. This was evident in the\nfollowing response (PD 1):\nI think it would be extremely beneficial if EVERYONE had\nmore input and responsibility in this process, not just the\nprogram directors. Had I had input prior to becoming program\ndirector, I might have been less lost and may have seen the value\nin the process. It would seem assessment should be part of\neveryone's responsibility to providing/maintaining quality\nprograms for our students and excellent teachers for our state\nand beyond.\nA second program director (PD 4) confirmed the need for\nfaculty input:\nMuch of this necessarily falls on Program Directors. We are the\nonly ones with access to the data at the program level; it falls to\nus to pull that data and make it accessible to faculty. It turns\nmore into a presentation of possible focus areas and assent by\nother faculty.\nA third program director added insight about the role of\nadministrators in the assessment process (PD 6):\nProgramfacultyneedtoseeprogramimprovementandassessment\nas a PROPERTY of teaching at a university. As long as that\nmessage is disseminated only to directors, directors will bear the\nbrunt of assessment for the entire program. Program faculty need\nto hear this message from administration--at all levels.\nThe results indicated that more ownership around assess-\nment needs to be cultivated in our college, and administra-\ntors need to find ways to support program directors as they\nlead faculty through this process.\nImplications\nFrom the three themes that emerged from this study,\nOutcomes of Collaborative Analysis, Use of Data for\nCollaborative Analysis, and Facilitating Collaborative\nAnalysis, four implications for implementing collaborative\nanalysis as a component of a unit assessment system were\nidentified. These implications are discussed below.\nCommitment is Crucial\nOne implication that emerged from this study was the need\nfor a strong commitment from administrators, program\ndirectors, and faculty for successful implementation of the\ncollaborative analysis process. Efforts to implement collab-\norative analysis must be systematic and continuous, espe-\ncially if data are to be aggregated for unit-level results. Too\noften, data analyses are not conducted frequently enough or\nin enough depth to reap the beneficial results of formative\nassessment (Hoover & Abrams, 2013). To use collaborative\nanalysis for accreditation, all programs should engage in col-\nlaboration on an ongoing basis to maximize the potential for\nsignificant change.\nCollaborative analysis should become a primary focus for\nimprovement purposes rather than an ancillary activity.\nFullan and Quinn (2016) stated that it is easy to create frag-\nmentation when responding to multiple mandates for\naccountability and improvement. They propose that to build\ncoherence in a system, the right drivers must be set in action.\nToo often, the wrong drivers of punitive accountability, indi-\nvidualistic strategies, technology, and ad hoc policies drive\nefforts for reform. Utilizing the right drivers of focused\ndirection, cultivating collaborative cultures, securing\naccountability, and deepening learning will help organiza-\ntions \"become immersed in work that develops focus and\nFindings from this study suggested that program directors\nstruggled, at times, to implement collaborative analysis\nbecause faculty investment and administrative direction\nwere lacking. As researchers for this study, we learned that a\nstronger commitment to the process and a strategic plan for\nfostering an investment from program faculty would likely\nhave created more coherency and commitment.\nImplement CASL With Fidelity\nA second implication that emerged from this study was the\nimportance of implementing CASL with fidelity to the origi-\nnal model. CASL is not the only approach for focusing teams\non collaborative analysis of data, but it is a comprehensive\nmodel that has been refined for over a decade (Colton et al.,\ngrams would have benefited from stricter adherence to the\noriginal definition of student work and to the reflective\ninquiry phases. Data from this study provided evidence that\nprograms tended to revert back to familiar patterns for exam-\nining data in lieu of a more formal process for analyzing stu-\ndent work. This practice is noted in the literature (Gallimore\nVan Lare & Brazer, 2013). Nearly half of the graduate pro-\ngrams examined aggregated quantitative results from cap-\nstone projects to determine what percentage of graduate\nstudents were proficient. These data were used to identify\nareas for program improvement without confirmation from\nstudent work. In addition, many of our programs used data\nthat were only indirectly related to student learning. Programs\nused faculty and student perceptions, disposition data, docu-\nments, and grades to determine areas for improvement.\nWhile we acknowledge that examining multiple sources of\ndata is beneficial, the process established in our college did\nnot provide the impetus for examining student work that\nwould help faculty \"reexamine, clarify, and transform their\nthinking so that they can help students succeed\" (Langer\nfoster collaborative discussion, it did not stimulate the type\nof reflective inquiry that provides a deep understanding of\nstudent learning as outlined in the CASL method. As Fullan\nand Quinn (2016) suggested, \"we must shift from a focus on\nteaching or inputs to a deeper understanding of the process of\nlearning and how we can influence it\" (p. 79). This does not\nmean that programs should only review student work in col-\nlaborative sessions; indeed, collaboration of all types has\npotential for improving student learning (Ronfeldt et al.,\n2015). However, in a continuous improvement cycle, there is\na place for critical discussions based on examining real stu-\ndent work. True CASL sessions should have a place in an\nassessment system where program faculty analyze student\nwork resulting from course and program assessments to criti-\ncally reflect on practice.\nDesign Protocols and Processes for Critical Inquiry\nA third implication for educators intending to implement\nCASL is to thoughtfully design inquiry-based protocols and\nprocesses. As evidenced in the literature, it is critical that\nfaculty and administrators create protocols that provide the\nstructure for focused opportunities that support reflection\non practice. The assessment reflection template used in our\ncollege lacked questions that would support the effective\nuse of data (Schildkamp et al., 2014) and stimulate critical\npreparation programs might have benefited from a more\nfocused attempt to develop social routines in our faculty\nthat fostered productive conversations about teaching and\nlearning that were \"intellectually ambitious\" (Thompson\nfacilitate a data analysis meeting, but they did not cultivate\ncritical reflection about student learning. This was evi-\ndenced by some of the more perfunctory reports we\nreceived. This was also evident in the survey results. Some\nprogram directors found the prompts to be useful while oth-\ners found them to be a restrictive. In addition, the assess-\nment reflection template could have been deliberately\ndesigned to more directly align with continuous improve-\nment as outlined in CAEP Standard 5. Including prompts\nthat assisted in tracking results over time (5.3), examining\ncompleter impact on K-12 student growth (5.4), and includ-\ning a variety of stakeholders in evaluation (5.5) would have\nbenefited our data collection.\nAs a result of this study, we learned how important it is to\nestablish SLOs prior to undertaking CASL. Better defined\nSLOs in programs and deliberate attention to using SLOs in\nprotocols and processes would have produced more focused\nand productive CASL sessions. Fullan and Quinn (2016)\nconfirmed the need for SLOs to become the foundation of\ncollaborative work:\nThe first step in building precision and consistent practices is to\nbe clear about the learning goals. For the last quarter century,\neducation has been giving superficial lip service to 21st century\nskills without much concerted action or impact. The energy has\nbeen invested in describing sets of skills without much robust\nimplementation or effective ways to measure them. (p. 83)\nAddress Professional Learning Needs\nA fourth implication that emerged from this study was that\nguidance, support, and resources must be provided to pro-\ngram directors and faculty implementing CASL. Addressing\nthe needs of directors and faculty is crucial for success.\nProgram directors need assistance in selecting appropriate\ndata to analyze and in leading collaborative discussions. In\naddition, directors and faculty need to develop data-based\ndecision-making skills. This is not surprising given the\nresearch in this area. Teachers find it difficult to interpret evi-\ndence and use data to draw accurate conclusions (Ronka,\nSchneider & Andrade, 2013), and professional development\nis critical for learning how to use data for improvement pur-\nSchneider & Gowan, 2013). Modeling and coaching are\nhighly successful professional learning strategies (Aguilar,\n2013), and program directors would benefit from participat-\ning in CASL at the unit level before facilitating sessions with\ntheir faculty. Finally, a data expert can help program direc-\ntors select appropriate data and provide a focus for collabora-\ntive sessions (Schildkamp et al., 2014).\nConclusion\nThe findings from this study provide insight into the poten-\ntial of CASL for program improvement that supports internal\nand external accountability (Fullan & Quinn, 2016). The\nCASL process can foster collective responsibility for student\nlearning, but only with a strong commitment from adminis-\ntrators and faculty, purposefully designed protocols and pro-\ncesses, fidelity to the program, and a focus on professional\ndevelopment. Faculty become engaged in critical reflection\nabout student learning with the right structures, routines, and\ntools. Through CASL, programs have the ability to produce\nmeaningful data related to student learning and program\nimprovement. Findings from this study confirm that using\nany method, strategy, or system for both internal and external\nevaluation, while promising, has its challenges. Norman and\nSherwood (2015) confirmed this finding: \"While we advo-\ncate a model of program improvement that embraces both\ninternal and external evaluation components, such a model is\nnot without challenge\" (p. 20).\nIt is our hope that CASL will become a method used in our\ncollege for both program improvement that leads to greater\nstudent learning and evidence of engagement in a continuous\nimprovement cycle for accreditation purposes. Perhaps as we\nprepare for our first CAEP accreditation, we will come to\nunderstand that \"securing accountability is not about pleasing\nthe system (although there is nothing wrong with this) but\nabout acting in ways that are in your own best interest\"\n(Fullan & Quinn, 2016). The new CAEP standards embrace\nsome of the same principles for continuous improvement that\nthis college values. CAEP allows for institutions to create\nmeaningful assessment systems that support student learning,\nprogram quality, and continuous improvement. We believe\nthat CASL, or other well-defined methods for critical conver-\nsation, can provide coherency to a college-level assessment\nsystem. As Fullan and Quinn (2016) stated, \"The key to a\ncapacity building approach lies in developing a common\nknowledge and skill base across all leaders and educators in\nthe system, focusing on a few goals, and sustaining an intense\neffort over multiple years\" (p. 57). Perhaps it is time for edu-\ncator preparation colleges to more directly \"know thy impact\"\n(Hattie, 2015) and take their place in \"building communities\nof actors whose collective work is aimed at the improvement\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect\nto the research, authorship, and/or publication of this article.\nFunding\nThe author(s) received no financial support for the research and/or\nauthorship of this article.\nReferences\nAguilar, E. (2013). The art of coaching: Effective strategies for\nschool transformation. San Francisco, CA: Jossey-Bass.\nBest, J. W., & Kahn, J. V. (1998). Research in education (8th ed.).\nNeedham Heights, MA: Allyn & Bacon.\nBuhle, R., & Blachowicz, C. L. Z. (2009). The assessment plan.\nCoburn, C. E., & Talbert, J. E. (2006). Conceptions of evidence use\nin school districts: Mapping the terrain. American Journal of\nColton, A. B., Langer, G. M., & Goff, L. S. (2015). Create a safe\nCosta, A. L., & Garmston, R. J. (2002). Cognitive coaching: A\nfoundation for renaissance schools (2nd ed.). Norwood, MA:\nChristopher-Gordon Publishers.\nCouncil for the Accreditation of Educator Preparation. (2015).\nCAEP standards introduction. Available from http://caepnet.\norg\nCouncil for the Accreditation of Educator Preparation. (2016). CAEP\naccreditation handbook. Available from http://caepnet.org\nCreswell, J. W. (2014). Research design: Qualitative, quantitative,\nand mixed methods approaches (4th ed.). Thousand Oaks, CA:\nDarling-Hammond, L. (2015). Can value added add value to teacher\nDuFour, R. (2015). How PLCs do data right. Educational\nDuFour, R., & Eaker, R. E. (1998). Professional learning communi-\nties at work: Best practices for enhancing student achievement.\nBloomington, IN: National Education Service.\nFullan, M., & Quinn, J. (2016). Coherence. Thousand Oaks, CA:\nCorwin Press.\nFuller, E. J. (2014). Shaky methods, shaky motives: A critique of the\nNational Council of Teacher Quality's review of teacher prepa-\nGallimore, R., Ermeling, B. A., Saunders, W. M., & Goldenberg,\nC. (2009). Moving the learning of teaching closer to practice:\nTeacher education implications of school-based inquiry teams.\nHarris, E. N., & Herrington, C. D. (2015). Editors' introduc-\ntion: The use of teacher value-added measures in schools:\nNew evidence, unanswered questions, and future prospects.\nHattie, J. A. C. (2012). Visible learning for teachers. Maximizing\nimpact on achievement. London, England: Routledge.\nHattie, J. A. C. (2015). The applicability of Visible Learning to\nhigher education. Scholarship of Teaching and Learning in\nHoover, N. R., & Abrams, L. M. (2013). Teachers' instructional use\nof summative student assessment data. Applied Measurement\nJimerson, J. B. (2014). Thinking about data: Exploring the devel-\nopment of mental models for \"data use\" among teachers and\nschool leaders. Studies in Educational Evaluation, 42, 5-14.\nKatz, S., & Dack, L. A. (2014). Towards a culture of inquiry for\ndata use in schools: Breaking down professional learning bar-\nriers through intentional interruption. Studies in Educational\nLanger, G., Colton, A., & Goff, L. (2003). Collaborative analysis\nof student work: Improving teaching and learning. Alexandria,\nVA: Association for Supervision and Curriculum Development.\nLincove, J. A., Osborne, C., Dillon, A., & Mills, N. (2014). The\npolitics and statistics of value-added modeling for account-\nability of teacher preparation programs. Journal of Teacher\nMarsh, J. A., & Farrell, C. C. (2015). How leaders can support\nteachers with data-driven decision making: A framework for\nunderstanding capacity building. Educational Management\nMitchell, J. (2014). Academics learning a new language: Developing\ncommunities of practice in faculty professional development.\nInternational Journal of Adult, Community and Professional\nNorman, P. J., & Sherwood, S. A. S. (2015). Using internal and\nexternal evaluation to shape teacher preparation curriculum:\nA model for continuous program improvement. The New\nReeves, D. (2007). From the bell curve to the mountain: A new\nvision for achievement, assessment, and equity. In D. Reeves\n(Ed.), Ahead of the curve: The power of assessment to trans-\nform teaching and learning (pp. 1-14). Bloomington, IN:\nSolution Tree.\nRonfeldt, M., Farmer, S. O., McQueen, K., & Grissom, J. A.\n(2015). Teacher collaboration in instructional teams and stu-\ndent achievement. American Educational Research Journal,\nRonka, D., Lachat, M. A., Slaughter, R., & Meltzer, J. (2009).\nAnswering the questions that count. Educational Leadership,\nSchildkamp, K., Karbautzki, L., & Vanhoof, J. (2014). Exploring\ndata use practices around Europe: Identifying enablers and bar-\nSchmoker, M. (2005). Here and now: Improving teaching and\nlearning. In R. Dufour, R. Eaker, & R. Dufour (Eds.), On com-\nmon ground: The power of professional learning communities\n(pp. xi-xvi). Bloomington, IN: Solution Tree Press.\nSchneider, M. C., & Andrade, H. (2013). Teachers' and admin-\nistrators' use of evidence of student learning to take action:\nConclusions drawn from a special issue on formative assess-\nSchneider, M. C., & Gowan, P. (2013). Investigating teachers'\nskills in interpreting evidence of student learning. Applied\nSparks-Langer, G. M., Simmons, J. M., Pasch, M., Colton, A., &\nStarko, A. (1990). Reflective pedagogical thinking: How can\nwe promote it and measure it? Journal of Teacher Education,\nStallings, J., Needels, M., & Sparks, G. (1987). Observation for\nthe improvement of teaching. In D. Berliner & B. Rosenshine\nRandom House.\nStaman, L., Visscher, A. J., & Luyten, H. (2014). The effects of\nprofessional development on the attitudes, knowledge and\nskills for data-driven decision making. Studies in Educational\nThompson, J., Hagenah, S., Lohwasser, K., & Laxton, K. (2015).\nProblems without ceilings: How mentors and novices frame and\nwork on problems-of-practice. Journal of Teacher Education,\nVan Lare, M. D., & Brazer, S. D. (2013). Analyzing learning in\nprofessional learning communities: A conceptual framework.\nWayman, J. C., & Jimerson, J. B. (2014). Teacher needs for\ndata-related professional learning. Studies in Educational\nWayman, J. C., & Stringfield, S. (2006). Data use for school\nimprovement: School practices and research perspectives.\nYoung, V. M. (2006). Teachers' use of data: Loose coupling, agenda\nsetting, and team norms. American Journal of Education, 112,\nAuthor Biographies\nDr. Susan Colby is Professor and Chair in the Department of\nCurriculum and Instruction at Appalachian State University. Her\nresearch interests include assessment of student learning, teacher\npreparation program effectiveness, and the evaluation of preservice\nand practicing teachers. She has published in journals such as\nStudies in Educational Evaluation, Action in Teacher Education,\nReading Research and Instruction, Teaching Education, and\nEducational Leadership.\nDr. Monica Lambert is Professor and Associate Dean in the Reich\nCollege of Education at Appalachian State University. Her research\ninterests include effective P-12 partnerships, clinical practice, pro-\ngram improvement, assessment of student learning, and methods\nfor teaching students with learning disabilities. She has published in\njournals such as Intervention in School and Clinic and Learning\nDisabilitiy Quaterly.\nDr. Jennifer McGee is Assistant Professor in the Department of\nCurriculum and Instruction. Her research interests include valid-\nity and measurement, program evaluation, self-efficacy, assess-\nment, and STEM education. She has published in journals such\nas the Journal of Psychoeducational Assessment, Action in\nTeacher Education, and the International Journal of Educational\nResearch."
}