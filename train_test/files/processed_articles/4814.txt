{
    "abstract": "Abstract\nBecause of overwhelming evidence of publication bias in psychology, techniques to correct meta-analytic estimates\nfor such bias are greatly needed. The methodology on which the p-uniform and p-curve methods are based has great\npromise for providing accurate meta-analytic estimates in the presence of publication bias. However, in this article,\nwe show that in some situations, p-curve behaves erratically, whereas p-uniform may yield implausible estimates\nof negative effect size. Moreover, we show that (and explain why) p-curve and p-uniform result in overestimation\nof effect size under moderate-to-large heterogeneity and may yield unpredictable bias when researchers employ\np-hacking. We offer hands-on recommendations on applying and interpreting results of meta-analyses in general and\np-uniform and p-curve in particular. Both methods as well as traditional methods are applied to a meta-analysis on\nthe effect of weight on judgments of importance. We offer guidance for applying p-uniform or p-curve using R and a\nuser-friendly web application for applying p-uniform.\n",
    "reduced_content": "Perspectives on Psychological Science\nReprints and permissions:\nsagepub.com/journalsPermissions.nav\npps.sagepub.com\nMeta-analysis is the standard technique to synthesize\neffect sizes of several studies on the same phenomenon.\nA well-known problem of meta-analysis is that effect size\ncan be overestimated because of publication bias (e.g.,\ndefine publication bias as the tendency for studies with\nstatistically significant results to be published at a higher\nrate than studies with results that are not statistically sig-\nnificant. Because evidence of publication bias is over-\nwhelming across many scientific disciplines (Fanelli,\n2012), it is important to develop techniques that correct\nthe meta-analytic estimate for publication bias (Moreno,\nSutton, Ades, etal., 2009). Recently, van Assen, van Aert,\nand Wicherts (2015) and Simonsohn, Nelson, and Simmons\n(2014a) independently developed methods aiming to\nprovide an accurate meta-analytic estimate in the pres-\nence of publication bias. Both of their methods, p-uniform\nand p-curve, respectively, make use of the distribution of\nstatistically significant results yet differ in implementation.\nOur goals in this article are to introduce and explain both\nmethods and their differences, to provide straightforward\nrecommendations applying to meta-analysis, and to for-\nmulate guidelines for applying and interpreting results of\np-uniform and p-curve.\nA Primer on p-Uniform and p-Curve\nSimonsohn, Nelson, and Simmons (2014b) described how\nstatistically significant p values of studies on an effect\ncould be used to test this effect against the null hypothesis\nthat the effect equals zero. This idea was not new; Fisher\n(1925) developed a method for testing the null hypothesis\nof no effect by combining p values. However, the novelty\nCorresponding Author:\nRobbie C. M. van Aert, Department of Methodology and Statistics,\nE-mail: R.C.M.vanAert@tilburguniversity.edu\nConducting Meta-Analyses Based\non p Values: Reservations and\nRecommendations for Applying\np-Uniform and p-Curve\nRobbie C. M. van Aert1, Jelte M. Wicherts1, and\nMarcel A. L. M. van Assen1,2\n1Department of Methodology and Statistics, Tilburg University and;\n2Department of Social and Behavioral Sciences, Utrecht University\n Keywords\np-uniform, p-curve, meta-analysis, p-hacking, heterogeneity\nof p-curve lies in its use of only the statistically significant\np values, which arguably are not affected by publication\nbias. The method was called p-curve because it can be\nused to analyze the curve or distribution of p values. In\nthe logic of the p-curve method, there is no effect in the\nstudies in the meta-analysis if the p values are uniformly\ndistributed (i.e., p-curve is flat), whereas there is an effect\nif the p value distribution or p-curve is right skewed (Hung,\nA disadvantage of p-curve at that time was that effect\nsize could not be estimated. Van Assen et al. (2015) devel-\noped another method of analyzing statistically significant\np values called p-uniform, which can be used to estimate\nthe effect size in a set of studies. These researchers called\ntheir method p-uniform because the effect-size estimate is\nequal to the value for which the p value distribution con-\nditional on that value is uniform (as we explain later).\nBesides estimating the effect size, p-uniform also can be\nused to estimate a confidence interval (CI) around the\neffect-size estimate, in addition to testing for publication\nbias and, similar to p-curve (Simonsohn etal., 2014b), for\ntesting the null hypothesis of no effect. Simonsohn etal.\n(2014a) later extended p-curve to estimate effect size as\nwell. However, p-curve provides neither a CI nor a test for\npublication bias. In the present study, we focus on effect-\nsize estimation using both p-curve and p-uniform.\nThe strengths of p-uniform and p-curve and the logic\nupon which they are based were convincingly illustrated\nThey showed that the methods provide accurate effect-\nsize estimates in the presence of publication bias, even\nwhen the number of statistically significant studies is small.\nSimilarly, both methods were found to perform well when\nstudies have the same sample sizes or different sample\nsizes and when there is (small) heterogeneity of effect size\n(i.e., when the underlying population effect sizes actually dif-\nfer between studies in the meta-analysis). Moreover, results\nof Simonsohn etal. (2014a) suggested that p-hacking--or\nthe original researchers' use of strategies to achieve statisti-\ncal significance (Simmons, Nelson, & Simonsohn, 2011)--\nleads to an underestimation of effect size in analyses\nbased on p-curve, whereas it leads to overestimation of\neffect size in traditional meta-analysis (Bakker, van Dijk, &\nThree reservations\nAlthough we are convinced of the potential and validity\nof the logic of p-uniform and p-curve, we have added\nthree important reservations to the application of the\nmethods and the general methodology in its current state.\nMore specifically, we first show that p-uniform and p-curve\nmay yield implausible negative (p-uniform) or inaccurate\n(p-curve) estimates in meta-analyses with p values close\nto the significance level (considered equal to .05 in the\npresent article). Second, we explain why and show that\np-hacking does not always cause the effect sizes of p-curve\nand p-uniform to be underestimated as was stated in\nSimonsohn etal. (2014a). Finally, we show that in contrast\nto the results in Simonsohn etal. (2014a), p-uniform and\np-curve cannot deal with a substantial amount of hetero-\ngeneity (i.e., there is no single true effect size underlying\nthe studies in the meta-analysis but rather a distribution of\ntrue effect sizes). Based on our explanation of the meth-\nods and the reservations, we have formulated recommen-\ndations for applying meta-analysis in general and\ninterpreting results of p-uniform and p-curve in particular.\nThese hands-on recommendations are summarized in\nTable 1. Scientists who consider using these methods\nshould be aware of conditions in which the methods either\nTable 1. Recommendations for Meta-Analysis and Application of p-Uniform and p-Curve\nCheck for evidence of p-hacking in the primary studies.\nIn case of strong evidence or strong indications of p-hacking, be reluctant in interpreting estimates of traditional meta-analytic\ntechniques and p-uniform and p-curve because their effect-size estimates may be biased in any direction depending on the\ntype of p-hacking used.\nApply fixed-effect and random-effects meta-analysis, as well as p-uniform or p-curve, and report results conforming to the\nCheck for direct or indirect evidence of publication bias.\nIn case of evidence of publication bias, interpret results of p-uniform or p-curve rather than those of fixed-effect and random-\neffects meta-analysis; in the absence of such evidence, interpret results of fixed-effect and random-effects meta-analysis.\nSet the effect-size estimate of p-uniform or p-curve equal to zero if the average p value of the statistically significant studies is\nIf the effect size is homogeneous or if the heterogeneity is small to moderate (I2 < 0.5), interpret the estimates of p-uniform\nand p-curve as estimates of the average population effect size; otherwise, these methods result in overestimates of average\npopulation effect size and should be interpreted as estimates of the average true effect size of only the set of statistically\nsignificant studies.\nIn case of substantial heterogeneity (and if desired), create homogeneous subgroups of primary studies on the basis of\ntheoretical or methodological considerations to estimate with p-uniform and p-curve the average population effect size\nunderlying the studies in each subgroup\nConducting Meta-Analyses Based on p Values 715\nshould not be interpreted or should be interpreted with\ncaution.\nIn the remainder of the article, we illustrate major\nissues involved in applying p-curve and p-uniform by\nconsidering a recent meta-analysis of studies on the effect\nof weight on judgment of importance (Rabelo, Keller,\nPilati, & Wicherts, 2015). We briefly describe other meth-\nods of meta-analysis using statistically significant effect\nsizes, introduce the basic idea underlying p-uniform and\np-curve, and illustrate the logic of and computations in\np-uniform and p-curve in Appendix A. The analyses that\nform the basis of our three reservations and recommen-\ndations are presented in the next sections. Readers who\ndo not want to delve into the (technical) details of\n\np-uniform and p-curve can skip these sections and move\nover to the Discussion and Conclusion section, where we\nexplain the recommendations in Table 1. R code for all\nour analyses is available in the Supplemental Materials.\nExample\nRabelo et\nal. (2015) conducted a meta-analysis on the\neffect of weight on judgments of importance. The theory\nunderlying the studies included in the meta-analysis is\nthat the physical experience of weight (e.g., holding a\nheavy object) influences how much importance people\nassign to things, issues, and people (IJzerman, Padiotis, &\ninstance, in their second study, Jostmann et\nfound that participants who held a heavy clipboard attrib-\nuted more importance to fairness in decision making than\ndid participants holding a light clipboard. Table B1 in the\nAppendix B provides the full references, sample sizes (n\ni\nand n\ni\n), t values, and p values from the 25 studies of this\nkind published in the embodiment literature.\nAccording to the first recommendation, we should\nconsider the presence of p-hacking in the primary studies\nincluded in the meta-analysis. We believe that the studies\non the link between weight and importance are mostly\nstudies in which the specifics of the analysis often are\nneither preregistered nor clearly restricted by theory.\nHence, according to Recommendation 1, we would use\ncaution in interpreting the current results and await new\n(preferably preregistered) studies in this field.\nFour different meta-analytic estimates of the (mean)\neffect size underlying the weight-importance studies are\npresented in Table 2. In line with Recommendation 2, we\nfirst fitted traditional fixed-effect and random-effects\nmeta-analysis. Both analyses yielded the same effect size\na medium-to-large effect of the experience of weight on\nhow much importance people assign to things (see\nTable 2). The results of p-uniform's publication bias test\nsuggested evidence of publication bias (z = 5.058, p <\n.001), so the results of p-uniform or p-curve should be\ninterpreted rather than the standard meta-\nanalytic estimates\n(Recommendation 3). Because the average p value of the\n23 statistically significant studies equaled .0281, we set the\neffect size estimate of p-uniform and p-curve equal to zero,\nin line with Recommendation 4. When the estimate is not\nset to zero, application of p-curve and p-uniform yields a\nnonsignificant negative effect size (see Table 2), and the\neffect size is small at best.\nThe null hypothesis of no heterogeneity among the\nwhich suggests that p-uniform and p-curve may accurately\nestimate the average population effect size (Recommenda-\ntion 5a). Note that due to the absence of heterogeneity,\neffect-size estimates of fixed-effect and random-effects meta-\nanalysis were identical. Although the lack of heterogeneity\nsuggests that the effects are homogeneous, in this particular\ninstance, homogeneity is excessive (with a p value of the\nQ test very close to 1). Such excessive homogeneity is unlikely\nto occur under normal sampling conditions (Ioannidis, Trika-\nlinos, & Zintzaras, 2006) and could be caused by publication\nbias (Augusteijn, 2015), possibly in combination with\n\np-hacking. Our preliminary conclusion about the effect of\nphysical experience of weight on importance would be that\nthere is as yet no evidence in the literature for such an effect.\nOther Methods Using p Values for\nEstimation\nSeveral other methods were developed in which p values\nare used to obtain an effect-size estimate corrected for\npublication bias. Hedges (1984) developed a method for\nTable 2. Results of p-Uniform, p-Curve, Fixed-Effect Meta-Analysis, and Random-Effects Meta-Analysis Applied to the\nMeta-Analysis Reported in Rabelo, Keller, Pilati, and Wicherts (2015)\nFixed-effect Random-effects\nTest of H0\n:  = 0 refers to the null hypothesis of no effect. CI = confidence interval.\ncorrecting meta-analytic effect sizes for publication bias\nthat is similar to p-uniform and p-curve. He derived the\nmaximum likelihood estimator of effect size under a\nmodel with only statistically significant results and stud-\nied the bias in the effect-size estimate. Although Hedges\n(1984) discussed the application to meta-analyses, he\nonly examined the bias in effect size of one statistically\nsignificant study. Hedges's method and its performance\nare not further examined in this article because it is cur-\nrently not applied in practice.\nOther methods for obtaining effect-size estimates cor-\nrected for publication bias are selection models (Hedges\n& Vevea, 2005). Selection models use an effect-size model\nand a weight function for correcting the effect-size esti-\nmates for publication bias. The effect-size model describes\nthe distribution of effect sizes in case all studies are pub-\nlished. The weight function yields probabilities of observ-\ning a particular study given its effect size or p value.\nEffect sizes of the studies then are weighted by these\nprobabilities in order to obtain an effect size corrected\nfor publication bias (for an overview on selection mod-\nels, see Hedges & Vevea, 2005). Drawbacks of selection\nmodels are that they require a large number of studies\n(i.e., more than 100) in order to avoid nonconvergence\nyield implausible weight functions (Hedges & Vevea,\n2005), are hard to implement, and require sophisticated\nassumptions and difficult choices (Borenstein, Hedges,\nalternative for selection models based on Bayesian statis-\ntics showed promising results and does not have conver-\ngence problems when the number of studies in the\nmeta-analysis is small (Guan & Vandekerckhove, 2015).\nHowever, a disadvantage of the latter method is that it\nmakes stronger assumptions on weight functions than\np-uniform and p-curve. In p-uniform and p-curve, the\nprobability of publishing a finding is assumed to be inde-\npendent of its p value given its statistical significance,\nwhereas the models in the method described in Guan\nand Vandekerckhove (2015) assume specific weights of\nfindings depending on their p value, significant or not.\nBecause both significant and nonsignificant p values are\nincluded, this Bayesian method makes assumptions about\nthe extent of publication bias, and its estimates are\naffected by the extent of publication bias. For these rea-\nsons, we will not discuss selection models and their prop-\nerties further.\nBasic Idea Underlying p-Uniform and\np-Curve\nIn both p-uniform and p-curve, the distribution of only\nthe statistically significant p values are used for estimating\neffect size for at least two reasons. First, collecting\nunpublished studies without the existence of study (or\ntrial) registers is often hard, and these unpublished stud-\nies may provide biased information on effect size just as\npublished studies do (Ferguson & Brannick, 2012). Sec-\nond, evidence for publication bias is overwhelming. For\ninstance, researchers have estimated that at least 90% of\nthe published literature within psychology contains sta-\ntistically significant results (e.g., Bakker et\nyielding overestimated effect sizes (e.g., Ioannidis, 2008;\nLane & Dunlap, 1978). Because most published findings\nare statistically significant, only a relatively small number\nof published but statistically nonsignificant studies (on\naverage up to 10%) need to be omitted from meta-analy-\nses by p-curve and p-uniform.\nBoth p-uniform and p-curve are founded on the statis-\ntical principle that the distribution of p values conditional\non the true effect size is uniform.1 This same statistical\nprinciple underlies standard null-hypothesis significance\ntesting, where the p values are uniformly distributed\nwhen the true effect size equals zero. In contrast to null-\nhypothesis significance testing, p values from p-uniform\nand p-curve are computed not only conditional on an\neffect size of zero (which would yield a simple transfor-\nmation of the traditional p values) but also conditional on\nother effect sizes (in which case the conditional p value\nis not a simple transformation of the traditional p value\nanymore). The effect-size estimate of p-uniform and\np-curve represents the effect size for which the condi-\ntional p values are uniformly distributed.2 What both pro-\ncedures do is to find an underlying effect, compute for\neach study the (conditional) p value given this effect, and\nsubsequently check whether these conditional p values\nshow a flat (i.e., uniform) distribution, which they should\nif indeed the studies reflect that underlying effect. The\nassumptions of p-uniform and p-curve are that all statisti-\ncally significant studies have the same probability of get-\nting published and being included in the meta-analysis and\nare statistically independent (i.e., they should not be based\non the same sample, van Assen etal., 2015). We describe the\nlogic underlying p-uniform and p-curve as well as how the\nconditional p value and effect-size estimate for p-uniform\nand p-curve are computed in Appendix A.\nIf Heterogeneity Is Moderate to Large,\np-Curve and p-Uniform Overestimate\nEffect Size\nSimonsohn et\nal. (2014a) stated that p-curve provides\naccurate effect-size estimates in the presence of hetero-\ngeneity (i.e., in cases where true effects underlying the\nobserved effects of the studies differ). In a blog post,\nSimonsohn (2015) qualified this statement as follows: \"If\nwe apply p-curve to a set of studies, it tells us what effect\nConducting Meta-Analyses Based on p Values 717\nwe expect to get if we run those studies again.\" In other\nwords, applying p-curve (and p-uniform) to a set of stud-\nies yields an accurate estimate of the average true effect\nsize of this exact set of studies. However, we note that it\nmay be impossible to run exactly the same studies again\nsince there will always be differences in, for instance, the\nparticipants included in the studies and the context in\nwhich the studies were conducted.\nBecause of the importance of its implications for the\ninterpretation of the p-curve's estimate, we provide a sim-\nple example with heterogeneous effect sizes. Assume that\nthe true effect size is equal to either 0 or 1 and that both\nunderlying effects are equally likely, which implies an\naverage true effect size, \u00b5 = .5. Also assume that both true\neffect sizes are investigated with the same number of\nstudies with a huge sample size, implying 5% and 100% of\nstudies with true effects equal to 0 and 1 are statistically\nsignificant, respectively. Because the sample sizes of the\nstudies are huge, the observed effect sizes of statistically\nsignificant studies are equal to (a number very close to) 0\nand 1. As a result, the p-curve's estimate equals (0.05 \u00d7 0\nunderlying true effect size of all the statistically significant\nstudies. However, it is much larger than the true popula-\ntion average of .5. Moreover, traditional random-effects\nmeta-analysis provide a more accurate estimate of true\naverage effect size (i.e., less positively biased) than\np-curve, even under extreme publication bias.\nIt is often unrealistic to assume homogeneous true\neffect sizes underlying primary studies in psychological\nmeta-analyses (e.g., Borenstein etal., 2009). Moreover,\nresearchers often want to estimate the true effect size in\nthe population instead of the average true effect size in\nthe studies included in the meta-analysis. That is, meta-\nanalysts wish to obtain an estimate of .5, rather than .952\nin our example. The reason that p-curve overestimates\neffect size under heterogeneity is that studies with an\nunderlying true effect of zero have a lower probability of\nbeing statistically significant, such that these studies are\nunderrepresented in the meta-analysis. In our example,\nstudies with large true effect size are 20 times more likely\nto be included in the meta-analysis than those with a\nzero effect size. Finally, we note that in this simple exam-\nple, we could deal with the heterogeneity rather easily if\ntrue effect size (0 or 1) is perfectly linked to an observed\ndichotomous study characteristic; applying p-curve or\np-uniform to studies of both groups (a so-called subgroup\nanalysis, e.g., Borenstein etal., 2009) yields the correct\nestimates of 0 and 1. We therefore recommend applying\nthese methods to subgroups of studies on the basis of the\ndifferent levels of a moderator in order to create more\nhomogeneous sets of studies (Recommendation 5b).\nHowever, in other realistic situations, the causes of het-\nerogeneity are not simply observed, and subgroup analy-\nsis will not completely solve the heterogeneity problem.\nTo illustrate the effect of heterogeneity of effect sizes\non the (over)estimation of effect size by p-curve and\np-uniform, we also performed a simulation study in which\nwe varied heterogeneity from moderate to large under the\nusual scenario: heterogeneity was modeled continuously\nusing a normal distribution of true effects, which is com-\nmonly assumed in meta-analysis (Raudenbush, 2009). As\nsignificant results were generated on which the meta-\nanalysis was conducted. All studies had two conditions\nwith 50 cases each, with population variance equal to 1 in\nboth conditions. Average population effect size was .397,\nand standard deviations of true effect size (denoted by )\n(i.e., ratio of heterogeneity to total variance; Higgins &\nity), .8 (large heterogeneity), .9, and .96 in the population\nof studies. Table 3 provides the estimates of p-curve,\np-uniform, fixed-effect meta-analysis, and random-effects\nmeta-analysis (with restricted maximum likelihood esti-\nmator for estimating the amount of heterogeneity) of all\nstudies with a statistically significant positive effect. For\np-uniform, we used the Irwin-Hall estimator and the so-\ncalled \"1 - p\" estimator, a variant based on Fisher's\nmethod, because this estimator is least affected by extreme\neffect sizes and therefore provides better estimates in case\nof heterogeneity (van Assen etal., 2015).\nThe first column confirms that p-curve and p-uniform\nprovide accurate estimates under homogeneity (effect-size\nTable 3. Estimates of Effect Size for 5,000 Studies With Statistically Significant Positive Effects\np-uniform \nNote. Fixed-effect and random-effects meta-analysis performed with restricted maximum likelihood for estimating the\namount of heterogeneity under different levels of heterogeneity (true effect .397).\nestimates are close to the true effect size of .397), whereas\nfixed-effect and random-effects meta-analysis (both .553)\noverestimate effect size. The other columns, however,\nshow that both p-curve and p-uniform overestimate the\nmean population effect size of .397 for moderate-to-large\nheterogeneity and that this bias increases with larger het-\nerogeneity. Note that the bias of fixed-effect and random-\neffects meta-analysis also increases with larger\nhetero\ngeneity and exceeds the bias of p-curve and\n\np-uniform in these cases. Although the 1 - p estimator of\np-uniform provides the best estimates, its bias is still so\nlarge that we do not recommend applying the methodol-\nogy in its current state to estimate the average population\neffect size in situations where moderate or large hetero-\ngeneity is present or suspected (Recommendation 5a).\nFor illustrative purposes, we show how p-curve and\np-uniform could still be used to diagnose heterogeneity\nby applying p-uniform to one simulated meta-analysis of\n20 studies with the aforementioned specifications; mean\npopulation effect size equal to .397, and large heteroge-\nneity ( = 1; I2 = .96). The 1 - p estimator of p-uniform\nyielded an effect size estimate of \ncomparison of the expected conditional p values with the\nobserved conditional p values for \nbility-probability (or P-P) plot in Figure 1 clearly indi-\ncated systematic misfit. Specifically, observed conditional\np values should be uniformly distributed, as the expected\nconditional p values. That is, all dots should fall on or\nclose to the diagonal. However, assuming a fixed effect\nsize of .795, the observed conditional p values were\neither (much) too small (dots below the diagonal to the\nleft) or (much) too large (dots above the diagonal to the\nright), signifying a large effect size variance. In other\nwords, deviations from the diagonal in the P-P plot may\nbe used to diagnose heterogeneity of effect size.\nTo conclude, if moderate to large heterogeneity is\npresent, then p-curve and p-uniform estimate the average\ntrue effect underlying all significant studies in the meta-\nanalysis. If the main goal of the meta-analysis is to esti-\nmate the average true effect of the whole population of\nstudies in the presence of heterogeneity (I2  .5), we do\nnot recommend using p-curve or p-uniform because\ndoing so generally overestimates average true effect size\n(Recommendation 5a). In opposition to mainstream\nmeta-analytic thinking, Simonsohn etal. (2014a) argued\nthat \"the\" average true effect size under heterogeneity\noften does not exist and even that it is meaningless\nbecause studies cannot be run randomly. However, we\nbelieve the average true effect size may be interpreted\nmeaningfully in the presence of heterogeneity in some\nsituations and consider heterogeneity to be both realistic\nfor psychological studies (e.g., in 50% of the replicated\npsychological studies in the Many Labs Replication Proj-\nect, heterogeneity was present; Klein etal., 2014) and\nimportant to take into consideration when estimating\naverage effect size.\nSensitivity to p Values Close to .05\nStatistically significant p values that are uniformly distrib-\nuted in [0, .05] are in line with a zero true effect size. A\ndistribution of p values with many p values close to .05\n(and, say, an average p value of more than .025) is not in\nline with a zero true effect size but may indicate a nega-\ntive true effect size. We now show that if most of studies\nin the meta-analysis have a p value just below the signifi-\ncance criterion of .05, then p-uniform yields implausible\nhighly negative effect size estimates and a very wide CI.\nSimilarly, under these conditions p-curve behaves\nerratically.\nTo illustrate the consequences of having many p val-\nues just below .05 on the estimates of p-uniform and\np-curve, consider doing a meta-analysis on the following\nthree observed effect sizes with two conditions having\np = .048. Several explanations exist for observing multi-\nple p values that barely pass the significance criterion as\nin this example. First, p-hacking such as optional stop-\nping or data peeking (Hartgerink, van Aert, Nuijten,\ntion of outliers to achieve statistical significance may\nyield a preponderance of p values just below .05 (Bakker\n& Wicherts, 2014b). Another explanation is (bad)\nFig. 1. Probability-probability (P-P) plot for a meta-analysis of 20 stud-\nies with large heterogeneity.\nConducting Meta-Analyses Based on p Values 719\nluck--when the meta-analysis consists of a small number\nof studies, and multiple studies coincidentally have p val-\nues close to .05. The fixed-effect meta-analytic estimate\nApplying p-curve to this set of studies yields an effect\nsize estimate of d = -1.898. Figure 2 displays the behavior\nof the Kolmogorov-Smirnov test statistic in p-curve with\ndots as a function of effect size. It shows that the\n\nKolmogorov-Smirnov statistic in p-curve does not behave\nas it should (decrease to one minimum, and then increase,\nand be continuous for all effect sizes). This erratic behav-\nior is caused by implementation of p-curve using the\nt distribution from the software R (R Core Team, 2015),\nbecause R yields inaccurate probabilities for very high\nt values in combination with an extreme noncentrality\nparameter (Witkovsk\u00fd, 2013). This inaccuracy may cause\nconditional p values to be negative or undefined (division\nby zero), which yields the discontinuities in Figure 2.\nTherefore, p-curve's estimate cannot be trusted for this\nexample.\nThe implementation of p-uniform differs from that of\np-curve because the normal distribution (instead of the\nt-distribution) is used for computing conditional p values.\nThe studies' effect sizes are transformed into standard-\nized effect sizes (Hedges' g) before the effect size is esti-\nmated. Consequently, extreme tail probabilities can be\ncomputed, and therefore p-uniform behaves as it should,\nas can be seen from the dashed line in Figure 2. At the\nsame time, the p-uniform's estimate, also based on the\nKolmogorov-Smirnov statistic to ease comparison with\np-curve, is -5.296, which is clearly peculiar. Because a\nconfidence interval cannot be computed with the\nKolmogorov-\nSmirnov statistic, we also calculated the\nIrwin-Hall estimates with p-uniform; \nestimator is correct, its effect size estimate (< -5) is unre-\nalistically low; the probability of obtaining three positive\nstatistically significant studies when  = -5.484 is essen-\ntially zero. Furthermore, the CI of p-uniform is very wide.\nWe explain in the Supplemental Materials why these\nimplausible negative estimates can be obtained and what\ncan be concluded from these estimates. To deal with the\nimplausibly negative estimates of p-uniform and the\nerratic behavior of p-curve, we recommend setting the\neffect-size estimate of p-uniform and p-curve to zero in\nmeta-analyses where the mean of the significant p values\nof the primary studies is larger than .025 (Recommenda-\ntion 4). The cutoff of .025 is natural for two reasons. First,\nif the average p value equals .025, p-uniform actually esti-\nmates \n^ = 0. Second, average p values higher than .025\nyield negative effect size estimates, making testing redun-\ndant because the p value of the test would be above .5 and\nhence could not be statistically significant. Of course, the\ntrue effect size can be below zero, but a left-tailed hypoth-\nesis test then is required to examine whether the effect is\nsmaller than zero.\nBias in Effect Size Estimates for\np-Uniform and p-Curve From p-Hacking\nSimonsohn etal. (2014a) examined the effect of p-hacking\non effect-size estimation in p-uniform, considering three\ndifferent p-hacking strategies: data peeking, selectively\nreporting using three dependent variables, and selectively\nexcluding outliers. In data peeking (or optional stopping),\nobservations are added whenever a test is not yet statisti-\ncally significant. Their p-hacking strategy with multiple\ndependent variables refers to a practice whereby depen-\ndent variables are considered one by one, until one is\nfound for which the test was statistically significant,\nwhich is then published. Selectively excluding outliers\nrefers to deleting outliers whenever a test is not yet statis-\ntically significant. From their simulations of specific\nexamples of these three practices, they concluded that\np-curve underestimates effect sizes. However, p-hacking\ncomprises a large number of behaviors, and Simonsohn\netal. (2014a) examined only three of these behaviors. We\nnow show that other types of p-hacking lead to overesti-\nmation of effect size in p-curve and p-uniform.\naffects the p-curve's estimate through the conditional\np value distribution. For instance, data peeking and selec-\ntively excluding outliers lead to a distribution with rela-\ntively more conditional p values corresponding to just\nFig. 2. Values for Kolmogorov-Smirnov (KS) test statistics in implementa-\ntion of p-curve and p-uniform for the example with three observed effect\nsizes and p values close to .05. D stat = test statistics of KS test.\nstatistically significant results, which pulls the p-curve\n(and the p-uniform) estimate downward, as we explained\nin the foregoing section. On the other hand, p-hacking\nthat yields relatively more small p values results in an\noverestimation of effect size. Ulrich and Miller (2015) and\nBruns and Ioannidis (2016) illustrated that multiple\np-hacking behaviors may result in relatively more small\np values, which leads to overestimation of effect size in\np-curve (and p-uniform).\nWe examined the effect of two methods of p-hacking\non effect-size estimation in p-curve and p-uniform. The\nfirst method again involves selectively reporting among\nthree dependent variables but differs from the procedure\nin Simonsohn etal. (2014a) in one crucial aspect: Instead\nof the first significant p value, the smallest of three\nsignificant p values is reported. The second method\ninvolves a \"multiple-conditions\" scenario, whereby mul-\ntiple experimental conditions are tested and compared\nwith the same control condition, and only the compari-\nson yielding the largest difference (and smallest p value)\nis reported. We note that a large portion of surveyed psy-\nchologists have admitted to using at least once selective\nreporting among different dependent variables (63.4%)\nand not reporting all experimental conditions (27.7%) in\ntheir work (John, Loewenstein, & Prelec, 2012).\nFigure 3 presents the estimates of p-uniform, as well\nas the true effect size and the effect size of fixed-effect\nmeta-analysis (see the Supplemental Materials for the\ndetails of our simulations). We do not show the p-curve\nresults because these are almost indistinguishable from\nFig. 3.Effect size estimates in p-uniform and fixed-effect meta-analysis in case of four types of p-hacking. FE = fixed-effect; DV = dependent\nvariable.\nConducting Meta-Analyses Based on p Values 721\nthe p-uniform results. Conditions of \"first significant\ndependent variable (DV)\" and \"data peeking\" replicate\nthe simulations in Simonsohn etal. (2014a), showing\nthat p-uniform and p-curve indeed underestimate effect\nsize under these conditions. The estimate is slightly\nbelow the true effect size for the \"first significant DV\"\nand about .2 lower on the scale of Cohen's d for \"data\npeeking\" for all true effect sizes from 0 (no effect) to .8\n(considered a large effect). Conversely, and as antici-\npated, both \"DV with lowest p value\" and \"multiple con-\nditions\" overestimate effect size, and this overestimation\nincreases for larger true effect sizes. What should also\nbe mentioned is that p-uniform and p-curve did not\nalways outperform traditional fixed-effect meta-analysis\nin the p-hacking scenarios we simulated. For instance,\nfixed-effect meta-analysis outperformed p-uniform and\np-curve (i.e., presented less-biased estimates) in the\ncase of data peeking (e.g., Francis, 2013; van Aert, Maas-\nsen, Wicherts, & van Assen, 2016). We therefore con-\ncluded that (a) p-hacking may bias p-uniform and\np-curve estimates in any direction depending on the\ntype of p-hacking and (b) p-uniform and p-curve esti-\nmates are not necessarily better than those of fixed-\neffect meta-analysis when p-hacking occurs. Thus,\np-uniform and p-curve can deal with publication bias,\nbut (just like traditional fixed-effect and random-effects\nmeta-analysis) neither method corrects for p-hacking or\nreacts predictably to it.\nBecause the validity of results of both traditional meta-\nanalytic methods and p-curve and p-uniform may be low-\nered by p-hacking, we recommend scrutinizing both data\nand studies included in the meta-analysis before apply-\ning meta-analytic methods. Underpowered primary stud-\nies (i.e., statistical power substantially below 0.8) and a\npreponderance of p values just below .05 are signals for\np-hacking. Other signals are unsystematic deletion of\noutliers and reporting results of other than commonly\nused measurement instruments. If there are signs of\np-hacking, we recommend that applied researchers be\nreluctant to interpret the results of any meta-analysis\n(Recommendation 1).\nDiscussion and Conclusion\nRecently, new methods were developed to provide an\naccurate meta-analytic estimate in the presence of publi-\ncation bias (Simonsohn etal., 2014a; van Assen etal.,\n2015). These methods, p-uniform and p-curve, are based\non the same basic idea but differ in implementation. The\nidea underlying these methods is to select only the statis-\ntically significant results and estimate the effect size using\nthe principle of statistical theory that the distribution of\n(conditional) p values based on the true effect size is\nuniform. The researchers van Assen et\nSimonsohn etal. (2014a) convincingly demonstrated the\npower of p-uniform and p-curve and the principles upon\nwhich the methods are based to carry out meta-analyses.\nIn this article, we explained the rationale and basics of\nboth methods, added three reservations (concerning het-\nerogeneity, incredible estimates, and p-hacking) to the\napplication of both methods, and offered hands-on rec-\nommendations for researchers.\nWe explained that p-curve behaves erratically and\nyields inaccurate estimates in situations in which multiple\nstudies in a meta-analysis have p values close to .05. Due\nto a difference in implementation, p-uniform does not\nexhibit this erratic behavior but provides implausible\nnegative estimates. These problems are solved by setting\nthe estimate in p-uniform and p-curve to zero whenever\nthe mean of p values in statistically significant studies\nexceeds .025 (i.e., whenever the p-uniform estimate is\nlower than zero). We also showed that p-hacking may\nbias the estimates of p-uniform and p-curve in any direc-\ntion depending on the particular type of \np-hacking, and\nestimates from these methods are not necessarily better\nthan those of fixed-effect meta-analysis when p-hacking\nhas taken place. Finally, we explained that p-curve and\np-uniform estimate the average true effect underlying\nall significant studies in the meta-\nanalysis but overesti-\nmate the average true effect of the whole population of\nstudies whenever moderate-to-large heterogeneity is\npresent.\nOn the basis of these and contemporary insights, we\nformulated the recommendations summarized in Table 1.\nThese recommendations hold for any meta-analysis and\nextend the Meta-Analysis Reporting Standards (MARS)\nproposed by the American Psychological Association\nFirst, we recommend that researchers be reluctant to\ninterpret the results of any meta-analytic technique if\nthere are indicators of p-hacking in the primary studies\n(Recommendation 1) because p-hacking may bias the\neffect-size estimates of meta-analysis in any direction.\nIndicators of potential p-hacking include the unsystem-\natic deletion of outliers in many primary studies, the\nusage and reporting of multiple and different measures\nfor the same dependent variable across primary studies,\nthe common use of small underpowered studies, incon-\nsistencies between sample size descriptions and degrees\nof freedom (Bakker & Wicherts, 2014a), and grossly mis-\nreported p values (Nuijten, Hartgerink, van Assen,\nEpskamp, & Wicherts, 2015). In addition, p-hacking can\nbe characteristic of a particular research field (e.g., differ-\nent measures of dependent variables in a research field)\nas well as of a single study or a set of studies. Researchers\ncan conduct a sensitivity analysis by comparing the\nresults of traditional meta-analysis methods and \np-uniform\nand p-curve with the results of these methods applied to\nonly the studies in which no p-hacking is suspected (e.g.\nbecause they involved the use of preregistered data col-\nlection and analysis plans). Meta-analysts will probably\nobserve indicators of p-hacking (if these are present) dur-\ning the literature search and data extraction and do not\nhave to go through all the primary studies again to gather\ninformation about the potential presence of p-hacking.\nSecond, we recommend applying fixed-effect and ran-\ndom-effects meta-analysis and p-uniform or p-curve (Rec-\nommendation 2). The selection of a fixed-effect or\nrandom-effects meta-analysis should be based on\nwhether a researcher wants to draw inferences on only\nthe studies included in the meta-analysis (fixed-effect) or\nwants to generalize the meta-analytic results to the whole\npopulation of studies (random-effects; see Borenstein\nelaborate discussion on selecting fixed-effect or random-\neffects meta-analysis). Moreover, the estimate of fixed-\neffect meta-analysis, compared with the estimate of\nrandom-effects meta-analysis, may signal publication\nbias; publication bias generally results in higher estimates\nof random effects than fixed-effect meta-analysis because\nthe studies with smaller sample sizes and (usually) over-\nestimated effect sizes get less weight in fixed-effect meta-\nNext, we recommend checking for direct and indirect\nevidence of publication bias (Recommendation 3). Direct\nevidence can be obtained using \np-uniform's publication\nbias test. Previous research has suggested that \np-uniform's\npublication bias test has higher statistical power than tra-\nditional tests (van Assen etal., 2015), which are known\nto have low statistical power (e.g., Borenstein etal., 2009;\nSterne & Egger, 2005). Moreover, use of the quite popular\ntrim-and-fill method is discouraged because it often pro-\nvides inaccurate results (Moreno, Sutton, Abrams, etal.,\nber of studies in the meta-analysis or a small amount of\npublication bias, p-uniform's publication bias test lacks\nsufficient statistical power. In these cases, indirect evi-\ndence of publication bias may be used. An example of\nindirect evidence is if 80% or more of the effect sizes in\nthe primary studies are statistically significant when at the\nsame time the sample sizes of the studies imply a power\nof .5 or less to detect a medium effect size (e.g., see Fran-\ncis, 2013). In case of (direct or indirect) evidence of pub-\nlication bias, we recommend that conclusions be based\non the results of p-uniform or p-curve, rather than on\nfixed-effect and random-effects meta-analysis, because\nthese traditional methods overestimate effect size in the\npresence of publication bias (e.g., Bakker etal., 2012;\n2015). Although p-uniform and p-curve also provide\naccurate effect-size estimates even in the absence of\npublication bias (Simonsohn et\netal., 2015), we recommend interpreting fixed-effect and\nrandom-effects meta-analysis in this case because these\ntraditional methods yield more efficient and precise\nestimates.\nWe recommend setting the estimates of p-uniform and\np-curve to 0 if the average p value of statistically signifi-\ncant studies is larger than .025 (Recommendation 4); an\naverage larger than .025 signals no evidence of an effect\nor the use of p-hacking in the set of included studies (in\nwhich case, effect-size estimation from meta-analytic\nmethods may be biased in any direction depending on\nthe type of p-hacking; see Recommendation 1). Inter-\npreting the estimates of p-uniform and p-curve as the\naverage population effect size estimate is discouraged if\nthe effect-size heterogeneity is large (Recommendation\n5a). In this case, the p-uniform and p-curve estimate\nreflects the average true effect underlying all significant\nstudies in the meta-analysis. The average population\neffect size is overestimated (although the addition of\np-hacking could complicate this pattern further) when\nthere is moderate or large heterogeneity (I2  .5), and the\naverage true effect of the whole population of studies is\nestimated. To deal with heterogeneous effect sizes and\nstill be able to accurately estimate the average true effect\nof the whole population of studies, one can apply p-\nuniform or p-curve to homogeneous subgroups of pri-\nmary studies created on the basis of theoretical (e.g.,\nsame population of participants being studied) or meth-\nodological considerations (using the same methodology,\ni.e. study design and measures; Recommendation 5b).\nThe implication of Recommendations 3 and 5 is that cur-\nrently no method provides accurate estimates of average\npopulation effect size in the presence of both publica-\ntion bias and heterogeneity.\nIn the example meta-analysis described earlier, we\napplied p-uniform and p-curve to a set of primary studies\non the effect of weight on judgment of importance\n(Rabelo etal., 2015). Researchers can also easily apply\np-uniform or p-curve to their own data. User-friendly R\ncode for applying p-uniform can be readily installed.4\nMoreover, we developed a user-friendly web application\nfor researchers who are not familiar with R (https://rva\nnaert.shinyapps.io/p-uniform). R code for estimating\neffect size with p-curve can be found in the supplemen-\ntary materials of Simonsohn etal. (2014a). Advantages\nthat p-uniform has over p-curve are that p-uniform also\nincludes a publication bias test and yields a CI around\nthe effect-size estimate.\nTo conclude, even though both p-uniform and p-curve\nare promising meta-analytic methods, the methodology\nunderlying them is still under development, and proper-\nties of these methods still need to be examined under\nmore stringent conditions (e.g., different forms of\nConducting Meta-Analyses Based on p Values 723\np-\nhacking). Moreover, both methods need to be extended\nto allow estimation of other effect sizes, such as odds\nratios, which have their own idiosyncrasies. Once the\ncurrent methodology is refined further--particularly by\nenabling accurate estimation in case of heterogeneity--\nwe believe it has the potential to become the standard\nmeta-analytic tool correcting for publication bias. At pres-\nent, however, researchers should follow the recommen-\ndations provided in Table 1 to avoid drawing erroneous\nconclusions from these still-developing methods.\nAppendix A\nIllustration of Logic of and Computations\nin p-Uniform and p-Curve\nA simple example involving one control condition and\none experimental condition with 25 observations each\nillustrates the logic underlying p-uniform and p-curve.\nImagine that the true effect size () is 0.5 and three sta-\ntistically significant effects are found with a two-tailed t\ntest ( = .05) for testing the null hypothesis of no effect--\n.0257). Applying traditional fixed-effect meta-analysis to\nthese three observed studies yields an overestimated\nConditional p values are used in p-curve and p-uniform\n(i.e., conditional on the effect size being statistically sig-\nnificant). More precisely, the conditional p value of an\nobserved effect size refers to the probability of observing\nthis effect size or larger, conditional on the observed\neffect size being statistically significant and given a par-\nticular population (or \"true\") effect size. Statistical signifi-\ncance has to be taken into account because p-uniform\nand p-curve focus only on the interval with p values\na\ndcv\ndobs\ndcv\ndobs\ndcv\ndobs\nb\nc\nFig. A1. Computation of conditional p values for Effect 3 (q3\n) for three effect sizes:  = 0;  = 0.5 (true\neffect size); and  = 0.748 (estimate of fixed-effect meta-analysis). Critical value on Cohen's d scale is\ndenoted by dcv\nand observed effect size is denoted by dobs\n.\nbetween 0 and .05 rather than on the interval from 0 to 1.\nFigure A1 depicts how this conditional p value of Effect 3\nis computed for three different candidates of the underly-\ning effect size, namely,  = 0,  = 0.5 (i.e., the true effect\nsize), and  = 0.748 (i.e., estimate of fixed-effect meta-\nanalysis). Figure A1a reflects the conditional p value for \n= 0, which is calculated by dividing the probability of\nobserving an effect size larger than the observed Effect 3\n(dark gray area in Figure A1a to the right of observed\neffect size, dobs\n) by the probability of observing an effect\nsize larger than the critical value (light and dark gray\nareas in Figure A1a to the right of critical value, dcv\n). For  =\n0, the null hypothesis being tested, this boils down to divid-\np value (denoted by q) for Effect 3 of q3\ntimes the traditional p value.\nIn computation of the conditional p values under\neffects that differ from zero, calculations closely resemble\nthe computation of statistical power of a test. Consider\nthe conditional p value of Effect 3 at  = 0.5 (Figure A1b).\nThe critical value (dcv\n) and the observed effect size (dobs\n)\non the Cohen's d scale remain the same, but the distribu-\ntion of true effect size () is now shifted to the right. The\nnumerator in computing the conditional p value expresses\nthe probability that the observed effect size dobs\nor larger, given  = 0.5 (dark gray area in Figure A1b to\nthe right of dobs\ndenominator expresses the probability that the observed\neffect size is statistically significant given  = 0.5 (light\nand dark gray areas in Figure A1b to the right of dcv\n),\nwhich equals 0.419 (i.e., the traditional power of the\nstudy, given its degrees of freedom and  = 0.5). This\nyields a conditional p value for Effect 3 at  = 0.5 of\nputed in a similar way: q3\nThe conditional p values of all three observed effect\nsizes in our example under the three different true effect\nsizes are presented in Figure A2. The solid black lines in\nthe left panel of Figure A2 shows the conditional p values\nThe dashed gray lines in the left panel illustrate uniformly\ndistributed conditional p values. In case of three studies,\nthese uniformly distributed conditional p values should\nequal \u00bc, \u00bd, and \u00be. Note that the observed conditional\nlower than their corresponding expected uniformly dis-\ntributed conditional p values, which sum to \u00bc + \u00bd + \u00be =\n1.5. Hence, we see that the conditional p values under\nthe null hypothesis ( = 0) as given in the left side of\nFigure A2 do not fit a uniform distribution.\nTo obtain the effect size estimate of p-uniform, effect\nsize () has to be shifted until the sum of conditional\np values equals 1.5, which is the expected value of the\nFig. A2. Observed conditional p values (solid black lines) and conditional p values under uniformity (dashed gray lines) for the example\nwith three observed effect sizes. The three panels refer to the conditional p values for the p-uniform hypothesis test of no effect ( = 0),\np-uniform effect size estimate ( = 0.5), and effect size obtained by fixed-effect (FE) meta-analysis ( = 0.748).\nConducting Meta-Analyses Based on p Values 725\nsum under uniformity (i.e. given the true effect size). Fig-\nure A3 shows the effect of shifting  on the conditional\np values from -.5 to 1.5 for the three observed effect sizes\nin our example. Each conditional p value increases when\nthe true effect size gets larger. For instance, the conditional\ntrue effect size is increased from 0 to .5, and it is further\nincreased to .459 if true effect size is increased to .748. As\na consequence of these increases, the sum of conditional\np values also increases as true effect size increases.\nThe middle panel in Figure A2 presents the condi-\ntional p values in case the effect size is shifted to  = 0.5.\nThese conditional p values are also shown in Figure A3\nas the intersections of the three curves with the vertical\nline representing  = 0.5 and equal q1\nThese conditional p values exactly match (and studies\nwere selected to exactly match) the expected conditional\np values under uniformity. Consequently, the sum of the\nconditional p values also equals the sum of the condi-\ntional p values under uniformity (1.5). This indicates that\nthe effect size estimate of p-uniform will be equal to the\ntrue effect size of 0.5.\nThe right panel in Figure A2 and the intersections of\nthe curves of the study with line  = 0.748 in Figure A3\nshow the conditional p values conditional on the effect\nsize  = 0.748, which was the estimate of traditional\nfixed-effect meta-analysis:\nAll are higher than their corresponding expected condi-\ntional p values under uniformity, and their sum (2.031) is\nlarger than the expected sum under uniformity (1.5).\nThese results indicate that traditional fixed-effect meta-\nanalysis overestimated the effect size. In such a case, it is\nnot farfetched to suppose that publication bias exists (i.e.\nsome nonsignificant results are missing from the set of\nstudies included in the meta-analysis).\nTable A1 shows the results of applying p-uniform and\np-curve to the example. The effect size estimated by\np-uniform is exactly equal to the true effect size of  = 0.5.\nand the finding that both the null hypothesis of no effect\n(p = .0737) and the hypothesis of no publication bias (p =\n.147) cannot be rejected.6 The output of p-curve incorpo-\nrates neither a confidence interval nor a publication bias\ntest. The p-curve estimate of .511 is slightly larger than\nthe true effect size,7 and the p-curve result of the test of\nno effect is p = .086. Why are the results of both methods\ndifferent if they are based on the same logic? This is\nbecause the methods differ in implementation, which we\nexplain in the Supplemental Materials.\nFig. A3.Conditional p values as a function of true effect size (x axis)\nfor each of the three observed effect sizes in the example. Effect sizes\nzero ( = 0), true effect size ( = 0.5), and estimated by fixed-effect\nmeta-analysis ( = 0.748) are indicated by vertical lines.\nTable A1. Results of p-Uniform and p-Curve Applied to the\nExample Based on Three Observed Effect Sizes\nMeasurement p-uniform p-curve\nTest of H0\n:  = 0 refers to the null hypothesis of no effect. CI =\nconfidence interval.\nTable B1. Studies and Corresponding Sample Sizes (Group 1: n\ni\nand Group 2: n\ni\n), t Values and Two-\nTailed p Values Included in the Meta-Analysis Described in Rabelo, Keller, Pilati, and Wicherts (2015)\nArticle and experiment n\ni\ni\nt p (two-tailed)\nNote. Exp. = experiment.\nAppendix B\n"
}