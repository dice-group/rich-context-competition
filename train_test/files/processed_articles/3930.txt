{
    "abstract": "Abstract\nThe Snowden revelations and the emergence of `Big Data' have rekindled questions about how security practices are\ndeployed in a digital age and with what political effects. While critical scholars have drawn attention to the social, political\nand legal challenges to these practices, the debates in computer and information science have received less analytical\nattention. This paper proposes to take seriously the critical knowledge developed in information and computer science\nand reinterpret their debates to develop a critical intervention into the public controversies concerning data-driven\nsecurity and digital surveillance. The paper offers a two-pronged contribution: on the one hand, we challenge the\ncredibility of security professionals' discourses in light of the knowledge that they supposedly mobilize; on the other,\nwe argue for a series of conceptual moves around data, human\u00adcomputer relations, and algorithms to address some of\nthe limitations of existing engagements with the Big Data-security assemblage.\n",
    "reduced_content": "Original Research Article\nThe (Big) Data-security assemblage:\nKnowledge and critique\nClaudia Aradau1 and Tobias Blanke2\n Keywords\nBig Data, security, critique computing and information science, assemblage, surveillance\nIntroduction\nIn the wake of the Snowden revelations, the question of\nhow data is used for security purposes has re-emerged\nas a political problem. Critical inquiries around data\nand security are not new: from the production of trace-\nability (e.g. Bonditti, 2008), proactive and pre-emptive\nmanagement through data analytics (Aradau and van\nmanagement, algorithmic governance or dataveillance\nin the `war on terror' (Amoore and de Goede, 2005;\nRaley, 2013), the literature on security practices has\nanalysed the multiple facets of the social and political\ntransformations that the proliferation and increased\nuse of data have entailed. The Snowden revelations\nand the emergence of `Big Data' have rekindled these\ndebates and prompted new inquiries into how digital\npractices and Big Data devices are deployed for the\npurposes of security and with what political effects.\nThese transformations have so far been analysed as\npart of the `computational turn' in security governance,\nwith data mining, predictive analytics and algorithmic\ndecision-making playing an important role in the digital\ntransformation of security (Amoore, 2011; Rouvroy,\n2012). Big Data is an indicator of the transformations\nthat digital information has brought about by being\n`too big to know' (Weinberger, 2011). These dynamics\nhave far-reaching implications for privacy and data\nprotection, alongside civil liberties and human rights,\nwhich appear to be most at stake (Bauman et al., 2014;\nlars have concentrated mostly on social, political and\nlegal challenges to Big Data and have paid less atten-\ntion to the controversies around concepts and devices\nin computer and information science. The debates\nabout the transformations that are underway have\noften tended to embrace particular dominant represen-\ntations of this perceived computational or, more\nrecently, Big Data `revolution' rather than attending\n1Department of War Studies, King's College London, Strand, London, UK\n2Department of Digital Humanities, King's College London, Strand,\nLondon, UK\nCorresponding author:\nTobias Blanke, Department of Digital Humanities, King's College London,\nEmail tobias.blanke@kcl.ac.uk\nBig Data & Society\nbds.sagepub.com\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 3.0 License (http://\nwww.creativecommons.org/licenses/by/3.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nto the contestations and controversies about computing\nand digital knowledge formulated in these disciplines.\nThis paper investigates how the supposed `novelty'\nimplied by a digital transformation of security practices\nhas been particularly put to use by security and intelli-\ngence experts in order to justify an urgent need for\nnovel responses to anticipate and pre-empt the `next\nterrorist attack'. For security professionals, Big Data,\nin particular, stands for the promise of solutions to\ncontemporary security problems. Here, they are not\nso different from other institutional actors and strate-\ngies, from the development of e-science to new forms of\ne-government or a renewed focus in commercial organ-\nizations on their data assets. Critical scholars have simi-\nlarly embraced this discourse of novelty in which Big\nData is a `game changer' (Hildebrandt, 2013: 8). We\ncontend that these assumptions of novelty of a Big\nData `revolution' and game change in science \u00ad and\nby extension in governance practices \u00ad limit the poten-\ntial of critical engagement. In analysing an emerging\nBig Data-security assemblage which brings together\nheterogeneous modes of knowledge, devices, institu-\ntions and methods, we address the impasses of critical\ndiscourses as formulated by civil liberties activists as\nwell as critical scholars of security and Big Data.\nAlthough the methods used by intelligence agencies\nlike the National Security Agency (NSA) and\nGovernment Communications Headquarters (GCHQ)\nare secret, we start from the assumption that it is\nunlikely that their methods would be widely different\nfrom the state of the art in computing and informa-\ntion science, and the practices developed in dealing\nwith Big Data in academic organizations and commer-\ncial institutions. In this sense, the paper understands\nintelligence agencies as Big Data organizations that\nemploy data-driven methods to anticipate future dan-\ngers. A collaboration between social and computer\nscientists, as this paper proposes, can help go\nbeyond the inscrutability of algorithmic methods in\nsecurity practices.\nIn so doing, this paper develops a contribution to\ncritical data studies (Kitchin, 2014) and critical\napproaches to security and surveillance (Amoore,\nto take seriously the critical knowledge developed in\ninformation and computer science and reinterpret\nthese debates to offer a critique of the common repre-\nsentations by security professionals of the digital trans-\nformation of their practices. The paper makes three\nmoves that recast existing critical engagements with\ndata-driven security: from data/metadata distinctions\nto the production of data as a complex epistemic\nentity; from a `computational turn' in surveillance to\nthe division of labour between humans and computers\nin socio-technical assemblages; from an underlying\nlogic of algorithms to algorithmic practices and meth-\nods in security analytics.\nThese moves are developed through the analysis of\nthree sites of public controversy about NSA and\nGCHQ surveillance and the use of Big Data for security\ngovernance in the wake of the Snowden revelations.\nThe first controversy concerns the definition of digital\ndata, and particularly the distinction between metadata\nand content, which has been used repeatedly by security\nprofessionals to justify bulk data collection. The second\ncontroversy we discuss relates to mass surveillance and\nthe relationship between humans and machines in jus-\ntifying surveillance practices. Here, a `computational\nturn' in surveillance is used by security professionals\nas a reassurance that no privacy is invaded as only\ncomputers look at bulk-collected data. A third justifi-\ncation that security professionals have promoted is that\nthey need to collect everything and make data big for\nalgorithms to develop anticipatory knowledge of the\n`next terrorist attack'. Contra these discourses of nov-\nelty, we show how debates in computer and informa-\ntion science can be mobilized to challenge the security\nprofessionals' claims to credible knowledge.\n(Meta)data and the remaking of\nsecurity knowledge\nSince the Snowden revelations, a new concept has\nentered the public vocabulary: metadata. Long used by\narchivists and computer experts, metadata has more\nrecently been at the heart of controversies about security\npractices. President Obama argued that the NSA pro-\ngramme was not gathering data but metadata, namely\nhow long a call was or where it was made from. He\nreinforced that metadata collection would then be differ-\nent from surveillance and proceeded to allay the public's\nfears: `Nobody is listening to your telephone calls. That's\nnot what this program is about' (Obama, 2013).\nMetadata is therefore seen as not encroaching upon\nrights or privacy, as it does not reach content. In the\nUK, Theresa May has also claimed that privacy con-\ncerns only come into effect `at the point at which the\ncommunication is opened' (Wheeler, 2014). Metadata is\nsupposed to not convey information about what people\nsay or do in their homes. In that sense, metadata is\nrendered as the opposite of the telephone tap and\nthe secret agent listening in for revealing clues. It is\nalso the opposite of the camera, the extended CCTV-\nsurveillance that appears to pry into the intimate and\nintricate details of everyday life. The content/metadata\ndistinction therefore justifies the practices of intelli-\ngence agencies, as ultimately they are deemed to be\nusing a qualitatively different form of data.\nYet, this benign reading of metadata has been chal-\nlenged by critics, who have argued, on the one hand,\n2 Big Data & Society\nthat metadata is data about content and that the uses of\nmetadata were no more devoid of knowledge than data\nis, on the other. For instance, Edward Snowden has\npointed out that `[m]etadata is what allows an actual\nenumerated understanding, a precise record of all the\nprivate activities in all of our lives' (Plume, 2014). In an\nAmici curiae brief for the ACLU v Clapper case in the\nUSA, filed by a coalition of non-governmental organ-\nizations (NGOs) in the wake of the Snowden revela-\ntions, several computer scientists have emphasized the\nsensitivity of metadata in similar terms and challenged\nthe hierarchy that President Obama and the intelligence\nexperts set in place. `The pool of telephony metadata\ncollected by the government', they note, `reveals a\nwealth of deeply personal and intimate information\nabout millions of Americans' (Abelson et al., 2014).\nThese critics draw attention to the relationships\nbetween (telephony) metadata, content, and informa-\ntion. Networks of people can coalesce around certain\nphone numbers of, for instance, their local sports\ncentre. Thus, telephony metadata oozes with mean-\ning, which makes its distinction from content\nproblematic.\nIndeed, this distinction between metadata and con-\ntent has been contested by critical scholars and civil lib-\nerties activists alongside whistle-blowers and computer\nscientists. However, we contend that there are limita-\ntions to the critical arguments that metadata\u00bc data \u00bc\nknowledge. In information science, the field of know-\nledge engineering covers the transformation of content\ninto data that computers can process. For knowledge\nengineering, content is really anything that can be\nexpressed digitally like any video, software, text, or\naudio. In our case, content is, for instance, a phone con-\nversation. Knowledge engineering sets out to discover all\ncomputer-actionable data in this content.\nCommunications (meta)data such as location and time\nis an important part of this data. Computers prefer\n(meta)data to content, as the former is structured\nand semantically defined for their processing. The dis-\ntinction between content and (meta)data, which is\ninvoked and justified by the professionals of security\nand politics, relies on an implicit hierarchy of knowledge\nproduction that underlies the field of knowledge engin-\neering. In computing and information science, this hier-\narchy is generally referred to as data-information-\nknowledge (DIK) and is widely discussed as one of the\nfoundational issues of these disciplines. The taxonomy\nof data-information-knowledge starts with `raw' data\nand systematically builds information and finally\nknowledge.\nFor the purposes of our argument, what counts is\nthe fact that professionals of security and politics rele-\ngate (meta)data to the bottom of the supposed DIK\nhierarchy. This has important implications, as it\nmight mean that the collection of data in all its forms\nis regarded as irrelevant and `encourages the mindless\nand meaningless collection of data in the hope that one\nday it will ascend to information \u00ad pre-emptive acqui-\nsition' (Fricke\narchy enables Obama and others to claim that the bulk\ncollection of metadata is not politically significant and\nonly becomes problematic once it has been transformed\ninto information and knowledge. Data becomes a raw\nand neutral material ready to be mined.\nAs the hierarchy of DIK is translated into the polit-\nical realm, security professionals take up what is an\nessentially messy distinction from computer and infor-\nmation science and transform it into an absolute one in\norder to justify knowledge production about `known\nand unknown terrorists'. In a declassified opinion\nfrom the US Foreign Intelligence Surveillance Court\n(FISC) in the wake of the Snowden revelations, Judge\nClaire Eagen noted that `it is necessary to obtain the\nbulk collection of a telephone company's metadata\nto determine those connections between known\nand unknown international terrorist operatives . . .'\nTo avoid this move to the irrelevance of (meta)data,\ncomputer and information scholars have challenged\nthe strict separation of categories and the DIK hier-\narchy. This move is not quite the same as the recent\nre-evaluation and claim by critical scholars that `there is\nno such thing as raw data' (Gitelman, 2013; Lyon,\n2014) or that data is not simply `pre-analytical and\npre-semantic' (Markham, 2013). Information scientists\nhave indeed long argued that the imagination of data as\n`discrete objects that can be located in time and space'\nor as `raw numbers and facts' (Alavi and Leidner,\nfurthest by stating that `data is more than knowledge'\n(1999). Yet, they have also emphasized that data needs\nto be understood in the context of more fluid bound-\naries between data, information and knowledge (Alavi\nAmici curiae mentioned supra, computer scientists have\npointed out that:\nAlthough the law may try to draw hard and fast dis-\ntinctions between the two, see, e.g., Smith v. Maryland,\nand typically depends on context. A change in technical\nprotocols or standards can cause information tradition-\nally regarded as metadata to be treated as content,\nand vice-versa. But the task here is not to define ``meta-\ndata,'' nor do amici believe it practical or useful to do so\nin a categorical way. (Abelson et al., 2014,\nemphasis ours)\nAradau and Blanke 3\nWhile the interpretation of metadata as yielding infor-\nmation about individuals has ultimately come to be\naccepted by the US Court of Appeals of the Second\nCircuit in its judgement in ACLU v Clapper, the\nCourt's interpretation of metadata remains beholden\nand does not take up the computer scientists' argument\nabout the problematic boundaries between kinds\nof data.\nData and metadata both refer to particular practices\nof knowledge production in the context of the field of\nknowledge engineering, which simultaneously draw\nboundaries (often through standards) between struc-\ntured/unstructured data, information and knowledge.\nThese distinctions are compounded by political distinc-\ntions between kinds of data and metadata. The UK\nParliament's Intelligence and Security Committee\nrecent report on security and privacy recognises these\nfluid epistemic and political boundaries:\nMetadata is a term commonly used in the USA, but it\nhas no legal definition in the RIPA and therefore no\nbearing on the UK system of interception. For exam-\nple, in the UK a record of a website visited (e.g. http://\nwww.google.com) is treated as CD [communications\ndata], whereas the full web address, which\nincludes the precise words searched (e.g. http://www.\ngoogle.co.uk/search?q\u00bcISC), is treated as content.\nThe critical move is not to ask for clarity and defin-\nitions more adapted to digital technologies, as many\nNGOs and legal scholars suggest.2 It is also not suffi-\ncient to subsume these to an overarching concept of\nknowledge. Translating concepts of knowledge from\nsocial science to this debate risks reproducing a similar\ndis-counting of data. It also risks downplaying the pro-\ncesses of (meta)data production. (Meta)data is not\nsimply a question of interpretation by analysts\n(Bauman et al., 2014) or of data always being\n`cooked'. (Meta)data can simply be technical, like the\ntime and place of a particular phone call, and still be\nmeaningful.3\nThe turn to metadata in the emerging Big Data-\nsecurity assemblage needs to be understood in the con-\ntext of an economy of Big Data production where\n`digital sources create data as a by-product' (Ruppert\net al., 2013) and we become `walking data generators'\n(McAfee and Brynjolfsson, 2012). Data itself is a com-\nplex epistemic object, and distinctions between kinds of\ndata are produced depending on how data is actionable\nby digital devices. In another Amici curiae submitted by\nthe Electronic Frontier Foundation and ACLU in\nanother legal case, Klayman v Obama, we are reminded\nthat `structured data, including telephony metadata, is\nideally suited for computational analysis' (EFF and\ntion and the critique of the DIK hierarchy are import-\nant moves that challenge the justificatory discourses of\nsecurity professionals. At the same time, the problem-\natization of metadata in the emerging Big Data-security\nassemblage needs to be supplemented by the under-\nstanding of how kinds of data are produced as action-\nable by digital devices. It is structured data that is an\nessential component in the work of Big Data organiza-\ntions (Ekbia et al., 2015). A critique of NSA\nand GCHQ surveillance practices entails taking ser-\niously the production of data as a heterogeneous epi-\nstemic and political object. `Making up people'\n(Hacking, 1999) needs to be supplemented today by\n`making up data'.\nSeeing like a computer? Big Data\nas artificial intelligence\nA second argument in the controversies about digital\nsurveillance has been formulated in terms of controlled\nentries to and views onto data. Critics have drawn\nattention to the practices of `mass surveillance' and\ntheir effects on human rights and democracy (Bauman\nPoullet, 2009). Security professionals have attempted to\njustify these practices as non-intrusive by arguing that it\nis computers that read data first and foremost, while\nhumans only see little of what is otherwise processed by\nmachines. Justificatory discourses of `bulk data collec-\ntion' and targeted rather than mass surveillance have\nhinged upon a distinction between humans and\nmachines, computers and analysts. In an op-ed for\nthe New York Times, Charles A Shanor, a professor\nof law, asks:\nshouldn't I be concerned that F.B.I. agents are tram-\npling my rights, just like the I.R.S. might have trampled\nthe rights of certain organizations seeking tax-exempt\nstatus? As it turns out, the answer is no. The raw `meta-\ndata' requested will not be directly seen by any F.B.I.\nHe goes on to argue that it is in fact a computer that\nsorts `through the millions of calls and isolates a very\nsmall number for further scrutiny' (Shanor, 2013) and\nfinds it a better option than the transparency advocated\nby human rights activists. In the ACLU v Clapper case\nmentioned earlier, Judge William Pauley III based his\ndecision to dismiss the case on similar arguments put\nforth by Theresa Shea, Director of Signals Intelligence\nDirectorate at NSA, that `only a very small percentage\nof the total data collected is ever reviewed by intelligence\nanalysts' (2013). Shea explicates that `[a]lthough bulk\n4 Big Data & Society\nmetadata are consolidated and preserved by the NSA\npursuant to Section 215, the vast majority of that infor-\nmation is never seen by any person' (2013: 8, emphasis\nours).\nShea and other intelligence experts invoke an ana-\nlogy between NSA bulk data processing and targeted\nsurveillance. Ultimately, the assumption is that there is\nno surveillance where data in not `seen' by a human\nbeing. The human/machine distinction, with humans\nsupposedly only coming in at the end of the data pro-\ncessing, aims to render these practices of data collection\nand processing legitimate by enacting a strong separ-\nation between humans and machines. These justifica-\ntions do not only deactivate criticisms about mass\nsurveillance, but are mobilized to disable claims\nbefore the law. James R Clapper, the Director of US\nNational Intelligence, pursues this strategy to ask for a\ndismissal in ACLU v Clapper: `those injuries could arise\nonly if metadata associated with plaintiffs' calls were\nactually reviewed by a person, and plaintiffs do not\ndispute that only a small fraction of the Section 215\ntelephony metadata is actually reviewed by any\nIn the UK, the 2013 Annual Report of the\nInterception of Communications Commissioner reiter-\nates this logic, where GCHQ data mining is deemed\nlegal given that `intrusion in this context into the priv-\nacy of innocent persons would require sentient examin-\nation of individuals' communications' (May, 2014,\nemphasis ours). In the UK Parliament's recent public\ninquiry on security and privacy, the reasoning follows\nthe same strong binary of human/machine, as the\nreport concludes that only `a tiny fraction' of collected\ndata is `ever seen by human eyes' (Intelligence and\nThe justification of the separation between humans\nand computers is difficult to sustain if we understand the\ncomputing involved in terms of socio-technical assem-\nblages, an unstable and contingent collection of hetero-\ngeneous elements (Latour, 2005). Yet, this distinction\nhas been much more resistant to critique than the meta-\ndata/data distinction. To push our critical vocabularies\nfurther, it is not enough to point out that the Big Data-\nsecurity assemblage is socio-technical. We need to\nunderstand not just human\u00adnon-human relationality,\nbut also `the content of the relationships that hold\nin the Big Data-security assemblage is how the relation\nbetween humans and computers gains content, and how\nthe assembling of humans and computers is both an\nassociation and a division of labour.\nBig Data organizations are characterized by a new\ndivision of labour between humans and computers\nabout integrating human and machine reasoning at\neach step of processing relevance and learning how to\ndistinguish significant and non-significant information\nfrom each other. In this division of labour, humans and\nmachines are brought together in the same infrastruc-\ntures to process the data. Recent advances in informa-\ntion systems have focused on connections between\nhumans and computers, given a fundamental shift in\nhow these systems are designed as artificial intelligence\napplications. A historical perspective on information\nsystems for artificial intelligence helps shed light on\nthis division of labour.\nIn social sciences, debates about automation,\nrobotics, and data-driven science also suggest a\nchange in human/non-human assemblages. Much crit-\nical work has focused on a shift towards computational\ndecision-making and has downplayed the division of\nlabour between humans and computers. We propose\na different reading of human\u00adcomputer assemblages\nby revisiting historical debates and transformations in\nthe wake of the so-called `winter of artificial intelli-\ngence' (AI). The phrase captures the perceived failure\nin the late 1980s of attempts to develop analytical capa-\ncities that would bring computing machines close to\nhuman intelligence. All the enthusiasm and the early\npromises of creating the thinking machines seemed\ngone. However, theoretical and practical breakthroughs\nin artificial intelligence capacities have since put in doubt\ndeveloped a new type of `intelligence' by combining\nhuman and computer reasoning.\nIn a reply to Noam Chomsky's critique of artificial\nintelligence, Peter Norvig (2012), Google's Director of\nResearch, captures the epistemic transformation that\ntook place in the 1980s. The `winter of AI' was related\nto an over-reliance on logical models to simulate\nhuman reasoning and their subsequent failure.\nComputing scientists wanted to create an artificial\nintelligence that replicated human intelligence, but\nwas separate from humans. Norvig (2012) locates\nan epistemico-material transformation from logical\nmodels to statistical models which `have achieved a\ndominant (although not exclusive) position.' Unlike\nlogical models, statistical models focus on assemblages\nof humans and machines that can process data. Rather\nthan re-creating a black box that veils how humans\nreason, artificial intelligence has focused on building\nmodels that can solve particular problems. These\nmodels are developed within complex workflows of\nhuman\u00adcomputer interaction, starting with large test-\nbeds to tune algorithms so that they simulate human\njudgement on information relevance or expressed sen-\ntiments. In order to perform, algorithms need to be\nconstantly evaluated for their effectiveness.\nThe epistemic crisis of AI has led to a reconfigur-\nation and integration of human work and intelligence\nAradau and Blanke 5\ninto new human\u00adcomputer assemblages. Information\nsystems to develop analytical capacities have thus\nbeen built in a similar way since the winter of AI.\nSecurity systems in all their components share with\nother data science applications that they are also the\nresult of the winter of AI, when research and funding\nshifted from `wild-eyed dreams' of creating a human-\nlike machine and started to concentrate on particular\napplications that were made possible by fostering dif-\nferent connections and division of labour between\nhumans and computers.\nIn these models, humans do not interact with the\ndata just at the end, but are involved at every stage\nthrough evaluation, optimization, training, etc. Yet,\nthis does not mean that computers are simply passive\ntools, because as much processing as possible needs to\nbe computerized. Computers should learn `unsuper-\nvised' or, to put it differently, develop agency in these\nassemblages. A successful unsupervised learning tech-\nnique is, for instance, the so-called topic modelling,\nwhich auto-summarizes a collection of documents\ninto a number of common topics. In security applica-\ntions, for instance, topic modelling is used to summar-\nize cyber-threats in web data mining. However, these\ntopic models can be too suggestive and require careful\nintervention by humans (Schmidt, 2011). Even so-\ncalled `unsupervised' techniques are not black boxes\nbut a human\u00adcomputer assemblage.\nSimilarly, since 2006 we have witnessed yet another\ntransformation in machine learning techniques through\n`deep learning' for speech and image recognition.\nAgain, however, as soon as one analyses the compo-\nnents of this new black box, it becomes apparent that\neven deep learning as the state-of-the-art in unsuper-\nvised computational learning requires human participa-\ntion. Rather than becoming autonomous, computers\nare still enrolled in a socio-technical assemblage. Two\nleading machine-learning researchers, Socher and\nManning, thus define machine learning as the `numer-\nical optimization of weights for human-designed repre-\nsentations and features' (2013). Security analytics is no\ndifferent from other domains that enrol computers,\nartificial intelligence practices and data scientists.\nIn analysing this emerging assemblage through the\ntransformation of AI, we have built upon critical work\nthat draws attention to the socio-technical character of\nBig Data (Lyon, 2014). Yet, our analysis also shows\nhow relations in a socio-technical assemblage gain con-\ntent historically. Debates in AI have reconfigured the\nhuman\u00adcomputer assemblage through an epistemico-\nmaterial division of labour between humans and com-\nputers. This re-reading of Big Data through the history\nof AI also allows us to challenge the credibility of the\ndistinctions that security professionals attempt to insti-\ntute between computer-based targeted surveillance and\nmass surveillance by showing how the `winter of AI'\nhas led to particular modes of assembling computers,\ntechniques of machine learning and (data and security)\nanalysts.\nAlgorithmic security: Anticipation\nand probabilities\nA third and related site of controversy in the emerging\nBig Data-security assemblage has focused on the epi-\nstemic capabilities of algorithms. Intelligence experts\nspeak about these capacities as finding the `needle in\na haystack' and the NSA's General Alexander as `con-\nnecting the dots'. In an intervention before the US\nCongress following the Snowden revelations, former\nFederal Bureau of Investigation (FBI) Director\nRobert Mueller noted that `If you narrow [the scope\nof surveillance], you narrow the dots and that might be\nthe dot that prevents the next Boston' (Roberts, 2013).\nThe shift in reasoning towards the anticipation of pos-\nsibilities, conjecture and speculation underpins the jus-\ntifications that security professionals proffer for the\nnecessity of extensive Big Data mining. Critical security\nand surveillance scholars have analysed the recalibra-\ntion of security practices through anticipatory know-\ndata studies have highlighted to the predictive fallacy\nof Big Data, the tension between correlation and caus-\nation, the `return to empiricism', and the opacity of\nalgorithms (Boyd and Crawford, 2012; Ekbia et al.,\nAs important as these criticisms are, they do not\naddress the perceived necessity to `collect it all', the\nwhole `haystack' of data, in order to enhance the cap-\nabilities of algorithms. As Judge Pauley III glosses in\nACLU v Clapper, the bulk metadata collection pro-\ngramme is `a wide net that could find and isolate gos-\nsamer contacts among suspected terrorists in an ocean\nof seemingly disconnected data' (ACLU v Clapper,\n2015). Security professionals talk about `needles in hay-\nstacks' and take the necessity of collecting and creating\nan ever-larger `haystack' of data for granted. Thus, the\nfact that all the data is needed for the purposes of ter-\nrorism prevention becomes unquestionable:\nNo doubt, the bulk telephony metadata collection pro-\ngram vacuums up information about virtually every tele-\nphone call to, from, or within the United States. That is\nby design, as it allows the NSA to detect relationships so\nattenuated and ephemeral they would otherwise escape\nThese statements become meaningful through the\npromise of algorithms to unveil the `unknown\n6 Big Data & Society\nterrorists' through the anomalous clues and features\nthat cannot be easily clustered and do not fall under\na normal pattern.4 Algorithms appear to institute the\nnew: new processes, rationalities, and techniques of\ndecision-making. Critical discussions about algorithms\nand algorithmic reasoning have focused on the `ontol-\nogy of association' (Amoore, 2011) and the secret\nnature of the algorithms used by intelligence agencies\nand businesses (Pasquale, 2015). We argue that critical\ndiscourses need to engage more closely with algorithmic\npractices. Here, we focus on two elements of algorith-\nmic practices: the relation between data and algorithms,\nand probabilistic methods.\nFirstly, we take seriously Norvig's widely quoted\nclaim that Google does not necessarily have better algo-\nrithms than everybody else, but more data (quoted in\nSchutt and O'Neil, 2013). Marissa Mayer, Google's\nformer VP of Search Products and User Experience,\nhad also noted `that having access to large amounts\nof data is in many instances more important than creat-\ning great algorithms' (quoted in Perez, 2007). Data has\nbecome more important than algorithms themselves,\nbecause `[t]here is no single scientific breakthrough\nbehind Big Data. On the contrary, the methods used\nhave been well known and established for quite some\nwe analyse algorithms through the probabilistic meth-\nods that all algorithms deploy independent of their\nexact design in particular institutions. Discourses of\nthe `novelty' of Big Data and algorithmic capacities\nneed to be located within probabilitistic methods and\ntheir limitations, which are the foundation of Big Data\nanalytics as they allow reasoning about uncertainty\n(Bengio et al., in preparation).\nJustificatory discourses of the capabilities of Big\nData for security governance activate a particular\nimagination of the relation between part and whole.\nThe haystack metaphor that security professionals are\nusing is no longer a metaphor for a sample size but for\nBig Data, where `N \u00bc all' (Mayer-Scho\n\u00a8 nberger and\nCukier, 2013). All data appears now to be needed or,\nin a formula repeated by Big Data enthusiasts, data is\n`unreasonably effective' (Halevy et al., 2009). This not\nonly drives Big Data to become even bigger but it also\nmotivates engineers to trust probabilistic reasoning as a\nway to render uncertainty mathematically (Fricke\n\u00b4 ,\nShifting from the focus on algorithms to the data\nthat algorithms need and the modes of probabilistic\nreasoning designed for algorithmic processes allows us\nto develop a critical vocabulary about the `needle in a\nhaystack'. Security professionals have justified this need\nfor an infinitely expanding haystack through the idea\nthat having all the data can algorithmically reveal\nbetter knowledge about potential terrorists, which\nwould make pre-emptive action possible. However,\nthis imaginary of Big Data that yields better knowledge\nhas been challenged in debates about Big Data. Bigger\ndata is not better `without limit' (Fricke\nLet us take an example, which has been at the heart\nof claims about the unreasonable effectiveness of data:\nGoogle. Google engineers can rely on the `unreasonable\neffectiveness' of web page data, because these pages\nconsist of words whose meaning can be derived from\nthe frequencies with which these words appear within\nweb pages. Over time, `human language has already\nevolved words for the important concepts' (Halevy\nhand, share with Big Data applications in humanities\nand social sciences the interest in shifting concepts and\n`minority' vocabularies, rare words and rare data.\nWhile standard algorithmic reasoning can be `success-\nful for a lot of things' that follow regularities, `it\namounts to a deliberate neglect of rare words' (White,\n2011). Statistical algorithms tend to ignore rare data,\nespecially as the models get more complex in social\ndomains and are tightly fitted to their original training\ndata. In the language of machine learning for Big Data,\nthese models are `overfitted' to the training data and are\nchallenged by new unknown data items. According to\nistical learning' requires us to add more and more\ndimensions to our data and move further away from\nthe rare variables.\nAnother example of the fallacy to `collect it all' for\nthe purposes of data analytics is the analysis by the\ncomputer scientist and outspoken critic of NSA data\nmining Edward Felten (2014), where he shows how in\ntypical data reasoning reduced data sets can lead to\nbetter results than large data sets. He argues that if\nthe NSA covers larger numbers of phone connection\nhops, starting from a phone number they have under\nsurveillance, this will not render the security analytics\nmore effective. As Felten shows using typical algo-\nrithms, the fewer the hops involved (i.e. the smaller\nthe sample), the better the results. The NSA drive to\ncollect and mine Big Data understood as `all data' is\nrevealed as a myth.\nThe drive to collect more and more data is not just\nbased on a Big Data myth but can be counterproduct-\nive not just for the fitting of the machine learning algo-\nrithms but also for their methods, as it generally leads\nto many false positives. Computer scientists involved in\nmining Big Data have argued for a long time that the\nlarger the haystack, the more likely it is that only mean-\ningless events are added. A comparison of random and\nnon-random events will lead to an overestimation of\nthe non-random ones. One of the standard textbook\nintroductions to machine learning makes an important\npoint for our discussion (Leskovec et al., 2011). In\nAradau and Blanke 7\noffering a standard example of predictive data analytics\nfor counter-terrorism, the authors develop a criticism of\nthe assumption that if the haystack is just big enough\nand enough data is collected, the ocean of data will\nautomatically also increase the possibilities to find the\nneedle in the data.\nStarting with a typical data mining case for counter-\nterrorism, Leskovec et al. (2011) work with the assump-\ntion that two people are considered to be suspicious if\nthey stayed more than once in the same hotel at the\nsame time, which is a typical assumption in security\nanalytics. In their example, if one billion people were\nto be tracked over a period of 1000 days, there will be a\nwell-defined probability that two random people stay at\nthe same hotel more than once. This probability is quite\nsmall and might therefore look promising for identify-\ning potential suspects. The problem is, however, that in\nthese millions of people there are many possible pairs of\npeople who could have stayed at the same hotel. In\n`suspicious' pairs (Leskovec et al., 2011). Therefore, as\ncomputer scientists know, the error in data mining\nmethods actually rises if more and more data is col-\nlected, which is contrary to the belief that an `ocean\nof data' makes it possible to identify `gossamer con-\ntacts' with more certainty.\nThe production of large numbers of false positives in\nthe `ocean of data' raises another question for Big Data\nmining \u00ad how to distinguish `real' and `false' suspects.\nAccording to Felten (2014), rather than predicting new\nterrorist activities, typical NSA security analytics algo-\nrithms are designed for more realistic use cases in order\nto avoid overfitting and overestimation of false posi-\ntives. These algorithms are designed to substantiate\nthe suspicion a security analyst already has, rather\nthan predict new suspects or suspect behaviour, as the\nanalogy of the `needle in a haystack' might suggest.\nThese algorithms are thus not good at eliminating sus-\npicion. In general, they do not help to find the needle in\nthe haystack but help confirm whether there is reason\nto assume that there might be a needle. Big Data\nmining has never been good at evaluating false posi-\ntives, which is less of an issue for an Internet search\nengine where users get used to skipping irrelevant\nresults, but is an issue in a security context where inno-\ncent people become suspicious.\nContra the FBI director Robert Mueller's concern\nthat a narrower focus of surveillance will miss the dots\nthat need to be connected in order to prevent the `next\nBoston' (Roberts, 2013), computer scientists have\nshown that it is often the wider focus that has this\neffect. If data grows big, both the apparent risk of a\nterrorist attack and the number of suspicious people\nwill vastly increase. In terms of the theory of data\nmining, this problem can be seen as typical of distribu-\ntions with heavy tails, which are those distributions that\nare extremely skewed with a long tail of events that\nhappen sometimes but not very often.\nThe `next terrorist attack' is a possible event of low\nprobability such as the `rare words' for which Google's\ndata enthusiasm is less reasonably effective, as dis-\ncussed supra. Security-relevant activities are generally\nheavy-tailed, as they are conducted by a small number\nof people compared to the overall population. The cor-\nresponding heavy-tailed distributions require special\nmethods in order to apply algorithmic reasoning\n(Clauset et al., 2007). General statistical assumptions\nabout what can be reasonably expected as the next\nevent do not work, because these are `distributions\nevent, for instance, does not reveal anything about\nthe expected behaviour of the distribution. For heavy-\ntailed distributions, events outside the tail do not indi-\ncate anything about events in the tail where the suspects\ncan be found. Similarly, the random behaviour of those\nwho are not suspicious does not tell us much about the\nbehaviour of suspects and vice versa. Making the data\n`big' will therefore not reveal anything that can be used\nto identify suspects, as long as we do not know whether\nthis data belongs to the tail.\nIn order for data mining to work on heavy-tailed\ndistributions, these are often split up into various sub-\ngroups where each of them is dealt with individually.\nTo make these divisions, cut-off points need to be\nfound that analysts consider as indicative of suspicious\nbehaviour in the tail. For instance, all those are sus-\npects who call the same phone number in Maryland.\nThese features generally cannot be just read from the\ndata, but an analyst needs to make a decision as to\nwhich features count to identify a suspect (Janert,\nories and domain assumptions like the feature that sus-\npects call the same number in Maryland. Formally\nspeaking, without assumptions about the data, Big\nData mining algorithms do not perform better on\nnew information than any random prediction; this is\nindependent of the size of the data set (Wolpert and\nThe metaphor of finding the needle in the haystack is\na powerful one used by Big Data professionals and then\nreused by security professionals, judges and politicians.\nWhile it seemingly justifies the huge effort of collecting\nmore and more data in order to cover all possibilities,\nthe metaphor effaces debates in computer science about\nthe epistemic limits of algorithmic practices and meth-\nods. Developing a critique of anticipatory algorithmic\nsecurity entails an immanent critique of algorithmic\npractices and methods.\n8 Big Data & Society\nConclusion: Theses on critique\nSecond Circuit overturned the decision in ACLU v\nClapper that granted the government's motion to dis-\nmiss and argued that the programme was unlawful\ndraws on the declaration by Edward Felten and other\nAmici curiae information, which have disputed the gov-\nernment's justification of telephone (meta)data collec-\ntion. While the Court does not address the challenge of\nconstitutionality, the decision highlights the importance\nof computing knowledge to challenge the credibility of\nclaims made by security professionals. At the same\ntime, the judges have left a series of other assumptions\nabout surveillance by computers and the algorithmic\ncapabilities unquestioned.\nIn order to address the impasses of critique in public\nand academic controversies, this paper has proposed a\nseries of reformulations of existing critical vocabularies\nof the Big Data-security assemblage. A collaboration\nbetween social sciences, and computer and information\nsciences, as developed here, can challenge the credibility\nof the justifications promoted by intelligence agencies\nsuch as the NSA and the GCHQ and contribute to\nresearch agendas in critical approaches to security\nand surveillance, on the one hand, and critical data\nstudies, on the other. Rather than assuming that\ninformatics is `the discipline of choice of liberal\na series of debates in knowledge engineering, artificial\nintelligence and data mining in order to address the\nlimits and impasses of critical engagements with Big\nData in security governance. Methodologically, we\nhave been able to eschew the invocation of secrecy\naround NSA and GCHQ practices by engaging with\nthe state-of-the-art in computer and information sci-\nences. As employers of computer scientists, mathemat-\nicians, and physicists, we have contended that the NSA\nand the GCHQ are unlikely to have developed technol-\nogies and methods beyond the latest research in aca-\ndemic and commercial organizations. Yet, despite the\napparent credibility of their discourses being grounded\nin the scientificity of these disciplines, we have shown\nthat debates in computer and information science chal-\nlenge this credibility. We have combined this engage-\nment with computer and information science with an\nanalysis of legal cases, public inquiries, parliamentary\nand media reports, and declassified documents in the\nwake of the Snowden revelations.\nWe have developed a two-pronged argument. On the\none hand, we have shown that the invocations of priv-\nacy, mass surveillance and decisions by algorithms run\ninto political and epistemic impasses as they have been\ncontinually challenged by professionals of security and\npolitics. On the other, we have argued for different\nvocabularies of critical intervention in the sites of con-\ntroversy in the emerging Big Data-security assemblage:\nfrom metadata privacy to the production of data as a\ncomplex epistemic entity; from mass surveillance by\ncomputers to the division of labour between humans\nand computers in artificial intelligence; from an under-\nlying algorithmic logic of security to algorithmic prac-\ntices and probabilistic methods. These interventions\nsuggest several trajectories for developing critical\nresearch about data in security governance. By way of\nconclusion we propose four theses on critique:\nBig Data theories and methods are not as new or revo-\nlutionary as the justificatory discourses of security\nprofessionals and many critical academic analyses of\nBig Data suggest. Rather than assuming a `Big Data\nrevolution', the deployment of Big Data for security\npurposes needs to be understood in terms of what\nMichel Foucault has called the `inflection of the\nData is a complex epistemic object. Therefore, data\nneeds to be approached as an object of inquiry rather\nthan subsumed to knowledge. Meanings of data, meta-\ndata, and knowledge differ between social sciences and\ninformation science. Rather than simply contending\nthat there is no such thing as raw data, critical data\nstudies need to analyse the `making up' of data and\nthe production of kinds of data at the juncture between\ninformation and computer science, on the one hand,\nand politics, on the other.\nThe digital age does not mean that decisions are simply\ntransferred from humans to computers and algorithms.\nWe are only beginning to understand the transform-\nation of intelligence agencies (and other security organ-\nizations) into Big Data organizations. There is little\nexisting work on the details of this organizational\nchange, which entails understanding histories of Big\nData as artificial intelligence, and particularly the\nreconceptualization of the relation between humans\nand computers in AI. Analyses of data-security assem-\nblages need to attend not just to the modes of associ-\nation and heterogeneous elements of an assemblage,\nbut equally to how it gains content through a division\nof labour between humans and computers.\nAradau and Blanke 9\nThere are no `unreasonably effective' algorithms.\nCritical approaches to security and surveillance need\nto engage with the methods and routine practices of\nBig Data-security analytics. Algorithms continue to\nrely on probabilistic methods, which depend on and\nare challenged by the increased amounts of data pro-\nduced today. Unless we analyse the algorithmic prac-\ntices and methods from information and computing\nsciences, we might write and critique only the science\nfiction of security analytics.\nDeclaration of conflicting interest\nThe author(s) declared no potential conflicts of interest with\nrespect to the research, authorship, and/or publication of this\narticle.\nFunding\nThe author(s) received no financial support for the research,\nauthorship, and/or publication of this article.\nNotes\n1. For critical engagements, which challenge the relevance of\nliberal concepts of privacy and human rights for the digital\nage, see for instance Finn Brunton and Nissenbaum\n2. See the International Principles on the Application of\nHuman Rights to Communications Surveillance, a global\nconsultation on a framework to evaluate digital surveil-\nlance practices (https://en.necessaryandproportionate.org/\ntext).\n3. Although for reasons of space we cannot discuss the vari-\nous kinds of metadata, we should mention here that in\ninformation science there is a distinction between admin-\nistrative and technical metadata as well as descriptive\nmetadata. The latter are, for instance, keywords that\ndescribe the content of a phone call, while the former\nare, e.g., timestamps of phone calls. All these types of\nmetadata are used in surveillance and the boundaries\nbetween them are fluid (Miller, 2011). Especially technical\nmetadata such as timestamps is easily structured data in\nthe processing pipelines of knowledge engineering.\n4. The lack of evidence of a high number of thwarted terror-\nist plots by the NSA and the GCHQ did not undermine the\nargument that having the whole haystack could potentially\npredict the `next terrorist attack'.\nReferences\nAbelson H, Appel AW, Bellovin SM, et al. (2014) Amici\ncuriae brief of experts in computer and data science in sup-\nport of appellants and reversal, 3 March. Available at:\nwww.nysd.uscourts.gov/cases/show.php?db\u00bcspecial&id\nthe Second Circuit (7 May 2015). Available at: http://\nAlavi M and Leidner DE (2001) Review: Knowledge manage-\nment and knowledge management systems: Conceptual\nfoundations and research issues. MIS Quarterly 25(1):\nAmoore L (2011) Data derivatives. Theory, Culture & Society\nAmoore L (2014) The Politics of Possibility: Risk and Security\nBeyond Probability. Durham, NC: Duke University Press.\nAmoore L and de Goede M (2005) Governance, risk and\ndataveillance in the war on terror. Crime, Law and\nAradau C and van Munster R (2011) Politics of Catastrophe:\nGenealogies of the Unknown. Abingdon: Routledge.\nBauman Z, Bigo D, Esteves P, et al. (2014) After Snowden:\nRethinking the impact of surveillance. International\nBengio Y, Goodfellow IJ and Courville A (in preparation)\nDeep learning. Available at: http://www.iro.umon-\nBigo D (2008) Globalized (in)security: The field and the ban-\nopticon. In: Bigo D and Tsoukala A (eds) Terror,\nInsecurity and Liberty: Illiberal practices of liberal states\nBlanke T (2014) Digital Asset Ecosystems: Rethinking Crowds\nand Clouds. Oxford: Chandos/Elsevier.\nBonditti P (2008) Homeland security through traceability.\nIn: Dunn Cavelty M and Soby-Kristensen K (eds)\nSecuring `the Homeland': Critical Infrastructure, Risk and\nBoyd D and Crawford K (2012) Critical questions for big\ndata: Provocations for a cultural, technological, and schol-\narly phenomenon. Information, Communication & Society\nBrunton F and Nissenbaum H (2011) Vernacular resistance\nto data collection and analysis: A political theory of obfus-\ncation. First Monday 16(5). Available at: http://firstmon-\nBrynjolfsson E and McAfee A (2011) Race Against the\nMachine. Lexington: Digital Frontier Press.\nClapper JR, et al. (2014) Brief for defendants \u00ad Appellees. In:\nDocket No 14-42. United States Court of Appeals for the\nSecond Circuit.\nClauset A, Young M and Gleditsch KS (2007) On the fre-\nquency of severe terrorist events. Journal of Conflict\nCooper M (2008) Life as Surplus: Biotechnology and\nCapitalism in the Neoliberal Era. Seattle, WA: University\nof Washington Press.\nDe Goede M (2012) Speculative Security: The Politics of\nPursuing Terrorist Monies. Minneapolis: University of\nMinnesota Press.\n10 Big Data & Society\nDe Hert P and Gutwirth S (2006) Privacy, data protection\nand law enforcement. Opacity of the individual and trans-\nparency of power. In: Claes E, Duff A and Gutwirth S\n(eds) Privacy and the Criminal Law. Antwerpen:\nEFF and ACLU (2014) Amici Curiae. The United States Court\nof Appeal for the District of Columbia Circuit 2014.\nAvailable at: https://www.eff.org/en-gb/document/eff-and-\naclu-amicus-brief-klayman (accessed 4 April 2015).\nEkbia H, Mattioli M, Kouper I, et al. (2015) Big data, bigger\ndilemmas: A critical review. Journal of the Association for\nFelten E (2014) Technical Tradeoffs in NSA's Mass Phonecall\nData Program 2014. Available at: https://www.youtube.\nFISC (2013) Re application of the federal bureau of investi-\nIntelligence Surveillance Court (FISC). Available at:\nhttps://www.aclu.org/files/assets/br13-09-primary-\nto Knowledge. 3 vols. London: Penguin Books.\nFricke\n\u00b4 M (2009) The knowledge pyramid: A critique of the\nDIKW hierarchy. Journal of Information Science 35(2):\nFricke\n\u00b4 M (2014) Big data and its epistemology. Journal of the\nAssociation for Information Science and Technology 66(4):\nGitelman L (2013) Raw Data is an Oxymoron. Cambridge,\nMA: MIT Press.\nHacking I (1999) Making up people. In: Biagioli M (ed.)\nThe Science Studies Reader. New York: Routledge,\nHalevy A, Norvig P and Pereira F (2009) The unreasonable\neffectiveness of data. Intelligent Systems, IEEE 24(2):\nHildebrandt M (2013) Slaves to big data. Or are we? IDP.\nRevista de Internet, Derecho y Politica (17). Available at:\nhttp://journals.uoc.edu/index.php/idp/article/viewFile/\nIntelligence and Security Committee (2015) Privacy and\nSecurity: A Modern and Transparent Legal Framework.\nHouse of Commons. Available at: http://isc.independent.\ngov.uk/committee-reports/special-reports (accessed 12\nJanert PK (2010) Data Analysis with Open Source Tools.\nCambridge, MA: O'Reilly Media.\nKitchin R (2014) Big data, new epistemologies and paradigm\nKurzweil R (2005) The Singularity is Near. New York, NY:\nViking.\nLatour B (2005) Reassembling the Social: An Introduction to\nActor-network-theory. Oxford: Oxford University Press.\nLehikoinen J and Koistinen V (2014) In big data we trust?\nLeskovec J, Rajaraman A and Ullman JD (2011) Mining of\nMassive Datasets. Cambridge: Cambridge University\nPress.\nLyon D (2014) Surveillance, Snowden, and Big Data:\nCapacities, consequences, critique. Big Data & Society\nMarkham AN (2013) Undermining `data': A critical examin-\nation of a core term in scientific inquiry. First Monday\n18(10). Available at: http://journals.uic.edu/ojs/\nnications commissioner. London: Interception of\nCommunication Commissioner's Office.\nMayer-Scho\n\u00a8 nberger V and Cukier K (2013) Big Data: A\nRevolution that will Transform how We Live, Work, and\nThink. London: John Murray.\nMcAfee A and Brynjolfsson E (2012) Big data: The manage-\nMiller SJ (2011) Metadata for Digital Collections: A How-to-\ndo-it Manual. New York, NY: Neal-Schuman Publishers.\nNorvig P (2012) Colorless green ideas learn furiously:\nChomsky and the two cultures of statistical learning.\nObama B (2013) Transcript: Obama's remarks on NSA\nControversy (7 June 2013). The Wall Street Journal.\ntranscript-what-obama-said-on-nsa-controversy/\nCambridge, MA: Harvard University Press.\nPerez J (2007) Google wants Your Phonemes. Available at:\nhttp://www.infoworld.com/t/data-management/google-\nPlume K (2014) Snowden, Greenwald Urge Caution of Wider\nGovernment Monitoring at Amnesty Event (5 April 2014).\nReuters. Available at: http://www.reuters.com/article/\nRaley R (2013) Dataveillance and countervailance.\nIn: Gitelman L (ed.) ``Raw data'' is an Oxymoron.\nRoberts D (2013) FBI Chief Mueller says spy tactics could\nRouvroy A (2012) The end(s) of critique: Data-behaviourism\nvs. Due-process. In: Hildebrandt M and de Vries K (eds)\nPrivacy, Due Process and the Computational Turn: The\nPhilosophy of Law Meets the Philosophy of Technology.\nRouvroy A and Poullet Y (2009) The right to informational\nself-determination and the value of self-development:\nReassessing the importance of privacy for democracy.\nIn: Gutwirth S, Poullet Y, de Hert P, et al. (eds)\nReinventing Data Protection? New York, NY: Springer,\nRuppert E, Law J and Savage M (2013) Reassembling social\nscience methods: The challenge of digital devices. Theory,\nSchmidt B (2011) Compare and contrast. In: Sapping\nAttention. Digital Humanities: Using tools from the 1990s\ncentury America. Available at: http://\nAradau and Blanke 11\nSchutt R and O'Neil C (2013) Doing Data Science: Straight\nTalk from the Frontline. Sebastopol, CA: O'Reilly Media,\nInc.\nShanor CA (2013) Making a mountain out of a digital mole-\nhill. New York Times, 7 June. Available at: http://\ntain-out-of-a-digital-molehill.html?_r=0 (accessed 28\nShea T (2013) Declaration of Teresa H Shea, Signals\nIntelligency Director, National Security Agency. In:\nSouthern District of New York.\nSocher R and Manning C (2013) Deep learning for natural\nlanguage processing (without magic). Available at: http://\nTuomi I (1999) Data is more than knowledge: Implications of\nthe reversed knowledge hierarchy for knowledge\nmanagement and organizational memory. Journal of\nVan Dijck J (2014) Datafication, dataism and dataveillance:\nBig data between scientific paradigm and ideology.\nWeinberger D (2011) Too Big to Know: Rethinking Knowledge\nNow that the Facts Aren't the Facts, Experts Are\nEverywhere, and the Smartest Person in the Room is the\nRoom. New York, NY: Basic Books.\nlect communications data `haystack'. BBC 2014. Available\nWhite G (2011) Semantics, hermeneutics, statistics: Some\nreflections on the semantic web. Paper read at BCS-HCI\n`11 Proceedings of the 25th BCS Conference on Human\u00ad\nWolpert DH and Macready WG (1997) No free lunch the-\norems for optimization. Evolutionary Computation, IEEE\n12 Big Data & Society"
}