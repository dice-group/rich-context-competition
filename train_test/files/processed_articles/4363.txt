{
    "abstract": "Abstract\nBig social data have enabled new opportunities for evaluating the applicability of social science theories that were\nformulated decades ago and were often based on small- to medium-sized samples. Big Data coupled with powerful\ncomputing has the potential to replace the statistical practice of sampling and estimating effects by measuring phenomena\nbased on full populations. Preparing these data for analysis and conducting analytics involves a plethora of decisions, some\nof which are already embedded in previously collected data and built tools. These decisions refer to the recording,\nindexing and representation of data and the settings for analysis methods. While these choices can have tremendous\nimpact on research outcomes, they are not often obvious, not considered or not being made explicit. Consequently, our\nawareness and understanding of the impact of these decisions on analysis results and derived implications are highly\nunderdeveloped. This might be attributable to occasional high levels of over-confidence in computational solutions as\nwell as the possible yet questionable assumption that Big Data can wash out minor data quality issues, among other\nreasons. This article provides examples for how to address this issue. It argues that checking, ensuring and validating the\nquality of big social data and related auxiliary material is a key ingredient for empowering users to gain reliable insights\nfrom their work. Scrutinizing data for accuracy issues, systematically fixing them and diligently documenting these\nprocesses can have another positive side effect: Closely interacting with the data, thereby forcing ourselves to under-\nstand their idiosyncrasies and patterns, can help us to move from being able to precisely model and formally describe\neffects in society to also understand and explain them.\n",
    "reduced_content": "Commentary\nSmall decisions with big impact\non data analytics\nJana Diesner\n Keywords\nValidation, evaluation, error analysis, data preprocessing, social network analysis, data mining\nIntroduction\nBig social data have enabled new opportunities for\nevaluating the applicability of social science theories\nthat were formulated decades ago and often based on\nsmall- to medium-sized samples in today's contexts and\nfor social agents operating in contemporary socio-tech-\nnical infrastructures. These data, which includes large-\nscale traces of social interactions and natural language\nuse, are also essential for developing new knowledge\nand methods based on bigger and broader datasets\nthan those typically used in the past (Lazer et al., 2009).\nIn some cases, Big Data coupled with powerful com-\nputing have the potential to replace the statistical prac-\ntice of sampling and estimating effects by measuring\nphenomena based on full populations. For instance,\nthe social networks concept of small worlds, which\nbasically means that a randomly picked pair of people\nis linked through a small number of intermediaries or\nsocial circles, is based on a few experiments where, for\narrived at their predefined target (i.e. a person\nunknown to the first sender); with a median of 5.2\n(N about 16) intermediaries (Travers and Milgram,\n1969). This study has recently been replicated based\non Facebook data (a graph with about 721 million\nnodes), using a bird's eye view (automated efficient\ngraph search) rather than a frog's perspective (local\nintermediaries was found for the Facebook data.\nCorresponding author:\nEmail: jdiesner@illinois.edu\nBig Data & Society\nReprints and permissions:\nsagepub.co.uk/journalsPermissions.nav\nbds.sagepub.com\nCreative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons Attribution-\nNonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use, reproduction\nand distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access pages\n(https://us.sagepub.com/en-us/nam/open-access-at-sage).\nThis shorter distance might be due to the fact that the\nparticipants in Milgram's studies had incomplete know-\nledge about chains of acquaintances beyond their ego-\nnetwork or friend-of-a-friend network (1.5 to 2 of\nseparation), while in the Facebook study, algorithms\nperformed the search task. An alternative explanation\nwould be that the average social distance has decreased\nover time, e.g., due to the wide diffusion of communi-\ncation and social interaction technologies. Has our\nsocial world truly become smaller over the last 50\nyears? Finding and proofing reasons and explanations\nfor such empirical observations require more work.\nAnother example comes from political science, where\nscholars have been collaborating over decades to define\nand update categorization schemas for geopolitical\nactors and events with the goal of enabling the analysis\nof international relations, especially conflicts (Schrodt\napproach, the ``Global Database of Events, Language\nand Tone'' (GDELT)1 serves the same purpose but is\nbased on big event data: Enabled by computing infra-\nstructure providers, a large number of national and\ninternational news content providers, and research in\ninformation science, GDELT provides a continuously\nupdated geopolitical event database with over a quarter-\nIn the (computational) social sciences and (digital)\nhumanities, researchers have long started to use readily\navailable technologies, including APIs, computing\nenvironments like R, and scripting languages like\nPython, to enhance methods common in their fields,\ne.g. close reading and text coding, as well as advanced\nsocial data analytics and visualization techniques, such\nas topic modeling, sentiment analysis, and clustering\nods are then applied to big social and cultural data.\nThe outlined leap forward in supporting a better\nunderstanding of society, culture, and socio-technical\nsystems at scale benefits from a combination of devel-\nopments: Electronic repositories and collections of data\n(e.g. the Stanford Large Network Dataset Collection2),\ncode (e.g. GitHub3), scientific publications (e.g.\nDBLP4), and user-generated (e.g. Wikipedia5) as well\nas traditional print content (e.g. HATHI Trust6) sup-\nport the reproducibility of findings based on Big Data\nand the sharing of material--at least theoretically\ncopyright law, such as the Creative Commons\nLicenses,7 which apply to Wikipedia data for instance,\nand open source software licenses, such as the Apache\nLicense8 or the GNU General Public Licenses,9 further\nease the process of sharing and reusing information and\ntools. Also, some commercial providers, platforms, and\nwebsites that receive, manage, and synthesize social\nmedia content offer APIs that allow researchers and\npractitioners to access and analyze big social data,\nSo what's the problem?\nPreparing big social data for analysis and conducting\nactual analytics involves a plethora of decisions, some\nof which are already embedded in previously collected\nThese decisions refer to the recording, indexing, and\nrepresentation of data and to the settings and (paramet-\nric) choices for analysis methods. For example, when\nfusing data from various social media sites (``v'' for\nvariety of Big Data), one needs to think about how to\nidentify identical users across different plat-\nforms--where the same person might use different\nnames on different sites or multiple names on the\nsame site, and different people might use the same\nname on different sites--and what errors in resolving\nthese ambiguities mean for the accuracy of the data and\nobtained findings (Iofciu et al., 2011; Zafarani and Liu,\n2013). Means for communicating and learning about\nthese decisions include data annotation, such as meta-\ndata, and diligent documentation. In the given disam-\nbiguation example, one might need to dig deep into\nmeta-data, individual language use, or interaction pat-\nterns, to tell users apart or merging them. Another\ninstance of possible issues with reusing Big Data is\nthe considerable amounts of false positives that have\nbeen found in GDELT, where these problems result\nfrom the same event being reported by multiple sources\nand difficulties with automatically disambiguating these\nredundancies (Hammond and Weidmann, 2014).\nWhile such decisions can have tremendous impact on\nresearch outcomes, they are often not obvious, not con-\nsidered, or not made explicit (De Choudhury et al., 2010;\nConsequently, our awareness and understanding of the\nimpact of these decisions on analysis results and derived\nimplications are highly underdeveloped. This might be\nattributable to occasional high levels of over-confidence\nin computational solutions as well as the possible yet\nquestionable assumption that Big Data might wash out\nminor data quality issues, among other reasons.\nUltimately, it is up to the users of big social data to\nleverage given resources in a responsible and meaningful\nway and to bring relevant questions and appropriate\nanalysis techniques to the data. More research on the\nquality of big social data can aid this process.\nIn our work, we have begun to address this issue by\nidentifying the quantitative and qualitative impact of\nsmall decisions made prior to and throughout the\nresearch process on its outcomes. More specifically,\nwe have been measuring the effect of inaccuracies in\n2 Big Data & Society\nresolving entities when constructing social network data\nand of refining previously built lexical resources on ana-\nlysis results and the interpretation of findings (Diesner,\nprovides one example for each type of research.\nBefore turning to this main point, I am concluding\nthe problem statement with the brief mention of a\nrelated issue that is only tangential to this article and\nwill therefore not be further addressed herein: One chal-\nlenge that scholars seeking to use big social data need to\nface is that the terms of service and intellectual prop-\nerty/copyright regulations for publicly accessible data,\nespecially those synthesized, organized, and hosted by\ncommercial providers, can put a damper on educational\nand research aspirations. The fact that large-scale inter-\naction and language use data are visible and available\nto the public does not necessarily mean that researchers\nare also allowed to collect and analyze them. APIs can\nhelp to mitigate this issue. The relationship between the\nlegality, feasibility, and ethics of data acquisition and\nanalytics is an evolving aspect of big social data science.\nDiscussing related policies and regulations might be\nanother essential ingredient for making progress with\npractical and computational solutions.\nScrutinizing the impact of data quality\nissues on social computing research\nAmbiguity of social network data\nEntity resolution involves two tasks: First, locating and\nconsolidating the different references to a single unique\nentity. This applies, for example, to (a) social media\nnetworks, where individuals might use different names\non different platforms; (b) social networks constructed\nfrom text data, where people and organizations might\nbe referred to by various ways of expressing their names\nand roles (John Kerry, United States Secretary of State)\nand pronouns; and (c) scientific collaboration and coci-\ntation networks, where authors might be indexed with\ndifferent variations of their names, e.g., with and with-\nout middle name initials. Multiple computational, algo-\nrithmic, and human-in-the-loop solutions have been\ndeveloped to address these problems. For example,\nthe ORCID project13 was started to resolve ambiguous\nnames of authors of scientific publications relying on\nthe input and data verification from scholars. Other\nproviders of information about scientific publications\nand citations use highly accurate algorithmic solutions,\nsometimes coupled with the manual resolution of\nambiguous cases, e.g. DBLP. The second entity reso-\nlution task is splitting up nodes that represent multiple\ndistinct entities that are referred to by the same name.\nThis can happen, for instance, when some people's\nnames entail common first and last names. For exam-\nple, the University of Michigan has two established\nscholars with the name of Mark Newman: a physicist\nwho studies networks14 and an HCI scholar15 (for the\ncurious reader: the http://howmanyofme.com/ webpage\ngives an idea of how many US-based people share a\ncertain first name/last name combination). Telling\nboth Dr Newmans apart requires knowledge about\ntheir middle names, details about their institutional\naffiliation (i.e. meta-data), or contextual information\nabout their work. Prior work has shown that the prob-\nlem of telling people with identical names apart is par-\nticularly important when working with Asian names\n(Zhao and Strotmann, 2011). Overall, prior research\non entity resolution has resulted in highly accurate\nand automated techniques to both the consolidation\nand splitting of names (see, for example, Fegley and\nWe have been bringing the following question to this\nproblem: How much does entity resolution matter for\nbig (and small) social network analysis? Our results\nsuggest that commonly reported network metrics, as\nwell as derived implications, can strongly deviate\nfrom the truth--as established based on ground truth/\ngold standard data or approximations thereof--\ndepending on the efforts dedicated to the data prepro-\ncessing step of entity resolution (Diesner et al., 2015).\nWe found the identification of key players to be less\nsensitive to entity resolution errors than variations in\nnetwork metrics. For working with email data, our\nresults have shown that failing to consolidate email\naddresses (i.e. indexing all email addresses that a\nperson uses as one node) can make email networks\nappear less coherent and integrated and also bigger\nthan they really are, potentially suggesting a false\nneed for more coordination and communication. For\ncopublishing networks, failing to split up nodes that\nrepresent multiple individuals with the same name can\nmake scientific communities look denser and more\ncohesive than they are, and make individual authors\nappear more productive, collaborative, and diversified\nthan truth has it, potentially downgrading the need for\n(interdisciplinary) collaboration and funding. We\nobserved that in coauthorship networks, incorrect or\nskipped entity resolution can even lead to the misiden-\ntification of applicable network topologies, e.g. detect-\ning power law distributions of node degrees and\nassuming an underlying preferential attachment process\nwhere there is insufficient empirical evidence for this\nScope of auxiliary lexical resources\nThe Big Data wave also eased access to sizable auxiliary\nmaterial, which can help to disambiguate and\ncontextualize information, among other uses.16 For\ninstance, Wikipedia offers additional information on a\nlarge variety of social entities, e.g. sociodemographic\nmeta-data on individuals and product portfolios for\ncompanies. Also, Freebase,17 which inherited its content\nfrom Metaweb and later fed into Google's knowledge\ngraph, used to be a big repository of structured and\ncategorized information about a large variety of types\nof entities and phenomena. Freebase obtained its con-\ntent from various sources, including user-generated con-\ntributions.18 Another example is WordNet,19 which\nonymous words and provides a database of relationships\nbetween words, including hyperonyms (super-subordi-\nnate relations), meronyms (part-whole relations), and\nvice has since been provided for many other languages as\nwell, including Afrikaans, Persian, and Latin.20\nSimilarly, a plethora of research initiatives has provided\ndictionaries, i.e. registers of word-category pairs (occa-\nsionally enriched with additional information such as\nparts of speech), that can be used to assess, for example,\nthe subjectivity, emotionality, honesty, and morale of\n(pieces of) text data or their authors (Graham et al.,\n2005). Using such resources is not only efficient, but\nalso a scientifically solid strategy as many of these\nhelper tools have been previously validated--by experts\nor crowds of ordinary people--and documented.\nMoreover, this general approach puts the idea of sharing\nand reuse into action. However, when leveraging readily\navailable material for Big Data analytics, we have a lack\nof understanding, ground truth data, and pertinent\nbenchmark results for how much adjustment of preexist-\ning resources to a given dataset, domain, or time period\nis needed in order to obtain reliable and comprehensive\nresults.\nWe have started to address this issue by asking this\nquestion: How much of a difference does the tuning of\nlexical auxiliary material make for text mining projects?\nTo give an example, we have used a previously built\nand widely adopted subjectivity lexicon (Wilson et al.,\n2005) in order to identify the emotionality and senti-\nment of information exchanged via emails. Our overall\npurpose with this work was to develop a novel method\nfor efficiently assessing structural balance in large-scale\ncommunication networks (Diesner and Evans, 2015).\nThis idea was motivated by the aforementioned oppor-\ntunity and need to test the validity of social science\ntheories in today's contexts and interaction environ-\nments; a precondition for advancing theory and sub-\nstantive knowledge about social networks. This bigger\ngoal involves small decisions throughout the research\nprocess: The original subjectivity lexicon that we used\nwas built based on world press data (Wiebe et al.,\n2005), while the data for our study were email conver-\nsations from a company, namely Enron. The original\nlexicon associates syntactically disambiguated (via\nparts of speech) terms as well as their stemmed versions\nwith a value for polarity (positive, negative, or neutral)\nand strength of polarity (weakly or strongly subjective).\nNot knowing to what degree this lexicon would gener-\nalize to the business domain, we identified and cor-\nrected for false negatives (prevalent subjective terms\ncontained in the email data but not in the lexicon)\nand false positives (subjective terms included in the lexi-\ncon that were a misfit for our data). In our example,\ncorrecting for false negatives involved the detection of\nsalient terms, such as words with a high weighted term\nfrequency, inspecting them one by one to decide\nwhether they should be added to the lexicon, and if\nso deciding on the best fitting part of speech, polarity,\nand strength. Removing false positives from the lexicon\nmeant to compute a list of lexicon terms that frequently\noccurred in the email data, inspecting them, and mod-\nifying their values or dropping them from the lexicon.\nOverall, we added 34 terms (a tiny fraction, the original\nand modified 30 entries. Overall, we changed less than\n8% of the original dictionary. Even though computer-\nassisted, this process is tedious. Is it worth it? In our\nexample, adjusting a lexical resource to a different\ndomain and corpus leads to similar overall findings\nabout balance as with using the original lexicon, but\nresulted in more empirical evidence (we annotated\nvalues) and lower statistical variance of the results\nand correcting for false positives (false alarms) and\nfalse negatives (blind spots) should be common practice\nwhen reusing lexical resources for big social data ana-\nlytics. More research is needed to identify best prac-\ntices, stopping criteria (how much is enough?), and\nevaluation procedures and metrics for this step.\nConclusions\nIn summary, this article argues that checking, ensuring,\nand validating the quality of big social data and related\nauxiliary material is a key ingredient for empowering\nusers to gain reliable insights from these data. This\nmain point is further substantiated by the fact that\noftentimes, assessing the accuracy and validity of Big\nData and respective findings is difficult to infeasible.\nThis is due to population size (``v'' for volume in Big\nData) and the continuously evolving and changing\nnature (``v'' for velocity of Big Data) of social systems.\nFurthermore, when working with large-scale digital\ntrace data, such as social interaction and information\ndissemination data from Facebook and Twitter, it is\n4 Big Data & Society\ncommon practice not to interact with the user popula-\ntion, but rather to harvest and mine the information\nthey produce and leave behind, and occasionally infer\nor predict additional information based on that. Ethical\nstandards around this process keep being discussed\n(Kosinski et al., 2015). All of these characteristics\nmake the collection of gold-standard or ground-truth\ndata and the creation of benchmarks for validation a\ndaunting and costly task.\nScrutinizing big social data for accuracy and integ-\nrity issues, systematically fixing them, and diligently\ndocumenting these processes can have another positive\nside effect beyond boosting the reliability of results and\nthe reusability of material: Closely interacting with the\ndata, thereby forcing ourselves to understand their idio-\nsyncrasies and patterns and learning about the content\ndomain, can help to move us from being able to pre-\ncisely model and formally describe effects in society to\nalso understand and explain them.\n"
}