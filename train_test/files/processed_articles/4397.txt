{
    "abstract": "Abstract\nPrior research finds that statistically significant results are overrepresented in scientific publications. If significant results\nare consistently favored in the review process, published results could systematically overstate the magnitude of their\nfindings even under ideal conditions. In this paper, we measure the impact of this publication bias on political science\nusing a new data set of published quantitative results. Although any measurement of publication bias depends on the\nprior distribution of empirical relationships, we determine that published estimates in political science are on average\nsubstantially larger than their true value under a variety of reasonable choices for this prior. We also find that many\npublished estimates have a false positive probability substantially greater than the conventional  = 0.05 threshold for\nstatistical significance if the prior probability of a null relationship exceeds 50%. Finally, although the proportion of\npublished false positives would be reduced if significance tests used a smaller , this change would not solve the problem\nof upward bias in the magnitude of published results.\n",
    "reduced_content": "Research and Politics\nrap.sagepub.com\nCreative Commons Non Commercial CC-BY-NC: This article is distributed under the terms of the Creative Commons\nAttribution-NonCommercial 3.0 License (http://www.creativecommons.org/licenses/by-nc/3.0/) which permits non-commercial use,\nreproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open\nAccess pages (https://us.sagepub.com/en-us/nam/open-access-at-sage).\nIntroduction\nMany academic papers (and especially the first few arti-\ncles on a topic) describe relationships that turn out to be\nillusory upon closer examination (Ioannidis, 2005).\nAdditionally, the typical published estimate is probably of\nlarger magnitude than the true relationship (Ioannidis,\n2008). Recent large-scale attempts to replicate social sci-\nentific findings have discovered that many of these find-\nings become substantially smaller and more uncertain than\ninitially indicated (Boekel et al., 2015; Camerer et al.,\nScience Collaboration, 2015); the \"replication crisis\" has\nplagued fields in the hard sciences as well (e.g. Begley and\nReplicability problems are exacerbated by researcher\nbehaviors like p-hacking (analyzing the same data in mul-\ntiple ways but only reporting the most statistically signifi-\ncant findings).1 But even if behaviors like this were\neliminated, the problems would continue to exist because\nthe publication process privileges statistically significant\net al., 1995), including by influencing authors' decision to\nwrite up and publicize their findings (Franco et al., 2014).\nWhen null findings are not published, they cannot place\nanomalously large and statistically significant results into\ntheir proper context; such anomalous results can attract a\ngreat deal of scientific interest because of their novelty and\ncounterintuitiveness.2 These problems are often collec-\ntively referred to as publication bias (Scargle, 2000).\nAlthough the publication bias created by \"misunderstand-\ning or misuse of statistical inference is only one cause of\nthe `reproducibility crisis' ... to our community, it is an\nWhile much of the previous work in this area focuses on\nestablishing that publication bias is real and pervasive in\ndisciplines that use statistical evidence (e.g. by using \"cali-\nper tests\" of published p-values, as in Gerber and Malhotra\nMeasuring the effects of publication\nbias in political science\nJustin Esarey and Ahra Wu\n Keywords\nHypothesis testing, publication bias, statistical significance\nDepartment of Political Science, Rice University, USA\nCorresponding author:\nJustin Esarey, Assistant Professor of Political Science, Rice University,\nEmail: justin@justinesarey.com\nResearch Article\n2 Research and Politics \ndetermine how publication bias has affected the accumu-\nlated body of knowledge in political science. We measure\nthe impact of publication bias on political science using a\nnew data set of published quantitative results. Although any\nmeasurement of publication bias depends on the prior dis-\ntribution of empirical relationships, we estimate that pub-\nlished results in political science are distorted to a\nsubstantively meaningful degree under a variety of reason-\nable choices for this prior.\nWe come to three conclusions. First, published estimates\nof relationships in political science are on average substan-\ntially larger than their true value. The exact degree of\nupward bias depends on the choice of prior, but at the high\nend we estimate that the true value of published relation-\nships is on average 40% smaller than their published value.\nMore optimistic priors yield a lower average bias, but still\nfind that at least 14% of results are biased upward by 10%\nor more. Second, we find that many published results have\na false positive probability substantially greater than the\nconventional  = 0.05 threshold for statistical significance\nif the prior probability of a null relationship exceeds 50%.\nThese two findings are quantitatively and qualitatively sim-\nilar to results uncovered by the large scale replication stud-\nies noted above, suggesting that publication bias can\nexplain much of the \"replication crisis\" these studies have\nobserved.4 Finally, we find that both the upward bias in\nmagnitude and the probability of being a false positive is\nsmaller for results with p-values further from the threshold\nfor significance. Our last finding suggests that requiring a\nmore stringent statistical significance test (with a smaller )\nfor publication might be effective at combating publication\nbias (Johnson, 2013). Unfortunately, although the propor-\ntion of published false positives would be reduced by this\nthat such a reform would not solve the problem of upward\nbias in published results: published results near the new\nthreshold of significance would still be (on average) sub-\nstantially biased upward.\nMeasurement strategy\nTrying to measure the degree of upward bias in an estimate\n\n of some parameter , or the prevalence of false positives\n(statistically significant estimates of \n when the null\n = 0 is true), is tricky. Any measurement depends on an\nassumption about the true value of  (or a probability distri-\nbution of beliefs about its value, f ( )\n ). For example, con-\nsider the distribution of statistically significant estimates\n\n associated with a true value of . Publication bias\nimplies that E t t\n  \n\n\n\n\n\n\n \n , where t = /\n   is\nthe t-statistic, \n\n \nis the estimated standard error of \n, and\nt\nd\nf\n\n\n\n\n\n\n is the critical t value for a two-tailed significance\ntest under a null hypothesis  = 0 setting  = Pr(significant |\nmany degrees of freedom. For a\nfixed and known , we could calculate the degree of publi-\ncation bias as\nE t t\nd\nd\nf\nf\n  \n \n \n\n \n\n\n| ,\n=\n| , ,\n\n\n\n\n\n\n-\n-\n\n\n\n\n\n\n\n\n-\n-\n\n\n\n\n \n\n\n \n \n\n  \n\n\n\n\n\n\n\n\n+\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd\nd d\nf\n| , ,\n\n\n\n\n\n\n\n\n\n- \n\nwhere   \n=\nt\nd\nf\n\n\n\n\n\n\n\n \n(that is, the smallest statistically\nsignificant \n ) and  is the t probability density function.\nThat is, we define bias as the difference between the\nexpected value of statistically significant estimates and the\ntrue value of the estimand.5\nHowever, in a published work,  is unknown. We must\ntherefore calculate\nE E t t\nE E t t\nd\nf\nd\nf\n  \n \n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n| ,\n= | ,\n\n= | ,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nE t t\nd\nf \n\n\n\n-\n\n\n\n\n\n\n\n\n  \nf d\n( )\n\n\n\nunder some reasonable assumptions about our prior beliefs\nabout , f \n( ). This estimate of publication bias will obvi-\nously be a function of our choice of f \n( ), and conse-\nquently it is advisable to estimate publication bias under a\nvariety of choices for f \n( ) to ensure robust results.\nWe estimate the degree of expected publication bias in the\npolitical science literature as a proportion of the published\nresult, E E t t\nd\nf\n    \n-\n( ) ( )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsign pub\n| , /\n\n .\nThe sign \n( ) term means that we measure the degree of bias\nin the direction of the true  (that is, as a function of the dis-\ntance of the relationship from zero); this allows us to meas-\nure the degree to which the average published result\nexaggerates the true magnitude of a relationship.6 The pub-\nlished estimate \npub\ninforms our assumption about the prior\nf ( )\n to recognize that each project comes out of a different\nfamily of projects pertaining to different subfields and topics\nwhose magnitudes are difficult to compare across families.\nWe consider two classes of f ( )\n :\nEsarey and Wu 3\n(a) aspike-and-slabdistributionwithaspikeat  = 0\nand a uniform slab between -\n\n\n\n\n\n\n \n \npub pub\n;\nand\n(b) a spike-and-normal distribution with a spike at\n = 0 added to a normal distribution with stan-\ndard deviation equal to \npub\n.\nThe first distribution represents a 33% probability prior\nbelief that a non-zero  \npub\n , while the second repre-\nsents a  68% probability prior belief that a non-zero\n\n \npub\n; our results are robust to other reasonable\nchoices for the boundaries of the spike-and-slab prior and\nthe standard deviation of the normal prior.7 We systemati-\ncally vary the height of the spike, Pr( = 0\n ), to determine\nhow different expectations for the baseline rate of null rela-\ntionships changes our view of the published literature.\nFinally, we repeat our analysis with no spike at  = 0, to\nrecognize the possibility that a point null hypothesis is\nWe use our prior belief density f ( )\n to determine the\nrelationship between true relationships and observed esti-\ndraws \n from f ( )\n for each published study. For each\ndraw of \n , we simulate a sample estimate    \n\ns\n= + ,\n \n\nd f\n( ) where \n\n \nis the published standard error of\n\npub\nand  is the t-density with d f\ndegrees of freedom\nequivalent to the published study.8 We determine which of\nthese results is statistically significant by comparing\nts s\n= /\n  \n  \nto the t\nd\nf\n\n\n\n\n\n\nvalue from a t-density with the appropriate degrees of free-\ndom. Finally, we calculate   \ns\n-\n( ) ( )\n\n\n\n\n\n\nsign for each of the\nstatistically significant draws. The average of this quantity is\nour estimate of E E t t\nd\nf\n   \n-\n( ) ( )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsign | ,\n .\n\nWe then divide this by the absolute value of the published\nresult, \npub\n, to calculate percentage bias.\nData set\nWe estimate the effect of publication bias on the literature in\npolitical science using a new data set of quantitative work\nrecently published in prominent, general interest journals.\nOur data set is composed of 314 quantitative articles pub-\nlished in the American Political Science Review (APSR: 139\nAmerican Journal of Political Science (AJPS: 175 articles in\nsis, we analyze only articles with continuous and unbounded\ndependent variables. Among the 173 articles with continu-\nous and unbounded dependent variables,10 6 articles have at\nleast one missing value regarding their estimates or sample\nsizes, which are necessary for our analysis. Therefore, we\nremove these 6 articles from our analysis. Consequently, we\nare left with 167 quantitative articles published in the APSR\n(70 articles) and the AJPS (97 articles). Finally, 25 studies\nout of these 167 quantitative articles (or 15% of that num-\nber) report statistically insignificant results as their main\nrelationship under a two-tailed test with  = 0.05, although\n17 of the 25 studies are statistically significant if using a\none-tailed test with  = 0.05.11 We omit these studies from\nour analysis as their interpretation is unclear in the context\nof assessing publication bias when using an  = 0.05 two-\ntailed significance test, leaving 142 studies for analysis. The\nconsequence of omitting statistically insignificant results is\nthat our estimates are upper bounds on the degree of publi-\ncation bias in the literature: the more likely it is that statisti-\ncally insignificant results will be published, the smaller that\npublication bias will be.\nA complete list of the rules we used to identify and code\nobservations in our data set is provided in Appendix 2; we\nsummarize the procedure here.12 Each observation of the col-\nlected data set represents one article and contains the article's\nmain finding (viz., an estimated marginal effect, \npub\n).\nDefining the main finding of an article can be complicated,\nas many articles present multiple results.13 We code the main\nfinding in the following way. First, if there is any expression\nsuch as \"the key independent variable\" or \"the main finding\nof this paper,\" we consider that relationship the main finding.\nIf there is no such explicit phrasing, we consider the finding\nthat is emphasized in the abstract or in the conclusion of a\npaper as the main finding. If there are multiple hypotheses\nthat receive almost equal attention, we record the informa-\ntion of the first hypothesis (\"H1\" or \"the first hypothesis\").\nResults\nThe result of applying this technique to the published (and\nstatistically significant) marginal effects estimates in our\ndata set reveals a substantial tendency toward upward bias\nin magnitude, as illustrated in Table 1. As the table shows,\nif we have a baseline expectation that only 10% of our\nhypotheses correctly predict a relationship a priori, then\nover 50% of published findings are expected to be at least\n10% larger in magnitude than the true relationship. The\ntypical published result in this scenario is on average at\nleast 29% larger than the true relationship. Even if there are\nno relationships that are exactly zero under a normal prior\ndensity (with standard deviation equal to \npub\nof published results have 10% upward bias in magnitude.\nIn general, the magnitude of the bias problem scales posi-\ntively with the assumed underlying proportion of null\nresults in the population of research ideas ( Pr  = 0\n( ) ).\nThe implication of the analysis is that a substantial por-\ntion of published results overestimate the true size of the\nrelationship being studied because statistical significance\ntests are used to screen results for publication. Biases that\nare large enough to be substantively meaningful are not\n4 Research and Politics \nuncommon; if our assumptions about f ( )\n are a good\nrepresentation of the background rate of null relation-\nships, we would expect many empirical findings (perhaps\neven a majority) to exaggerate the size of the true relation-\nships that they measure. Moreover, the high end of our\nestimates (viz., that the true value of published relation-\nships is on average 40% smaller than their published\nvalue) matches recent empirical estimates by large-scale\nreplication projects. For example, the Open Science\nCollaboration (OSC) estimated that originally published\neffect sizes in psychology are on average around 50%\nsmaller than effect sizes in replication studies of the same\nexperimental economics found that replication effect sizes\nwere on average only 65.9% of the size of the original\nmean bias; this suggests that either (a) the proportion of\nnull results in the population of studies is higher than we\ncontemplated, or (b) other factors (opportunistic model\nselection by the original researchers, a bias against suc-\ncess among the researchers performing the replication,\nand/or many alternative possibilities) may be working in\nconcert with publication bias to explain prior empirical\nresults.\nNot all publications are equally susceptible to bias, as\nseen in Figure 1. The figure shows that individual results\nvary greatly in terms of expected publication bias, regard-\nless of the prior probability of a null effect Pr  = 0\n( ).\nIndeed, Figure 1(d) shows substantial bias, and substantial\nTable 1. Expected bias in a sample of published marginal effects from APSR and AJPS.\nAssumed population Pr( 0)\n  Spike-and-slab prior Spike-and-normal prior\nMean % bias % of estimates with\nMean % bias % of estimates with\nThe table shows the estimated prevalence of upward bias in estimate magnitude in a sample of 167 articles from the American Political Science Review\nand the American Journal of Political Science; the sample size is 142 after 25 statistically insignificant results are excluded. We generate 100,000 draws\n\n from   \n\nU -\n\n\n\n\n\n\n^ ^\npub pub\nor  \n\n( ) with probability (1 )\n- p and  = 0 with probability p for each published study; the assumed\nvalue of p is listed in column 1. For each draw of \n, we simulate a sample estimate    \n\ns\n= + ,  \n\nd\nf\n( ), where \n\n \nis the published standard\nerror of \npub\nand  is the t-density with d\nf\ndegrees of freedom equivalent to the published study. We determine which of these estimates is\nstatistically significant by comparing t\ns s\n= /\n  \n  \nto the t\nd\nf\n( )\n critical value for an  = 0.05 test (two-tailed) from a t-density with degrees of\nfreedom equivalent to the published study. Finally, we calculate    \n-\n( ) ( )\n\n\n\n\n\n\nsign\npub\n/ for each of the statistically significant draws. Columns 2\nand 4 list the mean value of these replicates, our estimate of E E t t\nd\nf\n    \n-\n( ) ( )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n( )\nsign\npub\n| , /\n\n , across all 142 results for the prior\ndistribution indicated in the column heading. Columns 3 and 5 lists the corresponding proportion of estimates that are greater than or equal to 10%.\nvariation among published results, for the normal prior even\nwith no spike at  = 0 . Importantly, the expected bias is\nstrongly associated with the published p-value of the result:\nsmaller p-values are associated with smaller expected bias.\nOur finding underscores a point made in the American\nStatistical Association's statement on p-values: \"the wide-\nspread use of `statistical significance' (generally interpreted\n ') as a license for making a claim of a scientific\nfinding (or implied truth) leads to considerable distortion of\nthe scientific process\" (Wasserstein and Lazar, 2016: 9).\nCalculating susceptibility to false positives\nStatistical significance testing is designed to lower the risk of\nconcluding that a relationship exists when the evidence could\nbe consistent with no relationship at all. However, it is well\nestablished (though perhaps not widely understood) that sta-\ntistical significance testing is often insufficient to reduce the\nchance of a false positive to an acceptable level when the\nprior probability of studying a null relationship is very high\nSiegfried, 2010). A key factor is the prior probability that the\nnull hypothesis is true (i.e., the a priori expectation that the\nrelationship being studied does not actually exist). That is:\nPr |\nPr( | )Pr( )\n\n \n=\n( ) =\n= =\nstatistically significant\nstat. sig\nD\nD\nD = = = +\n - =\nPr( | )Pr( )\nPr( | )( Pr( ))\nstat. sig\nstat. sig\n \n \nEsarey and Wu 5\nWe can use this formula to calculate this probability for the\nobservations in our data set; this is similar to a calculation\nusing Bayes'factors and to a closely related formula offered\nby Maniadis et al. (2014). To establish a lower bound for\nPr  = 0 |stat.sig.\n( ), we set Pr stat. sig.| 0 = 1\n \n( ) to\nmaximize the denominator of equation (3). We then set the\nprior probability Pr  = 0\n( ) to a fixed value and calculated\nPr  = 0 |stat.sig.\n( ) for a range of Pr (stat.sig.|  = 0) \n[0,0.05]. The results for four different values of Pr  = 0\n( )\nare shown in Figure 2; the histogram in this figure indi-\ncates the distribution of p-values (i.e. the value of\nPr stat. sig.| = 0\n\n( )) in our data set.\nAs the figure shows, not all published work has an equal\nexpected probability of being a false positive. Results that\nare close to the boundary of statistical significance (with\np  0.05) have the greatest expected probability of being a\nfalse positive. Results that are further from this boundary\n ) are at substantially lower risk of\nbeing false positives. This finding is consistent with the\nprior work of the Open Science Collaboration (2015:\naac4716-5), whose replications of notable findings in psy-\nchology discovered that \"a negative correlation of replica-\ntion success with the original study p value indicates that\nthe initial strength of evidence is predictive of reproducibil-\nFigure 1. Histogram of expected bias calculations from APSR and AJPS. (a) Spike-and-slab prior, Pr( 0) =10%\n  . (b) Spike-and-\n  . Each histogram shows\nthe proportion of articles in a sample of 167 articles from the American Political Science Review and the American Journal of Political\nScience corresponding to a degree of expected bias; the sample size is 142 after 25 statistically insignificant results are excluded.\nExpected bias E E t t\nd\nf\n    \n-\n( ) ( )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsign\npub\n| , /\n\n is calculated using the prior density indicated in the sub-figure's\ncaption and the procedure described in Table 1. The color of the bar indicates a published result p -value in the range listed by the\nsub-figure's legend.\n6 Research and Politics \nbetween p-values and replicability. The finding is consist-\nent with the calculations of Goodman (2001) and Bayarri\net al. (2016), who shows that lower p-values are associated\nwith greater reductions in the posterior probability of the\nnull hypothesis (relative to its prior probability).\nFigure 2 indicates that our concern about the likelihood\nof a false positive should be geometrically related to our\nprior belief about Pr  = 0\n( ) and almost linearly related to\na result's p-value. When Pr  = 0 0.5\n( ) , the probability of a\nfalse positive never exceeds 5% in our calculation. However,\n( ) , we calculate that 10.6% of the pub-\nlished results in our data have Pr .\nstat. sig.\n( )\n( ) , 25.4% of the published results in\nour data set have Pr  = 0 | 10%\nstat. sig.\nthese results have have Pr  = 0 | 10%\nstat. sig.\n( ) if\n( ) . Our finding may explain why so many\nresults fail to replicate. For example, the Open Science\nCollaboration was able to successfully replicate only 39 of\n100 relationships from the psychology literature that it\ntested in its study (Open Science Collaboration, 2015: aac-\n4716-5). A survey of researchers in psychology and allied\nfields by Hartshorne and Schachner (2012: 3) found that\nonly 49% of attempted replications were able to fully repli-\ncate a study's original findings. In economics, Camerer\net al. (2016) were able to successfully replicate only 11 of\nthe 18 studies they examined, a 61% success rate. Even in\nmedicine, a recent study by Prinz et al. (2011) found that\ntheir laboratory was only able to completely replicate\nbetween 20% and 25% of the published work examined.\nConclusions and implications\nThe problem of publication bias has been studied for years\nand permeates all scientific disciplines that use statistical\nthe problem has been reignited by effort to replicate results\nin multiple disciplines that have met with a surprisingly\nhigh rate of failure (Boekel et al., 2015; Camerer et al.,\nScience Collaboration, 2015). Prior work has established\nthat statistically significant results are favored in political\nscience (e.g. Gerber and Malhotra, 2008a), but to what\nextent does it distort substantive knowledge in the\nFigure 2. Expected lower bound false positive probability calculations. The figure shows the relationship between the probability\nthat the null hypothesis is true given a statistically significant result Pr null|st. sig.\n( )\n( ) as a function of the probability of obtaining\na statistically significant result when the null hypothesis is true Pr st. sig.|null\n( )\n( ) that is implied by equation (3). To establish a\nlower bound for Pr null|st. sig.\n( ), we set Pr stat. sig.| 0 =1\n \n( ) in equation (3). We set Pr null\n( ) to several alternative values,\nas indicated in the figure's legend. The histogram shows the proportion of p -values in bins of width 0.005 for 142 published and\nstatistically significant results in our data set.\nEsarey and Wu 7\ndiscipline? Are our findings contaminated by results that\nare biased upward in magnitude? Are false positive find-\nings published too often in that literature? The answers\ndepend on the unknown prior distribution of true relation-\nships. But we find evidence for both problems in the pub-\nlished political science literature, and the problems are\nlarge enough to be qualitatively meaningful under a wide\nvariety of different prior distributions. If these problems\nexist, they occur because statistically significant results are\nfavored in the publication process: smaller values in an\nestimate's sampling distribution are disproportionately\nignored and null relationships are still likely to be published\n1995). We believe that our paper complements the findings\nof large-scale replication projects by placing them into a\nclearer theoretical context: under reasonable assumptions\nfor the prior distribution of effects f ( )\n , the results of\nthese studies are what we should expect given (a) the exist-\nence of a publication process that favors statistically sig-\nnificant results and (b) the distribution of published results\nin the literature. In short, our findings suggest that publica-\ntion bias is a reasonable explanation for at least part of the\n\"replication crisis.\"\nBased on our evidence, results with smaller p-values are\nless affected by publication bias because they are further\nfrom the  = 0.05 threshold. These results are also at lesser\nrisk of being a false positive (Bayarri et al., 2016; Goodman,\n2001). However, using a decreased threshold for statistical\nsignificance (i.e. only publishing results that can pass a sig-\nnificance test with  lower than 0.05), as suggested by\nJohnson (2013), simply recreates the problem for results\nnear the new threshold. Consider the simulations of Table 1\nfor the spike-and-slab prior when Pr  = 0 = 90%\n( ) : using\na significance threshold of  = 0.01 results in 66.9% of esti-\nmates exceeding 10% magnitude in bias (compared to\n( ) under the same prior, using a significance\ning 10% magnitude in bias (compared to 14.1% of esti-\nmates using the  = 0.05 threshold).\nThe empirical \"credibility revolution\" in economics\nand political science has rightfully made us ask harder\nquestions of the quality of our research designs on a\npaper-to-paper basis (Angrist and Pischke, 2010). But as\nlong as statistically significant results are privileged in the\npublication process, even researchers who do everything\nright from a causal identification perspective could still\nproduce a literature with results that are (on average) biased\nupward and overpopulated with false positives. Just as\nthe credibility revolution has made us more skeptical of\nsome research designs, we believe that our findings (and\nthe larger universe of findings concerning replicability)\ndemand increased skepticism of novel results. This is par-\nticularly true if the result is only marginally statistically\nsignificant, because marginally significant results are at\nincreased risk of being false positives. Consequently, it\nmay be prudent to place less importance on the novelty\nand originality of a scholar's output in evaluating his or\nher contribution to the discipline-- recognizing, of course,\nthat these are still important and valuable qualities!-- and\nmore importance on work that checks the robustness of\nexisting findings, including replication studies. We should\nalso be careful about allowing the initial discovery of a\nnew phenomenon to shape our research agenda before the\nphenomenon is thoroughly replicated. In the event that the\ndiscovery is a false positive, researchers seeking to apply\nthe findings to other areas will necessarily be building\ntheir work on a null finding, thereby raising the overall\nprior probability of null hypotheses (viz. Pr( = 0)\n ) in the\npopulation and making the overall problem of publication\nbias even worse. We think that these changes constitute a\nsubstantial revision to the status quo, but one that is\nimportant to safeguarding the reliability of the findings\nthat we communicate to each other, to our students, and to\nthe larger world.\n"
}