{
    "abstract": "Abstract\nDecisions based on algorithmic, machine learning models can be unfair, reproducing biases in historical data used to train\nthem. While computational techniques are emerging to address aspects of these concerns through communities such as\ndiscrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their\npractical implementation faces real-world challenges. For legal, institutional or commercial reasons, organisations might\nnot hold the data on sensitive attributes such as gender, ethnicity, sexuality or disability needed to diagnose and mitigate\nemergent indirect discrimination-by-proxy, such as redlining. Such organisations might also lack the knowledge and\ncapacity to identify and manage fairness issues that are emergent properties of complex sociotechnical systems.\nThis paper presents and discusses three potential approaches to deal with such knowledge and information deficits in\nthe context of fairer machine learning. Trusted third parties could selectively store data necessary for performing\ndiscrimination discovery and incorporating fairness constraints into model-building in a privacy-preserving manner.\nCollaborative online platforms would allow diverse organisations to record, share and access contextual and experiential\nknowledge to promote fairness in machine learning systems. Finally, unsupervised learning and pedagogically interpret-\nable algorithms might allow fairness hypotheses to be built for further selective testing and exploration. Real-world\nfairness challenges in machine learning are not abstract, constrained optimisation problems, but are institutionally and\ncontextually grounded. Computational fairness tools are useful, but must be researched and developed in and with\nthe messy contexts that will shape their deployment, rather than just for imagined situations. Not doing so risks real,\nnear-term algorithmic harm.\n",
    "reduced_content": "Original Research Article\nFairer machine learning in the real\nworld: Mitigating discrimination\nwithout collecting sensitive data\nMichael Veale1 and Reuben Binns2\n Keywords\nAlgorithmic accountability, algorithms, discrimination, machine learning, personal data, privacy\nIntroduction\nAs machine learning techniques are taken up in an ever-\nwider array of sectors for decision-making and deci-\nsion-support, many have pointed to harms that might\nresult from their careless or malicious implementation.\nSome harms surround fairness, as it proves to be diffi-\ncult to make systems that do not exhibit bias, indirectly\nor in subsets of data (Hajian, 2013; Kamiran et al.,\nwithin a range of linked concerns, including algorithmic\ntransparency and accountability (Burrell, 2016; Keats\nNissenbaum, 1996), in-the-wild reliability (Z\n liobaite\n_\net al., 2016); security against adversaries (Huang\ninequality (boyd and Crawford, 2012; Harcourt,\n2006); risks to privacy and due process (Hildebrandt\nand Gutwirth, 2008); and the enablement of ambient,\nubiquitous surveillance systems (Hildebrandt, 2015;\nKitchin and Dodge, 2011). These have mobilised a\n1Department of Science, Technology, Engineering and Public Policy\n(STEaPP), University College London, UK\n2Department of Computer Science, University of Oxford, UK\nCorresponding author:\nMichael Veale, University College London, 36\u00ad38 Fitzroy Square, London\nEmail: m.veale@ucl.ac.uk\nCreative Commons CC-BY: This article is distributed under the terms of the Creative Commons Attribution 4.0 License (http://\nwww.creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without further\npermission provided the original work is attributed as specified on the SAGE and Open Access pages (https://us.sagepub.com/en-us/nam/open-access-\nat-sage).\nBig Data & Society\njournals.sagepub.com/home/bds\nwide array of researchers and practitioners to consider\nhow these technologies can be utilised whilst minimis-\ning the pitfalls and risks that might accompany them.\nThis paper focuses on how fairness and discrimin-\nation in machine learning systems can be mitigated\nwithin practical institutional constraints. Machine\nlearning systems, which identify and utilise patterns in\ndata, are designed to discriminate. We use these systems\nto distinguish data points from each other based on\ncertain predictive characteristics. Some forms of dis-\ncrimination however are considered unacceptable\n(Hellman, 2008). Legally `protected characteristics',\nusually including disability, race, sexuality, gender,\npregnancy, among others, are broadly illegal to use in\nmost decision-making. These are not set in stone.\nFor example, while single-sex sports clubs, toilets, or\nspecific types of job advert (e.g. modelling) are usually\nnot illegal, acceptance of them is changing.\nDiscrimination usually also requires cases to be other-\nwise comparable. In some situations, sex might not be\nconsidered discriminatory where decisions hinge on dif-\nferences in statistical life expectancies (Berendt and\nOther bars for measuring fairness are less universal.\nJudging based on appearance; on events that occurred\nsome time ago; on limited data; on actions an individ-\nual has already been sanctioned for, or in conditions\nof high uncertainty and rapid change, is sometimes\nacceptable, sometimes not. Judging based on arbitrary\ncharacteristics, like favouring those who access online\nforms with custom web browsers (Pinsker, 2015), might\nalso seem unfair, perhaps because of the opportunistic\nshort-lived nature of such correlations as well as the\nassociated ways it might discriminate against those\naccessing forms from schools, or from libraries.\nSome sources of unfair machine learning systems\nThere are several interacting ways that deployment of\nmachine learning can potentially lead to unfair or dis-\ncriminatory outcomes.\nUnfairness in data, their collection and their processing. Many\nof the fairness issues in machine learning are primarily\nthought to arise from data. Some think, falling for\nwhat could be called the `neutrality fallacy', that\nmachine learning will provide a more even and object-\nive treatment of individuals (Sandvig, 2015). As Latour\nindicates, we are often more than happy to declare\nvalue-laden issues as matters of fact, and let machines\nsettle them for us (1999). This is rarely appropriate.\nThe high demand for labelled data in the context of\nsupervised machine learning \u00ad the focus of this paper \u00ad\ncan usually only be met by using data from previous\ndecision-making. If these historical data reflect existing,\nunwanted discrimination in society, the model that is\nlearned from it \u00ad essentially a similarity engine \u00ad will\nlikely encode these same patterns, risking reproduction\nof past disparities. Machine learning algorithms are\nsupposed to discriminate between data points \u00ad that is\nwhy we use them \u00ad yet some logics of discrimination,\neven if predictively valid, are not societally acceptable.\nFurthermore, if some sub-groups are historically\nundersampled, or exhibit more complicated, nuanced\nor under-evidenced patterns compared to others,\nmodels might exhibit differential performance. It is\nnot practically possible to have data on all individuals,\nquantifying or classifying all factors important to some\nsocial phenomenon. People, or aspects of their lives, are\nalways missing. These skews fast make their way into\ndata-driven systems.\nData are often also cleaned and transformed before\nuse, in subjective ways. `Feature engineering', where\ninput variables are transformed to make them more\namenable to modelling, has crucial downstream\nimpact on the behaviour of machine learning systems.\nFeature engineering emphasises aspects of certain vari-\nables through augmentation, aggregation and summar-\nisation of characteristics whilst downplaying others.\nFor instance, aggregating those who subscribe to dif-\nferent branches of a religious doctrine (e.g. Catholic,\nProtestant; Shia, Sunni) within a single overarching\ndoctrine (Christian, Muslim) might collapse distinc-\ntions which are highly relevant to questions of fairness\nand discrimination within certain contexts. Including a\nstandard deviation of a characteristic as an input vari-\nable will make it easier for a machine learning model\nto emphasise divergence from a constructed average.\nAs with many issues in machine learning, the political\nnature of this classifying and sorting has long been\nrecognised (Bowker and Star, 1999). Categorisation\ndoes not just label people, it can create groups and\nalter future outcomes (Hacking, 1995; Harcourt,\n2006), just as feature engineering can in machine learn-\nUnfairness from selecting and specifying a machine learning\nsystem. Humans carry their worldviews and make\nvalue-laden choices, with both foreseeable and unfore-\nseeable consequences, during the whole modelling pro-\ncess. While machine learning is often portrayed as\nautomated, a great deal of subjective human labour is\ninvolved in system design and deployment. Model\nchoice itself can be political. Neural networks or\nrandom forests are more amenable to capturing syn-\nergy between variables than linear regression. Use of\nregression might omit important contextual variance,\nfor example. Within a model family, further hyperpara-\nmeters must be specified. Higher regularisation param-\neters penalise complexity in a model, which might help it\n2 Big Data & Society\ngeneralise but might trade-off for certain complicated or\nrare patterns not being retained. Different evaluation\nmechanisms for models emphasise different aspects of\nUnfortunately, `neutral' choices in machine learning sys-\ntems do not exist \u00ad candidates for these, such as software\ndefaults, are best thought of as arbitrary.\nFinally, once a model has been built, there are vari-\nous ways it can be deployed in practice which may\nintroduce additional fairness issues. The extent to\nwhich a model may have different impacts on different\ngroups may only become evident once that model is put\ninto a decision-making system; for instance, the setting\nof thresholds for positive and negative outcomes could\nhave significant consequences for different groups\nwhich may not be evident by merely studying the\nmodel itself. The introduction of an algorithmic\nsystem may also provide spurious justification for deci-\nsions which would otherwise have been more open\nto challenge under a purely human decision-making\nAs with any sociotechnical, value-laden problem,\nwe cannot expect to find simple or universal panaceas.\nWe are stuck with layered, messy techniques to\ndefine, resolve and manage these complex challenges.\nThis paper zooms in to examine one piece of this\nchallenge \u00ad how potentially unfair patterns in datasets\nthat make their way into modelling and decision-\nmaking processes might be remedied in practical\nrather than theoretical machine learning situations.\nWe emphasise situations where actors designing and\ndeploying such systems wish to avoid bias themselves,\nfor regulatory and reputation-related reasons, rather\nthan adversarial situations where external investigators\nwish to discover bias against the will of the organisa-\ntions undertaking analysis. Legislative discussion\nwithin a European context of the ability to investigate\nalgorithmic systems can be found in Edwards and\nCan we statistically `debias' data and algorithms?\nComputational techniques to prevent machine learning\nmethods from perpetuating these forms of bias have\nbeen proposed in recent years by research communities\nsuch as discrimination-aware data mining (DADM)\nand fairness, accountability and transparency in\nmachine learning (FATML). They involve altering\nusual data science processes in order to correct these\nforms of bias. They can operate at several stages,\nincluding pre-processing, in-processing and post-\nprocessing (Hajian and Domingo-Ferrer, 2013). In\neach case, the aim is to induce patterns that do not\nlead to discriminatory decisions despite the possibility\nof biases in the training data.\nAnti-discrimination law has particularly motivated\nDADM and FATML communities, who have\nattempted to formalise these requirements for mathem-\natical implementation. For instance, heuristics such as\nthe US Equal Employment Opportunity Commission's\n`80% rule', which provides a suggested level of permis-\nsible disparity between protected groups and the\ngeneral population, have been used to set parameters\nfor fairness-aware models (Feldman et al., 2015).\nWithin European contexts, non-discrimination and\ndata protection are rights enshrined in the EU\nCharter of Fundamental Rights, and both potentially\nrelate to the risks of unfairness inherent in machine\nof the EU General Data Protection Regulation\n(GDPR) refers in particular to fairness-aware data\nmining technologies and organisational measures.\nThere are multiple ways to define fairness formally\nin machine learning contexts. Most measures focus\non differences in treatment between protected and\nnon-protected groups, but there are multiple ways to\nmeasure differences in outcomes. These include:\n`disparate impact' or `statistical/demographic parity',\nwhich considers classification rates between groups;1\n`accuracy equity', which considers the overall accuracy\nof a predictive model for each group (Angwin et al.,\nequity', which considers the accuracy of a predictive\nmodel for each group, conditional on their predicted\nclass (Dieterich et al., 2016); `equality of opportunity',\nwhich considers whether each group is equally likely to\nbe predicted a desirable outcome given the actual base\nrates for that group (Hardt et al., 2016); and `disparate\nmistreatment', a corollary which considers differences\nin false positive rates between groups (Zafar et al., 2016).\nOther measures focus not just on actual outcomes and\ntheir relation to true/false positives/negatives, but on\ncounterfactual scenarios wherein members of the pro-\ntected groups are instead members of the non-protected\ngroup (i.e. a woman classified by the system should get\nthe same classification she would have done had she\nEach of these measures of fairness are arguably rea-\nsonable ways to measure fairness. One might therefore\nhope that a fair system would satisfy all of these con-\nstraints. But unfortunately, recent work has formally\nproven that it is impossible for a model to satisfy sev-\neral of these constraints at the same time, except in\nexceptional cases which are unlikely to hold in the\nKleinberg et al., 2016). As a result, choices between\nthe different measures will have to be made. In some\ncases it may be more important to focus on differences\nbetween positive classifications (e.g. loan applications),\nand therefore an `equality of opportunity' measure\nVeale and Binns 3\nmight be preferable; in others, the cost of a false nega-\ntive might be higher (e.g. the risk a violent criminal\nmight pose to the public). Thus the choice of a particu-\nlar fairness measure therefore ought to be sensitive to\nthe context.\nSetting aside these definitional problems, fairness-\naware machine learning techniques are increasingly\nseen as desirable, viable and even in some cases legally\nrecommended or required. However, an important\nchallenge remains. To be successful, these techniques\ndepend on knowledge about the potential correl-\nations between features in the training data and\nprotected characteristics that are the subject of anti-\ndiscrimination and data protection law. In practice,\nthis is a condition that is either not always met, or\nnot always desirable to meet.\nWhy knowledge of protected characteristics\nis both necessary and problematic?\nTo see why knowledge of protected characteristics is\nnecessary, it is helpful to consider why certain nai\u00a8ve\napproaches to removing bias from modelling are inad-\nequate. One could simply delete any sensitive variables\nrelated to discrimination, e.g. age, gender, race, or reli-\ngion, from the training data. Unfortunately, this does\nnot guarantee non-discrimination in the models that\nare trained on this data, as non-discriminatory items\nmight exist which in some conditions are closely corre-\nlated with the sensitive attributes. Where geography\nserves as a sensitive proxy, this phenomenon is\ntermed `redlining'. More broadly, it can be seen as an\nissue of redundant encoding.\nIn order to discover redlining in training data, one\nneeds to be able to find out whether sensitive attributes\nmight be encoded by other, apparently benign ones.\nFor instance, to discover whether ZIP codes in a data-\nset are correlated with, e.g. race, it will be necessary to\neither have race as an attribute in the dataset, or to\nhave background knowledge about the demographics\nof the areas in question (for instance, from census rec-\nords). Proposed approaches to non-discriminatory\nmachine learning assume that whoever is implementing\nthe technique has access to the sensitive attributes\nwhich might be encoded (e.g. Hajian and Domingo-\nnecessary for assurance of computationally non-\ndiscriminatory models (Z\n liobaite\nDespite this, in many cases organisations deploying\nmachine learning will lack this necessary access, often\nfor legitimate reasons.\nFirst, the collection of personal data inevitably\ncreates privacy risks. Many organisations have interna-\nlised the dictum of regulators and privacy advocates\nonly to collect data that is necessary for their purposes.\nThe concepts of data minimisation and purpose limita-\ntion within the GDPR are intended to prevent col-\nlection and processing of data for unspecified\nor disproportionate ends. Furthermore, the kinds of\nprotected characteristics involved in cases of discrimin-\nation raise higher privacy and data protection risks\nthan other kinds of data, and are given special protec-\ntion under both the GDPR and other laws (Edwards\nand Veale, 2017). The proposition that organisations\nought to collect a wide range of sensitive data that\nisn't directly necessary for their primary purposes\ncontradicts this general dictum. Yet fairness-aware\nmachine learning seems to require organisations to do\nexactly that to adequately inspect and modify their\nIt is not our aim here to analyse the extent to which\nprivacy and data protection law and best practice is\nsubstantively in conflict with the collection and process-\ning of sensitive attributes for the purposes of fairness-\naware machine learning.3 It may be that collection and\nprocessing for such purposes is legitimate; however, it\nmay still not be desirable. It would require data subjects\nto share sensitive attributes along with non-sensitive\nones every time their data was to be used to train a\nmodel. The general result would be much more sensi-\ntive data in the hands of data controllers \u00ad a security\nrisk even if it is intended to be used for the legitimate\npurposes of avoiding discriminatory outcomes. Even if\norganisations are permitted to collect and process such\ndata, requiring consumers to provide it might make\ntheir service less competitive, or less trusted. For pur-\nposes of building a model that serves some narrowly\nprescribed goal, they may not see the need to collect\nsensitive data. In the context of data minimisation, the\ndata controller must argue that it is proportionate to\ncollect and process sensitive categories of data, and\nthey may not be sufficiently incentivised to do so.\nWhere individuals fear they are being treated unfairly,\nthe collection of sensitive data by the organisation in\nquestion, even to explicitly remedy fairness issues,\nmight not alleviate that perception-based fear.\nIt could even make it worse.\nSome approaches have been proposed to transform\ntraining data with anonymisation procedures to protect\nthe sensitive attributes. This can be performed in\ntandem with pre-processing techniques to prevent dis-\ncrimination (Hajian et al., 2014; Hajian and Domingo-\nFerrer, 2012). While promising, this still mandates the\ncomprehensive collection of sensitive attributes from\nindividuals in training data for each form of discrimin-\nation for which mitigation is desired. Despite meaning-\nful privacy protections, the concerns raised above are\nstill likely to apply. Individuals are unlikely to be happy\nproviding a comprehensive range of sensitive personal\ndata to the very organisations who are in position to\n4 Big Data & Society\ndiscriminate, no matter how technically robust their\nanonymisation process is.\nThree approaches for appraising and\nimproving fairness with limited data\nOrganisations developing learning systems need strate-\ngies to mitigate discrimination concerns in the absence\nof sensitive data. The challenge is to implement the\ntechniques, such as those outlined above, without\nhaving to take on the additional burden and risk of\ncollecting detailed sensitive data on the training sample.\nWe present three alternative approaches to over-\ncome this challenge. The first is based on a multi-\nparty data governance model, suited to contexts\nwhere little background knowledge about discrimin-\nation exists and a comprehensive assessment of poten-\ntial forms of discrimination is needed. The second\ninvolves a collaborative knowledge sharing approach\nin which organisations can learn from each other's\nexperiences in similar contexts as well as relevant socio-\nlogical and demographic correlations. The third\ninvolves exploratory analysis to build hypotheses of\npotential unfair characteristics of the data or system,\nwhich can be more formally tested as part of a due\ndiligence process. Figure 1 pictographically illustrates\nthese three distinct approaches.\nWe do not argue that these three methods are\nperfect, nor that they provide complete solutions or\nassurances to the multitude of challenges surrounding\nmachine learning systems. We argue instead that these\nare avenues that are important to explore to make\nfairer machine learning a practical reality in the multi-\ntude of settings that automated and semi-automated\ndecisions will be occurring in our society in the\ncoming years and decades.\nTrusted third parties holding protected\ncharacteristics\nVarious proposals have been made for the involvement\nof external parties in the evaluation and auditing of\nreflected in law. Article 35 of the GDPR obliges organ-\nisations to undertake `data protection impact assess-\nments' wherever `profiling' is used to automatically\nmake decisions which have legal or significant effects\non data subjects. In some cases these assessments may\nbe audited by a data protection authority (Recital 84).\nIn most governance approaches, external auditors are\ngiven access to an organisation's policies, personnel,\ndata collection procedures, training data, models, pro-\nprietary code, and other relevant aspects, in order to\nassess the ethical dimensions and legal compliance of a\nparticular algorithmic system (see Binns, 2017).\nThis model assumes that the relevant information\nrequired to perform an audit will lie in the hands of\nthe organisation being audited. As argued above, this\nmight not be the case, rendering external audit process\nincapable of ensuring the kinds of algorithmic fairness\nthat DADM and FATML techniques aim for.\nThis might be different, were trusted third parties\nenlisted to work alongside organisations from when\ndata collection begins. This proposal could be achieved\nwith a variety of different institutional and technical\narrangements. Below, we illustrate several possible\nimplementations.\nTrusted third party approach\nFirst party\n(e.g. insurer)\nCivil society\nModeller\nAcademic\nThird party\n(e.g. NGO, government)\nKnowledge base approach Exploratory fairness analysis\napproach\ntraining data\n(e.g. income,\neducation,\nexperience)\nsensitive data\n(e.g. sex, ethnicity,\nreligion, sexuality)\nproduces model\nmight this latent\nvariable be\nsensitive?\nproduces\ndiscrimination\nanalysis\ndecision\nsubject ity\ndata\ninput\nquery by\nrecord\nwith API\n\"In our CV\nwe found...\"\npotentially\nnon-sensitive\nvariable\npotentially\nnon-sensitive\nvariable\nlatent variable\npotentially\nnon-sensitive\nvariable\ndoes this logic or\nrule seem\nsuspicious?\nblack-boxed\nmodel\nmore\ninterpretable\nmodel\n\"Might any of\nour variables be\ncorrelated\nwith...?\"\n\"In a study, we\ndiscovered a\ncorrelation\nbetween...\"\n\"Does this data\ncontroller have data\nthey could use to\ndiscriminate?\"\n\"Given this\ncorrelation, are all\nrelevant organisation\ntaking mitigating\naction?\"\n\"How might\nwe responsibly\nmodel this\nphenomenon?\"\nreturn\npredictions\ndata input\nwww\ntrain with\nrepeated\nqueries\nFigure 1. Three approaches to fairness-aware machine learning without holding sensitive characteristics.\nVeale and Binns 5\nThe first party (the organisation implementing the\nalgorithmic decision-making system) has access to his-\ntorical data relevant to the classification or prediction\ntask for which they are building a model. However, the\nfirst party does not and should not have access to any\nof the protected characteristics associated with the\npopulation used to train the model.\nAs discussed above, in order to statistically test the\nmodel for potential discrimination, the protected char-\nacteristics need to be linked somehow to the records\nused in the training data. To achieve this, a trusted\nthird party is enlisted to collect data on the protected\ncharacteristics of those individuals whose data is used\nto train the model. For each individual, protected char-\nacteristics like race, gender, religious beliefs or health\nstatus are collected by the third party in parallel to the\ncollection of the non-protected characteristics by the\nthird party. The channel for communicating this infor-\nmation from the individual to the third party may\ndepend on the platform (e.g. online, telephone, or in-\nperson). It could be as part of a separate collection\nprocess, although this prove unwieldy, or be encrypted\nsimultaneously and seamlessly at the point of collection\n(e.g. locally through JavaScript in a web browser4) with\nthe public key of a third party, and transmitted to the\norganisation in question.5\nConsider the following illustrative example:\nAn insurer wishes to use a machine learning model to\nhelp determine customers' premiums. They have access\nto historical customer, and use it to train a model to\npredict the amount of compensation a customer will\nclaim over the term of their cover given certain attri-\nbutes (e.g. postcode, occupation, qualifications). The\nestimated size of a potential claim \u00ad the output of the\nmodel \u00ad is used to automatically set premiums.\nThe insurer enlists a third party organisation (for\ninstance, a consumer rights group) to simultaneously\ncollect protected characteristics about each customer\nas they purchase their insurance policy. For online\npurchases, the customer is directed to the consumer\nrights group's domain, and asked to provide protected\ncharacteristics for the purposes of discrimination\nprevention.\nBased on this multi-party data governance model,\nthere are multiple ways to proceed, depending on\nwhether the goal is merely to detect bias or to both\ndetect and prevent it, and what prevention techniques\nwill be used (e.g. pre-processing, in-processing, or post-\nprocessing). We outline a set of possible variations here,\nand discuss their relative advantages and drawbacks.\nVariation 1: Third party as ex post disparate impact\ndetector. In cases where the third party's only role is\nto detect discrimination (but not prevent it), the third\nparty need only collect protected characteristics from\neach individual featured in the dataset used to train\n(and test) the model, along with an identifier. The rec-\nords held by the first party for the purposes of model\ntraining could be linked by this identifier to the records\nheld by the third party which contain the protected\ncharacteristics. The third party would be given access\nto the model developed by the first party (either directly\nor via an application programming interface (API)).\nBy testing the outputs of the model on each of the indi-\nviduals in their sensitive attribute dataset (using the\nindividual's identifier), the third party could detect dis-\nparate impacts.\nAn advantage of this variation is that the third party\ncan only access the sensitive attributes, not the poten-\ntially non-sensitive ones. Since each record only\ncontains sensitive attributes and an identifier this\nrepresents a lesser privacy risk; while the data itself is\nsensitive, it would be harder to re-identify an individual\nwithout other data types. This may also be beneficial\nfrom the perspective of a first party concerned about\nkeeping their proprietary model secret, as it has been\nshown that unlimited access to a query interface for a\nprediction model can allow an attacker to extract and\nreconstruct a model (Trame\nwhile the third party would have unrestricted ability to\nquery the model by individual identifiers, and thus learn\nthe distributions of outputs for each protected charac-\nteristic, they would not be able to reverse-engineer the\nmodel without access to the other, non-protected\ncharacteristics.\nThe disadvantage of this variation is that it only\nprovides the first party with evidence of the disparate\nimpact of their model. Disparate impact is a blunt\nmeasure of discrimination, because some disparities\nmay be `explicable', in the sense that the disparities\nmight be accountable by reference to attributes which\nare legitimate grounds for differential treatment (Zafar\n liobaite\nures of disparate impact may not be sufficient for the\nfirst party to actually change their model to prevent it\nfrom being discriminatory. For instance, to remove bias\nfrom the training data, the first party would have to\nknow which data points to relabel, massage or re-\nweight \u00ad i.e. the protected characteristics of the specific\nindividuals, which they would lack. More generally,\nwithout the ability to check for redundant encoding\nof protected characteristics by non-protected attributes,\nit will be difficult for the first party to revise their\nmodel.\nNevertheless, the mere ability to detect disparate\nimpact may be valuable in allowing third parties to\nflag up problems, which can then be dealt with by\nallowing the first party access to the necessary\n6 Big Data & Society\nadditional data to investigate and transform their\nmodel accordingly. Separating out detection of dispar-\nate impact and prevention could thus prevent unneces-\nsary sharing of sensitive attributes and enable the third\nparty to perform continuous monitoring.\nVariation 2: Third party as ex ante discrimination\nmitigator. Alternatively, the third party could collect\nboth the protected attributes and the other features\nused to train the model. This would enable the third\nparty to play a more significant role, not only detecting\ndisparate impact in model outputs but also helping to\nensure the disparities are attributable to disparate mis-\ntreatment (i.e. that they are not explainable), and also\nto ensure that the model can be bias-free.\nThird party as redlining detector. In this approach,\nthe third party has both the sensitive and potentially\nnon-sensitive characteristics, and puts them through a\ncommon framework to produce summary information\nthat aims to flag obvious issues that might occur during\nmodel building. Upon acquisition of a cleaned dataset,\nthe third party calculates and returns a set of redundant\nencodings and their strengths. The returning document\nmight note that `race is correlated to zip code by 0.8';\n`gender is correlated with aspects of profession by 0.2',\nand so on. The first party could use this knowledge to\nmake trade-offs in the model \u00ad removing certain fea-\ntures, or engaging in further discussions with the third\nparty about potential procedures to scrub unwanted\ncorrelations from a model.\nNaturally, such a framework could suffer from flaws\nwhich made it unsuitable for some types of data or\nproblems, particularly highly contextual ones. Yet this\napproach would create a focal point for the improve-\nment of discrimination detection methods for certain\ncontexts and data types, which would foster active dis-\ncussion and debate about best practices and processes\nthat could be translated into on-the-ground practice\nwith relative ease.\nThird party as data pre-processor. Another approach\nwould see the third party pre-process the training data\nin such a way as to preserve anonymity and remove\nbias, before handing it over to the first party. This\ncould be achieved by modifying the data to preserve\ndegrees of anonymity (using techniques such as statis-\ntical disclosure control (Hundepool et al., 2012;\nWillenborg and de Waal, 2012), and privacy-preserving\ndata mining (Agrawal and Srikant, 2000), which allow\nthe statistical properties of the data to be maintained),\nfollowed by applying one of a range of anti-biasing\ntechniques described in the DADM/FATML litera-\ntures (e.g. Feldman et al., 2015; Hajian and\nIt would even be possible, if it were desired, to introduce\npositive discrimination at this point, and some methods\nhave been proposed for how this could be achieved\n(Verwer and Calders, 2013). As mentioned above,\nmore recently proposed techniques aim to render data-\nsets both k-anonymous and non-discriminatory in a\nsingle procedure with limited loss of accuracy (Hajian\ntransformed the data to increase privacy and remove\nbias, the third party could then hand it over to the first\nparty for model development.\nThe advantage of this variation is that the first party\ncan develop whatever kind of model they like, without\nthe risk of it learning biases from the training data.\nIt also limits the involvement of the third party to a\nsingle step, after which the data could be deleted.\nFinally, it encourages the development of expertise on\nthe part of the specialist third party and doesn't require\nthe first party to have in-house knowledge about fair-\nness-aware machine learning. The disadvantage of this\napproach is that the anonymisation techniques only\nprovide a degree of (quantifiable) anonymity. There is\na clear trade-off between degrees of anonymity and util-\nity of the dataset (Loukides and Shao, 2008), such that\nuseful datasets will still likely carry re-identification\nrisks. To the extent that such risks persist, the first\nparty could learn more about individuals' sensitive\ncharacteristics in this variation than it could in the\nother variations.\nWho could act as a third party? We have thus far assumed\nthe existence of a suitable trusted third party, but it is\nworth considering what kinds of organisations might\nfulfil this role. This will likely depend on which of the\nvariations are adopted. Each might pose different\nrequirements of trustworthiness, technical expertise\nand incentivisation. In the case of a third party whose\nrole is merely to detect disparate impact, relatively little\ntechnical expertise would be required, making it suit-\nable for organisations with fewer resources and tech-\nnical skills. The fact that disparate impact is already the\nfocus of many civil society groups' research activities\nmay make them well situated to take on this role. Many\npotentially affected minority groups already have active\nrepresentatives who could benefit from more formal\nauditing roles. Depending on the application context,\nit may be appropriate to involve different organisa-\ntions; for instance, trade unions might be more\nequipped to address the fairness of algorithmic\nmodels deployed in human resources decisions.\nIf the third party is expected to be an ex ante dis-\ncrimination mitigator, they will require more data col-\nlection and particular expertise in fairness-aware\ntechniques. It may therefore need to be a specialist\norganisation, potentially working in collaboration\nVeale and Binns 7\nwith appropriate civil society organisations. It could be\nanticipated that consultancy or accountancy firms\nmight provide these services to corporate clients, as\nthey do with other forms of social auditing.7\nAnother option might be statutory or chartered\nbodies whose remit includes monitoring discrimination,\npromoting equality, or enforcing law. For instance, the\nEquality and Human Rights Commission in the UK, or\nthe Equal Employment Opportunity Commission in the\nUS, are statutory bodies responsible for enforcing\nequalities laws. While traditionally involved in review-\ning of individual cases for litigation, providing legal\nassistance and intervening in proceedings, these\nbodies could also take on more ongoing, data-driven\nmonitoring of data-driven discrimination. Bodies more\nlinked to data governance might help here too, such as\nthe Conseil national du nume\u00b4rique (French Digital\nCouncil) or the data stewardship body recently recom-\nmended by the Royal Society and the British Academy\n(2017). State-sponsored API frameworks such as\nGOV.UK Verify, where the public sector certifies com-\npanies to provide verification services to third parties,\nmight also serve as a framework to allow auditors to\nquery trusted bodies for protected characteristics.\nKnowledge bases about fairness\nin data and models\nExperiential knowledge concerning the construction or\nattempted construction of ethical algorithmic systems\nhas been largely neglected in the DADM and FATML\ncommunities. This has created a not insignificant know-\nledge gap that we believe has problematic consequences\non-the-ground. This neglect is surprising for several\nreasons.\nAs data governance tools move increasingly towards\nex ante prevention and anticipation of harms, particu-\nlarly through data protection and privacy impact assess-\nsolely on in-data analysis of unfairness appears not just\nat tension with on-the-ground regulatory needs \u00ad it\ncould even be described as paradoxical. It certainly\nseems problematic to have to link the data and train a\nsystem before you can decide whether you should even\nbe doing either of those things. Many organisations\ncannot legally or practically proceed with any data\nwork, even basic data access, cleaning, linking or explor-\nation, until this stage is passed. Yet DADM and\nFATML approaches often implicitly assume that all\nthe ingredients are on the table to build the tool, and\nthe only decision to be made is whether to deploy or not.\nMachine learning is a generic technology with\nsector-specific applications. High profile, consequential\ndomains have included anticipating the geospatial dis-\nWetenschappelijke Raad voor het Regeringsbeleid\n(WRR), 2016), the need for child protection\n(Vaithianathan et al., 2013) and the detection of tax\nfraud (Khwaja et al., 2011; Sharma and Kumar\nPanigrahi, 2012). Some ethical issues are sector- or\neven location-specific, but others are likely to be\nshared. Highly problematic issues might only appear\nrarely, limiting their propensity to capture with in-\ndata analysis.\nLimited implementation and education surrounding\nDADM and FATML technologies threatens our ability\nto cope with pressing issues in today's machine learning\nsystems. Even though this research field has some his-\nware libraries remain largely unavailable, and little\ntraining exists. Given the current lack of practical\nethics education in computer science curricula, rapid\nSpradling et al., 2008). A stopgap is sorely needed.\nDiagnosing and addressing social and ethical issues\nin machine learning systems can be a high capacity\ntask, and one difficult to plan and execute alone\nor from scratch. Ethical challenges or appropriate\nmethods to tackle them might lurk within aspects of\nenvisaged that are easy overlooked, such as hyperpara-\nmeters, model structure, or quirks in data formatting or\ncleaning. Some issues that might arise might also not\nhave their origins in the models or the data, but sur-\nrounding social, cultural and institutional contexts.\nIssues such as automation bias (Skitka et al., 1999),\nwhere individuals either place too much trust or too\nlittle trust in decision support systems, might be a syn-\nergistic result of both the model and the user interface.\nOther issues might have their origins in a model but\nlikely solutions elsewhere. For example, for fairness\ngrievances which are particularly difficult to detect or\nanticipate, better systems for decision subjects to feed-\nback to decision-makers might be required. These\nissues might not have one-size-fits-all answers, but\nthey are also unlikely to need to be treated as fresh\neach and every time they arise.\nIssues of changing data populations and correlations\nare both currently under-emphasised in DADM/\nFATML work and appear difficult to fully address\nwith in-data analysis. Concept drift or dataset shift\nrefers to either real or virtual (differently sampled)\nchanges in the conditional distributions of model\ninputs and outputs (Quin\nfor example, how changes in law might qualitatively\naffect prison population or the strategies of fraudsters.\nFairness and transparency are not static but moving\ntargets, and ensuring their reliability is important. But\nanticipating change is technically difficult. Knowledge\naround rates and causes of change can be tacit, obliging\n8 Big Data & Society\nus to carefully consider how best to use expert input\n(Gama et al., 2013). In particular, these phenomena can\nbe hard to examine when changes are nuanced, or even\nare a result of the actions of previous machine learning\nsupported decisions themselves. An important key role\nfor domain experts going forward is to explain and\nrecord how and why certain types of concept drift\noccur, rather than just help in their detection\n(Z\n liobaite\nPractical aspects of a knowledge base for fairness. Given the\nabove factors, we propose that a structured, commu-\nnity-driven data resource containing practical experi-\nences of fair machine learning and modelling could\nserve as a useful resource both in the direct absence\nof sensitive data, and more broadly in its own right.\nSuch a resource, held online, would allow modellers\nto record experiences with problematic correlations\nand redundant encoding while modelling certain phe-\nnomena, as well as sociotechnical ethical issues more\nbroadly (such as interpretability, reliability and auto-\nmation bias), and detail the kinds of solutions and\napproaches they used or sought to remedy them.\nIt could operate on a relatively open, trust-based\nmodel, such as Wikipedia, or have third-party gate-\nkeepers, such as NGOs or sectoral regulators verifying\ncontributions and attempting to instil anonymity where\npossible or desired. It would create a stepping-stone to\nenable practical, albeit rudimentary, fairness evalu-\nations to be carried out today.\nLinked data technologies have already seen sig-\nnificant adoption in sectors where cross-organisational\ncollaboration around data is necessary (Bizer et al.,\n2009). This does not necessarily mean an industry-\nwide, comprehensive, rigid ontology for the purposes\nof addressing the ethical challenges of machine learning\nhas to be adopted. Rather, a minimal adoption of\ncommon practices would enable different organisations\nto collaboratively annotate and describe the resource.\nSeveral challenges would need to be addressed\nbefore such a database could be implemented.\nSimilar variables and entities would need to be aligned\nin order to make such a dataset structured and\nnavigable. Higher level common identifiers might be\nneeded to group variables even if the levels of such\nvariables were different. Some categorisations might\nhave given individuals the chance to specify non-\nbinary gender identities, or to opt out from this\nquestion \u00ad but this is unlikely to make any correl-\nations or lessons found completely irrelevant or non-\ntransferable in practice. Database ontologies should\nincorporate broader parts of the modelling process,\nsuch as cleaning or user interfaces, but the best\nformat to do this is unclear. Arriving at it will likely\nbe a result of trial-and-error.\nMetadata should also be standardised. What kind of\ndiscrimination discovery methods were being utilised?\nHow could effect strength or statistical significance be\ncaptured across these? It is likely that a descriptive\nvignette would also be useful, particularly concerning\nsocial processes and organisational context, but should\nor could this take a standardised format whilst remain-\ning effective?\nSuch a dataset might benefit from discussion and\ninput from different viewpoints both within the organ-\nisations submitting the information, but also externally.\nOpen annotation or discussion technologies might\ncontribute questions and context to the methods and\ncontent of dataset entries (Pellissier Tanon et al.,\n ic\n\u00b4\nand Kro\nStackExchange, a question and answer network ini-\ntially aimed at developers, but recently with wider\nadoption, have proved practically popular technical\nand social tools for solving issues around software.\nSuch a database could take inspiration from the factors\nthat make knowledge communities run effectively in\nthese virtual environments. Allowing organisations to\ntrace the sources of the data in such collaborative\nknowledge bases would also be key; in this respect,\nmuch could be learned from proposed solutions to simi-\nlar challenges in scientific data collaboration (Missier\nMost data scientists are already used to working col-\nlaboratively online, through leading technologies in this\nspace such as Git, MediaWiki, or StackExchange. Yet\ndata scientists form only one part of the puzzle.\nAs discussed, fairness issues can concern different\nparts of the modelling process, and as such viewpoints\nfrom others such as user interface developers, project\nmanagers and decision subjects would likely be valid\nand useful. The technologies chosen should be clear\nand accessible to those who are not used to working\nin these virtual spaces, whilst incorporating the features\nand extensibility that more developed solutions bring.\nIf they are not, they are likely to become exclusionary\nand not see the widespread adoption that would make\nthem most useful.\nIt is not just modellers who can contribute informa-\ntion to this knowledge base. Quantitative and qualita-\ntive findings in the research literature that might be\nrelevant to particular fields or data sources could be\nadded. For example, considerable amounts of research\nexist on areas such as financial literacy, recidivism\nor child protection which are carried out with the\naims of improving their fields, but not directly to\nmake or inform decision support or decision-making.\nThese forms of evidence could be used to directly\ninform model structure, or to inform in-data analysis\nand search for ethical issues and concerns. Many of\nVeale and Binns 9\nthese pieces of evidence are currently hard to locate \u00ad\nthey are published across disciplines, behind paywalls,\nor with research questions that do not make clear the\ncorrelations that the research also unearths. In the\nmedium term, text mining and natural-language pro-\ncessing might help populate such a database semi-\nautomatically.\nDADM/FATML methods, given their own tech-\nnical opacity to laypersons, come with their own\nissues of transparency and legitimacy. Individuals are,\nunder the GDPR, entitled to know when automated\nprocessing of their personal data is occurring, and for\nwhat purposes, although there are practical caveats\nregarding these rights (Edwards and Veale, 2017). Yet\nfor them to understand the potential harms that could\naccrue to them by consenting is much trickier. Both\nthey and trusted independent third parties usually\nlack the source data for investigative purposes. Even\nif they had it, it is unclear that it would be hugely\nuseful or revealing given the rapidly changing nature\nof these datasets and the patterns within and the\nample possibilities for data linkage that usually exist.\nYet what they are (usually) interested in is not the data\nthemselves, but the potentially problematic patterns the\ndata support. An evidence base might help individuals\nor organisations understand what insights are held in\ndifferent forms of data.\nPotentially confounding issues. The proposal is largely\ngrounded on the idea that organisations would be will-\ning to spend time and money on cooperating to create a\ncommon resource. Primarily, this is a collective action\nproblem, as there are great incentives to free ride and\nlet others provide the information, which could result in\nnon-provision (Olson, 1971). This is compounded by\nintellectual property concerns. If insights from data\nare viewed through an IP or a trade secrets lens, this\ncould make organisations reticent to share.\nYet sharing of data for ethical purposes between\nfirms is far from unheard of, particularly in other sec-\ntors facing similarly tricky societal challenges. Social\nand environmental issues in the global clothing sector\nare pervasive due to uncertainties around the environ-\nmental impact of processes, materials and chemicals,\nand uncertainties in the on-the-ground production sys-\ntems characterised by multi-layered subcontracting.\nThe Sustainable Apparel Coalition (SAC) emerged as\nbers representing well over a third of all clothing and\nfootwear sold on the planet. Together with the US\nEnvironmental Protection Agency (EPA), and with\nseveral large data donations and collection projects\ninvolving members, they have been developing the\nopen-source Higg Index to give designers tools to\nbetter and more rigorously anticipate potential\nproducts' sustainability further upstream. In some\nways, withholding data about ethical concerns and\npotentially salient social issues could itself be seen as\na controversial, reputational risk.\nFurthermore, the institutional field of the technology\nsector does not seem unamenable to this form of\ncooperation. Institutional fields create like-minded\ncommunities of practice through three main mechan-\nisms \u00ad coercive pressure, where influence from actors or\nactants enforces homogeneity; mimetic pressures, which\nstem from standard, imitative responses to uncertainty;\nand normative pressures, which stem from how a field\ncoalesces and becomes professionalised (DiMaggio and\nPowell, 1983). Some promising normative pressures can\nbe seen across the machine learning modelling field that\ngive hope for this \u00ad communities of voluntary support\non question\u00adanswer networks such as Cross Validated8\n(which themselves support mimetic pressures); pro-\nbono data science for non-profits on the weekends\nthrough growing organisations like DataKind; virtual\ndiscussions and events from field leaders on /r/\nMachineLearning and Quora; expectations of contribu-\ntions to open source software, to name a few. Proposed\ncoercive pressures, such as professional bodies, charters\nor certification for data scientists might also play a role\nhere in the future.\nIdentifying and creating databases of `good' or `best'\npractices is a common but also a problematic policy\napproach to complex socio-technical challenges. This\napproach can mislead, as practices are usually assumed\nto lead to good outcomes rather than being treated as\nhypotheses subject to serious monitoring and evalu-\nation. Even where evidence suggests good practices\nwork in one context, they may fail elsewhere\n(Cartwright and Hardie, 2012). Instead of prescribing\n`good practice', a database of experiences would serve a\nmore exploratory function. Several organisations are\nwell positioned to start or collaborate on such initia-\ntives: private think-tanks such as Data and Society in\nthe United States, proposed bodies such as the national\ndata stewardship body described in a recent report by\nthe Royal Society and the British Academy (2017), or\none of many interdisciplinary collaborative melding\ncomputer science and social science in universities\nacross the world. It might also connect individuals\nfacing similar challenges across the globe, creating cre-\native, discussion-enabling support networks that help\nlike-minded individuals share advice, strategies and\neven code to tackle the trickiest challenges together.\nExploratory fairness analysis (EFA)\nThe situations above assume that information on pro-\ntected characteristics are either possible to obtain, or\navailable in parallel cases. Yet there may be situations\n10 Big Data & Society\nwhere such data is restrictively difficult to obtain at all.\nAmbient computing, for example, judges people based\non rather disembodied and abstracted features that\nenvironmental sensors can pick up, rather than through\na data-entry method. Yet these systems might also exhi-\nbit fairness concerns; fairness concerns which might be\nparticularly tricky to deal with.\nThese situations, where the protected data are not\nknown, pose a difficult challenge for computational\nfairness tools. Yet we propose that there are concrete\nmethods for these issues that while imperfect, could\nprove useful practices to both explore and develop in\nthe future.\nBuilding ex ante unfairness hypotheses with unsupervised\nlearning methods. Before building the model, data can\nbe examined for patterns that might lead to bias.\nExploratory data analysis is a core part of data ana-\nlysis, but teaching, research and practice into it has\nbeen historically marginalised (Behrens, 1997; Tukey,\n1980). Results of previous research, such as DCUBE-\nGUI or D-Explorer, have shown how visual tools\nmight help with the understanding of potentially dis-\ncriminatory patterns in datasets (Gao, 2015; Gao and\nBerendt, 2011), even for novice users (Berendt and\nPreibusch, 2014). Still, as with other methods, these\ntools broadly come with the assumption that the sensi-\ntive characteristics are available in the dataset, which\nwe have argued is often unrealistic.\nIf we assume that immediately sensitive data are\nunavailable, simply understanding the correlations in\nthe dataset is of less use. Instead, the exploratory chal-\nlenge can be seen primarily an unsupervised learning\nproblem. Unsupervised learning attempts to draw out\nand formalise hidden structure in datasets. Through\nunsupervised learning, we can hope to build an idea\nof the structure of correlations within data. As we do\nnot have the sensitive characteristics, confirmatory ana-\nlysis is difficult. This does not mean there is nothing to\nbe done. Exploratory data analysis has much to con-\ntribute in the building of hypothesis and the directing of\nfuture data and evidence collection as part of a broader\nprocess of due diligence.\nA relevant subset of unsupervised learning methods\nwe zoom in on here attempt to understand dataset\nstructure through estimating latent variables that\nappear to be present. Some methods, such as principal\ncomponent analysis (PCA), try to create a lower dimen-\nsional version of the data that captures as much vari-\nance as possible with a smaller number of variables.\nSome social science methods such as Q-methodology\n(McKeown and Thomas, 2013) use this approach to\ntry and pick up latent dimensions such as subjective\nviewpoints. Other methods, such as Gaussian mixture\nmodels, assume that datasets are generated from\nseveral different Gaussian distributions, and attempt\nto locate and model these clusters.\nThese forms of analysis can be used to build hypoth-\neses about fairness in datasets. For example, upon clus-\ntering or identifying subgroups within a dataset (which\nmay or may not be related to any protected character-\nistics), these groups can be qualitatively examined,\ndescribed and characterised. Experimental and sam-\npling techniques might be used to gain more contextual\ninformation about the individuals in these clusters \u00ad for\nexample, if their sensed or captured behaviour correl-\nates with any sociodemographic attributes. These clus-\nters can be used before or during the model building\nprocess to understand performance on different sub-\ngroups present in the data.\nBuilding ex post unfairness hypotheses with interpretable\nmodels. A second approach to in-data analysis without\naccess to protected characteristics examines trained\nmodels, rather than the input data alone. Once\nmodels have been trained, even complex models, there\nare several methods that are available for trying to\nunderstand their core logics in human-interpretable\nways.\nThe literature on understanding models such as\nneural networks has traditionally distinguished between\ndecompositional interpretation and pedagogical9 inter-\nDecompositional approaches focus on how to represent\npatterns in data in a way that both optimises predictive\nperformance whilst the internal logics remain semantic-\nally understandable to designers. Proponents of peda-\ngogical systems on the other hand noted that not only\nwas it difficult to get a semantically interpretable logic\nfrom models such as neural networks, although some\ntry (Jin et al., 2006). The tactic they have adopted,\nwhich is broadly the domain of most current research\nin interpreting complex systems, is to see the interpret-\nation as a separate optimisation problem to be\nconsidered.\nThe concept of pedagogically interpretable models is\nrelatively simple to explain. The basic idea is to wrap a\ncomplex model with a simpler one, which through\nquerying the more complex model like an oracle, can\nestimate its core logics. Candidates include logistic\nregression or decision trees. Increasingly, proposals\nfor the analysis of more complex models acknowledge\nthat the gap between the logics that can be represented\nby the simpler model and the logics latent in a more\ncomplex model are too vast to translate appropriately.\nImage recognition is a case in point. Instead, proposals\nin this area have tried to estimate the logics that locally\nsurround a given input vector \u00ad such as an image \u00ad to\nunderstand why it was classified as it was (Ribeiro\nExploratory fairness analysts might manually exam-\nine mechanisms behind a model's core logics and ask if\nthey made sense. Specifically, analysts might wish to\nconsider whether they would be happy publishing such\ninformation behind a model, or whether the public\nmight take issue with the way and reasons behind deci-\nsions being made as they were. Some recent research that\nhas highlighted gender bias in word embedding systems,\nwhich place words in relation to each other in high\ndimensional spaces to attempt to map different dimen-\nsions of their meaning, has gathered attention: and the\nmethods of bias identification in this area are related to\nwhat we discuss here (Bolukbasi et al., 2017; Caliskan\net al.. 2017). Future research should tangibly explore\nwhether meaningful and relevant information about\ndatasets or models known to be somehow biased can\nbe discerned through this type of analysis.\nDiscussions and directions\nThree approaches, three purposes\nThe three distinct approaches we have outlined in this\npaper point to three possible avenues for exploration in\nthe research and practice of fairer machine learning.\nEach of them is suited for different purposes.\nThe third-party approach, where another organisa-\ntion holds sensitive characteristics that they use to\ndetect and potentially mitigate discrimination from\ndata and models, is primarily useful where trust in the\norganisation interested in model building is low, or\npotential reputational risk is high. Insurance or hiring\nseem like prime cases here, particularly as they are areas\nhistorically associated with bias over protected vari-\nables. A challenge with this approach is that it is not\neasy to set up in low-resourced situations, or\nunilaterally.\nThe collaborative knowledge base approach, where\nlinked databases featuring fairness issues noted and\nexperienced by global researchers and practitioners,\ncould be useful in a broad array of situations. It\nmight provide benefit where general uncertainty is\nacute, risk assessment must be undertaken pre-\nemptively, or risks are complex, changing and socio-\ntechnical. Yet this requires a change of mindset.\nOrganisations involved in modelling should overcome\na reluctance to openly discuss their models, and will\nneed to dedicate time and money to give to as well as\ntake from such a shared resource. Anonymous contri-\nbutions could work as a model, but issues of who veri-\nfies provenance of the information given, and how\neasily it is to re-identify organisations based on model-\nling purpose would abound.\nThe exploratory approach requires the least organ-\nisational set-up, as it can be undertaken unilaterally\non data where sensitive characteristics are not held.\nYet while this approach enables the construction of\nquestions and the probing of certain types of anomal-\nous or potentially problematic patterns in the data, on\nits own it provides by far the least assurance that fair-\nness issues have been comprehensively identified,\nassessed and mitigated. Further work should seek to\nformalise methods of exploring data for these kinds\nof patterns, and test modellers and processes for their\nefficacy in identifying a range of synthetically induced\nissues.\nThere are, unsurprisingly, limits to the effectiveness\nof technological or managerial fixes to contested con-\ncepts such as fairness. Unsupervised learning is particu-\nlarly challenging to evaluate fairness upon, given that\ngroups discovered are latent, although there has been\nsome recent work beginning to explore this space\n(Chierichetti et al., 2017). Understanding fairness by\ndemographic will also be hard to grasp when those\ndemographics are latent \u00ad such as treating individuals\nholding particular political views similarly in regards to\nmoderating content online (Binns et al., 2017). More\nimportantly, even though the three approaches we out-\nline deal with different levels of formality and different\nways of understanding or conceiving fairness, they all\nremain broadly centred on the software artefacts them-\nselves. We do not suggest that either these approaches\nor the broad mindsets that underpin them are sufficient\nfor understanding equity or mitigating discrimination\nin a digital age. We do, however, tentatively suggest\nthat where these software artefacts are used to make\nand support decisions, tackling technical aspects of\nthese issues is likely a necessary piece of the puzzle \u00ad\nneither more nor less important than others, such as\norganisational culture, social methods of oversight, or\ndecisions about the intention or direction of deploy-\nment. We also would draw attention to larger chal-\nlenges with predictive systems: that they might not\nachieve social or policy goals at all by their nature\n(Harcourt, 2006), or that fairness might not be the\nmost relevant issue as much as ideas of stigmatisation,\nover-surveillance, or the devaluing of particular cul-\ntural notions, such as family units (Blank et al.,\n2015). Where there are inherent conflicting interests\nbetween organisations deploying such systems and\nthose affected by them, co-operation may not be feas-\nible or desirable; affected groups may instead be drawn\n(understandably) to more adversarial forms of resist-\nance and political action (Brunton and Nissenbaum,\nDirections for empirical research\nThese three proposals illustrate how alternative institu-\ntional set-ups and ways of knowing might help in the\n12 Big Data & Society\ngovernance of fairness in the context of machine learn-\ning. It focuses on one identified practical constraint \u00ad\nthe absence of sensitive data. Each approach introduces\nlimitations, caveats and provides few guarantees of per-\nformance. This might irritate researchers in this space,\nyet it reflects the messy reality of many contemporary\non-the-ground situations.\nWe believe there are opportunities amidst the con-\nstraints. The practical limitations of fairness-improving\napproaches, including these three, will only become\napparent upon their introduction and reflexive study\nwithin real-world settings. In particular, our second\nand third suggestions, concerning knowledge bases\nand exploratory analyses, are not amenable to the\nsort of mathematical guarantees that the DADM litera-\ntures may find comforting. In these situation, process\nevaluation is much more important than outcome evalu-\nation. Understanding the questions and challenges that\nthese methods do (or do not) address during the real\nbuilding, deployment and management of predictive\nsystems is key here. Only a small amount of work has\nbeen done in this space (see Veale, 2017 for one exam-\nple), and we argue strongly that this should increase.\nAs we have noted, it is often unrealistic to assume\nmathematically sound `debiasing' on-the-ground is pos-\nsible, and this means it often unhelpful to apply the\nvalidity conditions of traditional research in statistics\nand computer science to discrimination-aware machine\nlearning. New technologies of this type should be at\nleast partially assessed on the extent of new capabilities\nfor responsible practices they afford practitioners \u00ad a\ndifficult, transdisciplinary and heavily value-laden task,\nbut a very necessary one.\nWithout this dimension, designed tools are likely to\nstumble in surprising and even mundane ways, which\nwill affect their ability to deal with unfairness and dis-\ncrimination in the wild. It seems unlikely that statistical\nguarantees of fairness will translate smoothly to indi-\nviduals feeling that decisions about them were made\nfairly \u00ad something as much a result of process as of\noutcome. Researchers working in this space should\ntrial their proposed solutions, monitoring their imple-\nmentation using rich and rigorous qualitative methods\nsuch as ethnography and action research, and feed\nfindings from this back into tool revision and rethink-\ning. To adequately address fairness in the context of\nmachine learning, researchers and practitioners work-\ning towards `fairer' machine learning need to recognise\nthat this is not just an abstract constrained optimisation\nproblem. It is a messy, contextually-embedded and\nnecessarily sociotechnical problem, and needs to be\ntreated as such. This requires technical scholars to\nbetter grasp the social challenges and contexts; but\nalso for social scholars to grapple more rigorously\nwith the technical proposals placed on the table,\nand to ensure that critiques with operational implica-\ntions reach the ears of the computing community.\nDeclaration of conflicting interests\nThe author(s) declared no potential conflicts of interest with\nrespect to the research, authorship, and/or publication of this\narticle.\nFunding\nThe author(s) disclosed receipt of the following financial sup-\nport for the research, authorship, and/or publication of this\narticle: The UK Engineering and Physical Science Research\nCouncil (EPSRC) provided support to both Michael Veale\n(SOCIAM: The Theory and Practice of Social Machines,\nNotes\n1. Some of these measures have obvious shortcomings. In\nparticular, disparate impact has been criticised because it\nfails to account for discrimination which is explainable in\nterms of legitimate grounds (Dwork et al., 2012). For\ninstance, attempting to enforce equal impact between\nmen and women in recidivism prediction systems, if\nmen have higher reoffending rates, could result in\nwomen remaining in prison longer despite being less\nlikely to reoffend.\n2. Some have argued that a principle of data minmumisation\nwould enable better governance of these issues, rather\nthan mimalisation (van der Sloot, 2012). In some ways,\nit could be argued that the existing regulation could be\nalready read through such a lens, but text interpretation\nis not the focus of this paper.\n3. For consideration of this question, see van der Sloot\n liobaite\n4. See the Web Cryptography API recommendation from\nW3C https://www.w3.org/TR/WebCryptoAPI/\n5. A range of privacy preserving communication solutions\ncould be applicable here, yet we do not seek to treat the\nmodelling organisation as a malicious adversary. The\nmethods here implicitly focus on organisations actively\nwishing to increase trust and reduce discriminatory\noutcomes.\n6. Many researchers (e.g. Ossia et al., 2017) are currently\nexploring how to bring parts of model training activities\naway from cloud servers and onto a user's own device for\nthe purposes of increasing privacy, utilising mathematical\ntools such as homomorphic encryption or zero-knowl-\nedge proofs, and integration of fairness into these more\ndecentralised systems will likely be a research area for\nfuture exploration.\n7. Some relevant consultancies already offer services in this\nspace, such as Trilateral Research (http://trilateralre-\nsearch.com/services/impact-assessment/) or ORCAA\n(http://www.oneilrisk.com/).\n8. Cross Validated is the statistical question\u00adanswer site on\nthe StackExchange network, http://stats.stackexchange.\ncom/\n9. Pedagogical interpretation has recently been described as\n`model-agnostic' interpretation (Ribeiro et al., 2016a).\n10. This method is not exclusive to pedagogical methods, and\nsome recent work has shown how decompositional meth-\nods, which use components of model structure rather\nthan just treating it like a black box, also display strong\npromise in this space (Montavon et al., 2017).\nReferences\nAgrawal R and Srikant R (2000) Privacy-preserving data\nSIGMOD International Conference on Management of\nAndrews R, Diederich J and Tickle AB (1995) Survey and\ncritique of techniques for extracting rules from trained\nartificial neural networks. Knowledge-Based Systems 8(6):\nAngwin J, Larson J, Mattu S, et al. (2016) Machine bias:\nThere's software used across the country to predict\nfuture criminals, and it's biased against blacks.\nAzavea (2015) HunchLab: Under the Hood. Philadelphia, PA:\nAuthor. Available at: http://cdn.azavea.com/pdfs/hunchlab/\nHunchLab-Under-the-Hood.pdf.\nBarocas S and Selbst AD (2016) Big Data's disparate impact.\nBehrens JT (1997) Principles and procedures of exploratory\nBerendt B and Preibusch S (2014) Better decision support\nthrough exploratory discrimination-aware data mining:\nFoundations and empirical evidence. Artificial\nBerk R, Heidari H, Jabbari S, et al. (2017) Fairness in\ncriminal justice risk assessments: The state of the art.\narXiv [stat.ML]. Available at: http://arxiv.org/abs/\nBinns R (2017) Data protection impact assessments: A meta-\nregulatory approach. International Data Privacy Law 7(1):\nBinns R, Veale M, Van Kleek M, et al. (2017) Like trainer,\nlike bot? Inheritance of bias in algorithmic content mod-\neration. In: Ciampaglia G, et al. (eds) Social informatics:\n9th international conference, SocInfo 2017, Oxford, UK,\n13\u00ad15 September, Proceedings, Part II. Cham: Springer,\nBizer C, Heath T and Berners-Lee T (2009) Linked data \u00ad The\nstory so far. In: Sheth A (ed.) Semantic Services,\nInteroperability and Web Applications: Emerging\nBlank A, et al. (2015) Ethical Issues for Maori in Predictive\nRisk Modelling to Identify New-Born Children who are at\nHigh Risk of Future Maltreatment. Wellington, NZ:\nMinistry of Social Development/Te Manatu Whakahiato\nOra. Available at: https://www.msd.govt.nz/documents/\nabout-msd-and-our-work/publications-resources/\nresearch/predictive-modelling/00-ethical-issues-for-maori-\nin-predictive-risk-modelling.pdf (accessed 16 February\nBolukbasi T, Chang K-W, Zou J, et al. (2017) Man is to\ncomputer programmer as woman is to homemaker?\nDebiasing word embeddings. In: Proceedings of the 30th\nAnnual Conference on Neural Information Processing\nSpain.\nBowker G and Star SL (1999) Sorting Things Out:\nClassification and Its Consequences. Cambridge, MA:\nThe MIT Press.\nboyd d and Crawford K (2012) Critical questions for big data:\nProvocations for a cultural, technological, and scholarly\nphenomenon. Information, Communication and Society\nBrunton F and Nissenbaum H (2015) Obfuscation: A User's\nGuide for Privacy and Protest. Cambridge: MIT Press.\nBurrell J (2016) How the machine ``thinks'': Understanding\nopacity in machine learning algorithms. Big Data &\nCaliskan A, Bryson JJ and Narayanan A (2017) Semantics\nderived automatically from language corpora contain\nCartwright N and Hardie J (2012) Evidence-based Policy: A\nPractical Guide to doing it Better. Oxford: Oxford\nUniversity Press.\nChierichetti F, Kumar R, Lattanzi S, et al. (2017) Fair clus-\ntering through fairlets. Presented as a talk at the 4th\nWorkshop on Fairness, Accountability, Transparency in\nMachine Learning (FAT/ML 2017), Halifax, Nova Scotia,\nCanada. Available at: http://www.fatml.org/media/docu-\nments/fair_clustering_through_fairlets.pdf (accessed 30\nChouldechova A (2017) Fair prediction with disparate\nimpact: A study of bias in recidivism prediction instru-\nments. Presented as a poster at FAT/ML 2016, New\nCusters B, Calders T, Schermer B, et al. (eds) (2013)\nDiscrimination and Privacy in the Information Society.\nDanaher J (2016) The threat of algocracy: Reality, resistance\nand accommodation. Philosophy and Technology 29(3):\nDieterich W, Mendoza C and Brennan T (2016) COMPAS\nrisk scales: Demonstrating accuracy equity and predictive\nparity. Technical report, Northpointe, July 2016.\nAvailable at: http://www.northpointeinc.com/north-\nDiMaggio PJ and Powell WW (1983) The iron cage revisited:\nInstitutional isomorphism and collective rationality in\norganizational fields. American Sociological Review 48(2):\nDwork C, Hardt M, Pitassi T, et al. (2012) Fairness through\nawareness. In: Proceedings of the 3rd Innovations in\nTheoretical Computer Science Conference, ITCS `12,\n14 Big Data & Society\nEdwards L and Veale M (2017) Slave to the algorithm? Why a\n`right to an explanation' is probably not the remedy you\nare looking for. Duke Law and Technology Review 15(2),\nFeldman M, Friedler SA, Moeller J, et al. (2015) Certifying\nand removing disparate impact. In: Proceedings of the 21th\nACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining (KDD '15). Sydney, NSW,\nAustralia, 10\u00ad13 August. Available at: http://dx.doi.org/\nGama J, Z\n liobaite\n_ I, Bifet A, et al. (2013) A survey on concept\ndrift adaptation. ACM Computing Surveys 1(1): 1\u00ad35.\nGao B (2015) Exploratory visualization design towards online\nsocial network privacy and data literacy. PhD Thesis, KU\nLeuven. Available at: https://lirias.kuleuven.be/bitstream/\nGao B and Berendt B (2011) Visual data mining for higher-\nlevel patterns: Discrimination-aware data mining and\nbeyond. In: Benelearn 2011. Proceedings of the Twentieth\nBelgian Dutch Conference on Machine Learning (Benelearn\nGellert R, de Vries K, de Hert P, et al. (2013) A comparative\nanalysis of anti-discrimination and data protection legisla-\ntions. In: Custers B, Calders T, Schermer B, et al. (eds)\nDiscrimination and Privacy in the Information Society.\nBerlin: Springer, pp. 61\u00ad89. Available at: http://\nGoldweber M, Barr J, Clear T, et al. (2013) A framework for\nenhancing the social good in computing education: A\nGoldweber M, Davoli R, Little JC, et al. (2011) Enhancing\nthe social issues components in our computing curriculum:\nHacking I (1995) The looping effects of human kinds.\nIn: Premack D, Sperber D and Premack AJ (eds)\nCausal Cognition: A Multidisciplinary Debate. Oxford:\nHajian S (2013) Simultaneous discrimination prevention and\nprivacy protection in data publishing and mining. PhD\nThesis, Universitat Rovira I Virgili. Available at: https://\nHajian S and Domingo-Ferrer J (2012) A study on the impact\nof data anonymization on anti-discrimination. In: IEEE\n12th International Conference on Data Mining Workshops\nHajian S and Domingo-Ferrer J (2013) A methodology for\ndirect and indirect discrimination prevention in data\nmining. IEEE Transactions on Knowledge and Data\nHajian S, Domingo-Ferrer J and Farra\nGeneralization-based privacy preservation and discrimin-\nation prevention in data publishing and mining. Data\nHarcourt BE (2006) Against Prediction: Profiling, Policing,\nand Punishing in an Actuarial Age. Chicago, IL:\nUniversity of Chicago Press.\nHardt M, Price E and Srebro N (2016) Equality of opportun-\nity in supervised learning. In: Lee DD, Sugiyama M,\nLuxburg UV, et al. (eds) Advances in Neural Information\nProcessing Systems 29, Red Hook, NY: Curran\npapers.nips.cc/paper/6374-equality-of-opportunity-in-\nHellman D (2008) When is Discrimination Wrong?\nCambridge, MA: Harvard University Press.\nHildebrandt M (2015) Smart Technologies and the End(s) of\nLaw. Cheltenham: Edward Elgar.\nHildebrandt M and Gutwirth S (eds) (2008) Profiling the\nEuropean Citizen: Cross-Disciplinary Perspectives.\nDordrecht: Springer.\nHuang L, Joseph AD, Nelson B, et al. (2011) Adversarial\nmachine learning. AISec'11. Available at: http://\nHundepool A, Domingo-Ferrer J, Franconi L, et al. (2012)\nStatistical Disclosure Control. London: John Wiley &\nSons.\nJapkowicz N and Shah M (2011) Evaluating Learning\nAlgorithms: A Classification Perspective. Cambridge:\nCambridge University Press.\nJin Y, Sendhoff B and Ko\nation of accurate and interpretable neural network classi-\nfiers. In: Jin Y (ed.) Multi-objective Machine Learning,\nKamiran F, Calders T and Pechenizkiy M (2012) Techniques\nfor discrimination-free predictive models. In: Custers B,\nCalders T, Schermer B, et al. (eds) Discrimination and\nPrivacy in the Information Society. Berlin: Springer,\nKeats Citron D and Pasquale F (2014) The scored society:\nDue process for automated predictions. Washington Law\nKhwaja MS, Awasthi R and Loeprick J (2011) Risk-Based\nTax Audits: Approaches and Country Experiences. New\nYork, NY: World Bank.\nKitchin R and Dodge M (2011) Code/space: Software and\nEveryday Life. London: MIT Press.\nKleinberg J, Mullainathan S and Raghavan M (2016)\nInherent trade-offs in the fair determination of risk\nscores. arXiv [cs.LG]. Available at: http://arxiv.org/abs/\nKroll JA, Huey J, Barocas S, et al. (2016) Accountable algo-\nrithms. University of Pennsylvania Law Review 165:\nKusner MJ, Loftus JR, Russell C, et al. (2017) Counterfactual\nfairness. arXiv [stat.ML]. Available at: http://arxiv.org/\nLatour B (1999) Pandora's Hope: Essays on the Reality of\nScience Studies. Cambridge, MA: Harvard University\nPress.\nLoukides G and Shao J (2008) Data utility and privacy pro-\ntection trade-off in k-anonymisation. In: Proceedings of\nthe 2008 International Workshop on Privacy and\nAnonymity in the Information Society (PAIS), March 25,\npp. 36\u00ad45. Nantes, France: ACM, Available at: http://\nLyon D (2007) Resisting Surveillance. The Surveillance Studies\nMcDaniel P, Papernot N and Berkay Celik Z (2016) Machine\nlearning in adversarial settings. IEEE Security & Privacy\nMcKeown B and Thomas DB (2013) Q Methodology.\nMantelero A (2016) Personal data for decisional purposes in\nthe age of analytics: From an individual to a collective\ndimension of data protection. Computer Law & Security\nMissier P, Luda\n\u00a8 scher B, Bowers S, et al. (2010) Linking mul-\ntiple workflow provenance traces for interoperable collab-\norative science. In: 5th Workshop on Workflows in Support\nNew Orleans, LA, USA. pp. 1\u00ad8. Available at: http://\nMontavon G, Lapuschkin S, Binder A, et al. (2017)\nExplaining nonlinear classification decisions with deep\nNissenbaum H (1996) Accountability in a computerized soci-\nOlson M (1971) The Logic of Collective Action: Public Goods\nand the Theory of Groups. Cambridge, MA: Harvard\nUniversity Press.\nOssia SA, Shamsabadi AS, Taheri A, et al. (2017) A hybrid\ndeep learning architecture for privacy-preserving mobile\nanalytics. arXiv. Available at: https://arxiv.org/abs/\nPasquale F (2010) Beyond innovation and competition: The\nneed for qualified transparency in internet intermediaries.\nPedreshi D, Ruggieri S and Turini F (2008) Discrimination-\naware data mining. KDD `08. Las Vegas, NV: ACM,\nPellissier Tanon T, Vrandec\n ic\nFrom freebase to Wikidata: The great migration. In:\nProceedings of the 25th International Conference on\nPerry WL, McInnis B, Price CC, et al. (2013) Predictive\nPolicing: The Role of Crime Forecasting in Law\nEnforcement Operations. Washington, DC: RAND\nCorporation. Available at: http://www.rand.org/content/\nPinsker J (2015) What your choice of browser says about you\nas an employee. The Atlantic. Available at: http://\nQuin\n~ onero-Candela J, Sugiyama M, Schwaighofer A, et al.\n(2009) Dataset Shift in Machine Learning. Cambridge,\nMA: The MIT Press.\nRibeiro MT, Singh S and Guestrin C (2016a) Model-agnostic\ninterpretability of machine learning. Presented at 2016\nICML Workshop on Human Interpretability in Machine\nLearning (WHI 2016). New York, NY. Available at:\nRibeiro MT, Singh S and Guestrin C (2016b) ``Why should I\ntrust you?'': Explaining the predictions of any classifier.\nIn: Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining\nRouvroy A (2011) Technology, virtuality and utopia.\nIn: Hildebrandt M and Rouvroy A (eds) Law, Human\nAgency and Autonomic Computing. London: Routledge,\nSandvig C (2015) Seeing the sort: The aesthetic and industrial\nSandvig C, Hamilton K, Karahalios K, et al. (2014) Auditing\nalgorithms: Research methods for detecting discrimination\non internet platforms. Paper presented to ``Data and\nDiscrimination: Converting Critical Concerns into\nProductive Inquiry,'' a preconference at the 64th Annual\nMeeting of the International Communication Association.\nSharma A and Kumar Panigrahi P (2012) A review of finan-\ncial accounting fraud detection based on data mining tech-\nSimperl E and Luczak-Ro\nogy engineering: A survey. Knowledge Engineering Review\nSkitka LJ, Mosier KL and Burdick M (1999) Does automa-\ntion bias decision-making? International Journal of\nSpradling C, Soh L-K and Ansorge C (2008) Ethics training and\ndecision-making: Do computer science programs need help?\nIn: Proceedings of the 39th SIGCSE Technical Symposium on\nComputer Science Education, SIGCSE 2008, Portland, OR,\nThe Royal Society and the British Academy (2017) Data\nmanagement and use: Governance in the 21st century.\nAvailable at: http://royalsociety.org/topics-policy/pro-\nTickle AB, Andrews R, Golea M, et al. (1998) The truth will\ncome to light: Directions and challenges in extracting the\nknowledge embedded within trained artificial neural net-\nworks. IEEE Transactions on Neural Networks 9(6):\nTrame\n` r F, Zhang F, Juels A, et al. (2016) Stealing machine\nlearning models via prediction APIs. Available at: https://\nwww.usenix.org/sites/default/files/conference/protected-\nfiles/security16_slides_tramer.pdf (accessed 16 February\nTukey JW (1980) We need both exploratory and confirma-\nTutt A (2016) An FDA for algorithms. Available at: https://\n16 Big Data & Society\nVaithianathan R, Maloney T, Putnam-Hornstein E, et al.\n(2013) Children in the public benefit system at risk of\nmaltreatment: Identification via predictive modeling.\nvan der Sloot B (2012) From data minimization to data mini-\nmummization. In: Custers B, Calders T, Schermer B, et al.\n(eds) Discrimination and Privacy in the Information\nVeale M (2017) Logics and practices of transparency and\nopacity in real-world applications of public sector machine\nlearning. Presented as a talk at the 4th Workshop on\nFairness, Accountability, Transparency in Machine\nLearning (FAT/ML 2017), Halifax, Nova Scotia.\nVedder A (1999) KDD: The challenge to individualism.\nVerwer S and Calders T (2013) Introducing positive discrim-\nination in predictive models. In: Custers B, Calders T,\nSchermer B, et al. (eds) Discrimination and Privacy in the\nVrandec\n ic\n\u00b4 D and Kro\n\u00a8 tzsch M (2014) Wikidata: A free collab-\norative knowledgebase. Communications of the ACM\nWetenschappelijke Raad voor het Regeringsbeleid (WRR)\n[Dutch Scientific Council for Government Policy] (2016)\nBig Data in een vrije en veilige samenleving [Big Data in a\nfree and safe society]. Den Haag: WRR. Available at:\nhttp://www.wrr.nl/publicaties/publicatie/article/big-data-\nin-een-vrije-en-veilige-samenleving/ (accessed 16 February\nWillenborg L and de Waal T (2012) Elements of Statistical\nDisclosure Control. Berlin: Springer.\nWright D and de Hert P (2012) Privacy Impact Assessment.\nDordrecht: SpringerAvailable at: http://dx.doi.org/\nZafar MB, Valera I, Rodriguez MG, et al. (2016) Fairness\nbeyond disparate treatment & disparate impact: Learning\nclassification without disparate mistreatment. arXiv\nZ\n liobaite\n_ I and Custers B (2016) Using sensitive personal data\nmay be necessary for avoiding discrimination in data-\ndriven decision models. Artificial Intelligence and Law\nZ\n liobaite\n_ I, Kamiran F and Calders T (2011) Handling con-\nditional discrimination. In: Proceedings of the 2011 IEEE\n11th International Conference on Data Mining Workshops\nZ\n liobaite\n_ I, Pechenizkiy M and Gama J (2016) An overview of\nconcept drift applications. In: Japkowicz N and\nStefanowski J (eds) Big Data Analysis: New Algorithms\nfor a New Society, Studies in Big Data. Berlin: Springer,"
}