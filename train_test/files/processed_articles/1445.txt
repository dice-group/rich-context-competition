{
    "abstract": "Abstract\nBackground: We present a potentially useful alternative approach based on support vector machine (SVM)\ntechniques to classify persons with and without common diseases. We illustrate the method to detect persons\nwith diabetes and pre-diabetes in a cross-sectional representative sample of the U.S. population.\nMethods: We used data from the 1999-2004 National Health and Nutrition Examination Survey (NHANES) to\ndevelop and validate SVM models for two classification schemes: Classification Scheme I (diagnosed or\nundiagnosed diabetes vs. pre-diabetes or no diabetes) and Classification Scheme II (undiagnosed diabetes or pre-\ndiabetes vs. no diabetes). The SVM models were used to select sets of variables that would yield the best\nclassification of individuals into these diabetes categories.\nResults: For Classification Scheme I, the set of diabetes-related variables with the best classification performance\nincluded family history, age, race and ethnicity, weight, height, waist circumference, body mass index (BMI), and\nhypertension. For Classification Scheme II, two additional variables\u00adsex and physical activity\u00adwere included. The\ndiscriminative abilities of the SVM models for Classification Schemes I and II, according to the area under the\nreceiver operating characteristic (ROC) curve, were 83.5% and 73.2%, respectively. The web-based tool-Diabetes\nClassifier was developed to demonstrate a user-friendly application that allows for individual or group assessment\nwith a configurable, user-defined threshold.\nConclusions: Support vector machine modeling is a promising classification approach for detecting persons with\ncommon diseases such as diabetes and pre-diabetes in the population. This approach should be further explored\nin other complex diseases using common variables.\nBackground\nA supervised machine learning method, the support vec-\ntor machine (SVM) algorithm [1], has demonstrated\nhigh performance in solving classification problems in\nmany biomedical fields, especially in bioinformatics\n[2,3]. In contrast to logistic regression, which depends\non a pre-determined model to predict the occurrence or\nnot of a binary event by fitting data to a logistic curve,\nSVM discriminates between two classes by generating a\nhyperplane that optimally separates classes after the\ninput data have been transformed mathematically into a\nhigh-dimensional space. Because the SVM approach is\ndata-driven and model-free, it may have important dis-\ncriminative power for classification, especially in cases\nwhere sample sizes are small and a large number of\nvariables are involved (high-dimensionality space). This\ntechnique has recently been used to develop automated\nclassification of diseases and to improve methods for\ndetecting disease in the clinical setting [4,5].\nTo test the potential power of SVM as an approach\nfor classifying individuals into groups defined by disease\nstatus, we chose diabetes as an example. In the U.S., dia-\nbetes affects an estimated 23.6 million people, of whom\nabout one third are unaware that they have the disease\n[6]. Another 57 million people have pre-diabetes, with\nelevated blood glucose levels that increase their risk of\ndeveloping diabetes, heart disease, and stroke. Recent\nstudies indicate that diabetes can be prevented by\n* Correspondence: wyu@cdc.gov\nNational Office of Public Health Genomics, Coordinating Center for Health\nPromotion, Centers for Disease Control and Prevention, Atlanta, GA, USA\n\u00a9 2010 Yu et al; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in\nany medium, provided the original work is properly cited.\nlifestyle changes or pharmacotherapy among persons\nwith pre-diabetes [7-9]. Early screening and diagnosis is\nthus central to effective prevention strategies [10]. To\nthis end, numerous risk scores and prediction equations\nhave been developed to identify people at high risk of\ndeveloping diabetes or with pre-diabetes based on com-\nmon risk factors such as body mass index (BMI) and\nfamily history of diabetes [11-13]. For example, a\nrecently published risk calculator uses logistic regression\nto identify people with pre-diabetes and undiagnosed\ndiabetes by employing combinations of common risk\nvariables [14]. Our objective was to generate an SVM-\nbased approach to distinguish people with either undiag-\nnosed diabetes or pre-diabetes from people without\neither of these conditions. The variables used to gener-\nate the SVM models were limited to simple clinical\nmeasurements that do not require laboratory tests. Pre-\ndictions from this approach were compared with the\npredictions from logistic regression models containing\nthe same set of variables. A final goal was to demon-\nstrate the applicability of the SVM approach by creating\na demonstration web-based classification tool.\nMethods\nData source\nNational Health and Nutrition Examination Survey\n(NHANES) to generate the SVM algorithm. NHANES is\nan ongoing, cross-sectional, probability sample survey of\nthe U.S. population. It collects demographic, health his-\ntory, and behavioral information from participants in\nhome interviews. Participants are also invited for\ndetailed physical, physiological, and laboratory examina-\ntions that are performed by trained personnel in spe-\ncially equipped mobile centers [15].\nWe limited our study to non-pregnant participants\naged 20 or older. Participants were considered to have\ndiagnosed diabetes if they answered \"yes\" to the ques-\ntion \"Have you ever been told by a doctor or health pro-\nfessionals that you have diabetes?\" Participants who\nanswered \"no\" to this question but who had a measured\nfasting plasma glucose  126 mg/dl were considered to\nhave undiagnosed diabetes; those with a fasting plasma\nbetes. Participants with fasting glucose <100 mg/dl were\nconsidered to not have diabetes (Table 1).\nWe devised two different classification schemes\n(Table 1). In Classification Scheme I, the group of\npersons with diabetes (diagnosed or undiagnosed) was\ndistinguished from those without diabetes, including\npersons with pre-diabetes. In Classification Scheme II,\nthe group of persons with either undiagnosed diabetes\nor pre-diabetes was distinguished from those without\ndiabetes. The models were developed using a sample of\n80% of the individuals in each group and validated in\nVariable selection\nWe selected 14 simple variables commonly associated\nwith the risk for diabetes: family history, age, gender,\nrace and ethnicity, weight, height, waist circumference,\nBMI, hypertension, physical activity, smoking, alcohol\nuse, education, and household income. Variable selec-\ntion was performed according to an automatic approach\ndeveloped by Chen et al. [16]. The significance of the\nautomatically selected set of variables was further manu-\nally evaluated by fine tuning parameters. The variables\nincluded in the final selection were those with the best\ndiscriminative performance.\nModel generation\nSupport Vector Machine (SVM) is a supervised machine\nlearning technique that is widely used in pattern recogni-\ntion and classification problems. The SVM algorithm per-\nforms a classification by constructing a multidimensional\nhyperplane that optimally discriminates between two\nclasses by maximizing the margin between two data clus-\nters. This algorithm achieves high discriminative power by\nusing special nonlinear functions called kernels to trans-\nform the input space into a multidimensional space [17].\nThe basic idea behind the SVM technique is to con-\nstruct an n-1 dimensional separating hyperplane to dis-\ncriminate two classes in an n-dimensional space. A data\npoint is viewed as an n-dimensional vector. For example,\ntwo variables in a dataset will create a two-dimensional\nspace; the separating hyperplane would be a straight line\n(one dimensional) dividing the space in half. When\nmore dimensions are involved, SVM searches for an\noptimal separating hyperplane called the maximum-\nmargin separating hyperplane. The distance between the\nhyperplane and the nearest data point on each side\n(called support vectors) is maximized. The best scenario\nis that two classes are separated by a linear hyperplane.\nHowever, real-world situations are not always that sim-\nple. Some data points in the two classes might fall into\na \"grey\" area that is not easy to be separated. SVM\nsolves this problem by 1) allowing some data points to\nthe wrong side of the hyperplane by introducing a user-\nspecified parameter C that specifies the trade-off\nbetween the minimization of the misclassifications and\nmaximization of margin; 2) using kernel functions\n(usually including linear, polynomial, sigmoid, and radial\nbasis functions (RBF)) to add more dimensions to the\nlow dimensional space, as a result that two classes could\nbe separable in the high dimensional space. Figure 1\nshows an example of an inseparable two-dimensional\nspace that becomes separable after the transformation of\nthe input space from low dimensional to multi\ndimensional. The SVM approach tends to classify enti-\nties without providing estimates of the probabilities of\nclass membership in the dataset, which is a fundamental\ndifference from multiple logistic regression.\nTwo key parameters for the kernels, C and gamma,\nneed to be pre-selected to generate an optimal SVM\nmodel. Parameter C controls over-fitting of the model\nby specifying tolerance for misclassification. Parameter\ngamma controls the degree of nonlinearity of the model.\nWe used LibSVM [18], a freely available SVM soft-\nware library, to generate the SVM models. To generate\nthe data set for model training, we randomly selected a\nnumber of non-cases to match the number of cases in\nthe training data set (see Table 1 for the definitions of\ncases and non-cases). According to the required data\nformat input, values of selected features were normal-\nized to values from - 1 to +1. Values of categorical vari-\nables such as Race are arbitrarily assigned to numbers\nrepresents non-Hispanic white, non-Hispanic black,\nMexican American, other, other Hispanic respectively.\nValues of continuous variables were transformed into\nvalues between -1 and +1 by dividing them by an appro-\npriate number. For example, the age values were divided\nby 100. In the training data set, the first column of the\ninput data was set to the known outcome, i.e., 1 for\npositive, - 1 for negative. A utility included in the\nLibSVM package (grid.py) was used to find the optimal\nparameters for penalty parameter C and gamma under\n5-fold cross-validation. Different kernel functions,\nincluding linear, polynomial, sigmoid, and radial basis\nfunctions (RBF), were tested and selected for the models\nbased on performance.\nMultiple logistic regression modeling (MLR) was per-\nformed using the same selected risk variables or features\nand case status (as specified previously and in Table 1)\nas the outcome variable. The logistic regression analysis\nwas performed with the training data set using\nTable 1 Description of the National Health and Nutrition Examination Survey data set used for the study\nDiagnostic\ncategory\nDefinition N Classification\nScheme I\nClassification\nScheme II\nDiagnosed\ndiabetes\nAnswered \"yes\" to question \"Have you ever been told by a doctor or health\nprofessionals that you had diabetes?\"\nanalysis\nUndiagnosed\ndiabetes\nAnswered \"no\" to question \"Have you ever been told by a doctor or health\nprofessionals that you had diabetes?\"\nAND\nFasting plasma glucose level  126 mg/dl\nNo diabetes Fasting plasma glucose level <100 mg/dl 3,277 Non-cases Non-cases\nNotes: Total number of the cases for classification scheme I = 1461\nTotal number of the non-cases for classification scheme I = 4853\nTotal number of the cases for classification scheme II = 1709\nTotal number of the non-cases for classification scheme II = 3206\nFigure 1 Demonstration of finding a separating hyperplane in high dimensional space vs in low dimensional space.\nSAS-callable SUDAAN version 9, a procedure specific\nfor complex survey design. Then, the estimated b coeffi-\ncients were applied to the test data set to calculate for\neach individual the probability of being a case.\nModel evaluations\nEvaluation in the test data sets\nTest data sets were used to assess the performance of\nthe models. Validation using the test data sets avoided\npotential bias of the performance estimate due to over-\nfitting of the model to training data sets. For the SVM\nmodel, the data files in the test data sets were formatted\naccording to the requirement that variable values be\nnormalized to values from - 1 to +1; the first column of\nthe input data set (indicating case status) was set to 0.\nPrediction program Java code from the LibSVM library\nwas modified to output the decision value (internal\nscore generated by SVM tool) for each member of the\ntest data set. For the logistic regression model, the pre-\ndiction value for each member of the test data set was\nestimated by using the logistic regression function gen-\nerated during the training step.\n10-fold cross-validation in the training data set\nTo evaluate the robustness of the estimates from the\nSVM models, a 10-fold cross-validation was performed\nin the training data set. The training data set was parti-\ntioned into 10 equal-size subsets. Each subset was used\nas a test data set for a model trained on all cases and an\nequal number of non-cases randomly selected from the\n9 remaining data subsets. This cross-validation process\nwas repeated 10 times, allowing each subset to serve\nonce as the test data set. To generate summary perfor-\nmance estimates, we averaged the area under the curve\n(AUC) of the receiver operating characteristic (ROC)\ncurve and other statistics (sensitivity, specificity, positive\npredictive value [PPV], negative predictive value [NPV])\nof the cross-validations.\nStatistics for performance evaluation\nROC curves were generated based on the predicted out-\ncome and true outcome. The AUCs for the test data\nsets were calculated and used to compare the discrimi-\nnative powers of the models. We used Delong's method\nto calculate P-values to compare the AUCs based on\nresults of the SVM models and MLR models [19].\nSensitivity, specificity, PPV, and NPV were calculated\nbased on the following formulas when the cutoff value\nwas set to default value (0) in the SVM model.\nSensitivity\nTP\n\n\nSpecificity\nTN\n\n\nPPV\nTP\n\n\nNPV\nTN\n\n\nwhere TP, FP, TN, and FN represent the number of\ntrue positives, false positives, true negatives, and false\nnegatives, respectively.\nDemonstration web-based classification tool\nimplementation\nWe implemented the SVM model as a web-based tool\nthat we called Diabetes Classifier. The application was\nbuilt by using J2EE technology [20] and other Java\nopen-source frameworks such as Hibernate [21] and\nStrut [22]. LibSVM open-source Java codes were modi-\nfied and embedded in the system source codes for pre-\ndiction. The lookup tables for cutoff values and\ncorresponding statistics (sensitivity, specificity) were\ngenerated from the calculations on each data point in\nthe test data sets. Diabetes Classifier is freely accessible\nvia http://www.hugenavigator.net/DiseaseClassification-\nPortal/startPageDiabetes.do\nResults and Discussion\nIn Classification Scheme I (diagnosed or undiagnosed\ndiabetes vs. no diabetes or pre-diabetes), 8 variables\u00ad\nfamily history, age, race and ethnicity, weight, height,\nwaist circumference, BMI, and hypertension\u00adyielded the\nbest performance. In Classification Scheme II (undiag-\nnosed diabetes or pre-diabetes vs. no diabetes), 10 vari-\nables\u00adfamily history, age, race and ethnicity, weight,\nheight, waist circumference, BMI, hypertension, sex, and\nphysical activity\u00adperformed best. Kernel functions were\nevaluated in terms of their discriminative accuracy by\nAUC. The RBF kernel function performed best in Clas-\nsification Scheme I, and the linear kernel function per-\nformed best in Classification Scheme II (Table 2).\nPerformance parameters such as the AUC, sensitivity,\nspecificity, positive predictive value, and negative predic-\ntive value are presented in Table 3. The overall discrimi-\nnative ability of Classification Schemes I and II are\nrespectively; Figure 2).\nThe AUC values for logistic regression analyses of the\nrespectively (Figure 2). Comparing the AUCs from our\nSVM and MLR models revealed no statistically signifi-\ncant difference in their discriminative abilities (P =\nII, respectively); thus, the SVM approach appears to per-\nform as well as the traditional logistic regression model.\nDiabetes Classifier, the web-based demonstration tool,\nwas built based on the two SVM models. By selecting\none of the diabetes classifications, the user is asked to\nenter the values for 8 or 10 common variables; the clas-\nsification result is then presented on the next page,\nusing the default cutoff value (0). This application pro-\nvides an interface that allows the user to select the cut-\noff values. Each cutoff value has specific values of\nsensitivity and specificity so that the user can decide\nhow the tool could be used in screening for diabetes.\nDiabetes Classifier can also be used in batch mode to\nclassify observations in an uploaded file containing\nappropriately formatted values of required variables.\nConclusions\nIn this study, we tested two classification schemes to\ndetect cases of diabetes and pre-diabetes in the U.S. popu-\nlation. Both schemes are examples of the potential use of\nsupport vector machine techniques in the classification of\ncommon diseases. Our results demonstrated that the dis-\ncriminative performance of SVM models was equivalent\nto the epidemiological method commonly used for this\npurpose, multivariate logistic regression. To our knowl-\nedge, this is the first report that the SVM approach can be\nused successfully to detect a common disease with simple\nclinical measurements, without laboratory tests. Based on\nthese results, we also developed a web-based tool for clas-\nsification of diabetes and pre-diabetes. This tool demon-\nstrates useful features for the potential application of\nclassification algorithms in health care.\nSVM is a model-free method that provides efficient\nsolutions to classification problems without any assump-\ntion regarding the distribution and interdependency of\nthe data. In epidemiologic studies and population health\nsurveys, the SVM technique has the potential to perform\nbetter than traditional statistical methods like logistic\nregression, especially in situations that include multivari-\nate risk factors with small effects (e.g., genome-wide\nassociation data and gene expression profiles), limited\nTable 2 The performance of support vector machine\nmodels with four kernel functions for the Classification I\nand Classification II\nModel Area under the curve\nLinear Polynomial Radial basis\nfunction\nSigmoid\nClassification\nScheme I*\nClassification\nScheme II*\n* see Table 1 for the definitions of Classification Schemes I and II\n**Best performance\nTable 3 The performance of support vector machine models for the Classification I and Classification II\nModel Data set Sensitivity Specificity PPV NPV AUC\nPPV, positive predictive value; NPV, negative predictive value; AUC, area under the curve.\n*See Table 1 for the definitions of Classification Schemes I and II.\nFigure 2 ROC curves for Classifications Schemes I (a) and II (b)\nwith SVM models and logistic regression models. Note: see\nTable 1 for the definitions of Classification Schemes I and II.\nsample size, and a limited knowledge of underlying bio-\nlogical relationships among risk factors. This is particu-\nlarly true in the case of common complex diseases\nwhere many risk factors, including gene-gene interac-\ntions and gene-environment interactions, have to be\nconsidered to reach sufficient discriminative power in\nprediction models [23]. Our work provides a promising\nproof of principle by demonstrating the predictive\npower of the SVM with just a small set of variables.\nThis approach can be extended to include large data\nsets, including many other variables, such as genetic bio-\nmarkers, as data become available.\nA major strength of this study is that we used the\nNHANES data set, which is a unique national weighted\nsurvey data that is representative of the U.S. population.\nOur results are comparable to those of other models\ntested in the same population. For example, Keikes et al.\n[24] developed a tool for detecting undiagnosed diabetes\nand pre-diabetes using logistic regression and a classifi-\ncation tree method to predict the risk of the diabetes in\nthe U.S. population. Although direct comparisons are\ndifficult because of the use of different NHANES data\nsets and different validation strategies, the discriminative\npowers in both studies seem to be equivalent. In our\nstudy, the AUC for the detection of diagnosed diabetes\nfor pre-diabetes or undiagnosed diabetes in the valida-\ntion test. In the study from Keikes et al., the AUC for\nundiagnosed diabetes were 82.19% (5-fold cross-valida-\ntion) and 75.03% (training data set) for pre-diabetes or\nundiagnosed diabetes. Schwarz et al. [25] recently pub-\nlished a comprehensive review of existing tools for pre-\ndicting the risk of type 2 diabetes or detecting\nundiagnosed diabetes. These tools were developed for\ndifferent populations under different methodologies\nusing different sets of variables. In general, the discrimi-\nnative power of our SVM method is within the range of\ndiscriminative powers reported for the tools included in\nthis review.\nWe cannot be certain that the models we developed\nby using the particular NHANES data set described here\nare applicable to other populations. Our SVM approach,\nhowever, is easily extended to other populations to gen-\nerate their own classification systems. Likewise, a similar\napproach could be used to develop SVM models for\nother complex diseases using a different set of relevant\nvariables.\nA critical step for determining the usefulness of a\nscreening test is to establish optimal cutoff values that\nyield optimal sensitivity and specificity values, which are\nparticularly important for cost-effectiveness analysis\n[26]. Our web-based application, Diabetes Classifier, dis-\nplays the trade-offs in sensitivity and specificity of the\nclassification method as the cutoff value is changed.\nThis feature is particularly relevant to clinical and public\nhealth programs, which can configure cutoff scores\naccording to the objectives of the program and other\nconsiderations including cost-effectiveness. Diabetes\nClassifier allows data to be fed automatically (via data\nbatch file uploading) for classification and provides an\ninterface capable of sharing information with other sec-\ntors of a health care system. Web-based tools such as\nDiabetes Classifier can also serve as self-assessment\ntools for use by the general public.\nSupport vector machine modeling is a promising clas-\nsification approach for detecting a complex disease like\ndiabetes using common, simple variables. Validation\nindicated that the discriminative powers of our two\nSVM models are comparable to those of commonly\nused multivariable logistic regression methods. Our Dia-\nbetes Classifier tool, a web-based tool developed for\ndemonstration purposes only, illustrates a potential use\nof the SVM technique: the identification of people with\nundetected common diseases such as diabetes and pre-\ndiabetes. This approach needs to tested and validated in\nother studies.\nAuthors' contributions\nWY designed and developed the methodology, built the demo web-based\nsystem, and drafted the manuscript. TL performed the data preparation and\nstatistical analysis, RV provided expertise on diabetes and helped in\nmanuscript preparation. MG provided advice on the project and revised the\ndraft manuscript. MJK oversaw the project and revised the draft manuscript.\nAll authors read and approved the final document.\nCompeting interests\nThe authors declare that they have no competing interests.\nReferences\n1. Cortes C, Vapnik V: Support-vector networks. Machine Learning 1995,\n2. Ng KL, Mishra SK: De novo SVM classification of precursor microRNAs\nfrom genomic pseudo hairpins using global and intrinsic folding\n3. Rice SB, Nenadic G, Stapley BJ: Mining protein function from text using\nterm-based support vector machines. BMC Bioinformatics 2005, 6(Suppl 1):\n4. Maglogiannis I, Loukis E, Zafiropoulos E, Stasis A: Support Vectors Machine-\nbased identification of heart valve diseases using heart sounds. Comput\n5. Thurston RC, Matthews KA, Hernandez J, De La TF: Improving the\nperformance of physiologic hot flash measures with support vector\n6. American Diabetes Association. 2009 [http://www.diabetes.org/].\n7. Pi-Sunyer FX: How effective are lifestyle changes in the prevention of\n8. Knowler WC, Barrett-Connor E, Fowler SE, Hamman RF, Lachin JM,\nWalker EA, Nathan DM: Reduction in the incidence of type 2 diabetes\n9. Fruchter O: Prevention of type 2 diabetes mellitus by changes in\n10. Global Guideline for Type 2 Diabetes: recommendations for standard,\n11. Thomas C, Hypponen E, Power C: Type 2 diabetes mellitus in midlife\nestimated from the Cambridge Risk Score and body mass index. Arch\n12. Franciosi M, De BG, Rossi MC, Sacco M, Belfiglio M, Pellegrini F, Tognoni G,\nValentini M, Nicolucci A: Use of the diabetes risk score for opportunistic\nscreening of undiagnosed diabetes and impaired glucose tolerance: the\nIGLOO (Impaired Glucose Tolerance and Long-Term Outcomes\n14. Heikes KE, Eddy DM, Arondekar B, Schlessinger L: Diabetes Risk Calculator:\na simple tool for detecting undiagnosed diabetes and pre-diabetes.\n15. National Health and Nutrition Examination Survey(NHANES). 2009\n[http://www.cdc.gov/nchs/nhanes/about_nhanes.htm].\n16. Yi-Wei Chen C-JL: Combining SVMs with Various Feature Selection\nStrategies. Feature Extraction: Foundations and Applications (Studies in\nFuzziness and Soft Computing) Isabelle Guyon SGMNLAZ 2005.\n17. Cover TC: Geometrical and statistical properties of system of linear\ninequalities with applications in pattern recognition [abstract]. IEEE Trans\n18. Chang CC, Lin CJ: LIBSVM: a library for supportvector machines, 2001.\n2007 [http://www.csie.ntu.edu.tw/~cjlin/libsvm/].\n19. DeLong ER, DeLong DM, Clarke-Pearson DL: Comparing the areas under\ntwo or more correlated receiver operating characteristic curves: a\n20. Java J2EE. Sun Microsystems, Inc. 2006 [http://java.sun.com/javaee/].\n21. Hibernate. JBoss Enterprise Middleware System. 2006 [http://www.\nhibernate.org/].\n22. Apache Struts. The Apache Software Foundation. 2006 [http://struts.\napache.org/].\n23. Schadt EE, Friend SH, Shaywitz DA: A network view of disease and\n24. Heikes KE, Eddy DM, Arondekar B, Schlessinger L: Diabetes Risk Calculator:\na simple tool for detecting undiagnosed diabetes and pre-diabetes.\n25. Schwarz PE, Li J, Lindstrom J, Tuomilehto J: Tools for predicting the risk of\n26. Cantor SB, Sun CC, Tortolero-Luna G, Richards-Kortum R, Follen M:\nA comparison of C/B ratios from studies using receiver operating\nPre-publication history\nThe pre-publication history for this paper can be accessed here: http://www.\nCite this article as: Yu et al.: Application of support vector machine\nmodeling for prediction of common diseases: the case of diabetes and\nSubmit your next manuscript to BioMed Central\nand take full advantage of:\n\u00b7 Convenient online submission\n\u00b7 Thorough peer review\n\u00b7 No space constraints or color figure charges\n\u00b7 Immediate publication on acceptance\n\u00b7 Inclusion in PubMed, CAS, Scopus and Google Scholar\n\u00b7 Research which is freely available for redistribution\nSubmit your manuscript at\nwww.biomedcentral.com/submit",
    "reduced_content": "Application of support vector machine modeling\nfor prediction of common diseases: the case of\ndiabetes and pre-diabetes\nWei Yu*, Tiebin Liu, Rodolfo Valdez, Marta Gwinn, Muin J Khoury"
}